09/16 12:14:42 PM: Git branch: master
09/16 12:14:42 PM: Git SHA: 93c1dfd555f3458ddbb66d458dfeca984f2d8527
09/16 12:14:42 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-None-only/",
  "exp_name": "experiments/ner-ontonotes-None-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-None-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/None",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/ner-ontonotes-None-only__run",
  "run_dir": "./experiments/ner-ontonotes-None-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 12:14:42 PM: Saved config to ./experiments/ner-ontonotes-None-only/run/params.conf
09/16 12:14:42 PM: Using random seed 1234
09/16 12:15:22 PM: Using GPU 0
09/16 12:15:22 PM: Loading tasks...
09/16 12:15:22 PM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-None-only/
09/16 12:15:22 PM: 	Creating task edges-ner-ontonotes from scratch.
09/16 12:15:24 PM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 12:15:24 PM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 12:15:24 PM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 12:15:25 PM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 12:15:25 PM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 12:15:25 PM: 	Building vocab from scratch.
09/16 12:15:25 PM: 	Counting units for task edges-ner-ontonotes.
09/16 12:15:27 PM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 12:15:28 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:15:28 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 12:15:28 PM: 	Saved vocab to ./experiments/ner-ontonotes-None-only/vocab
09/16 12:15:28 PM: Loading token dictionary from ./experiments/ner-ontonotes-None-only/vocab.
09/16 12:15:28 PM: 	Loaded vocab from ./experiments/ner-ontonotes-None-only/vocab
09/16 12:15:28 PM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 12:15:28 PM: 	Vocab namespace tokens: size 22840
09/16 12:15:28 PM: 	Vocab namespace bert_uncased: size 30524
09/16 12:15:28 PM: 	Vocab namespace chars: size 77
09/16 12:15:28 PM: 	Finished building vocab.
09/16 12:15:28 PM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 12:15:43 PM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-None-only/preproc/edges-ner-ontonotes__train_data
09/16 12:15:43 PM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 12:15:45 PM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-None-only/preproc/edges-ner-ontonotes__val_data
09/16 12:15:45 PM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 12:15:46 PM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-None-only/preproc/edges-ner-ontonotes__test_data
09/16 12:15:46 PM: 	Finished indexing tasks
09/16 12:15:46 PM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 12:15:46 PM: 	  Training on 
09/16 12:15:46 PM: 	  Evaluating on edges-ner-ontonotes
09/16 12:15:46 PM: 	Finished loading tasks in 24.031s
09/16 12:15:46 PM: 	 Tasks: ['edges-ner-ontonotes']
09/16 12:15:46 PM: Building model...
09/16 12:15:46 PM: Using BERT model (bert-base-uncased).
09/16 12:15:46 PM: LOADING A PRETRAINED MODEL
09/16 12:15:47 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpvo4up1ew
09/16 12:15:50 PM: copying /tmp/tmpvo4up1ew to cache at ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:50 PM: creating metadata file for ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:50 PM: removing temp file /tmp/tmpvo4up1ew
09/16 12:15:50 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:50 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 12:15:52 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpnprqi2m9
09/16 12:28:30 PM: copying /tmp/tmpnprqi2m9 to cache at ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:28:33 PM: creating metadata file for ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:28:33 PM: removing temp file /tmp/tmpnprqi2m9
09/16 12:28:33 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:28:45 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp4v9nbte2
09/16 12:28:47 PM: copying /tmp/tmp4v9nbte2 to cache at ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:28:47 PM: creating metadata file for ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:28:47 PM: removing temp file /tmp/tmp4v9nbte2
09/16 12:28:47 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-None-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:28:47 PM: Initializing parameters
09/16 12:28:47 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 12:28:47 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 12:28:47 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 12:28:47 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 12:28:47 PM:    _text_field_embedder.model.pooler.dense.bias
09/16 12:28:47 PM:    _text_field_embedder.model.pooler.dense.weight
09/16 12:28:47 PM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 12:29:29 PM: Model specification:
09/16 12:29:29 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 12:29:29 PM: Model parameters:
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:29 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:29 PM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 12:29:29 PM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 12:29:29 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 12:29:29 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 12:29:29 PM: Total number of parameters: 109688338 (1.09688e+08)
09/16 12:29:29 PM: Number of trainable parameters: 206098 (206098)
09/16 12:29:29 PM: Finished building model in 823.487s
09/16 12:29:29 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 12:29:37 PM: patience = 9
09/16 12:29:37 PM: val_interval = 1000
09/16 12:29:37 PM: max_vals = 250
09/16 12:29:37 PM: cuda_device = 0
09/16 12:29:37 PM: grad_norm = 5.0
09/16 12:29:37 PM: grad_clipping = None
09/16 12:29:37 PM: lr_decay = 0.99
09/16 12:29:37 PM: min_lr = 1e-06
09/16 12:29:37 PM: keep_all_checkpoints = 0
09/16 12:29:37 PM: val_data_limit = 5000
09/16 12:29:37 PM: max_epochs = -1
09/16 12:29:37 PM: dec_val_scale = 250
09/16 12:29:37 PM: training_data_fraction = 1
09/16 12:29:37 PM: type = adam
09/16 12:29:37 PM: parameter_groups = None
09/16 12:29:37 PM: Number of trainable parameters: 206098
09/16 12:29:37 PM: infer_type_and_cast = True
09/16 12:29:37 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:37 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:37 PM: lr = 0.0001
09/16 12:29:37 PM: amsgrad = True
09/16 12:29:37 PM: type = reduce_on_plateau
09/16 12:29:37 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:37 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:37 PM: mode = max
09/16 12:29:37 PM: factor = 0.5
09/16 12:29:37 PM: patience = 3
09/16 12:29:37 PM: threshold = 0.0001
09/16 12:29:37 PM: threshold_mode = abs
09/16 12:29:37 PM: verbose = True
09/16 12:29:37 PM: type = adam
09/16 12:29:37 PM: parameter_groups = None
09/16 12:29:37 PM: Number of trainable parameters: 206098
09/16 12:29:37 PM: infer_type_and_cast = True
09/16 12:29:37 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:37 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:37 PM: lr = 0.0001
09/16 12:29:37 PM: amsgrad = True
09/16 12:29:37 PM: type = reduce_on_plateau
09/16 12:29:37 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:37 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:37 PM: mode = max
09/16 12:29:37 PM: factor = 0.5
09/16 12:29:37 PM: patience = 3
09/16 12:29:37 PM: threshold = 0.0001
09/16 12:29:37 PM: threshold_mode = abs
09/16 12:29:37 PM: verbose = True
09/16 12:29:37 PM: Starting training without restoring from a checkpoint.
09/16 12:29:37 PM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 12:29:37 PM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 12:29:47 PM: Update 93: task edges-ner-ontonotes, batch 93 (93): mcc: 0.1414, acc: 0.1256, precision: 0.1601, recall: 0.2524, f1: 0.1959, edges-ner-ontonotes_loss: 0.3443
09/16 12:29:57 PM: Update 233: task edges-ner-ontonotes, batch 233 (233): mcc: 0.3509, acc: 0.3176, precision: 0.4038, recall: 0.3676, f1: 0.3849, edges-ner-ontonotes_loss: 0.2046
09/16 12:30:08 PM: Update 314: task edges-ner-ontonotes, batch 314 (314): mcc: 0.4469, acc: 0.4029, precision: 0.5129, recall: 0.4405, f1: 0.4740, edges-ner-ontonotes_loss: 0.1725
09/16 12:30:18 PM: Update 456: task edges-ner-ontonotes, batch 456 (456): mcc: 0.5466, acc: 0.4878, precision: 0.6303, recall: 0.5140, f1: 0.5662, edges-ner-ontonotes_loss: 0.1430
09/16 12:30:28 PM: Update 616: task edges-ner-ontonotes, batch 616 (616): mcc: 0.6185, acc: 0.5534, precision: 0.7071, recall: 0.5746, f1: 0.6340, edges-ner-ontonotes_loss: 0.1222
09/16 12:30:38 PM: Update 706: task edges-ner-ontonotes, batch 706 (706): mcc: 0.6450, acc: 0.5768, precision: 0.7342, recall: 0.5979, f1: 0.6591, edges-ner-ontonotes_loss: 0.1145
09/16 12:30:48 PM: Update 879: task edges-ner-ontonotes, batch 879 (879): mcc: 0.6805, acc: 0.6108, precision: 0.7674, recall: 0.6321, f1: 0.6932, edges-ner-ontonotes_loss: 0.1026
09/16 12:31:00 PM: Update 940: task edges-ner-ontonotes, batch 940 (940): mcc: 0.6911, acc: 0.6214, precision: 0.7767, recall: 0.6427, f1: 0.7033, edges-ner-ontonotes_loss: 0.0994
09/16 12:31:07 PM: ***** Step 1000 / Validation 1 *****
09/16 12:31:07 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:31:07 PM: Validating...
09/16 12:31:10 PM: Evaluate: task edges-ner-ontonotes, batch 46 (157): mcc: 0.7852, acc: 0.7087, precision: 0.8629, recall: 0.7344, f1: 0.7935, edges-ner-ontonotes_loss: 0.0698
09/16 12:31:19 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:31:19 PM: Best result seen so far for micro.
09/16 12:31:19 PM: Best result seen so far for macro.
09/16 12:31:19 PM: Updating LR scheduler:
09/16 12:31:19 PM: 	Best result seen so far for macro_avg: 0.821
09/16 12:31:19 PM: 	# validation passes without improvement: 0
09/16 12:31:19 PM: edges-ner-ontonotes_loss: training: 0.096748 validation: 0.058287
09/16 12:31:19 PM: macro_avg: validation: 0.820983
09/16 12:31:19 PM: micro_avg: validation: 0.000000
09/16 12:31:19 PM: edges-ner-ontonotes_mcc: training: 0.698565 validation: 0.813519
09/16 12:31:19 PM: edges-ner-ontonotes_acc: training: 0.628558 validation: 0.744995
09/16 12:31:19 PM: edges-ner-ontonotes_precision: training: 0.783288 validation: 0.883664
09/16 12:31:19 PM: edges-ner-ontonotes_recall: training: 0.650225 validation: 0.766606
09/16 12:31:19 PM: edges-ner-ontonotes_f1: training: 0.710580 validation: 0.820983
09/16 12:31:19 PM: Global learning rate: 0.0001
09/16 12:31:19 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:31:20 PM: Update 1009: task edges-ner-ontonotes, batch 9 (1009): mcc: 0.8256, acc: 0.7646, precision: 0.8722, recall: 0.7990, f1: 0.8340, edges-ner-ontonotes_loss: 0.0488
09/16 12:31:30 PM: Update 1117: task edges-ner-ontonotes, batch 117 (1117): mcc: 0.8261, acc: 0.7606, precision: 0.8871, recall: 0.7862, f1: 0.8336, edges-ner-ontonotes_loss: 0.0498
09/16 12:31:40 PM: Update 1192: task edges-ner-ontonotes, batch 192 (1192): mcc: 0.8321, acc: 0.7666, precision: 0.8926, recall: 0.7920, f1: 0.8393, edges-ner-ontonotes_loss: 0.0485
09/16 12:31:56 PM: Update 1253: task edges-ner-ontonotes, batch 253 (1253): mcc: 0.8301, acc: 0.7645, precision: 0.8913, recall: 0.7896, f1: 0.8374, edges-ner-ontonotes_loss: 0.0485
09/16 12:32:06 PM: Update 1338: task edges-ner-ontonotes, batch 338 (1338): mcc: 0.8205, acc: 0.7538, precision: 0.8860, recall: 0.7771, f1: 0.8280, edges-ner-ontonotes_loss: 0.0529
09/16 12:32:16 PM: Update 1417: task edges-ner-ontonotes, batch 417 (1417): mcc: 0.8189, acc: 0.7525, precision: 0.8861, recall: 0.7740, f1: 0.8263, edges-ner-ontonotes_loss: 0.0541
09/16 12:32:26 PM: Update 1498: task edges-ner-ontonotes, batch 498 (1498): mcc: 0.8194, acc: 0.7533, precision: 0.8874, recall: 0.7738, f1: 0.8267, edges-ner-ontonotes_loss: 0.0546
09/16 12:32:41 PM: Update 1557: task edges-ner-ontonotes, batch 557 (1557): mcc: 0.8193, acc: 0.7533, precision: 0.8878, recall: 0.7734, f1: 0.8266, edges-ner-ontonotes_loss: 0.0549
09/16 12:32:51 PM: Update 1655: task edges-ner-ontonotes, batch 655 (1655): mcc: 0.8232, acc: 0.7588, precision: 0.8909, recall: 0.7775, f1: 0.8303, edges-ner-ontonotes_loss: 0.0538
09/16 12:33:01 PM: Update 1775: task edges-ner-ontonotes, batch 775 (1775): mcc: 0.8275, acc: 0.7652, precision: 0.8937, recall: 0.7827, f1: 0.8345, edges-ner-ontonotes_loss: 0.0525
09/16 12:33:11 PM: Update 1868: task edges-ner-ontonotes, batch 868 (1868): mcc: 0.8318, acc: 0.7713, precision: 0.8964, recall: 0.7879, f1: 0.8387, edges-ner-ontonotes_loss: 0.0512
09/16 12:33:21 PM: Update 1897: task edges-ner-ontonotes, batch 897 (1897): mcc: 0.8314, acc: 0.7711, precision: 0.8959, recall: 0.7877, f1: 0.8383, edges-ner-ontonotes_loss: 0.0512
09/16 12:33:31 PM: Update 1992: task edges-ner-ontonotes, batch 992 (1992): mcc: 0.8325, acc: 0.7730, precision: 0.8962, recall: 0.7894, f1: 0.8394, edges-ner-ontonotes_loss: 0.0507
09/16 12:33:32 PM: ***** Step 2000 / Validation 2 *****
09/16 12:33:32 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:33:32 PM: Validating...
09/16 12:33:41 PM: Evaluate: task edges-ner-ontonotes, batch 89 (157): mcc: 0.8487, acc: 0.7965, precision: 0.9122, recall: 0.8041, f1: 0.8548, edges-ner-ontonotes_loss: 0.0498
09/16 12:33:47 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:33:48 PM: Best result seen so far for macro.
09/16 12:33:48 PM: Updating LR scheduler:
09/16 12:33:48 PM: 	Best result seen so far for macro_avg: 0.843
09/16 12:33:48 PM: 	# validation passes without improvement: 0
09/16 12:33:48 PM: edges-ner-ontonotes_loss: training: 0.050679 validation: 0.049460
09/16 12:33:48 PM: macro_avg: validation: 0.843101
09/16 12:33:48 PM: micro_avg: validation: 0.000000
09/16 12:33:48 PM: edges-ner-ontonotes_mcc: training: 0.832653 validation: 0.835961
09/16 12:33:48 PM: edges-ner-ontonotes_acc: training: 0.773306 validation: 0.783894
09/16 12:33:48 PM: edges-ner-ontonotes_precision: training: 0.896150 validation: 0.894370
09/16 12:33:48 PM: edges-ner-ontonotes_recall: training: 0.789787 validation: 0.797392
09/16 12:33:48 PM: edges-ner-ontonotes_f1: training: 0.839613 validation: 0.843101
09/16 12:33:48 PM: Global learning rate: 0.0001
09/16 12:33:48 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:33:51 PM: Update 2024: task edges-ner-ontonotes, batch 24 (2024): mcc: 0.8519, acc: 0.7982, precision: 0.9088, recall: 0.8131, f1: 0.8583, edges-ner-ontonotes_loss: 0.0413
09/16 12:34:01 PM: Update 2067: task edges-ner-ontonotes, batch 67 (2067): mcc: 0.8583, acc: 0.8086, precision: 0.9123, recall: 0.8215, f1: 0.8645, edges-ner-ontonotes_loss: 0.0420
09/16 12:34:11 PM: Update 2158: task edges-ner-ontonotes, batch 158 (2158): mcc: 0.8574, acc: 0.8075, precision: 0.9115, recall: 0.8207, f1: 0.8637, edges-ner-ontonotes_loss: 0.0422
09/16 12:34:21 PM: Update 2224: task edges-ner-ontonotes, batch 224 (2224): mcc: 0.8518, acc: 0.8008, precision: 0.9040, recall: 0.8174, f1: 0.8585, edges-ner-ontonotes_loss: 0.0432
09/16 12:34:31 PM: Update 2296: task edges-ner-ontonotes, batch 296 (2296): mcc: 0.8511, acc: 0.7991, precision: 0.9022, recall: 0.8177, f1: 0.8579, edges-ner-ontonotes_loss: 0.0431
09/16 12:34:41 PM: Update 2384: task edges-ner-ontonotes, batch 384 (2384): mcc: 0.8502, acc: 0.7972, precision: 0.8995, recall: 0.8186, f1: 0.8571, edges-ner-ontonotes_loss: 0.0429
09/16 12:34:51 PM: Update 2456: task edges-ner-ontonotes, batch 456 (2456): mcc: 0.8528, acc: 0.8003, precision: 0.9005, recall: 0.8225, f1: 0.8597, edges-ner-ontonotes_loss: 0.0423
09/16 12:35:04 PM: Update 2496: task edges-ner-ontonotes, batch 496 (2496): mcc: 0.8535, acc: 0.8012, precision: 0.9005, recall: 0.8237, f1: 0.8604, edges-ner-ontonotes_loss: 0.0419
09/16 12:35:14 PM: Update 2569: task edges-ner-ontonotes, batch 569 (2569): mcc: 0.8524, acc: 0.7999, precision: 0.8992, recall: 0.8229, f1: 0.8594, edges-ner-ontonotes_loss: 0.0422
09/16 12:35:24 PM: Update 2645: task edges-ner-ontonotes, batch 645 (2645): mcc: 0.8525, acc: 0.7995, precision: 0.8990, recall: 0.8233, f1: 0.8595, edges-ner-ontonotes_loss: 0.0420
09/16 12:35:34 PM: Update 2755: task edges-ner-ontonotes, batch 755 (2755): mcc: 0.8533, acc: 0.8002, precision: 0.8993, recall: 0.8244, f1: 0.8602, edges-ner-ontonotes_loss: 0.0417
09/16 12:35:44 PM: Update 2857: task edges-ner-ontonotes, batch 857 (2857): mcc: 0.8524, acc: 0.7994, precision: 0.8987, recall: 0.8233, f1: 0.8593, edges-ner-ontonotes_loss: 0.0424
09/16 12:35:54 PM: ***** Step 3000 / Validation 3 *****
09/16 12:35:54 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:35:54 PM: Validating...
09/16 12:35:54 PM: Evaluate: task edges-ner-ontonotes, batch 2 (157): mcc: 0.7421, acc: 0.7000, precision: 0.7899, recall: 0.7231, f1: 0.7550, edges-ner-ontonotes_loss: 0.0775
09/16 12:36:04 PM: Evaluate: task edges-ner-ontonotes, batch 145 (157): mcc: 0.8455, acc: 0.7957, precision: 0.9025, recall: 0.8072, f1: 0.8522, edges-ner-ontonotes_loss: 0.0478
09/16 12:36:05 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:36:05 PM: Best result seen so far for macro.
09/16 12:36:05 PM: Updating LR scheduler:
09/16 12:36:05 PM: 	Best result seen so far for macro_avg: 0.852
09/16 12:36:05 PM: 	# validation passes without improvement: 0
09/16 12:36:05 PM: edges-ner-ontonotes_loss: training: 0.043656 validation: 0.047357
09/16 12:36:05 PM: macro_avg: validation: 0.852059
09/16 12:36:05 PM: micro_avg: validation: 0.000000
09/16 12:36:05 PM: edges-ner-ontonotes_mcc: training: 0.850520 validation: 0.845264
09/16 12:36:05 PM: edges-ner-ontonotes_acc: training: 0.797576 validation: 0.796330
09/16 12:36:05 PM: edges-ner-ontonotes_precision: training: 0.898074 validation: 0.901286
09/16 12:36:05 PM: edges-ner-ontonotes_recall: training: 0.820496 validation: 0.807931
09/16 12:36:05 PM: edges-ner-ontonotes_f1: training: 0.857534 validation: 0.852059
09/16 12:36:05 PM: Global learning rate: 0.0001
09/16 12:36:05 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:36:14 PM: Update 3075: task edges-ner-ontonotes, batch 75 (3075): mcc: 0.8423, acc: 0.7865, precision: 0.8994, recall: 0.8042, f1: 0.8492, edges-ner-ontonotes_loss: 0.0511
09/16 12:36:24 PM: Update 3202: task edges-ner-ontonotes, batch 202 (3202): mcc: 0.8515, acc: 0.8036, precision: 0.9031, recall: 0.8175, f1: 0.8582, edges-ner-ontonotes_loss: 0.0470
09/16 12:36:34 PM: Update 3333: task edges-ner-ontonotes, batch 333 (3333): mcc: 0.8613, acc: 0.8171, precision: 0.9086, recall: 0.8304, f1: 0.8677, edges-ner-ontonotes_loss: 0.0436
09/16 12:36:46 PM: Update 3426: task edges-ner-ontonotes, batch 426 (3426): mcc: 0.8650, acc: 0.8220, precision: 0.9106, recall: 0.8353, f1: 0.8714, edges-ner-ontonotes_loss: 0.0421
09/16 12:36:56 PM: Update 3558: task edges-ner-ontonotes, batch 558 (3558): mcc: 0.8627, acc: 0.8180, precision: 0.9093, recall: 0.8323, f1: 0.8691, edges-ner-ontonotes_loss: 0.0421
09/16 12:37:06 PM: Update 3718: task edges-ner-ontonotes, batch 718 (3718): mcc: 0.8649, acc: 0.8206, precision: 0.9105, recall: 0.8352, f1: 0.8712, edges-ner-ontonotes_loss: 0.0414
09/16 12:37:16 PM: Update 3824: task edges-ner-ontonotes, batch 824 (3824): mcc: 0.8637, acc: 0.8188, precision: 0.9083, recall: 0.8351, f1: 0.8702, edges-ner-ontonotes_loss: 0.0413
09/16 12:37:26 PM: Update 3944: task edges-ner-ontonotes, batch 944 (3944): mcc: 0.8641, acc: 0.8188, precision: 0.9076, recall: 0.8365, f1: 0.8706, edges-ner-ontonotes_loss: 0.0409
09/16 12:37:32 PM: ***** Step 4000 / Validation 4 *****
09/16 12:37:32 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:37:32 PM: Validating...
09/16 12:37:36 PM: Evaluate: task edges-ner-ontonotes, batch 66 (157): mcc: 0.8386, acc: 0.7965, precision: 0.8898, recall: 0.8064, f1: 0.8460, edges-ner-ontonotes_loss: 0.0529
09/16 12:37:44 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:37:44 PM: Best result seen so far for macro.
09/16 12:37:44 PM: Updating LR scheduler:
09/16 12:37:44 PM: 	Best result seen so far for macro_avg: 0.859
09/16 12:37:44 PM: 	# validation passes without improvement: 0
09/16 12:37:44 PM: edges-ner-ontonotes_loss: training: 0.040587 validation: 0.045566
09/16 12:37:44 PM: macro_avg: validation: 0.858527
09/16 12:37:44 PM: micro_avg: validation: 0.000000
09/16 12:37:44 PM: edges-ner-ontonotes_mcc: training: 0.864300 validation: 0.851277
09/16 12:37:44 PM: edges-ner-ontonotes_acc: training: 0.818775 validation: 0.809600
09/16 12:37:44 PM: edges-ner-ontonotes_precision: training: 0.907513 validation: 0.893481
09/16 12:37:44 PM: edges-ner-ontonotes_recall: training: 0.836916 validation: 0.826206
09/16 12:37:44 PM: edges-ner-ontonotes_f1: training: 0.870786 validation: 0.858527
09/16 12:37:44 PM: Global learning rate: 0.0001
09/16 12:37:44 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:37:46 PM: Update 4021: task edges-ner-ontonotes, batch 21 (4021): mcc: 0.8612, acc: 0.8134, precision: 0.8942, recall: 0.8439, f1: 0.8683, edges-ner-ontonotes_loss: 0.0385
09/16 12:37:56 PM: Update 4080: task edges-ner-ontonotes, batch 80 (4080): mcc: 0.8612, acc: 0.8122, precision: 0.8964, recall: 0.8418, f1: 0.8682, edges-ner-ontonotes_loss: 0.0375
09/16 12:38:06 PM: Update 4193: task edges-ner-ontonotes, batch 193 (4193): mcc: 0.8653, acc: 0.8181, precision: 0.8997, recall: 0.8463, f1: 0.8722, edges-ner-ontonotes_loss: 0.0369
09/16 12:38:16 PM: Update 4319: task edges-ner-ontonotes, batch 319 (4319): mcc: 0.8676, acc: 0.8205, precision: 0.9033, recall: 0.8471, f1: 0.8743, edges-ner-ontonotes_loss: 0.0369
09/16 12:38:26 PM: Update 4386: task edges-ner-ontonotes, batch 386 (4386): mcc: 0.8643, acc: 0.8165, precision: 0.9011, recall: 0.8430, f1: 0.8711, edges-ner-ontonotes_loss: 0.0383
09/16 12:38:36 PM: Update 4514: task edges-ner-ontonotes, batch 514 (4514): mcc: 0.8604, acc: 0.8120, precision: 0.9006, recall: 0.8364, f1: 0.8673, edges-ner-ontonotes_loss: 0.0410
09/16 12:38:46 PM: Update 4630: task edges-ner-ontonotes, batch 630 (4630): mcc: 0.8583, acc: 0.8096, precision: 0.9000, recall: 0.8328, f1: 0.8651, edges-ner-ontonotes_loss: 0.0426
09/16 12:38:56 PM: Update 4701: task edges-ner-ontonotes, batch 701 (4701): mcc: 0.8584, acc: 0.8102, precision: 0.9003, recall: 0.8330, f1: 0.8653, edges-ner-ontonotes_loss: 0.0428
09/16 12:39:06 PM: Update 4840: task edges-ner-ontonotes, batch 840 (4840): mcc: 0.8616, acc: 0.8150, precision: 0.9029, recall: 0.8364, f1: 0.8683, edges-ner-ontonotes_loss: 0.0419
09/16 12:39:18 PM: Update 4982: task edges-ner-ontonotes, batch 982 (4982): mcc: 0.8645, acc: 0.8190, precision: 0.9053, recall: 0.8395, f1: 0.8711, edges-ner-ontonotes_loss: 0.0409
09/16 12:39:20 PM: ***** Step 5000 / Validation 5 *****
09/16 12:39:20 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:39:20 PM: Validating...
09/16 12:39:28 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.8415, acc: 0.7998, precision: 0.8929, recall: 0.8087, f1: 0.8488, edges-ner-ontonotes_loss: 0.0463
09/16 12:39:32 PM: Updating LR scheduler:
09/16 12:39:32 PM: 	Best result seen so far for macro_avg: 0.859
09/16 12:39:32 PM: 	# validation passes without improvement: 1
09/16 12:39:32 PM: edges-ner-ontonotes_loss: training: 0.041007 validation: 0.044553
09/16 12:39:32 PM: macro_avg: validation: 0.853454
09/16 12:39:32 PM: micro_avg: validation: 0.000000
09/16 12:39:32 PM: edges-ner-ontonotes_mcc: training: 0.864095 validation: 0.846347
09/16 12:39:32 PM: edges-ner-ontonotes_acc: training: 0.818455 validation: 0.804671
09/16 12:39:32 PM: edges-ner-ontonotes_precision: training: 0.904999 validation: 0.896287
09/16 12:39:32 PM: edges-ner-ontonotes_recall: training: 0.838911 validation: 0.814528
09/16 12:39:32 PM: edges-ner-ontonotes_f1: training: 0.870703 validation: 0.853454
09/16 12:39:32 PM: Global learning rate: 0.0001
09/16 12:39:32 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:39:38 PM: Update 5081: task edges-ner-ontonotes, batch 81 (5081): mcc: 0.8554, acc: 0.8084, precision: 0.9001, recall: 0.8275, f1: 0.8623, edges-ner-ontonotes_loss: 0.0426
09/16 12:39:49 PM: Update 5229: task edges-ner-ontonotes, batch 229 (5229): mcc: 0.8705, acc: 0.8276, precision: 0.9106, recall: 0.8454, f1: 0.8768, edges-ner-ontonotes_loss: 0.0387
09/16 12:40:00 PM: Update 5295: task edges-ner-ontonotes, batch 295 (5295): mcc: 0.8726, acc: 0.8313, precision: 0.9114, recall: 0.8485, f1: 0.8788, edges-ner-ontonotes_loss: 0.0378
09/16 12:40:10 PM: Update 5435: task edges-ner-ontonotes, batch 435 (5435): mcc: 0.8689, acc: 0.8253, precision: 0.9059, recall: 0.8469, f1: 0.8754, edges-ner-ontonotes_loss: 0.0381
09/16 12:40:20 PM: Update 5528: task edges-ner-ontonotes, batch 528 (5528): mcc: 0.8691, acc: 0.8248, precision: 0.9057, recall: 0.8475, f1: 0.8756, edges-ner-ontonotes_loss: 0.0378
09/16 12:40:33 PM: Update 5608: task edges-ner-ontonotes, batch 608 (5608): mcc: 0.8700, acc: 0.8252, precision: 0.9061, recall: 0.8488, f1: 0.8765, edges-ner-ontonotes_loss: 0.0374
09/16 12:40:43 PM: Update 5762: task edges-ner-ontonotes, batch 762 (5762): mcc: 0.8696, acc: 0.8241, precision: 0.9053, recall: 0.8488, f1: 0.8761, edges-ner-ontonotes_loss: 0.0373
09/16 12:40:53 PM: Update 5919: task edges-ner-ontonotes, batch 919 (5919): mcc: 0.8707, acc: 0.8252, precision: 0.9060, recall: 0.8502, f1: 0.8772, edges-ner-ontonotes_loss: 0.0369
09/16 12:41:02 PM: ***** Step 6000 / Validation 6 *****
09/16 12:41:02 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:41:02 PM: Validating...
09/16 12:41:03 PM: Evaluate: task edges-ner-ontonotes, batch 43 (157): mcc: 0.8625, acc: 0.8256, precision: 0.9023, recall: 0.8384, f1: 0.8692, edges-ner-ontonotes_loss: 0.0466
09/16 12:41:09 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:41:09 PM: Best result seen so far for macro.
09/16 12:41:09 PM: Updating LR scheduler:
09/16 12:41:09 PM: 	Best result seen so far for macro_avg: 0.861
09/16 12:41:09 PM: 	# validation passes without improvement: 0
09/16 12:41:09 PM: edges-ner-ontonotes_loss: training: 0.038017 validation: 0.044731
09/16 12:41:09 PM: macro_avg: validation: 0.860606
09/16 12:41:09 PM: micro_avg: validation: 0.000000
09/16 12:41:09 PM: edges-ner-ontonotes_mcc: training: 0.868324 validation: 0.853940
09/16 12:41:09 PM: edges-ner-ontonotes_acc: training: 0.822244 validation: 0.811799
09/16 12:41:09 PM: edges-ner-ontonotes_precision: training: 0.904630 validation: 0.904352
09/16 12:41:09 PM: edges-ner-ontonotes_recall: training: 0.847068 validation: 0.820898
09/16 12:41:09 PM: edges-ner-ontonotes_f1: training: 0.874903 validation: 0.860606
09/16 12:41:09 PM: Global learning rate: 0.0001
09/16 12:41:09 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:41:13 PM: Update 6067: task edges-ner-ontonotes, batch 67 (6067): mcc: 0.8519, acc: 0.8085, precision: 0.8952, recall: 0.8256, f1: 0.8590, edges-ner-ontonotes_loss: 0.0484
09/16 12:41:26 PM: Update 6192: task edges-ner-ontonotes, batch 192 (6192): mcc: 0.8509, acc: 0.8054, precision: 0.8966, recall: 0.8226, f1: 0.8580, edges-ner-ontonotes_loss: 0.0473
09/16 12:41:36 PM: Update 6261: task edges-ner-ontonotes, batch 261 (6261): mcc: 0.8541, acc: 0.8087, precision: 0.8987, recall: 0.8264, f1: 0.8610, edges-ner-ontonotes_loss: 0.0459
09/16 12:41:46 PM: Update 6436: task edges-ner-ontonotes, batch 436 (6436): mcc: 0.8653, acc: 0.8231, precision: 0.9065, recall: 0.8397, f1: 0.8718, edges-ner-ontonotes_loss: 0.0420
09/16 12:41:58 PM: Update 6538: task edges-ner-ontonotes, batch 538 (6538): mcc: 0.8704, acc: 0.8298, precision: 0.9095, recall: 0.8462, f1: 0.8767, edges-ner-ontonotes_loss: 0.0404
09/16 12:42:08 PM: Update 6670: task edges-ner-ontonotes, batch 670 (6670): mcc: 0.8692, acc: 0.8281, precision: 0.9086, recall: 0.8450, f1: 0.8756, edges-ner-ontonotes_loss: 0.0400
09/16 12:42:18 PM: Update 6799: task edges-ner-ontonotes, batch 799 (6799): mcc: 0.8719, acc: 0.8310, precision: 0.9103, recall: 0.8482, f1: 0.8782, edges-ner-ontonotes_loss: 0.0393
09/16 12:42:28 PM: Update 6889: task edges-ner-ontonotes, batch 889 (6889): mcc: 0.8715, acc: 0.8304, precision: 0.9093, recall: 0.8484, f1: 0.8778, edges-ner-ontonotes_loss: 0.0391
09/16 12:42:38 PM: ***** Step 7000 / Validation 7 *****
09/16 12:42:38 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:42:38 PM: Validating...
09/16 12:42:39 PM: Evaluate: task edges-ner-ontonotes, batch 5 (157): mcc: 0.7004, acc: 0.6567, precision: 0.7412, recall: 0.6925, f1: 0.7160, edges-ner-ontonotes_loss: 0.0822
09/16 12:42:46 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:42:46 PM: Best result seen so far for macro.
09/16 12:42:46 PM: Updating LR scheduler:
09/16 12:42:46 PM: 	Best result seen so far for macro_avg: 0.862
09/16 12:42:46 PM: 	# validation passes without improvement: 0
09/16 12:42:46 PM: edges-ner-ontonotes_loss: training: 0.038838 validation: 0.044227
09/16 12:42:46 PM: macro_avg: validation: 0.861683
09/16 12:42:46 PM: micro_avg: validation: 0.000000
09/16 12:42:46 PM: edges-ner-ontonotes_mcc: training: 0.871286 validation: 0.854323
09/16 12:42:46 PM: edges-ner-ontonotes_acc: training: 0.829416 validation: 0.814301
09/16 12:42:46 PM: edges-ner-ontonotes_precision: training: 0.908243 validation: 0.890471
09/16 12:42:46 PM: edges-ner-ontonotes_recall: training: 0.849095 validation: 0.834698
09/16 12:42:46 PM: edges-ner-ontonotes_f1: training: 0.877674 validation: 0.861683
09/16 12:42:46 PM: Global learning rate: 0.0001
09/16 12:42:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:42:49 PM: Update 7035: task edges-ner-ontonotes, batch 35 (7035): mcc: 0.8631, acc: 0.8144, precision: 0.8948, recall: 0.8469, f1: 0.8702, edges-ner-ontonotes_loss: 0.0376
09/16 12:42:59 PM: Update 7138: task edges-ner-ontonotes, batch 138 (7138): mcc: 0.8713, acc: 0.8265, precision: 0.9013, recall: 0.8559, f1: 0.8780, edges-ner-ontonotes_loss: 0.0357
09/16 12:43:09 PM: Update 7198: task edges-ner-ontonotes, batch 198 (7198): mcc: 0.8717, acc: 0.8260, precision: 0.9027, recall: 0.8551, f1: 0.8783, edges-ner-ontonotes_loss: 0.0356
09/16 12:43:19 PM: Update 7329: task edges-ner-ontonotes, batch 329 (7329): mcc: 0.8730, acc: 0.8276, precision: 0.9046, recall: 0.8558, f1: 0.8795, edges-ner-ontonotes_loss: 0.0355
09/16 12:43:29 PM: Update 7466: task edges-ner-ontonotes, batch 466 (7466): mcc: 0.8732, acc: 0.8275, precision: 0.9048, recall: 0.8559, f1: 0.8797, edges-ner-ontonotes_loss: 0.0355
09/16 12:43:39 PM: Update 7533: task edges-ner-ontonotes, batch 533 (7533): mcc: 0.8704, acc: 0.8247, precision: 0.9034, recall: 0.8521, f1: 0.8770, edges-ner-ontonotes_loss: 0.0372
09/16 12:43:49 PM: Update 7639: task edges-ner-ontonotes, batch 639 (7639): mcc: 0.8676, acc: 0.8214, precision: 0.9025, recall: 0.8478, f1: 0.8743, edges-ner-ontonotes_loss: 0.0387
09/16 12:44:02 PM: Update 7781: task edges-ner-ontonotes, batch 781 (7781): mcc: 0.8655, acc: 0.8190, precision: 0.9018, recall: 0.8445, f1: 0.8722, edges-ner-ontonotes_loss: 0.0399
09/16 12:44:12 PM: Update 7994: task edges-ner-ontonotes, batch 994 (7994): mcc: 0.8696, acc: 0.8251, precision: 0.9050, recall: 0.8490, f1: 0.8761, edges-ner-ontonotes_loss: 0.0388
09/16 12:44:12 PM: ***** Step 8000 / Validation 8 *****
09/16 12:44:12 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:44:12 PM: Validating...
09/16 12:44:22 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.8459, acc: 0.8062, precision: 0.8944, recall: 0.8156, f1: 0.8531, edges-ner-ontonotes_loss: 0.0469
09/16 12:44:25 PM: Updating LR scheduler:
09/16 12:44:25 PM: 	Best result seen so far for macro_avg: 0.862
09/16 12:44:25 PM: 	# validation passes without improvement: 1
09/16 12:44:25 PM: edges-ner-ontonotes_loss: training: 0.038743 validation: 0.045160
09/16 12:44:25 PM: macro_avg: validation: 0.855873
09/16 12:44:25 PM: micro_avg: validation: 0.000000
09/16 12:44:25 PM: edges-ner-ontonotes_mcc: training: 0.869807 validation: 0.848829
09/16 12:44:25 PM: edges-ner-ontonotes_acc: training: 0.825435 validation: 0.808993
09/16 12:44:25 PM: edges-ner-ontonotes_precision: training: 0.905140 validation: 0.897496
09/16 12:44:25 PM: edges-ner-ontonotes_recall: training: 0.849325 validation: 0.817941
09/16 12:44:25 PM: edges-ner-ontonotes_f1: training: 0.876344 validation: 0.855873
09/16 12:44:25 PM: Global learning rate: 0.0001
09/16 12:44:25 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:44:36 PM: Update 8094: task edges-ner-ontonotes, batch 94 (8094): mcc: 0.8890, acc: 0.8536, precision: 0.9165, recall: 0.8739, f1: 0.8947, edges-ner-ontonotes_loss: 0.0333
09/16 12:44:46 PM: Update 8221: task edges-ner-ontonotes, batch 221 (8221): mcc: 0.8802, acc: 0.8411, precision: 0.9144, recall: 0.8598, f1: 0.8862, edges-ner-ontonotes_loss: 0.0355
09/16 12:44:56 PM: Update 8340: task edges-ner-ontonotes, batch 340 (8340): mcc: 0.8798, acc: 0.8400, precision: 0.9140, recall: 0.8594, f1: 0.8859, edges-ner-ontonotes_loss: 0.0358
09/16 12:45:06 PM: Update 8421: task edges-ner-ontonotes, batch 421 (8421): mcc: 0.8788, acc: 0.8384, precision: 0.9135, recall: 0.8580, f1: 0.8849, edges-ner-ontonotes_loss: 0.0358
09/16 12:45:16 PM: Update 8533: task edges-ner-ontonotes, batch 533 (8533): mcc: 0.8764, acc: 0.8340, precision: 0.9103, recall: 0.8565, f1: 0.8826, edges-ner-ontonotes_loss: 0.0360
09/16 12:45:28 PM: Update 8643: task edges-ner-ontonotes, batch 643 (8643): mcc: 0.8769, acc: 0.8344, precision: 0.9097, recall: 0.8581, f1: 0.8832, edges-ner-ontonotes_loss: 0.0356
09/16 12:45:38 PM: Update 8732: task edges-ner-ontonotes, batch 732 (8732): mcc: 0.8768, acc: 0.8339, precision: 0.9095, recall: 0.8580, f1: 0.8830, edges-ner-ontonotes_loss: 0.0354
09/16 12:45:48 PM: Update 8858: task edges-ner-ontonotes, batch 858 (8858): mcc: 0.8764, acc: 0.8330, precision: 0.9088, recall: 0.8580, f1: 0.8827, edges-ner-ontonotes_loss: 0.0354
09/16 12:45:58 PM: Update 8960: task edges-ner-ontonotes, batch 960 (8960): mcc: 0.8766, acc: 0.8330, precision: 0.9086, recall: 0.8586, f1: 0.8829, edges-ner-ontonotes_loss: 0.0352
09/16 12:46:01 PM: ***** Step 9000 / Validation 9 *****
09/16 12:46:01 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:46:01 PM: Validating...
09/16 12:46:08 PM: Evaluate: task edges-ner-ontonotes, batch 117 (157): mcc: 0.8478, acc: 0.8028, precision: 0.8900, recall: 0.8231, f1: 0.8552, edges-ner-ontonotes_loss: 0.0482
09/16 12:46:10 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:46:10 PM: Best result seen so far for macro.
09/16 12:46:10 PM: Updating LR scheduler:
09/16 12:46:10 PM: 	Best result seen so far for macro_avg: 0.868
09/16 12:46:10 PM: 	# validation passes without improvement: 0
09/16 12:46:10 PM: edges-ner-ontonotes_loss: training: 0.035273 validation: 0.043990
09/16 12:46:10 PM: macro_avg: validation: 0.867877
09/16 12:46:10 PM: micro_avg: validation: 0.000000
09/16 12:46:10 PM: edges-ner-ontonotes_mcc: training: 0.876351 validation: 0.860956
09/16 12:46:10 PM: edges-ner-ontonotes_acc: training: 0.832642 validation: 0.818471
09/16 12:46:10 PM: edges-ner-ontonotes_precision: training: 0.908444 validation: 0.898790
09/16 12:46:10 PM: edges-ner-ontonotes_recall: training: 0.858285 validation: 0.839020
09/16 12:46:10 PM: edges-ner-ontonotes_f1: training: 0.882652 validation: 0.867877
09/16 12:46:10 PM: Global learning rate: 0.0001
09/16 12:46:10 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:46:18 PM: Update 9098: task edges-ner-ontonotes, batch 98 (9098): mcc: 0.8568, acc: 0.8093, precision: 0.8944, recall: 0.8355, f1: 0.8639, edges-ner-ontonotes_loss: 0.0437
09/16 12:46:28 PM: Update 9274: task edges-ner-ontonotes, batch 274 (9274): mcc: 0.8561, acc: 0.8085, precision: 0.8960, recall: 0.8328, f1: 0.8632, edges-ner-ontonotes_loss: 0.0452
09/16 12:46:38 PM: Update 9427: task edges-ner-ontonotes, batch 427 (9427): mcc: 0.8613, acc: 0.8160, precision: 0.9002, recall: 0.8383, f1: 0.8681, edges-ner-ontonotes_loss: 0.0432
09/16 12:46:48 PM: Update 9559: task edges-ner-ontonotes, batch 559 (9559): mcc: 0.8692, acc: 0.8265, precision: 0.9061, recall: 0.8473, f1: 0.8757, edges-ner-ontonotes_loss: 0.0406
09/16 12:47:01 PM: Update 9650: task edges-ner-ontonotes, batch 650 (9650): mcc: 0.8719, acc: 0.8304, precision: 0.9080, recall: 0.8506, f1: 0.8783, edges-ner-ontonotes_loss: 0.0397
09/16 12:47:11 PM: Update 9781: task edges-ner-ontonotes, batch 781 (9781): mcc: 0.8715, acc: 0.8297, precision: 0.9078, recall: 0.8499, f1: 0.8779, edges-ner-ontonotes_loss: 0.0394
09/16 12:47:21 PM: Update 9914: task edges-ner-ontonotes, batch 914 (9914): mcc: 0.8742, acc: 0.8327, precision: 0.9096, recall: 0.8532, f1: 0.8805, edges-ner-ontonotes_loss: 0.0386
09/16 12:47:30 PM: ***** Step 10000 / Validation 10 *****
09/16 12:47:30 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:47:30 PM: Validating...
09/16 12:47:31 PM: Evaluate: task edges-ner-ontonotes, batch 11 (157): mcc: 0.7709, acc: 0.7303, precision: 0.8172, recall: 0.7504, f1: 0.7823, edges-ner-ontonotes_loss: 0.0655
09/16 12:47:41 PM: Updating LR scheduler:
09/16 12:47:41 PM: 	Best result seen so far for macro_avg: 0.868
09/16 12:47:41 PM: 	# validation passes without improvement: 1
09/16 12:47:41 PM: edges-ner-ontonotes_loss: training: 0.038390 validation: 0.043845
09/16 12:47:41 PM: macro_avg: validation: 0.862574
09/16 12:47:41 PM: micro_avg: validation: 0.000000
09/16 12:47:41 PM: edges-ner-ontonotes_mcc: training: 0.874169 validation: 0.855328
09/16 12:47:41 PM: edges-ner-ontonotes_acc: training: 0.832814 validation: 0.816576
09/16 12:47:41 PM: edges-ner-ontonotes_precision: training: 0.909030 validation: 0.892811
09/16 12:47:41 PM: edges-ner-ontonotes_recall: training: 0.853678 validation: 0.834319
09/16 12:47:41 PM: edges-ner-ontonotes_f1: training: 0.880485 validation: 0.862574
09/16 12:47:41 PM: Global learning rate: 0.0001
09/16 12:47:41 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:47:41 PM: Update 10002: task edges-ner-ontonotes, batch 2 (10002): mcc: 0.8387, acc: 0.7771, precision: 0.8727, recall: 0.8229, f1: 0.8471, edges-ner-ontonotes_loss: 0.0377
09/16 12:47:51 PM: Update 10102: task edges-ner-ontonotes, batch 102 (10102): mcc: 0.8728, acc: 0.8254, precision: 0.9022, recall: 0.8577, f1: 0.8794, edges-ner-ontonotes_loss: 0.0360
09/16 12:48:01 PM: Update 10225: task edges-ner-ontonotes, batch 225 (10225): mcc: 0.8751, acc: 0.8300, precision: 0.9035, recall: 0.8606, f1: 0.8816, edges-ner-ontonotes_loss: 0.0345
09/16 12:48:11 PM: Update 10276: task edges-ner-ontonotes, batch 276 (10276): mcc: 0.8735, acc: 0.8282, precision: 0.9025, recall: 0.8586, f1: 0.8800, edges-ner-ontonotes_loss: 0.0348
09/16 12:48:21 PM: Update 10380: task edges-ner-ontonotes, batch 380 (10380): mcc: 0.8727, acc: 0.8274, precision: 0.9021, recall: 0.8577, f1: 0.8793, edges-ner-ontonotes_loss: 0.0349
09/16 12:48:31 PM: Update 10481: task edges-ner-ontonotes, batch 481 (10481): mcc: 0.8737, acc: 0.8284, precision: 0.9032, recall: 0.8585, f1: 0.8803, edges-ner-ontonotes_loss: 0.0348
09/16 12:48:43 PM: Update 10589: task edges-ner-ontonotes, batch 589 (10589): mcc: 0.8749, acc: 0.8298, precision: 0.9042, recall: 0.8596, f1: 0.8814, edges-ner-ontonotes_loss: 0.0346
09/16 12:48:53 PM: Update 10742: task edges-ner-ontonotes, batch 742 (10742): mcc: 0.8701, acc: 0.8242, precision: 0.9017, recall: 0.8531, f1: 0.8767, edges-ner-ontonotes_loss: 0.0371
09/16 12:49:08 PM: Update 10893: task edges-ner-ontonotes, batch 893 (10893): mcc: 0.8686, acc: 0.8230, precision: 0.9014, recall: 0.8507, f1: 0.8753, edges-ner-ontonotes_loss: 0.0385
09/16 12:49:15 PM: ***** Step 11000 / Validation 11 *****
09/16 12:49:16 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:49:16 PM: Validating...
09/16 12:49:18 PM: Evaluate: task edges-ner-ontonotes, batch 38 (157): mcc: 0.8619, acc: 0.8289, precision: 0.8967, recall: 0.8427, f1: 0.8689, edges-ner-ontonotes_loss: 0.0452
09/16 12:49:28 PM: Updating LR scheduler:
09/16 12:49:28 PM: 	Best result seen so far for macro_avg: 0.868
09/16 12:49:28 PM: 	# validation passes without improvement: 2
09/16 12:49:28 PM: edges-ner-ontonotes_loss: training: 0.038251 validation: 0.044146
09/16 12:49:28 PM: macro_avg: validation: 0.858945
09/16 12:49:28 PM: micro_avg: validation: 0.000000
09/16 12:49:28 PM: edges-ner-ontonotes_mcc: training: 0.870006 validation: 0.852085
09/16 12:49:28 PM: edges-ner-ontonotes_acc: training: 0.825326 validation: 0.810889
09/16 12:49:28 PM: edges-ner-ontonotes_precision: training: 0.902573 validation: 0.900874
09/16 12:49:28 PM: edges-ner-ontonotes_recall: training: 0.852157 validation: 0.820746
09/16 12:49:28 PM: edges-ner-ontonotes_f1: training: 0.876640 validation: 0.858945
09/16 12:49:28 PM: Global learning rate: 0.0001
09/16 12:49:28 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:49:28 PM: Update 11003: task edges-ner-ontonotes, batch 3 (11003): mcc: 0.9057, acc: 0.8713, precision: 0.9273, recall: 0.8947, f1: 0.9107, edges-ner-ontonotes_loss: 0.0304
09/16 12:49:38 PM: Update 11129: task edges-ner-ontonotes, batch 129 (11129): mcc: 0.8974, acc: 0.8639, precision: 0.9252, recall: 0.8812, f1: 0.9027, edges-ner-ontonotes_loss: 0.0316
09/16 12:49:49 PM: Update 11264: task edges-ner-ontonotes, batch 264 (11264): mcc: 0.8886, acc: 0.8524, precision: 0.9193, recall: 0.8706, f1: 0.8943, edges-ner-ontonotes_loss: 0.0329
09/16 12:49:59 PM: Update 11367: task edges-ner-ontonotes, batch 367 (11367): mcc: 0.8838, acc: 0.8462, precision: 0.9161, recall: 0.8648, f1: 0.8897, edges-ner-ontonotes_loss: 0.0338
09/16 12:50:12 PM: Update 11519: task edges-ner-ontonotes, batch 519 (11519): mcc: 0.8843, acc: 0.8467, precision: 0.9157, recall: 0.8661, f1: 0.8902, edges-ner-ontonotes_loss: 0.0341
09/16 12:50:22 PM: Update 11637: task edges-ner-ontonotes, batch 637 (11637): mcc: 0.8813, acc: 0.8418, precision: 0.9120, recall: 0.8641, f1: 0.8874, edges-ner-ontonotes_loss: 0.0345
09/16 12:50:32 PM: Update 11745: task edges-ner-ontonotes, batch 745 (11745): mcc: 0.8808, acc: 0.8405, precision: 0.9114, recall: 0.8636, f1: 0.8869, edges-ner-ontonotes_loss: 0.0344
09/16 12:50:44 PM: Update 11832: task edges-ner-ontonotes, batch 832 (11832): mcc: 0.8801, acc: 0.8395, precision: 0.9101, recall: 0.8636, f1: 0.8862, edges-ner-ontonotes_loss: 0.0343
09/16 12:50:54 PM: Update 11961: task edges-ner-ontonotes, batch 961 (11961): mcc: 0.8792, acc: 0.8380, precision: 0.9094, recall: 0.8627, f1: 0.8854, edges-ner-ontonotes_loss: 0.0343
09/16 12:50:56 PM: ***** Step 12000 / Validation 12 *****
09/16 12:50:56 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:50:56 PM: Validating...
09/16 12:51:04 PM: Evaluate: task edges-ner-ontonotes, batch 133 (157): mcc: 0.8570, acc: 0.8192, precision: 0.8959, recall: 0.8344, f1: 0.8641, edges-ner-ontonotes_loss: 0.0456
09/16 12:51:05 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:51:07 PM: Best result seen so far for macro.
09/16 12:51:07 PM: Updating LR scheduler:
09/16 12:51:07 PM: 	Best result seen so far for macro_avg: 0.868
09/16 12:51:07 PM: 	# validation passes without improvement: 0
09/16 12:51:07 PM: edges-ner-ontonotes_loss: training: 0.034350 validation: 0.043702
09/16 12:51:07 PM: macro_avg: validation: 0.868215
09/16 12:51:07 PM: micro_avg: validation: 0.000000
09/16 12:51:07 PM: edges-ner-ontonotes_mcc: training: 0.879229 validation: 0.861318
09/16 12:51:07 PM: edges-ner-ontonotes_acc: training: 0.837939 validation: 0.823552
09/16 12:51:07 PM: edges-ner-ontonotes_precision: training: 0.909237 validation: 0.899253
09/16 12:51:07 PM: edges-ner-ontonotes_recall: training: 0.862867 validation: 0.839248
09/16 12:51:07 PM: edges-ner-ontonotes_f1: training: 0.885445 validation: 0.868215
09/16 12:51:07 PM: Global learning rate: 0.0001
09/16 12:51:07 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:51:14 PM: Update 12099: task edges-ner-ontonotes, batch 99 (12099): mcc: 0.8794, acc: 0.8340, precision: 0.9084, recall: 0.8640, f1: 0.8856, edges-ner-ontonotes_loss: 0.0331
09/16 12:51:24 PM: Update 12182: task edges-ner-ontonotes, batch 182 (12182): mcc: 0.8673, acc: 0.8207, precision: 0.9003, recall: 0.8492, f1: 0.8740, edges-ner-ontonotes_loss: 0.0379
09/16 12:51:34 PM: Update 12305: task edges-ner-ontonotes, batch 305 (12305): mcc: 0.8653, acc: 0.8194, precision: 0.9009, recall: 0.8450, f1: 0.8721, edges-ner-ontonotes_loss: 0.0404
09/16 12:51:44 PM: Update 12424: task edges-ner-ontonotes, batch 424 (12424): mcc: 0.8641, acc: 0.8183, precision: 0.9004, recall: 0.8433, f1: 0.8709, edges-ner-ontonotes_loss: 0.0415
09/16 12:51:54 PM: Update 12485: task edges-ner-ontonotes, batch 485 (12485): mcc: 0.8645, acc: 0.8189, precision: 0.9010, recall: 0.8436, f1: 0.8713, edges-ner-ontonotes_loss: 0.0414
09/16 12:52:04 PM: Update 12609: task edges-ner-ontonotes, batch 609 (12609): mcc: 0.8693, acc: 0.8260, precision: 0.9040, recall: 0.8495, f1: 0.8759, edges-ner-ontonotes_loss: 0.0396
09/16 12:52:17 PM: Update 12762: task edges-ner-ontonotes, batch 762 (12762): mcc: 0.8744, acc: 0.8331, precision: 0.9076, recall: 0.8555, f1: 0.8808, edges-ner-ontonotes_loss: 0.0381
09/16 12:52:27 PM: Update 12879: task edges-ner-ontonotes, batch 879 (12879): mcc: 0.8745, acc: 0.8334, precision: 0.9077, recall: 0.8555, f1: 0.8808, edges-ner-ontonotes_loss: 0.0379
09/16 12:52:36 PM: ***** Step 13000 / Validation 13 *****
09/16 12:52:36 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:52:37 PM: Validating...
09/16 12:52:38 PM: Evaluate: task edges-ner-ontonotes, batch 7 (157): mcc: 0.7661, acc: 0.7241, precision: 0.8060, recall: 0.7522, f1: 0.7781, edges-ner-ontonotes_loss: 0.0628
09/16 12:52:48 PM: Evaluate: task edges-ner-ontonotes, batch 131 (157): mcc: 0.8518, acc: 0.8137, precision: 0.8903, recall: 0.8302, f1: 0.8592, edges-ner-ontonotes_loss: 0.0449
09/16 12:52:50 PM: Updating LR scheduler:
09/16 12:52:50 PM: 	Best result seen so far for macro_avg: 0.868
09/16 12:52:50 PM: 	# validation passes without improvement: 1
09/16 12:52:50 PM: edges-ner-ontonotes_loss: training: 0.037537 validation: 0.043230
09/16 12:52:50 PM: macro_avg: validation: 0.864344
09/16 12:52:50 PM: micro_avg: validation: 0.000000
09/16 12:52:50 PM: edges-ner-ontonotes_mcc: training: 0.875121 validation: 0.857197
09/16 12:52:50 PM: edges-ner-ontonotes_acc: training: 0.834123 validation: 0.819912
09/16 12:52:50 PM: edges-ner-ontonotes_precision: training: 0.907900 validation: 0.894604
09/16 12:52:50 PM: edges-ner-ontonotes_recall: training: 0.856525 validation: 0.836063
09/16 12:52:50 PM: edges-ner-ontonotes_f1: training: 0.881465 validation: 0.864344
09/16 12:52:50 PM: Global learning rate: 0.0001
09/16 12:52:50 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:53:00 PM: Update 13075: task edges-ner-ontonotes, batch 75 (13075): mcc: 0.8868, acc: 0.8508, precision: 0.9169, recall: 0.8694, f1: 0.8926, edges-ner-ontonotes_loss: 0.0333
09/16 12:53:10 PM: Update 13222: task edges-ner-ontonotes, batch 222 (13222): mcc: 0.8782, acc: 0.8359, precision: 0.9080, recall: 0.8622, f1: 0.8845, edges-ner-ontonotes_loss: 0.0341
09/16 12:53:20 PM: Update 13336: task edges-ner-ontonotes, batch 336 (13336): mcc: 0.8765, acc: 0.8338, precision: 0.9059, recall: 0.8611, f1: 0.8829, edges-ner-ontonotes_loss: 0.0342
09/16 12:53:31 PM: Update 13421: task edges-ner-ontonotes, batch 421 (13421): mcc: 0.8754, acc: 0.8318, precision: 0.9044, recall: 0.8604, f1: 0.8819, edges-ner-ontonotes_loss: 0.0345
09/16 12:53:41 PM: Update 13584: task edges-ner-ontonotes, batch 584 (13584): mcc: 0.8761, acc: 0.8321, precision: 0.9049, recall: 0.8613, f1: 0.8826, edges-ner-ontonotes_loss: 0.0344
09/16 12:53:51 PM: Update 13700: task edges-ner-ontonotes, batch 700 (13700): mcc: 0.8773, acc: 0.8331, precision: 0.9061, recall: 0.8624, f1: 0.8837, edges-ner-ontonotes_loss: 0.0340
09/16 12:54:01 PM: Update 13779: task edges-ner-ontonotes, batch 779 (13779): mcc: 0.8744, acc: 0.8297, precision: 0.9042, recall: 0.8587, f1: 0.8809, edges-ner-ontonotes_loss: 0.0353
09/16 12:54:11 PM: Update 13919: task edges-ner-ontonotes, batch 919 (13919): mcc: 0.8718, acc: 0.8268, precision: 0.9028, recall: 0.8552, f1: 0.8784, edges-ner-ontonotes_loss: 0.0369
09/16 12:54:16 PM: ***** Step 14000 / Validation 14 *****
09/16 12:54:16 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:54:16 PM: Validating...
09/16 12:54:21 PM: Evaluate: task edges-ner-ontonotes, batch 83 (157): mcc: 0.8649, acc: 0.8291, precision: 0.9067, recall: 0.8388, f1: 0.8714, edges-ner-ontonotes_loss: 0.0450
09/16 12:54:28 PM: Updating LR scheduler:
09/16 12:54:28 PM: 	Best result seen so far for macro_avg: 0.868
09/16 12:54:28 PM: 	# validation passes without improvement: 2
09/16 12:54:28 PM: edges-ner-ontonotes_loss: training: 0.037360 validation: 0.045028
09/16 12:54:28 PM: macro_avg: validation: 0.859395
09/16 12:54:28 PM: micro_avg: validation: 0.000000
09/16 12:54:28 PM: edges-ner-ontonotes_mcc: training: 0.871184 validation: 0.852479
09/16 12:54:28 PM: edges-ner-ontonotes_acc: training: 0.826305 validation: 0.813618
09/16 12:54:28 PM: edges-ner-ontonotes_precision: training: 0.902961 validation: 0.899859
09/16 12:54:28 PM: edges-ner-ontonotes_recall: training: 0.853971 validation: 0.822414
09/16 12:54:28 PM: edges-ner-ontonotes_f1: training: 0.877783 validation: 0.859395
09/16 12:54:28 PM: Global learning rate: 0.0001
09/16 12:54:28 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:54:33 PM: Update 14005: task edges-ner-ontonotes, batch 5 (14005): mcc: 0.8511, acc: 0.8050, precision: 0.8799, recall: 0.8390, f1: 0.8590, edges-ner-ontonotes_loss: 0.0448
09/16 12:54:44 PM: Update 14131: task edges-ner-ontonotes, batch 131 (14131): mcc: 0.8881, acc: 0.8524, precision: 0.9170, recall: 0.8718, f1: 0.8938, edges-ner-ontonotes_loss: 0.0348
09/16 12:55:00 PM: Update 14318: task edges-ner-ontonotes, batch 318 (14318): mcc: 0.8950, acc: 0.8616, precision: 0.9216, recall: 0.8803, f1: 0.9005, edges-ner-ontonotes_loss: 0.0327
09/16 12:55:10 PM: Update 14428: task edges-ner-ontonotes, batch 428 (14428): mcc: 0.8870, acc: 0.8506, precision: 0.9158, recall: 0.8710, f1: 0.8928, edges-ner-ontonotes_loss: 0.0341
09/16 12:55:20 PM: Update 14554: task edges-ner-ontonotes, batch 554 (14554): mcc: 0.8873, acc: 0.8511, precision: 0.9165, recall: 0.8709, f1: 0.8931, edges-ner-ontonotes_loss: 0.0337
09/16 12:55:34 PM: Update 14631: task edges-ner-ontonotes, batch 631 (14631): mcc: 0.8873, acc: 0.8510, precision: 0.9167, recall: 0.8707, f1: 0.8931, edges-ner-ontonotes_loss: 0.0337
09/16 12:55:44 PM: Update 14756: task edges-ner-ontonotes, batch 756 (14756): mcc: 0.8837, acc: 0.8451, precision: 0.9132, recall: 0.8675, f1: 0.8897, edges-ner-ontonotes_loss: 0.0340
09/16 12:55:54 PM: Update 14887: task edges-ner-ontonotes, batch 887 (14887): mcc: 0.8839, acc: 0.8446, precision: 0.9128, recall: 0.8680, f1: 0.8899, edges-ner-ontonotes_loss: 0.0338
09/16 12:56:04 PM: Update 14988: task edges-ner-ontonotes, batch 988 (14988): mcc: 0.8824, acc: 0.8425, precision: 0.9115, recall: 0.8666, f1: 0.8885, edges-ner-ontonotes_loss: 0.0339
09/16 12:56:05 PM: ***** Step 15000 / Validation 15 *****
09/16 12:56:05 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:56:05 PM: Validating...
09/16 12:56:12 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:56:12 PM: Best result seen so far for macro.
09/16 12:56:12 PM: Updating LR scheduler:
09/16 12:56:12 PM: 	Best result seen so far for macro_avg: 0.869
09/16 12:56:12 PM: 	# validation passes without improvement: 0
09/16 12:56:12 PM: edges-ner-ontonotes_loss: training: 0.033848 validation: 0.043752
09/16 12:56:12 PM: macro_avg: validation: 0.869219
09/16 12:56:12 PM: micro_avg: validation: 0.000000
09/16 12:56:12 PM: edges-ner-ontonotes_mcc: training: 0.882355 validation: 0.862148
09/16 12:56:12 PM: edges-ner-ontonotes_acc: training: 0.842343 validation: 0.822187
09/16 12:56:12 PM: edges-ner-ontonotes_precision: training: 0.911393 validation: 0.894686
09/16 12:56:12 PM: edges-ner-ontonotes_recall: training: 0.866595 validation: 0.845162
09/16 12:56:12 PM: edges-ner-ontonotes_f1: training: 0.888430 validation: 0.869219
09/16 12:56:12 PM: Global learning rate: 0.0001
09/16 12:56:12 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:56:14 PM: Update 15034: task edges-ner-ontonotes, batch 34 (15034): mcc: 0.8741, acc: 0.8301, precision: 0.9037, recall: 0.8587, f1: 0.8806, edges-ner-ontonotes_loss: 0.0356
09/16 12:56:24 PM: Update 15176: task edges-ner-ontonotes, batch 176 (15176): mcc: 0.8761, acc: 0.8317, precision: 0.9035, recall: 0.8626, f1: 0.8826, edges-ner-ontonotes_loss: 0.0339
09/16 12:56:34 PM: Update 15257: task edges-ner-ontonotes, batch 257 (15257): mcc: 0.8779, acc: 0.8346, precision: 0.9056, recall: 0.8639, f1: 0.8843, edges-ner-ontonotes_loss: 0.0338
09/16 12:56:45 PM: Update 15392: task edges-ner-ontonotes, batch 392 (15392): mcc: 0.8698, acc: 0.8251, precision: 0.9015, recall: 0.8529, f1: 0.8765, edges-ner-ontonotes_loss: 0.0381
09/16 12:56:55 PM: Update 15548: task edges-ner-ontonotes, batch 548 (15548): mcc: 0.8676, acc: 0.8231, precision: 0.9008, recall: 0.8494, f1: 0.8744, edges-ner-ontonotes_loss: 0.0397
09/16 12:57:05 PM: Update 15599: task edges-ner-ontonotes, batch 599 (15599): mcc: 0.8682, acc: 0.8239, precision: 0.9013, recall: 0.8500, f1: 0.8749, edges-ner-ontonotes_loss: 0.0396
09/16 12:57:15 PM: Update 15713: task edges-ner-ontonotes, batch 713 (15713): mcc: 0.8718, acc: 0.8289, precision: 0.9041, recall: 0.8539, f1: 0.8783, edges-ner-ontonotes_loss: 0.0384
09/16 12:57:25 PM: Update 15871: task edges-ner-ontonotes, batch 871 (15871): mcc: 0.8763, acc: 0.8355, precision: 0.9076, recall: 0.8590, f1: 0.8827, edges-ner-ontonotes_loss: 0.0372
09/16 12:57:35 PM: Update 15926: task edges-ner-ontonotes, batch 926 (15926): mcc: 0.8756, acc: 0.8348, precision: 0.9071, recall: 0.8582, f1: 0.8820, edges-ner-ontonotes_loss: 0.0372
09/16 12:57:41 PM: ***** Step 16000 / Validation 16 *****
09/16 12:57:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:57:41 PM: Validating...
09/16 12:57:45 PM: Evaluate: task edges-ner-ontonotes, batch 55 (157): mcc: 0.8734, acc: 0.8457, precision: 0.9027, recall: 0.8583, f1: 0.8799, edges-ner-ontonotes_loss: 0.0415
09/16 12:57:51 PM: Updating LR scheduler:
09/16 12:57:51 PM: 	Best result seen so far for macro_avg: 0.869
09/16 12:57:51 PM: 	# validation passes without improvement: 1
09/16 12:57:51 PM: edges-ner-ontonotes_loss: training: 0.036806 validation: 0.043233
09/16 12:57:51 PM: macro_avg: validation: 0.865695
09/16 12:57:51 PM: micro_avg: validation: 0.000000
09/16 12:57:51 PM: edges-ner-ontonotes_mcc: training: 0.876998 validation: 0.858842
09/16 12:57:51 PM: edges-ner-ontonotes_acc: training: 0.836477 validation: 0.821656
09/16 12:57:51 PM: edges-ner-ontonotes_precision: training: 0.908285 validation: 0.900672
09/16 12:57:51 PM: edges-ner-ontonotes_recall: training: 0.859637 validation: 0.833333
09/16 12:57:51 PM: edges-ner-ontonotes_f1: training: 0.883292 validation: 0.865695
09/16 12:57:51 PM: Global learning rate: 0.0001
09/16 12:57:51 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:57:55 PM: Update 16060: task edges-ner-ontonotes, batch 60 (16060): mcc: 0.8849, acc: 0.8453, precision: 0.9150, recall: 0.8679, f1: 0.8908, edges-ner-ontonotes_loss: 0.0332
09/16 12:58:05 PM: Update 16177: task edges-ner-ontonotes, batch 177 (16177): mcc: 0.8809, acc: 0.8420, precision: 0.9124, recall: 0.8629, f1: 0.8870, edges-ner-ontonotes_loss: 0.0351
09/16 12:58:15 PM: Update 16275: task edges-ner-ontonotes, batch 275 (16275): mcc: 0.8759, acc: 0.8342, precision: 0.9064, recall: 0.8593, f1: 0.8823, edges-ner-ontonotes_loss: 0.0354
09/16 12:58:25 PM: Update 16411: task edges-ner-ontonotes, batch 411 (16411): mcc: 0.8779, acc: 0.8358, precision: 0.9072, recall: 0.8623, f1: 0.8842, edges-ner-ontonotes_loss: 0.0347
09/16 12:58:37 PM: Update 16500: task edges-ner-ontonotes, batch 500 (16500): mcc: 0.8782, acc: 0.8359, precision: 0.9073, recall: 0.8628, f1: 0.8845, edges-ner-ontonotes_loss: 0.0344
09/16 12:58:48 PM: Update 16624: task edges-ner-ontonotes, batch 624 (16624): mcc: 0.8780, acc: 0.8352, precision: 0.9063, recall: 0.8634, f1: 0.8843, edges-ner-ontonotes_loss: 0.0343
09/16 12:58:58 PM: Update 16742: task edges-ner-ontonotes, batch 742 (16742): mcc: 0.8776, acc: 0.8343, precision: 0.9060, recall: 0.8629, f1: 0.8839, edges-ner-ontonotes_loss: 0.0343
09/16 12:59:09 PM: Update 16813: task edges-ner-ontonotes, batch 813 (16813): mcc: 0.8785, acc: 0.8353, precision: 0.9068, recall: 0.8639, f1: 0.8848, edges-ner-ontonotes_loss: 0.0341
09/16 12:59:19 PM: Update 16938: task edges-ner-ontonotes, batch 938 (16938): mcc: 0.8746, acc: 0.8308, precision: 0.9042, recall: 0.8592, f1: 0.8811, edges-ner-ontonotes_loss: 0.0358
09/16 12:59:25 PM: ***** Step 17000 / Validation 17 *****
09/16 12:59:25 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:59:25 PM: Validating...
09/16 12:59:29 PM: Evaluate: task edges-ner-ontonotes, batch 74 (157): mcc: 0.8679, acc: 0.8306, precision: 0.9110, recall: 0.8402, f1: 0.8742, edges-ner-ontonotes_loss: 0.0465
09/16 12:59:37 PM: Updating LR scheduler:
09/16 12:59:37 PM: 	Best result seen so far for macro_avg: 0.869
09/16 12:59:37 PM: 	# validation passes without improvement: 2
09/16 12:59:37 PM: edges-ner-ontonotes_loss: training: 0.036170 validation: 0.044803
09/16 12:59:37 PM: macro_avg: validation: 0.862552
09/16 12:59:37 PM: micro_avg: validation: 0.000000
09/16 12:59:37 PM: edges-ner-ontonotes_mcc: training: 0.874304 validation: 0.855806
09/16 12:59:37 PM: edges-ner-ontonotes_acc: training: 0.830640 validation: 0.816803
09/16 12:59:37 PM: edges-ner-ontonotes_precision: training: 0.904043 validation: 0.902969
09/16 12:59:37 PM: edges-ner-ontonotes_recall: training: 0.858727 validation: 0.825599
09/16 12:59:37 PM: edges-ner-ontonotes_f1: training: 0.880803 validation: 0.862552
09/16 12:59:37 PM: Global learning rate: 0.0001
09/16 12:59:37 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 12:59:39 PM: Update 17021: task edges-ner-ontonotes, batch 21 (17021): mcc: 0.8708, acc: 0.8261, precision: 0.9069, recall: 0.8494, f1: 0.8772, edges-ner-ontonotes_loss: 0.0403
09/16 12:59:50 PM: Update 17117: task edges-ner-ontonotes, batch 117 (17117): mcc: 0.8632, acc: 0.8156, precision: 0.9034, recall: 0.8388, f1: 0.8699, edges-ner-ontonotes_loss: 0.0426
09/16 01:00:00 PM: Update 17232: task edges-ner-ontonotes, batch 232 (17232): mcc: 0.8782, acc: 0.8370, precision: 0.9121, recall: 0.8582, f1: 0.8843, edges-ner-ontonotes_loss: 0.0377
09/16 01:00:10 PM: Update 17362: task edges-ner-ontonotes, batch 362 (17362): mcc: 0.8847, acc: 0.8461, precision: 0.9149, recall: 0.8675, f1: 0.8906, edges-ner-ontonotes_loss: 0.0358
09/16 01:00:20 PM: Update 17430: task edges-ner-ontonotes, batch 430 (17430): mcc: 0.8876, acc: 0.8502, precision: 0.9170, recall: 0.8709, f1: 0.8933, edges-ner-ontonotes_loss: 0.0348
09/16 01:00:30 PM: Update 17582: task edges-ner-ontonotes, batch 582 (17582): mcc: 0.8850, acc: 0.8470, precision: 0.9152, recall: 0.8679, f1: 0.8909, edges-ner-ontonotes_loss: 0.0350
09/16 01:00:40 PM: Update 17724: task edges-ner-ontonotes, batch 724 (17724): mcc: 0.8847, acc: 0.8463, precision: 0.9151, recall: 0.8673, f1: 0.8906, edges-ner-ontonotes_loss: 0.0347
09/16 01:00:50 PM: Update 17791: task edges-ner-ontonotes, batch 791 (17791): mcc: 0.8832, acc: 0.8445, precision: 0.9134, recall: 0.8663, f1: 0.8892, edges-ner-ontonotes_loss: 0.0347
09/16 01:01:00 PM: Update 17908: task edges-ner-ontonotes, batch 908 (17908): mcc: 0.8815, acc: 0.8419, precision: 0.9114, recall: 0.8651, f1: 0.8876, edges-ner-ontonotes_loss: 0.0348
09/16 01:01:06 PM: ***** Step 18000 / Validation 18 *****
09/16 01:01:06 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:01:06 PM: Validating...
09/16 01:01:10 PM: Evaluate: task edges-ner-ontonotes, batch 96 (157): mcc: 0.8582, acc: 0.8216, precision: 0.8940, recall: 0.8385, f1: 0.8654, edges-ner-ontonotes_loss: 0.0482
09/16 01:01:14 PM: Updating LR scheduler:
09/16 01:01:14 PM: 	Best result seen so far for macro_avg: 0.869
09/16 01:01:14 PM: 	# validation passes without improvement: 3
09/16 01:01:14 PM: edges-ner-ontonotes_loss: training: 0.034448 validation: 0.043872
09/16 01:01:14 PM: macro_avg: validation: 0.868621
09/16 01:01:14 PM: micro_avg: validation: 0.000000
09/16 01:01:14 PM: edges-ner-ontonotes_mcc: training: 0.881953 validation: 0.861455
09/16 01:01:14 PM: edges-ner-ontonotes_acc: training: 0.842289 validation: 0.823400
09/16 01:01:14 PM: edges-ner-ontonotes_precision: training: 0.911488 validation: 0.892489
09/16 01:01:14 PM: edges-ner-ontonotes_recall: training: 0.865757 validation: 0.845996
09/16 01:01:14 PM: edges-ner-ontonotes_f1: training: 0.888034 validation: 0.868621
09/16 01:01:14 PM: Global learning rate: 0.0001
09/16 01:01:14 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:01:20 PM: Update 18065: task edges-ner-ontonotes, batch 65 (18065): mcc: 0.8721, acc: 0.8272, precision: 0.9001, recall: 0.8584, f1: 0.8787, edges-ner-ontonotes_loss: 0.0335
09/16 01:01:30 PM: Update 18266: task edges-ner-ontonotes, batch 266 (18266): mcc: 0.8752, acc: 0.8306, precision: 0.9025, recall: 0.8619, f1: 0.8817, edges-ner-ontonotes_loss: 0.0337
09/16 01:01:42 PM: Update 18369: task edges-ner-ontonotes, batch 369 (18369): mcc: 0.8765, acc: 0.8324, precision: 0.9037, recall: 0.8632, f1: 0.8830, edges-ner-ontonotes_loss: 0.0334
09/16 01:01:52 PM: Update 18554: task edges-ner-ontonotes, batch 554 (18554): mcc: 0.8689, acc: 0.8239, precision: 0.8994, recall: 0.8531, f1: 0.8757, edges-ner-ontonotes_loss: 0.0376
09/16 01:02:02 PM: Update 18671: task edges-ner-ontonotes, batch 671 (18671): mcc: 0.8680, acc: 0.8231, precision: 0.8998, recall: 0.8512, f1: 0.8748, edges-ner-ontonotes_loss: 0.0384
09/16 01:02:12 PM: Update 18747: task edges-ner-ontonotes, batch 747 (18747): mcc: 0.8692, acc: 0.8249, precision: 0.9008, recall: 0.8524, f1: 0.8759, edges-ner-ontonotes_loss: 0.0381
09/16 01:02:22 PM: Update 18879: task edges-ner-ontonotes, batch 879 (18879): mcc: 0.8731, acc: 0.8305, precision: 0.9037, recall: 0.8568, f1: 0.8796, edges-ner-ontonotes_loss: 0.0370
09/16 01:02:35 PM: Update 18986: task edges-ner-ontonotes, batch 986 (18986): mcc: 0.8756, acc: 0.8341, precision: 0.9056, recall: 0.8597, f1: 0.8821, edges-ner-ontonotes_loss: 0.0363
09/16 01:02:36 PM: ***** Step 19000 / Validation 19 *****
09/16 01:02:36 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:02:36 PM: Validating...
09/16 01:02:45 PM: Evaluate: task edges-ner-ontonotes, batch 152 (157): mcc: 0.8531, acc: 0.8105, precision: 0.9011, recall: 0.8224, f1: 0.8599, edges-ner-ontonotes_loss: 0.0444
09/16 01:02:46 PM: Updating LR scheduler:
09/16 01:02:46 PM: 	Best result seen so far for macro_avg: 0.869
09/16 01:02:46 PM: 	# validation passes without improvement: 0
09/16 01:02:46 PM: edges-ner-ontonotes_loss: training: 0.036316 validation: 0.043846
09/16 01:02:46 PM: macro_avg: validation: 0.861220
09/16 01:02:46 PM: micro_avg: validation: 0.000000
09/16 01:02:46 PM: edges-ner-ontonotes_mcc: training: 0.875526 validation: 0.854438
09/16 01:02:46 PM: edges-ner-ontonotes_acc: training: 0.833942 validation: 0.811950
09/16 01:02:46 PM: edges-ner-ontonotes_precision: training: 0.905623 validation: 0.902317
09/16 01:02:46 PM: edges-ner-ontonotes_recall: training: 0.859472 validation: 0.823703
09/16 01:02:46 PM: edges-ner-ontonotes_f1: training: 0.881944 validation: 0.861220
09/16 01:02:46 PM: Global learning rate: 5e-05
09/16 01:02:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:02:56 PM: Update 19114: task edges-ner-ontonotes, batch 114 (19114): mcc: 0.8732, acc: 0.8299, precision: 0.9067, recall: 0.8542, f1: 0.8796, edges-ner-ontonotes_loss: 0.0366
09/16 01:03:06 PM: Update 19251: task edges-ner-ontonotes, batch 251 (19251): mcc: 0.8789, acc: 0.8379, precision: 0.9113, recall: 0.8602, f1: 0.8850, edges-ner-ontonotes_loss: 0.0356
09/16 01:03:16 PM: Update 19324: task edges-ner-ontonotes, batch 324 (19324): mcc: 0.8788, acc: 0.8381, precision: 0.9108, recall: 0.8606, f1: 0.8850, edges-ner-ontonotes_loss: 0.0352
09/16 01:03:26 PM: Update 19432: task edges-ner-ontonotes, batch 432 (19432): mcc: 0.8781, acc: 0.8365, precision: 0.9086, recall: 0.8614, f1: 0.8844, edges-ner-ontonotes_loss: 0.0350
09/16 01:03:36 PM: Update 19554: task edges-ner-ontonotes, batch 554 (19554): mcc: 0.8769, acc: 0.8344, precision: 0.9069, recall: 0.8608, f1: 0.8833, edges-ner-ontonotes_loss: 0.0349
09/16 01:03:46 PM: Update 19648: task edges-ner-ontonotes, batch 648 (19648): mcc: 0.8772, acc: 0.8342, precision: 0.9070, recall: 0.8612, f1: 0.8835, edges-ner-ontonotes_loss: 0.0346
09/16 01:03:56 PM: Update 19763: task edges-ner-ontonotes, batch 763 (19763): mcc: 0.8767, acc: 0.8334, precision: 0.9065, recall: 0.8608, f1: 0.8831, edges-ner-ontonotes_loss: 0.0346
09/16 01:04:06 PM: Update 19843: task edges-ner-ontonotes, batch 843 (19843): mcc: 0.8770, acc: 0.8336, precision: 0.9067, recall: 0.8612, f1: 0.8834, edges-ner-ontonotes_loss: 0.0345
09/16 01:04:17 PM: Update 19925: task edges-ner-ontonotes, batch 925 (19925): mcc: 0.8771, acc: 0.8335, precision: 0.9065, recall: 0.8615, f1: 0.8834, edges-ner-ontonotes_loss: 0.0343
09/16 01:04:22 PM: ***** Step 20000 / Validation 20 *****
09/16 01:04:22 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:04:22 PM: Validating...
09/16 01:04:27 PM: Evaluate: task edges-ner-ontonotes, batch 83 (157): mcc: 0.8674, acc: 0.8293, precision: 0.9098, recall: 0.8405, f1: 0.8738, edges-ner-ontonotes_loss: 0.0445
09/16 01:04:34 PM: Updating LR scheduler:
09/16 01:04:34 PM: 	Best result seen so far for macro_avg: 0.869
09/16 01:04:34 PM: 	# validation passes without improvement: 1
09/16 01:04:34 PM: edges-ner-ontonotes_loss: training: 0.035320 validation: 0.042618
09/16 01:04:34 PM: macro_avg: validation: 0.865434
09/16 01:04:34 PM: micro_avg: validation: 0.000000
09/16 01:04:34 PM: edges-ner-ontonotes_mcc: training: 0.875391 validation: 0.858825
09/16 01:04:34 PM: edges-ner-ontonotes_acc: training: 0.831658 validation: 0.818320
09/16 01:04:34 PM: edges-ner-ontonotes_precision: training: 0.905392 validation: 0.905476
09/16 01:04:34 PM: edges-ner-ontonotes_recall: training: 0.859444 validation: 0.828784
09/16 01:04:34 PM: edges-ner-ontonotes_f1: training: 0.881820 validation: 0.865434
09/16 01:04:34 PM: Global learning rate: 5e-05
09/16 01:04:34 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:04:40 PM: Update 20097: task edges-ner-ontonotes, batch 97 (20097): mcc: 0.8603, acc: 0.8128, precision: 0.8985, recall: 0.8380, f1: 0.8672, edges-ner-ontonotes_loss: 0.0418
09/16 01:04:53 PM: Update 20229: task edges-ner-ontonotes, batch 229 (20229): mcc: 0.8637, acc: 0.8179, precision: 0.9014, recall: 0.8416, f1: 0.8705, edges-ner-ontonotes_loss: 0.0425
09/16 01:05:03 PM: Update 20362: task edges-ner-ontonotes, batch 362 (20362): mcc: 0.8761, acc: 0.8344, precision: 0.9100, recall: 0.8564, f1: 0.8824, edges-ner-ontonotes_loss: 0.0383
09/16 01:05:13 PM: Update 20510: task edges-ner-ontonotes, batch 510 (20510): mcc: 0.8825, acc: 0.8435, precision: 0.9138, recall: 0.8646, f1: 0.8885, edges-ner-ontonotes_loss: 0.0361
09/16 01:05:23 PM: Update 20547: task edges-ner-ontonotes, batch 547 (20547): mcc: 0.8825, acc: 0.8436, precision: 0.9137, recall: 0.8648, f1: 0.8885, edges-ner-ontonotes_loss: 0.0360
09/16 01:05:33 PM: Update 20731: task edges-ner-ontonotes, batch 731 (20731): mcc: 0.8824, acc: 0.8438, precision: 0.9135, recall: 0.8647, f1: 0.8884, edges-ner-ontonotes_loss: 0.0356
09/16 01:05:43 PM: Update 20846: task edges-ner-ontonotes, batch 846 (20846): mcc: 0.8823, acc: 0.8437, precision: 0.9134, recall: 0.8646, f1: 0.8883, edges-ner-ontonotes_loss: 0.0355
09/16 01:05:53 PM: Update 20917: task edges-ner-ontonotes, batch 917 (20917): mcc: 0.8817, acc: 0.8426, precision: 0.9124, recall: 0.8644, f1: 0.8878, edges-ner-ontonotes_loss: 0.0354
09/16 01:05:59 PM: ***** Step 21000 / Validation 21 *****
09/16 01:06:00 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:06:00 PM: Validating...
09/16 01:06:03 PM: Evaluate: task edges-ner-ontonotes, batch 88 (157): mcc: 0.8682, acc: 0.8326, precision: 0.9042, recall: 0.8473, f1: 0.8748, edges-ner-ontonotes_loss: 0.0450
09/16 01:06:08 PM: Best result seen so far for edges-ner-ontonotes.
09/16 01:06:08 PM: Best result seen so far for macro.
09/16 01:06:08 PM: Updating LR scheduler:
09/16 01:06:08 PM: 	Best result seen so far for macro_avg: 0.870
09/16 01:06:08 PM: 	# validation passes without improvement: 0
09/16 01:06:08 PM: edges-ner-ontonotes_loss: training: 0.035223 validation: 0.042950
09/16 01:06:08 PM: macro_avg: validation: 0.870069
09/16 01:06:08 PM: micro_avg: validation: 0.000000
09/16 01:06:08 PM: edges-ner-ontonotes_mcc: training: 0.881260 validation: 0.863188
09/16 01:06:08 PM: edges-ner-ontonotes_acc: training: 0.841622 validation: 0.823931
09/16 01:06:08 PM: edges-ner-ontonotes_precision: training: 0.911653 validation: 0.899062
09/16 01:06:08 PM: edges-ner-ontonotes_recall: training: 0.864308 validation: 0.842887
09/16 01:06:08 PM: edges-ner-ontonotes_f1: training: 0.887349 validation: 0.870069
09/16 01:06:08 PM: Global learning rate: 5e-05
09/16 01:06:08 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:06:13 PM: Update 21066: task edges-ner-ontonotes, batch 66 (21066): mcc: 0.8817, acc: 0.8370, precision: 0.9081, recall: 0.8686, f1: 0.8879, edges-ner-ontonotes_loss: 0.0324
09/16 01:06:23 PM: Update 21207: task edges-ner-ontonotes, batch 207 (21207): mcc: 0.8765, acc: 0.8326, precision: 0.9034, recall: 0.8634, f1: 0.8830, edges-ner-ontonotes_loss: 0.0336
09/16 01:06:33 PM: Update 21371: task edges-ner-ontonotes, batch 371 (21371): mcc: 0.8784, acc: 0.8337, precision: 0.9059, recall: 0.8646, f1: 0.8848, edges-ner-ontonotes_loss: 0.0332
09/16 01:06:43 PM: Update 21506: task edges-ner-ontonotes, batch 506 (21506): mcc: 0.8771, acc: 0.8330, precision: 0.9046, recall: 0.8635, f1: 0.8836, edges-ner-ontonotes_loss: 0.0340
09/16 01:06:53 PM: Update 21625: task edges-ner-ontonotes, batch 625 (21625): mcc: 0.8739, acc: 0.8292, precision: 0.9032, recall: 0.8588, f1: 0.8804, edges-ner-ontonotes_loss: 0.0358
09/16 01:07:03 PM: Update 21770: task edges-ner-ontonotes, batch 770 (21770): mcc: 0.8722, acc: 0.8275, precision: 0.9030, recall: 0.8558, f1: 0.8788, edges-ner-ontonotes_loss: 0.0370
09/16 01:07:13 PM: Update 21843: task edges-ner-ontonotes, batch 843 (21843): mcc: 0.8729, acc: 0.8288, precision: 0.9035, recall: 0.8567, f1: 0.8795, edges-ner-ontonotes_loss: 0.0369
09/16 01:07:23 PM: Update 21991: task edges-ner-ontonotes, batch 991 (21991): mcc: 0.8758, acc: 0.8329, precision: 0.9059, recall: 0.8596, f1: 0.8822, edges-ner-ontonotes_loss: 0.0361
09/16 01:07:24 PM: ***** Step 22000 / Validation 22 *****
09/16 01:07:24 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:07:24 PM: Validating...
09/16 01:07:33 PM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.8531, acc: 0.8165, precision: 0.8983, recall: 0.8250, f1: 0.8601, edges-ner-ontonotes_loss: 0.0453
09/16 01:07:37 PM: Updating LR scheduler:
09/16 01:07:37 PM: 	Best result seen so far for macro_avg: 0.870
09/16 01:07:37 PM: 	# validation passes without improvement: 1
09/16 01:07:37 PM: edges-ner-ontonotes_loss: training: 0.036068 validation: 0.043145
09/16 01:07:37 PM: macro_avg: validation: 0.862430
09/16 01:07:37 PM: micro_avg: validation: 0.000000
09/16 01:07:37 PM: edges-ner-ontonotes_mcc: training: 0.875987 validation: 0.855587
09/16 01:07:37 PM: edges-ner-ontonotes_acc: training: 0.833149 validation: 0.818092
09/16 01:07:37 PM: edges-ner-ontonotes_precision: training: 0.906100 validation: 0.901165
09/16 01:07:37 PM: edges-ner-ontonotes_recall: training: 0.859869 validation: 0.826888
09/16 01:07:37 PM: edges-ner-ontonotes_f1: training: 0.882379 validation: 0.862430
09/16 01:07:37 PM: Global learning rate: 5e-05
09/16 01:07:37 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:07:43 PM: Update 22096: task edges-ner-ontonotes, batch 96 (22096): mcc: 0.9022, acc: 0.8706, precision: 0.9269, recall: 0.8886, f1: 0.9073, edges-ner-ontonotes_loss: 0.0298
09/16 01:07:53 PM: Update 22161: task edges-ner-ontonotes, batch 161 (22161): mcc: 0.8928, acc: 0.8577, precision: 0.9213, recall: 0.8764, f1: 0.8983, edges-ner-ontonotes_loss: 0.0315
09/16 01:08:03 PM: Update 22279: task edges-ner-ontonotes, batch 279 (22279): mcc: 0.8869, acc: 0.8495, precision: 0.9173, recall: 0.8694, f1: 0.8927, edges-ner-ontonotes_loss: 0.0331
09/16 01:08:14 PM: Update 22411: task edges-ner-ontonotes, batch 411 (22411): mcc: 0.8860, acc: 0.8486, precision: 0.9157, recall: 0.8692, f1: 0.8918, edges-ner-ontonotes_loss: 0.0335
09/16 01:08:24 PM: Update 22516: task edges-ner-ontonotes, batch 516 (22516): mcc: 0.8825, acc: 0.8431, precision: 0.9112, recall: 0.8670, f1: 0.8886, edges-ner-ontonotes_loss: 0.0339
09/16 01:08:35 PM: Update 22648: task edges-ner-ontonotes, batch 648 (22648): mcc: 0.8819, acc: 0.8419, precision: 0.9099, recall: 0.8673, f1: 0.8881, edges-ner-ontonotes_loss: 0.0337
09/16 01:08:45 PM: Update 22753: task edges-ner-ontonotes, batch 753 (22753): mcc: 0.8814, acc: 0.8406, precision: 0.9090, recall: 0.8672, f1: 0.8876, edges-ner-ontonotes_loss: 0.0335
09/16 01:08:55 PM: Update 22882: task edges-ner-ontonotes, batch 882 (22882): mcc: 0.8808, acc: 0.8394, precision: 0.9085, recall: 0.8665, f1: 0.8870, edges-ner-ontonotes_loss: 0.0335
09/16 01:09:05 PM: Update 22995: task edges-ner-ontonotes, batch 995 (22995): mcc: 0.8809, acc: 0.8393, precision: 0.9083, recall: 0.8668, f1: 0.8871, edges-ner-ontonotes_loss: 0.0334
09/16 01:09:05 PM: ***** Step 23000 / Validation 23 *****
09/16 01:09:05 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:09:05 PM: Validating...
09/16 01:09:14 PM: Best result seen so far for edges-ner-ontonotes.
09/16 01:09:14 PM: Best result seen so far for macro.
09/16 01:09:14 PM: Updating LR scheduler:
09/16 01:09:14 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:09:14 PM: 	# validation passes without improvement: 0
09/16 01:09:14 PM: edges-ner-ontonotes_loss: training: 0.033398 validation: 0.042754
09/16 01:09:14 PM: macro_avg: validation: 0.871668
09/16 01:09:14 PM: micro_avg: validation: 0.000000
09/16 01:09:14 PM: edges-ner-ontonotes_mcc: training: 0.880956 validation: 0.864881
09/16 01:09:14 PM: edges-ner-ontonotes_acc: training: 0.839487 validation: 0.826357
09/16 01:09:14 PM: edges-ner-ontonotes_precision: training: 0.908306 validation: 0.900752
09/16 01:09:14 PM: edges-ner-ontonotes_recall: training: 0.866982 validation: 0.844404
09/16 01:09:14 PM: edges-ner-ontonotes_f1: training: 0.887163 validation: 0.871668
09/16 01:09:14 PM: Global learning rate: 5e-05
09/16 01:09:14 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:09:15 PM: Update 23008: task edges-ner-ontonotes, batch 8 (23008): mcc: 0.8701, acc: 0.8255, precision: 0.8970, recall: 0.8578, f1: 0.8770, edges-ner-ontonotes_loss: 0.0379
09/16 01:09:25 PM: Update 23059: task edges-ner-ontonotes, batch 59 (23059): mcc: 0.8682, acc: 0.8211, precision: 0.9015, recall: 0.8498, f1: 0.8749, edges-ner-ontonotes_loss: 0.0387
09/16 01:09:35 PM: Update 23228: task edges-ner-ontonotes, batch 228 (23228): mcc: 0.8610, acc: 0.8152, precision: 0.8974, recall: 0.8404, f1: 0.8680, edges-ner-ontonotes_loss: 0.0426
09/16 01:09:49 PM: Update 23341: task edges-ner-ontonotes, batch 341 (23341): mcc: 0.8622, acc: 0.8172, precision: 0.8980, recall: 0.8421, f1: 0.8692, edges-ner-ontonotes_loss: 0.0424
09/16 01:09:59 PM: Update 23496: task edges-ner-ontonotes, batch 496 (23496): mcc: 0.8730, acc: 0.8318, precision: 0.9054, recall: 0.8550, f1: 0.8794, edges-ner-ontonotes_loss: 0.0388
09/16 01:10:09 PM: Update 23628: task edges-ner-ontonotes, batch 628 (23628): mcc: 0.8771, acc: 0.8376, precision: 0.9083, recall: 0.8599, f1: 0.8834, edges-ner-ontonotes_loss: 0.0373
09/16 01:10:19 PM: Update 23684: task edges-ner-ontonotes, batch 684 (23684): mcc: 0.8779, acc: 0.8385, precision: 0.9091, recall: 0.8604, f1: 0.8841, edges-ner-ontonotes_loss: 0.0369
09/16 01:10:29 PM: Update 23796: task edges-ner-ontonotes, batch 796 (23796): mcc: 0.8782, acc: 0.8388, precision: 0.9094, recall: 0.8609, f1: 0.8844, edges-ner-ontonotes_loss: 0.0367
09/16 01:10:39 PM: Update 23925: task edges-ner-ontonotes, batch 925 (23925): mcc: 0.8792, acc: 0.8403, precision: 0.9100, recall: 0.8620, f1: 0.8854, edges-ner-ontonotes_loss: 0.0363
09/16 01:10:49 PM: Update 23971: task edges-ner-ontonotes, batch 971 (23971): mcc: 0.8791, acc: 0.8402, precision: 0.9100, recall: 0.8619, f1: 0.8853, edges-ner-ontonotes_loss: 0.0361
09/16 01:10:51 PM: ***** Step 24000 / Validation 24 *****
09/16 01:10:51 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:10:51 PM: Validating...
09/16 01:10:59 PM: Updating LR scheduler:
09/16 01:10:59 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:10:59 PM: 	# validation passes without improvement: 1
09/16 01:10:59 PM: edges-ner-ontonotes_loss: training: 0.036082 validation: 0.042301
09/16 01:10:59 PM: macro_avg: validation: 0.867793
09/16 01:10:59 PM: micro_avg: validation: 0.000000
09/16 01:10:59 PM: edges-ner-ontonotes_mcc: training: 0.878927 validation: 0.860679
09/16 01:10:59 PM: edges-ner-ontonotes_acc: training: 0.839741 validation: 0.824613
09/16 01:10:59 PM: edges-ner-ontonotes_precision: training: 0.909714 validation: 0.894216
09/16 01:10:59 PM: edges-ner-ontonotes_recall: training: 0.861843 validation: 0.842887
09/16 01:10:59 PM: edges-ner-ontonotes_f1: training: 0.885132 validation: 0.867793
09/16 01:10:59 PM: Global learning rate: 5e-05
09/16 01:10:59 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:10:59 PM: Update 24001: task edges-ner-ontonotes, batch 1 (24001): mcc: 0.8648, acc: 0.8167, precision: 0.8947, recall: 0.8500, f1: 0.8718, edges-ner-ontonotes_loss: 0.0359
09/16 01:11:10 PM: Update 24081: task edges-ner-ontonotes, batch 81 (24081): mcc: 0.8735, acc: 0.8273, precision: 0.8995, recall: 0.8616, f1: 0.8802, edges-ner-ontonotes_loss: 0.0345
09/16 01:11:20 PM: Update 24204: task edges-ner-ontonotes, batch 204 (24204): mcc: 0.8770, acc: 0.8313, precision: 0.9026, recall: 0.8651, f1: 0.8835, edges-ner-ontonotes_loss: 0.0338
09/16 01:11:31 PM: Update 24302: task edges-ner-ontonotes, batch 302 (24302): mcc: 0.8770, acc: 0.8325, precision: 0.9023, recall: 0.8654, f1: 0.8835, edges-ner-ontonotes_loss: 0.0333
09/16 01:11:41 PM: Update 24474: task edges-ner-ontonotes, batch 474 (24474): mcc: 0.8780, acc: 0.8330, precision: 0.9046, recall: 0.8651, f1: 0.8844, edges-ner-ontonotes_loss: 0.0333
09/16 01:11:51 PM: Update 24629: task edges-ner-ontonotes, batch 629 (24629): mcc: 0.8777, acc: 0.8332, precision: 0.9051, recall: 0.8639, f1: 0.8841, edges-ner-ontonotes_loss: 0.0337
09/16 01:12:01 PM: Update 24766: task edges-ner-ontonotes, batch 766 (24766): mcc: 0.8743, acc: 0.8293, precision: 0.9037, recall: 0.8591, f1: 0.8809, edges-ner-ontonotes_loss: 0.0357
09/16 01:12:11 PM: Update 24890: task edges-ner-ontonotes, batch 890 (24890): mcc: 0.8729, acc: 0.8280, precision: 0.9035, recall: 0.8567, f1: 0.8795, edges-ner-ontonotes_loss: 0.0366
09/16 01:12:21 PM: Update 24937: task edges-ner-ontonotes, batch 937 (24937): mcc: 0.8738, acc: 0.8293, precision: 0.9041, recall: 0.8577, f1: 0.8803, edges-ner-ontonotes_loss: 0.0365
09/16 01:12:26 PM: ***** Step 25000 / Validation 25 *****
09/16 01:12:26 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:12:26 PM: Validating...
09/16 01:12:31 PM: Evaluate: task edges-ner-ontonotes, batch 80 (157): mcc: 0.8710, acc: 0.8343, precision: 0.9125, recall: 0.8446, f1: 0.8772, edges-ner-ontonotes_loss: 0.0430
09/16 01:12:36 PM: Updating LR scheduler:
09/16 01:12:36 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:12:36 PM: 	# validation passes without improvement: 2
09/16 01:12:36 PM: edges-ner-ontonotes_loss: training: 0.036235 validation: 0.043251
09/16 01:12:36 PM: macro_avg: validation: 0.863201
09/16 01:12:36 PM: micro_avg: validation: 0.000000
09/16 01:12:36 PM: edges-ner-ontonotes_mcc: training: 0.874829 validation: 0.856629
09/16 01:12:36 PM: edges-ner-ontonotes_acc: training: 0.830920 validation: 0.815893
09/16 01:12:36 PM: edges-ner-ontonotes_precision: training: 0.904764 validation: 0.906128
09/16 01:12:36 PM: edges-ner-ontonotes_recall: training: 0.859005 validation: 0.824158
09/16 01:12:36 PM: edges-ner-ontonotes_f1: training: 0.881291 validation: 0.863201
09/16 01:12:36 PM: Global learning rate: 5e-05
09/16 01:12:36 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:12:41 PM: Update 25055: task edges-ner-ontonotes, batch 55 (25055): mcc: 0.8978, acc: 0.8644, precision: 0.9253, recall: 0.8819, f1: 0.9031, edges-ner-ontonotes_loss: 0.0328
09/16 01:12:53 PM: Update 25210: task edges-ner-ontonotes, batch 210 (25210): mcc: 0.9009, acc: 0.8681, precision: 0.9262, recall: 0.8867, f1: 0.9060, edges-ner-ontonotes_loss: 0.0308
09/16 01:13:04 PM: Update 25316: task edges-ner-ontonotes, batch 316 (25316): mcc: 0.8897, acc: 0.8543, precision: 0.9183, recall: 0.8736, f1: 0.8954, edges-ner-ontonotes_loss: 0.0327
09/16 01:13:14 PM: Update 25444: task edges-ner-ontonotes, batch 444 (25444): mcc: 0.8897, acc: 0.8537, precision: 0.9182, recall: 0.8737, f1: 0.8954, edges-ner-ontonotes_loss: 0.0326
09/16 01:13:24 PM: Update 25529: task edges-ner-ontonotes, batch 529 (25529): mcc: 0.8878, acc: 0.8511, precision: 0.9168, recall: 0.8715, f1: 0.8936, edges-ner-ontonotes_loss: 0.0329
09/16 01:13:34 PM: Update 25630: task edges-ner-ontonotes, batch 630 (25630): mcc: 0.8849, acc: 0.8468, precision: 0.9134, recall: 0.8694, f1: 0.8908, edges-ner-ontonotes_loss: 0.0333
09/16 01:13:44 PM: Update 25767: task edges-ner-ontonotes, batch 767 (25767): mcc: 0.8838, acc: 0.8445, precision: 0.9120, recall: 0.8688, f1: 0.8899, edges-ner-ontonotes_loss: 0.0332
09/16 01:13:54 PM: Update 25852: task edges-ner-ontonotes, batch 852 (25852): mcc: 0.8833, acc: 0.8432, precision: 0.9111, recall: 0.8685, f1: 0.8893, edges-ner-ontonotes_loss: 0.0331
09/16 01:14:04 PM: Update 25996: task edges-ner-ontonotes, batch 996 (25996): mcc: 0.8825, acc: 0.8417, precision: 0.9103, recall: 0.8678, f1: 0.8886, edges-ner-ontonotes_loss: 0.0331
09/16 01:14:05 PM: ***** Step 26000 / Validation 26 *****
09/16 01:14:06 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:14:06 PM: Validating...
09/16 01:14:14 PM: Evaluate: task edges-ner-ontonotes, batch 130 (157): mcc: 0.8582, acc: 0.8200, precision: 0.8971, recall: 0.8354, f1: 0.8652, edges-ner-ontonotes_loss: 0.0451
09/16 01:14:16 PM: Updating LR scheduler:
09/16 01:14:16 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:14:16 PM: 	# validation passes without improvement: 3
09/16 01:14:16 PM: edges-ner-ontonotes_loss: training: 0.033146 validation: 0.042769
09/16 01:14:16 PM: macro_avg: validation: 0.870575
09/16 01:14:16 PM: micro_avg: validation: 0.000000
09/16 01:14:16 PM: edges-ner-ontonotes_mcc: training: 0.882476 validation: 0.863838
09/16 01:14:16 PM: edges-ner-ontonotes_acc: training: 0.841754 validation: 0.826509
09/16 01:14:16 PM: edges-ner-ontonotes_precision: training: 0.910352 validation: 0.902229
09/16 01:14:16 PM: edges-ner-ontonotes_recall: training: 0.867830 validation: 0.841068
09/16 01:14:16 PM: edges-ner-ontonotes_f1: training: 0.888583 validation: 0.870575
09/16 01:14:16 PM: Global learning rate: 5e-05
09/16 01:14:16 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:14:24 PM: Update 26086: task edges-ner-ontonotes, batch 86 (26086): mcc: 0.8792, acc: 0.8366, precision: 0.9042, recall: 0.8677, f1: 0.8856, edges-ner-ontonotes_loss: 0.0335
09/16 01:14:35 PM: Update 26149: task edges-ner-ontonotes, batch 149 (26149): mcc: 0.8812, acc: 0.8385, precision: 0.9069, recall: 0.8689, f1: 0.8875, edges-ner-ontonotes_loss: 0.0327
09/16 01:14:45 PM: Update 26261: task edges-ner-ontonotes, batch 261 (26261): mcc: 0.8704, acc: 0.8264, precision: 0.9006, recall: 0.8548, f1: 0.8771, edges-ner-ontonotes_loss: 0.0379
09/16 01:14:55 PM: Update 26368: task edges-ner-ontonotes, batch 368 (26368): mcc: 0.8679, acc: 0.8233, precision: 0.8998, recall: 0.8510, f1: 0.8747, edges-ner-ontonotes_loss: 0.0393
09/16 01:15:06 PM: Update 26453: task edges-ner-ontonotes, batch 453 (26453): mcc: 0.8671, acc: 0.8217, precision: 0.8999, recall: 0.8493, f1: 0.8739, edges-ner-ontonotes_loss: 0.0400
09/16 01:15:16 PM: Update 26583: task edges-ner-ontonotes, batch 583 (26583): mcc: 0.8721, acc: 0.8289, precision: 0.9036, recall: 0.8551, f1: 0.8787, edges-ner-ontonotes_loss: 0.0383
09/16 01:15:26 PM: Update 26706: task edges-ner-ontonotes, batch 706 (26706): mcc: 0.8763, acc: 0.8347, precision: 0.9064, recall: 0.8601, f1: 0.8827, edges-ner-ontonotes_loss: 0.0369
09/16 01:15:36 PM: Update 26795: task edges-ner-ontonotes, batch 795 (26795): mcc: 0.8782, acc: 0.8372, precision: 0.9081, recall: 0.8620, f1: 0.8844, edges-ner-ontonotes_loss: 0.0363
09/16 01:15:46 PM: Update 26921: task edges-ner-ontonotes, batch 921 (26921): mcc: 0.8785, acc: 0.8377, precision: 0.9086, recall: 0.8621, f1: 0.8847, edges-ner-ontonotes_loss: 0.0362
09/16 01:15:53 PM: ***** Step 27000 / Validation 27 *****
09/16 01:15:53 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:15:53 PM: Validating...
09/16 01:15:57 PM: Evaluate: task edges-ner-ontonotes, batch 46 (157): mcc: 0.8619, acc: 0.8340, precision: 0.8912, recall: 0.8481, f1: 0.8691, edges-ner-ontonotes_loss: 0.0435
09/16 01:16:04 PM: Updating LR scheduler:
09/16 01:16:04 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:16:04 PM: 	# validation passes without improvement: 0
09/16 01:16:04 PM: edges-ner-ontonotes_loss: training: 0.035829 validation: 0.042594
09/16 01:16:04 PM: macro_avg: validation: 0.868310
09/16 01:16:04 PM: micro_avg: validation: 0.000000
09/16 01:16:04 PM: edges-ner-ontonotes_mcc: training: 0.879769 validation: 0.861532
09/16 01:16:04 PM: edges-ner-ontonotes_acc: training: 0.839472 validation: 0.825144
09/16 01:16:04 PM: edges-ner-ontonotes_precision: training: 0.909788 validation: 0.901821
09/16 01:16:04 PM: edges-ner-ontonotes_recall: training: 0.863338 validation: 0.837200
09/16 01:16:04 PM: edges-ner-ontonotes_f1: training: 0.885954 validation: 0.868310
09/16 01:16:04 PM: Global learning rate: 2.5e-05
09/16 01:16:04 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:16:07 PM: Update 27035: task edges-ner-ontonotes, batch 35 (27035): mcc: 0.8813, acc: 0.8413, precision: 0.9133, recall: 0.8628, f1: 0.8873, edges-ner-ontonotes_loss: 0.0355
09/16 01:16:17 PM: Update 27106: task edges-ner-ontonotes, batch 106 (27106): mcc: 0.8800, acc: 0.8413, precision: 0.9097, recall: 0.8639, f1: 0.8862, edges-ner-ontonotes_loss: 0.0344
09/16 01:16:27 PM: Update 27258: task edges-ner-ontonotes, batch 258 (27258): mcc: 0.8782, acc: 0.8365, precision: 0.9059, recall: 0.8642, f1: 0.8845, edges-ner-ontonotes_loss: 0.0342
09/16 01:16:37 PM: Update 27392: task edges-ner-ontonotes, batch 392 (27392): mcc: 0.8779, acc: 0.8348, precision: 0.9050, recall: 0.8646, f1: 0.8843, edges-ner-ontonotes_loss: 0.0340
09/16 01:16:47 PM: Update 27560: task edges-ner-ontonotes, batch 560 (27560): mcc: 0.8792, acc: 0.8360, precision: 0.9054, recall: 0.8666, f1: 0.8856, edges-ner-ontonotes_loss: 0.0336
09/16 01:17:02 PM: Update 27705: task edges-ner-ontonotes, batch 705 (27705): mcc: 0.8784, acc: 0.8344, precision: 0.9050, recall: 0.8654, f1: 0.8848, edges-ner-ontonotes_loss: 0.0337
09/16 01:17:12 PM: Update 27826: task edges-ner-ontonotes, batch 826 (27826): mcc: 0.8759, acc: 0.8315, precision: 0.9041, recall: 0.8617, f1: 0.8824, edges-ner-ontonotes_loss: 0.0351
09/16 01:17:22 PM: Update 27937: task edges-ner-ontonotes, batch 937 (27937): mcc: 0.8743, acc: 0.8296, precision: 0.9033, recall: 0.8595, f1: 0.8809, edges-ner-ontonotes_loss: 0.0361
09/16 01:17:25 PM: ***** Step 28000 / Validation 28 *****
09/16 01:17:28 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:17:28 PM: Validating...
09/16 01:17:32 PM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.8759, acc: 0.8465, precision: 0.9072, recall: 0.8587, f1: 0.8823, edges-ner-ontonotes_loss: 0.0421
09/16 01:17:36 PM: Updating LR scheduler:
09/16 01:17:36 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:17:36 PM: 	# validation passes without improvement: 1
09/16 01:17:36 PM: edges-ner-ontonotes_loss: training: 0.036600 validation: 0.042755
09/16 01:17:36 PM: macro_avg: validation: 0.865303
09/16 01:17:36 PM: micro_avg: validation: 0.000000
09/16 01:17:36 PM: edges-ner-ontonotes_mcc: training: 0.873693 validation: 0.858645
09/16 01:17:36 PM: edges-ner-ontonotes_acc: training: 0.829060 validation: 0.819988
09/16 01:17:36 PM: edges-ner-ontonotes_precision: training: 0.902991 validation: 0.904557
09/16 01:17:36 PM: edges-ner-ontonotes_recall: training: 0.858608 validation: 0.829315
09/16 01:17:36 PM: edges-ner-ontonotes_f1: training: 0.880240 validation: 0.865303
09/16 01:17:36 PM: Global learning rate: 2.5e-05
09/16 01:17:36 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:17:42 PM: Update 28017: task edges-ner-ontonotes, batch 17 (28017): mcc: 0.8660, acc: 0.8195, precision: 0.9017, recall: 0.8455, f1: 0.8727, edges-ner-ontonotes_loss: 0.0399
09/16 01:17:52 PM: Update 28154: task edges-ner-ontonotes, batch 154 (28154): mcc: 0.8929, acc: 0.8595, precision: 0.9178, recall: 0.8800, f1: 0.8985, edges-ner-ontonotes_loss: 0.0325
09/16 01:18:09 PM: Update 28322: task edges-ner-ontonotes, batch 322 (28322): mcc: 0.8978, acc: 0.8650, precision: 0.9227, recall: 0.8844, f1: 0.9031, edges-ner-ontonotes_loss: 0.0312
09/16 01:18:19 PM: Update 28436: task edges-ner-ontonotes, batch 436 (28436): mcc: 0.8921, acc: 0.8574, precision: 0.9192, recall: 0.8772, f1: 0.8977, edges-ner-ontonotes_loss: 0.0322
09/16 01:18:29 PM: Update 28546: task edges-ner-ontonotes, batch 546 (28546): mcc: 0.8908, acc: 0.8558, precision: 0.9181, recall: 0.8759, f1: 0.8965, edges-ner-ontonotes_loss: 0.0326
09/16 01:18:46 PM: Update 28635: task edges-ner-ontonotes, batch 635 (28635): mcc: 0.8899, acc: 0.8543, precision: 0.9174, recall: 0.8748, f1: 0.8956, edges-ner-ontonotes_loss: 0.0326
09/16 01:18:56 PM: Update 28740: task edges-ner-ontonotes, batch 740 (28740): mcc: 0.8871, acc: 0.8503, precision: 0.9149, recall: 0.8720, f1: 0.8929, edges-ner-ontonotes_loss: 0.0329
09/16 01:19:06 PM: Update 28874: task edges-ner-ontonotes, batch 874 (28874): mcc: 0.8863, acc: 0.8481, precision: 0.9133, recall: 0.8720, f1: 0.8922, edges-ner-ontonotes_loss: 0.0329
09/16 01:19:18 PM: Update 28948: task edges-ner-ontonotes, batch 948 (28948): mcc: 0.8858, acc: 0.8470, precision: 0.9131, recall: 0.8714, f1: 0.8918, edges-ner-ontonotes_loss: 0.0329
09/16 01:19:22 PM: ***** Step 29000 / Validation 29 *****
09/16 01:19:22 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:19:22 PM: Validating...
09/16 01:19:28 PM: Evaluate: task edges-ner-ontonotes, batch 86 (157): mcc: 0.8644, acc: 0.8282, precision: 0.9007, recall: 0.8436, f1: 0.8712, edges-ner-ontonotes_loss: 0.0451
09/16 01:19:35 PM: Updating LR scheduler:
09/16 01:19:35 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:19:35 PM: 	# validation passes without improvement: 2
09/16 01:19:35 PM: edges-ner-ontonotes_loss: training: 0.032988 validation: 0.042712
09/16 01:19:35 PM: macro_avg: validation: 0.871120
09/16 01:19:35 PM: micro_avg: validation: 0.000000
09/16 01:19:35 PM: edges-ner-ontonotes_mcc: training: 0.885235 validation: 0.864301
09/16 01:19:35 PM: edges-ner-ontonotes_acc: training: 0.846032 validation: 0.825599
09/16 01:19:35 PM: edges-ner-ontonotes_precision: training: 0.912787 validation: 0.900186
09/16 01:19:35 PM: edges-ner-ontonotes_recall: training: 0.870607 validation: 0.843873
09/16 01:19:35 PM: edges-ner-ontonotes_f1: training: 0.891198 validation: 0.871120
09/16 01:19:35 PM: Global learning rate: 2.5e-05
09/16 01:19:35 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:19:38 PM: Update 29031: task edges-ner-ontonotes, batch 31 (29031): mcc: 0.8724, acc: 0.8270, precision: 0.9027, recall: 0.8565, f1: 0.8790, edges-ner-ontonotes_loss: 0.0359
09/16 01:19:48 PM: Update 29175: task edges-ner-ontonotes, batch 175 (29175): mcc: 0.8761, acc: 0.8310, precision: 0.9042, recall: 0.8619, f1: 0.8825, edges-ner-ontonotes_loss: 0.0339
09/16 01:20:01 PM: Update 29261: task edges-ner-ontonotes, batch 261 (29261): mcc: 0.8780, acc: 0.8330, precision: 0.9061, recall: 0.8636, f1: 0.8843, edges-ner-ontonotes_loss: 0.0334
09/16 01:20:11 PM: Update 29358: task edges-ner-ontonotes, batch 358 (29358): mcc: 0.8723, acc: 0.8275, precision: 0.9023, recall: 0.8567, f1: 0.8789, edges-ner-ontonotes_loss: 0.0361
09/16 01:20:21 PM: Update 29512: task edges-ner-ontonotes, batch 512 (29512): mcc: 0.8695, acc: 0.8251, precision: 0.9013, recall: 0.8525, f1: 0.8762, edges-ner-ontonotes_loss: 0.0382
09/16 01:20:32 PM: Update 29565: task edges-ner-ontonotes, batch 565 (29565): mcc: 0.8688, acc: 0.8244, precision: 0.9010, recall: 0.8514, f1: 0.8755, edges-ner-ontonotes_loss: 0.0386
09/16 01:20:42 PM: Update 29699: task edges-ner-ontonotes, batch 699 (29699): mcc: 0.8735, acc: 0.8312, precision: 0.9043, recall: 0.8570, f1: 0.8800, edges-ner-ontonotes_loss: 0.0373
09/16 01:20:52 PM: Update 29856: task edges-ner-ontonotes, batch 856 (29856): mcc: 0.8782, acc: 0.8376, precision: 0.9082, recall: 0.8620, f1: 0.8845, edges-ner-ontonotes_loss: 0.0359
09/16 01:21:02 PM: Update 29921: task edges-ner-ontonotes, batch 921 (29921): mcc: 0.8778, acc: 0.8371, precision: 0.9081, recall: 0.8613, f1: 0.8841, edges-ner-ontonotes_loss: 0.0359
09/16 01:21:09 PM: ***** Step 30000 / Validation 30 *****
09/16 01:21:09 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:21:09 PM: Validating...
09/16 01:21:12 PM: Evaluate: task edges-ner-ontonotes, batch 30 (157): mcc: 0.8463, acc: 0.8142, precision: 0.8815, recall: 0.8285, f1: 0.8542, edges-ner-ontonotes_loss: 0.0500
09/16 01:21:21 PM: Updating LR scheduler:
09/16 01:21:21 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:21:21 PM: 	# validation passes without improvement: 3
09/16 01:21:21 PM: edges-ner-ontonotes_loss: training: 0.035598 validation: 0.042136
09/16 01:21:21 PM: macro_avg: validation: 0.869541
09/16 01:21:21 PM: micro_avg: validation: 0.000000
09/16 01:21:21 PM: edges-ner-ontonotes_mcc: training: 0.879192 validation: 0.862857
09/16 01:21:21 PM: edges-ner-ontonotes_acc: training: 0.838854 validation: 0.826281
09/16 01:21:21 PM: edges-ner-ontonotes_precision: training: 0.909541 validation: 0.903598
09/16 01:21:21 PM: edges-ner-ontonotes_recall: training: 0.862504 validation: 0.837959
09/16 01:21:21 PM: edges-ner-ontonotes_f1: training: 0.885398 validation: 0.869541
09/16 01:21:21 PM: Global learning rate: 2.5e-05
09/16 01:21:21 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:21:22 PM: Update 30018: task edges-ner-ontonotes, batch 18 (30018): mcc: 0.8857, acc: 0.8527, precision: 0.9152, recall: 0.8692, f1: 0.8916, edges-ner-ontonotes_loss: 0.0351
09/16 01:21:32 PM: Update 30166: task edges-ner-ontonotes, batch 166 (30166): mcc: 0.8845, acc: 0.8473, precision: 0.9127, recall: 0.8694, f1: 0.8905, edges-ner-ontonotes_loss: 0.0332
09/16 01:21:42 PM: Update 30279: task edges-ner-ontonotes, batch 279 (30279): mcc: 0.8807, acc: 0.8401, precision: 0.9102, recall: 0.8646, f1: 0.8868, edges-ner-ontonotes_loss: 0.0339
09/16 01:21:52 PM: Update 30417: task edges-ner-ontonotes, batch 417 (30417): mcc: 0.8810, acc: 0.8396, precision: 0.9092, recall: 0.8661, f1: 0.8872, edges-ner-ontonotes_loss: 0.0334
09/16 01:22:02 PM: Update 30551: task edges-ner-ontonotes, batch 551 (30551): mcc: 0.8809, acc: 0.8385, precision: 0.9087, recall: 0.8666, f1: 0.8871, edges-ner-ontonotes_loss: 0.0333
09/16 01:22:12 PM: Update 30709: task edges-ner-ontonotes, batch 709 (30709): mcc: 0.8802, acc: 0.8367, precision: 0.9083, recall: 0.8656, f1: 0.8864, edges-ner-ontonotes_loss: 0.0333
09/16 01:22:28 PM: Update 30817: task edges-ner-ontonotes, batch 817 (30817): mcc: 0.8800, acc: 0.8364, precision: 0.9077, recall: 0.8657, f1: 0.8862, edges-ner-ontonotes_loss: 0.0332
09/16 01:22:38 PM: Update 30922: task edges-ner-ontonotes, batch 922 (30922): mcc: 0.8776, acc: 0.8338, precision: 0.9065, recall: 0.8625, f1: 0.8840, edges-ner-ontonotes_loss: 0.0345
09/16 01:22:44 PM: ***** Step 31000 / Validation 31 *****
09/16 01:22:44 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:22:44 PM: Validating...
09/16 01:22:48 PM: Evaluate: task edges-ner-ontonotes, batch 44 (157): mcc: 0.8686, acc: 0.8381, precision: 0.9015, recall: 0.8506, f1: 0.8753, edges-ner-ontonotes_loss: 0.0438
09/16 01:22:58 PM: Evaluate: task edges-ner-ontonotes, batch 155 (157): mcc: 0.8591, acc: 0.8200, precision: 0.9057, recall: 0.8290, f1: 0.8657, edges-ner-ontonotes_loss: 0.0428
09/16 01:22:58 PM: Updating LR scheduler:
09/16 01:22:58 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:22:58 PM: 	# validation passes without improvement: 0
09/16 01:22:58 PM: edges-ner-ontonotes_loss: training: 0.034949 validation: 0.042534
09/16 01:22:58 PM: macro_avg: validation: 0.866012
09/16 01:22:58 PM: micro_avg: validation: 0.000000
09/16 01:22:58 PM: edges-ner-ontonotes_mcc: training: 0.876676 validation: 0.859443
09/16 01:22:58 PM: edges-ner-ontonotes_acc: training: 0.832793 validation: 0.820291
09/16 01:22:58 PM: edges-ner-ontonotes_precision: training: 0.906265 validation: 0.906198
09/16 01:22:58 PM: edges-ner-ontonotes_recall: training: 0.860990 validation: 0.829239
09/16 01:22:58 PM: edges-ner-ontonotes_f1: training: 0.883048 validation: 0.866012
09/16 01:22:58 PM: Global learning rate: 1.25e-05
09/16 01:22:58 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:23:12 PM: Update 31121: task edges-ner-ontonotes, batch 121 (31121): mcc: 0.8597, acc: 0.8146, precision: 0.8967, recall: 0.8388, f1: 0.8668, edges-ner-ontonotes_loss: 0.0442
09/16 01:23:22 PM: Update 31262: task edges-ner-ontonotes, batch 262 (31262): mcc: 0.8794, acc: 0.8399, precision: 0.9104, recall: 0.8620, f1: 0.8855, edges-ner-ontonotes_loss: 0.0376
09/16 01:23:32 PM: Update 31412: task edges-ner-ontonotes, batch 412 (31412): mcc: 0.8879, acc: 0.8514, precision: 0.9163, recall: 0.8721, f1: 0.8937, edges-ner-ontonotes_loss: 0.0348
09/16 01:23:42 PM: Update 31485: task edges-ner-ontonotes, batch 485 (31485): mcc: 0.8872, acc: 0.8507, precision: 0.9163, recall: 0.8709, f1: 0.8930, edges-ner-ontonotes_loss: 0.0346
09/16 01:23:52 PM: Update 31621: task edges-ner-ontonotes, batch 621 (31621): mcc: 0.8863, acc: 0.8489, precision: 0.9163, recall: 0.8692, f1: 0.8921, edges-ner-ontonotes_loss: 0.0343
09/16 01:24:05 PM: Update 31747: task edges-ner-ontonotes, batch 747 (31747): mcc: 0.8861, acc: 0.8487, precision: 0.9158, recall: 0.8694, f1: 0.8920, edges-ner-ontonotes_loss: 0.0343
09/16 01:24:15 PM: Update 31859: task edges-ner-ontonotes, batch 859 (31859): mcc: 0.8839, acc: 0.8455, precision: 0.9138, recall: 0.8671, f1: 0.8899, edges-ner-ontonotes_loss: 0.0344
09/16 01:24:25 PM: Update 31976: task edges-ner-ontonotes, batch 976 (31976): mcc: 0.8837, acc: 0.8449, precision: 0.9131, recall: 0.8674, f1: 0.8896, edges-ner-ontonotes_loss: 0.0341
09/16 01:24:26 PM: ***** Step 32000 / Validation 32 *****
09/16 01:24:28 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:24:28 PM: Validating...
09/16 01:24:35 PM: Evaluate: task edges-ner-ontonotes, batch 122 (157): mcc: 0.8554, acc: 0.8143, precision: 0.8890, recall: 0.8382, f1: 0.8628, edges-ner-ontonotes_loss: 0.0451
09/16 01:24:38 PM: Updating LR scheduler:
09/16 01:24:38 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:24:38 PM: 	# validation passes without improvement: 1
09/16 01:24:38 PM: edges-ner-ontonotes_loss: training: 0.034160 validation: 0.042286
09/16 01:24:38 PM: macro_avg: validation: 0.871215
09/16 01:24:38 PM: micro_avg: validation: 0.000000
09/16 01:24:38 PM: edges-ner-ontonotes_mcc: training: 0.883343 validation: 0.864290
09/16 01:24:38 PM: edges-ner-ontonotes_acc: training: 0.844584 validation: 0.824537
09/16 01:24:38 PM: edges-ner-ontonotes_precision: training: 0.912466 validation: 0.897555
09/16 01:24:38 PM: edges-ner-ontonotes_recall: training: 0.867395 validation: 0.846375
09/16 01:24:38 PM: edges-ner-ontonotes_f1: training: 0.889360 validation: 0.871215
09/16 01:24:38 PM: Global learning rate: 1.25e-05
09/16 01:24:38 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:24:45 PM: Update 32077: task edges-ner-ontonotes, batch 77 (32077): mcc: 0.8830, acc: 0.8416, precision: 0.9088, recall: 0.8704, f1: 0.8892, edges-ner-ontonotes_loss: 0.0324
09/16 01:24:55 PM: Update 32260: task edges-ner-ontonotes, batch 260 (32260): mcc: 0.8806, acc: 0.8368, precision: 0.9072, recall: 0.8674, f1: 0.8869, edges-ner-ontonotes_loss: 0.0328
09/16 01:25:05 PM: Update 32365: task edges-ner-ontonotes, batch 365 (32365): mcc: 0.8803, acc: 0.8367, precision: 0.9071, recall: 0.8670, f1: 0.8866, edges-ner-ontonotes_loss: 0.0328
09/16 01:25:15 PM: Update 32484: task edges-ner-ontonotes, batch 484 (32484): mcc: 0.8741, acc: 0.8294, precision: 0.9030, recall: 0.8594, f1: 0.8807, edges-ner-ontonotes_loss: 0.0355
09/16 01:25:25 PM: Update 32613: task edges-ner-ontonotes, batch 613 (32613): mcc: 0.8720, acc: 0.8270, precision: 0.9022, recall: 0.8562, f1: 0.8786, edges-ner-ontonotes_loss: 0.0372
09/16 01:25:36 PM: Update 32677: task edges-ner-ontonotes, batch 677 (32677): mcc: 0.8717, acc: 0.8268, precision: 0.9024, recall: 0.8555, f1: 0.8783, edges-ner-ontonotes_loss: 0.0376
09/16 01:25:46 PM: Update 32818: task edges-ner-ontonotes, batch 818 (32818): mcc: 0.8756, acc: 0.8327, precision: 0.9052, recall: 0.8601, f1: 0.8820, edges-ner-ontonotes_loss: 0.0363
09/16 01:25:56 PM: Update 32934: task edges-ner-ontonotes, batch 934 (32934): mcc: 0.8776, acc: 0.8358, precision: 0.9070, recall: 0.8619, f1: 0.8839, edges-ner-ontonotes_loss: 0.0359
09/16 01:26:07 PM: Update 32990: task edges-ner-ontonotes, batch 990 (32990): mcc: 0.8787, acc: 0.8373, precision: 0.9080, recall: 0.8631, f1: 0.8850, edges-ner-ontonotes_loss: 0.0356
09/16 01:26:08 PM: ***** Step 33000 / Validation 33 *****
09/16 01:26:08 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:26:08 PM: Validating...
09/16 01:26:18 PM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.8585, acc: 0.8201, precision: 0.9050, recall: 0.8287, f1: 0.8652, edges-ner-ontonotes_loss: 0.0431
09/16 01:26:18 PM: Updating LR scheduler:
09/16 01:26:18 PM: 	Best result seen so far for macro_avg: 0.872
09/16 01:26:18 PM: 	# validation passes without improvement: 2
09/16 01:26:18 PM: Ran out of early stopping patience. Stopping training.
09/16 01:26:18 PM: edges-ner-ontonotes_loss: training: 0.035579 validation: 0.042412
09/16 01:26:18 PM: macro_avg: validation: 0.866772
09/16 01:26:18 PM: micro_avg: validation: 0.000000
09/16 01:26:18 PM: edges-ner-ontonotes_mcc: training: 0.878678 validation: 0.860259
09/16 01:26:18 PM: edges-ner-ontonotes_acc: training: 0.837357 validation: 0.821429
09/16 01:26:18 PM: edges-ner-ontonotes_precision: training: 0.908022 validation: 0.907229
09/16 01:26:19 PM: edges-ner-ontonotes_recall: training: 0.863015 validation: 0.829769
09/16 01:26:19 PM: edges-ner-ontonotes_f1: training: 0.884947 validation: 0.866772
09/16 01:26:19 PM: Global learning rate: 1.25e-05
09/16 01:26:19 PM: Saving checkpoints to: ./experiments/ner-ontonotes-None-only/run
09/16 01:26:19 PM: Stopped training after 33 validation checks
09/16 01:26:19 PM: Trained edges-ner-ontonotes for 33000 batches or 21.236 epochs
09/16 01:26:19 PM: ***** VALIDATION RESULTS *****
09/16 01:26:19 PM: edges-ner-ontonotes_f1 (for best val pass 23): edges-ner-ontonotes_loss: 0.04275, macro_avg: 0.87167, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.86488, edges-ner-ontonotes_acc: 0.82636, edges-ner-ontonotes_precision: 0.90075, edges-ner-ontonotes_recall: 0.84440, edges-ner-ontonotes_f1: 0.87167
09/16 01:26:19 PM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.05829, macro_avg: 0.82098, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.81352, edges-ner-ontonotes_acc: 0.74500, edges-ner-ontonotes_precision: 0.88366, edges-ner-ontonotes_recall: 0.76661, edges-ner-ontonotes_f1: 0.82098
09/16 01:26:19 PM: macro_avg (for best val pass 23): edges-ner-ontonotes_loss: 0.04275, macro_avg: 0.87167, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.86488, edges-ner-ontonotes_acc: 0.82636, edges-ner-ontonotes_precision: 0.90075, edges-ner-ontonotes_recall: 0.84440, edges-ner-ontonotes_f1: 0.87167
09/16 01:26:19 PM: Evaluating...
09/16 01:26:19 PM: Loaded model state from ./experiments/ner-ontonotes-None-only/run/edges-ner-ontonotes/model_state_target_train_val_23.best.th
09/16 01:26:19 PM: Evaluating on: edges-ner-ontonotes, split: val
09/16 01:26:37 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 01:26:37 PM: Finished evaluating on: edges-ner-ontonotes
09/16 01:26:37 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
09/16 01:26:38 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-None-only/run
09/16 01:26:38 PM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-None-only/run
09/16 01:26:38 PM: Evaluating on: edges-ner-ontonotes, split: test
09/16 01:26:47 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 01:26:47 PM: Finished evaluating on: edges-ner-ontonotes
09/16 01:26:47 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
09/16 01:26:48 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-None-only/run
09/16 01:26:48 PM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-None-only/run
09/16 01:26:48 PM: Writing results for split 'val' to ./experiments/ner-ontonotes-None-only/results.tsv
09/16 01:26:48 PM: micro_avg: 0.000, macro_avg: 0.874, edges-ner-ontonotes_mcc: 0.867, edges-ner-ontonotes_acc: 0.830, edges-ner-ontonotes_precision: 0.903, edges-ner-ontonotes_recall: 0.847, edges-ner-ontonotes_f1: 0.874
09/16 01:26:48 PM: Done!
09/16 01:26:48 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
