09/16 06:46:18 AM: Git branch: master
09/16 06:46:18 AM: Git SHA: fb3796f035a61c062bc75b422b0939a7eeec20ff
09/16 06:46:18 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/pos-ontonotes-random-memorization-top/",
  "exp_name": "experiments/pos-ontonotes-random-memorization-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/pos-ontonotes-random-memorization-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/random-memorization",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/pos-ontonotes-random-memorization-top__run",
  "run_dir": "./experiments/pos-ontonotes-random-memorization-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-pos-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 06:46:18 AM: Saved config to ./experiments/pos-ontonotes-random-memorization-top/run/params.conf
09/16 06:46:18 AM: Using random seed 1234
09/16 06:46:19 AM: Using GPU 0
09/16 06:46:19 AM: Loading tasks...
09/16 06:46:19 AM: Writing pre-preprocessed tasks to ./experiments/pos-ontonotes-random-memorization-top/
09/16 06:46:19 AM: 	Creating task edges-pos-ontonotes from scratch.
09/16 06:46:35 AM: Read=110514, Skip=5298, Total=115812 from ./probing_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased
09/16 06:46:36 AM: Read=15060, Skip=620, Total=15680 from ./probing_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased
09/16 06:46:39 AM: Read=11462, Skip=755, Total=12217 from ./probing_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased
09/16 06:46:49 AM: 	Task 'edges-pos-ontonotes': |train|=110514 |val|=15060 |test|=11462
09/16 06:46:49 AM: 	Finished loading tasks: edges-pos-ontonotes.
09/16 06:46:49 AM: 	Building vocab from scratch.
09/16 06:46:49 AM: 	Counting units for task edges-pos-ontonotes.
09/16 06:46:50 AM: 	Task 'edges-pos-ontonotes': adding vocab namespace 'edges-pos-ontonotes_labels'
09/16 06:46:51 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:46:51 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 06:46:52 AM: 	Saved vocab to ./experiments/pos-ontonotes-random-memorization-top/vocab
09/16 06:46:52 AM: Loading token dictionary from ./experiments/pos-ontonotes-random-memorization-top/vocab.
09/16 06:46:52 AM: 	Loaded vocab from ./experiments/pos-ontonotes-random-memorization-top/vocab
09/16 06:46:52 AM: 	Vocab namespace bert_uncased: size 30524
09/16 06:46:52 AM: 	Vocab namespace tokens: size 24015
09/16 06:46:52 AM: 	Vocab namespace edges-pos-ontonotes_labels: size 48
09/16 06:46:52 AM: 	Vocab namespace chars: size 81
09/16 06:46:52 AM: 	Finished building vocab.
09/16 06:46:52 AM: 	Task edges-pos-ontonotes (train): Indexing from scratch.
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (train): Saved 110514 instances to ./experiments/pos-ontonotes-random-memorization-top/preproc/edges-pos-ontonotes__train_data
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (val): Indexing from scratch.
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (val): Saved 15060 instances to ./experiments/pos-ontonotes-random-memorization-top/preproc/edges-pos-ontonotes__val_data
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (test): Indexing from scratch.
09/16 06:47:27 AM: 	Task edges-pos-ontonotes (test): Saved 11462 instances to ./experiments/pos-ontonotes-random-memorization-top/preproc/edges-pos-ontonotes__test_data
09/16 06:47:27 AM: 	Finished indexing tasks
09/16 06:47:27 AM: 	Creating trimmed target-only version of edges-pos-ontonotes train.
09/16 06:47:27 AM: 	  Training on 
09/16 06:47:27 AM: 	  Evaluating on edges-pos-ontonotes
09/16 06:47:27 AM: 	Finished loading tasks in 68.442s
09/16 06:47:27 AM: 	 Tasks: ['edges-pos-ontonotes']
09/16 06:47:27 AM: Building model...
09/16 06:47:27 AM: Using BERT model (bert-base-uncased).
09/16 06:47:27 AM: LOADING A FUNETUNED MODEL from: 
09/16 06:47:27 AM: models/random-memorization
09/16 06:47:27 AM: loading configuration file models/random-memorization/config.json
09/16 06:47:27 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "random-memorization",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 06:47:27 AM: loading weights file models/random-memorization/pytorch_model.bin
09/16 06:47:30 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpxcfdxmxl
09/16 06:47:32 AM: copying /tmp/tmpxcfdxmxl to cache at ./experiments/pos-ontonotes-random-memorization-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: creating metadata file for ./experiments/pos-ontonotes-random-memorization-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: removing temp file /tmp/tmpxcfdxmxl
09/16 06:47:32 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/pos-ontonotes-random-memorization-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: Initializing parameters
09/16 06:47:32 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 06:47:32 AM: 	Task 'edges-pos-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-pos-ontonotes"
}
09/16 06:47:36 AM: Model specification:
09/16 06:47:36 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-pos-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=48, bias=True)
    )
  )
)
09/16 06:47:36 AM: Model parameters:
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 24576 with torch.Size([48, 512])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 48 with torch.Size([48])
09/16 06:47:36 AM: Total number of parameters: 109703728 (1.09704e+08)
09/16 06:47:36 AM: Number of trainable parameters: 221488 (221488)
09/16 06:47:36 AM: Finished building model in 8.928s
09/16 06:47:36 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-pos-ontonotes 

09/16 06:48:37 AM: patience = 9
09/16 06:48:37 AM: val_interval = 1000
09/16 06:48:37 AM: max_vals = 250
09/16 06:48:37 AM: cuda_device = 0
09/16 06:48:37 AM: grad_norm = 5.0
09/16 06:48:37 AM: grad_clipping = None
09/16 06:48:37 AM: lr_decay = 0.99
09/16 06:48:37 AM: min_lr = 1e-06
09/16 06:48:37 AM: keep_all_checkpoints = 0
09/16 06:48:37 AM: val_data_limit = 5000
09/16 06:48:37 AM: max_epochs = -1
09/16 06:48:37 AM: dec_val_scale = 250
09/16 06:48:37 AM: training_data_fraction = 1
09/16 06:48:37 AM: type = adam
09/16 06:48:37 AM: parameter_groups = None
09/16 06:48:37 AM: Number of trainable parameters: 221488
09/16 06:48:37 AM: infer_type_and_cast = True
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: lr = 0.0001
09/16 06:48:37 AM: amsgrad = True
09/16 06:48:37 AM: type = reduce_on_plateau
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: mode = max
09/16 06:48:37 AM: factor = 0.5
09/16 06:48:37 AM: patience = 3
09/16 06:48:37 AM: threshold = 0.0001
09/16 06:48:37 AM: threshold_mode = abs
09/16 06:48:37 AM: verbose = True
09/16 06:48:37 AM: type = adam
09/16 06:48:37 AM: parameter_groups = None
09/16 06:48:37 AM: Number of trainable parameters: 221488
09/16 06:48:37 AM: infer_type_and_cast = True
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: lr = 0.0001
09/16 06:48:37 AM: amsgrad = True
09/16 06:48:37 AM: type = reduce_on_plateau
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: mode = max
09/16 06:48:37 AM: factor = 0.5
09/16 06:48:37 AM: patience = 3
09/16 06:48:37 AM: threshold = 0.0001
09/16 06:48:37 AM: threshold_mode = abs
09/16 06:48:37 AM: verbose = True
09/16 06:48:37 AM: Starting training without restoring from a checkpoint.
09/16 06:48:37 AM: Training examples per task, before any subsampling: {'edges-pos-ontonotes': 110514}
09/16 06:48:37 AM: Beginning training with stopping criteria based on metric: edges-pos-ontonotes_f1
09/16 06:48:47 AM: Update 81: task edges-pos-ontonotes, batch 81 (81): mcc: 0.0025, acc: 0.0024, precision: 0.0220, recall: 0.0898, f1: 0.0354, edges-pos-ontonotes_loss: 0.2837
09/16 06:48:57 AM: Update 173: task edges-pos-ontonotes, batch 173 (173): mcc: 0.0088, acc: 0.0109, precision: 0.0270, recall: 0.0516, f1: 0.0355, edges-pos-ontonotes_loss: 0.1763
09/16 06:49:07 AM: Update 268: task edges-pos-ontonotes, batch 268 (268): mcc: 0.0247, acc: 0.0269, precision: 0.0424, recall: 0.0533, f1: 0.0472, edges-pos-ontonotes_loss: 0.1393
09/16 06:49:17 AM: Update 337: task edges-pos-ontonotes, batch 337 (337): mcc: 0.0322, acc: 0.0311, precision: 0.0529, recall: 0.0514, f1: 0.0521, edges-pos-ontonotes_loss: 0.1247
09/16 06:49:27 AM: Update 414: task edges-pos-ontonotes, batch 414 (414): mcc: 0.0433, acc: 0.0376, precision: 0.0692, recall: 0.0535, f1: 0.0604, edges-pos-ontonotes_loss: 0.1137
09/16 06:49:37 AM: Update 488: task edges-pos-ontonotes, batch 488 (488): mcc: 0.0678, acc: 0.0547, precision: 0.1028, recall: 0.0679, f1: 0.0818, edges-pos-ontonotes_loss: 0.1053
09/16 06:49:47 AM: Update 560: task edges-pos-ontonotes, batch 560 (560): mcc: 0.1000, acc: 0.0762, precision: 0.1487, recall: 0.0879, f1: 0.1105, edges-pos-ontonotes_loss: 0.0988
09/16 06:49:58 AM: Update 627: task edges-pos-ontonotes, batch 627 (627): mcc: 0.1330, acc: 0.0980, precision: 0.1966, recall: 0.1089, f1: 0.1401, edges-pos-ontonotes_loss: 0.0935
09/16 06:50:09 AM: Update 678: task edges-pos-ontonotes, batch 678 (678): mcc: 0.1683, acc: 0.1204, precision: 0.2491, recall: 0.1311, f1: 0.1718, edges-pos-ontonotes_loss: 0.0903
09/16 06:50:19 AM: Update 733: task edges-pos-ontonotes, batch 733 (733): mcc: 0.2028, acc: 0.1426, precision: 0.3003, recall: 0.1531, f1: 0.2028, edges-pos-ontonotes_loss: 0.0872
09/16 06:50:29 AM: Update 797: task edges-pos-ontonotes, batch 797 (797): mcc: 0.2359, acc: 0.1644, precision: 0.3486, recall: 0.1750, f1: 0.2330, edges-pos-ontonotes_loss: 0.0839
09/16 06:50:39 AM: Update 862: task edges-pos-ontonotes, batch 862 (862): mcc: 0.2665, acc: 0.1848, precision: 0.3924, recall: 0.1955, f1: 0.2610, edges-pos-ontonotes_loss: 0.0809
09/16 06:50:49 AM: Update 916: task edges-pos-ontonotes, batch 916 (916): mcc: 0.2967, acc: 0.2056, precision: 0.4340, recall: 0.2168, f1: 0.2892, edges-pos-ontonotes_loss: 0.0786
09/16 06:50:59 AM: Update 963: task edges-pos-ontonotes, batch 963 (963): mcc: 0.3179, acc: 0.2201, precision: 0.4631, recall: 0.2318, f1: 0.3090, edges-pos-ontonotes_loss: 0.0767
09/16 06:51:06 AM: ***** Step 1000 / Validation 1 *****
09/16 06:51:06 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:51:06 AM: Validating...
09/16 06:51:09 AM: Evaluate: task edges-pos-ontonotes, batch 21 (157): mcc: 0.6630, acc: 0.4700, precision: 0.9246, recall: 0.4820, f1: 0.6336, edges-pos-ontonotes_loss: 0.0364
09/16 06:51:19 AM: Evaluate: task edges-pos-ontonotes, batch 89 (157): mcc: 0.6852, acc: 0.4998, precision: 0.9303, recall: 0.5112, f1: 0.6598, edges-pos-ontonotes_loss: 0.0358
09/16 06:51:29 AM: Evaluate: task edges-pos-ontonotes, batch 136 (157): mcc: 0.6935, acc: 0.5136, precision: 0.9271, recall: 0.5253, f1: 0.6707, edges-pos-ontonotes_loss: 0.0350
09/16 06:51:33 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:51:33 AM: Best result seen so far for micro.
09/16 06:51:33 AM: Best result seen so far for macro.
09/16 06:51:33 AM: Updating LR scheduler:
09/16 06:51:33 AM: 	Best result seen so far for macro_avg: 0.671
09/16 06:51:33 AM: 	# validation passes without improvement: 0
09/16 06:51:33 AM: edges-pos-ontonotes_loss: training: 0.075313 validation: 0.035075
09/16 06:51:33 AM: macro_avg: validation: 0.671220
09/16 06:51:33 AM: micro_avg: validation: 0.000000
09/16 06:51:33 AM: edges-pos-ontonotes_mcc: training: 0.334563 validation: 0.693893
09/16 06:51:33 AM: edges-pos-ontonotes_acc: training: 0.231632 validation: 0.514228
09/16 06:51:33 AM: edges-pos-ontonotes_precision: training: 0.485530 validation: 0.926531
09/16 06:51:33 AM: edges-pos-ontonotes_recall: training: 0.243850 validation: 0.526218
09/16 06:51:33 AM: edges-pos-ontonotes_f1: training: 0.324650 validation: 0.671220
09/16 06:51:33 AM: Global learning rate: 0.0001
09/16 06:51:33 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 06:51:39 AM: Update 1031: task edges-pos-ontonotes, batch 31 (1031): mcc: 0.6522, acc: 0.4616, precision: 0.8933, recall: 0.4832, f1: 0.6272, edges-pos-ontonotes_loss: 0.0389
09/16 06:51:49 AM: Update 1085: task edges-pos-ontonotes, batch 85 (1085): mcc: 0.6631, acc: 0.4771, precision: 0.8942, recall: 0.4989, f1: 0.6405, edges-pos-ontonotes_loss: 0.0378
09/16 06:51:59 AM: Update 1141: task edges-pos-ontonotes, batch 141 (1141): mcc: 0.6719, acc: 0.4892, precision: 0.8963, recall: 0.5108, f1: 0.6508, edges-pos-ontonotes_loss: 0.0370
09/16 06:52:09 AM: Update 1196: task edges-pos-ontonotes, batch 196 (1196): mcc: 0.6779, acc: 0.4980, precision: 0.8952, recall: 0.5206, f1: 0.6583, edges-pos-ontonotes_loss: 0.0364
09/16 06:52:20 AM: Update 1249: task edges-pos-ontonotes, batch 249 (1249): mcc: 0.6850, acc: 0.5081, precision: 0.8955, recall: 0.5312, f1: 0.6668, edges-pos-ontonotes_loss: 0.0357
09/16 06:52:30 AM: Update 1253: task edges-pos-ontonotes, batch 253 (1253): mcc: 0.6851, acc: 0.5084, precision: 0.8952, recall: 0.5315, f1: 0.6670, edges-pos-ontonotes_loss: 0.0357
09/16 06:52:41 AM: Update 1310: task edges-pos-ontonotes, batch 310 (1310): mcc: 0.6909, acc: 0.5167, precision: 0.8955, recall: 0.5402, f1: 0.6739, edges-pos-ontonotes_loss: 0.0350
09/16 06:52:51 AM: Update 1367: task edges-pos-ontonotes, batch 367 (1367): mcc: 0.6963, acc: 0.5245, precision: 0.8959, recall: 0.5484, f1: 0.6803, edges-pos-ontonotes_loss: 0.0344
09/16 06:53:01 AM: Update 1424: task edges-pos-ontonotes, batch 424 (1424): mcc: 0.7020, acc: 0.5325, precision: 0.8964, recall: 0.5569, f1: 0.6870, edges-pos-ontonotes_loss: 0.0338
09/16 06:53:11 AM: Update 1477: task edges-pos-ontonotes, batch 477 (1477): mcc: 0.7073, acc: 0.5403, precision: 0.8964, recall: 0.5653, f1: 0.6934, edges-pos-ontonotes_loss: 0.0334
09/16 06:53:21 AM: Update 1531: task edges-pos-ontonotes, batch 531 (1531): mcc: 0.7120, acc: 0.5472, precision: 0.8968, recall: 0.5724, f1: 0.6988, edges-pos-ontonotes_loss: 0.0330
09/16 06:53:31 AM: Update 1582: task edges-pos-ontonotes, batch 582 (1582): mcc: 0.7155, acc: 0.5523, precision: 0.8970, recall: 0.5778, f1: 0.7029, edges-pos-ontonotes_loss: 0.0326
09/16 06:53:41 AM: Update 1637: task edges-pos-ontonotes, batch 637 (1637): mcc: 0.7197, acc: 0.5587, precision: 0.8967, recall: 0.5848, f1: 0.7079, edges-pos-ontonotes_loss: 0.0322
09/16 06:53:51 AM: Update 1693: task edges-pos-ontonotes, batch 693 (1693): mcc: 0.7238, acc: 0.5647, precision: 0.8970, recall: 0.5911, f1: 0.7126, edges-pos-ontonotes_loss: 0.0317
09/16 06:54:01 AM: Update 1747: task edges-pos-ontonotes, batch 747 (1747): mcc: 0.7272, acc: 0.5699, precision: 0.8968, recall: 0.5968, f1: 0.7167, edges-pos-ontonotes_loss: 0.0314
09/16 06:54:11 AM: Update 1807: task edges-pos-ontonotes, batch 807 (1807): mcc: 0.7305, acc: 0.5749, precision: 0.8968, recall: 0.6022, f1: 0.7205, edges-pos-ontonotes_loss: 0.0311
09/16 06:54:22 AM: Update 1862: task edges-pos-ontonotes, batch 862 (1862): mcc: 0.7341, acc: 0.5803, precision: 0.8970, recall: 0.6079, f1: 0.7247, edges-pos-ontonotes_loss: 0.0307
09/16 06:54:32 AM: Update 1902: task edges-pos-ontonotes, batch 902 (1902): mcc: 0.7367, acc: 0.5842, precision: 0.8967, recall: 0.6123, f1: 0.7277, edges-pos-ontonotes_loss: 0.0305
09/16 06:54:42 AM: Update 1973: task edges-pos-ontonotes, batch 973 (1973): mcc: 0.7397, acc: 0.5887, precision: 0.8967, recall: 0.6173, f1: 0.7312, edges-pos-ontonotes_loss: 0.0301
09/16 06:54:45 AM: ***** Step 2000 / Validation 2 *****
09/16 06:54:45 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:54:45 AM: Validating...
09/16 06:54:52 AM: Evaluate: task edges-pos-ontonotes, batch 48 (157): mcc: 0.7839, acc: 0.6549, precision: 0.9170, recall: 0.6764, f1: 0.7786, edges-pos-ontonotes_loss: 0.0261
09/16 06:55:02 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8067, acc: 0.6905, precision: 0.9211, recall: 0.7124, f1: 0.8034, edges-pos-ontonotes_loss: 0.0240
09/16 06:55:12 AM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.8144, acc: 0.7024, precision: 0.9231, recall: 0.7243, f1: 0.8117, edges-pos-ontonotes_loss: 0.0230
09/16 06:55:12 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:55:12 AM: Best result seen so far for macro.
09/16 06:55:12 AM: Updating LR scheduler:
09/16 06:55:12 AM: 	Best result seen so far for macro_avg: 0.812
09/16 06:55:12 AM: 	# validation passes without improvement: 0
09/16 06:55:12 AM: edges-pos-ontonotes_loss: training: 0.029973 validation: 0.023034
09/16 06:55:12 AM: macro_avg: validation: 0.811728
09/16 06:55:12 AM: micro_avg: validation: 0.000000
09/16 06:55:12 AM: edges-pos-ontonotes_mcc: training: 0.740868 validation: 0.814387
09/16 06:55:12 AM: edges-pos-ontonotes_acc: training: 0.590390 validation: 0.702393
09/16 06:55:12 AM: edges-pos-ontonotes_precision: training: 0.896805 validation: 0.923078
09/16 06:55:12 AM: edges-pos-ontonotes_recall: training: 0.619106 validation: 0.724351
09/16 06:55:12 AM: edges-pos-ontonotes_f1: training: 0.732520 validation: 0.811728
09/16 06:55:12 AM: Global learning rate: 0.0001
09/16 06:55:12 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 06:55:22 AM: Update 2073: task edges-pos-ontonotes, batch 73 (2073): mcc: 0.7944, acc: 0.6731, precision: 0.9021, recall: 0.7060, f1: 0.7921, edges-pos-ontonotes_loss: 0.0243
09/16 06:55:32 AM: Update 2140: task edges-pos-ontonotes, batch 140 (2140): mcc: 0.7974, acc: 0.6782, precision: 0.9005, recall: 0.7126, f1: 0.7956, edges-pos-ontonotes_loss: 0.0238
09/16 06:55:42 AM: Update 2203: task edges-pos-ontonotes, batch 203 (2203): mcc: 0.8006, acc: 0.6826, precision: 0.9020, recall: 0.7169, f1: 0.7989, edges-pos-ontonotes_loss: 0.0234
09/16 06:55:52 AM: Update 2299: task edges-pos-ontonotes, batch 299 (2299): mcc: 0.8057, acc: 0.6887, precision: 0.9058, recall: 0.7229, f1: 0.8041, edges-pos-ontonotes_loss: 0.0226
09/16 06:56:02 AM: Update 2392: task edges-pos-ontonotes, batch 392 (2392): mcc: 0.8109, acc: 0.6957, precision: 0.9091, recall: 0.7295, f1: 0.8094, edges-pos-ontonotes_loss: 0.0220
09/16 06:56:12 AM: Update 2482: task edges-pos-ontonotes, batch 482 (2482): mcc: 0.8159, acc: 0.7024, precision: 0.9117, recall: 0.7362, f1: 0.8146, edges-pos-ontonotes_loss: 0.0214
09/16 06:56:22 AM: Update 2575: task edges-pos-ontonotes, batch 575 (2575): mcc: 0.8178, acc: 0.7055, precision: 0.9125, recall: 0.7390, f1: 0.8166, edges-pos-ontonotes_loss: 0.0216
09/16 06:56:32 AM: Update 2680: task edges-pos-ontonotes, batch 680 (2680): mcc: 0.8201, acc: 0.7090, precision: 0.9135, recall: 0.7421, f1: 0.8190, edges-pos-ontonotes_loss: 0.0215
09/16 06:56:42 AM: Update 2785: task edges-pos-ontonotes, batch 785 (2785): mcc: 0.8227, acc: 0.7131, precision: 0.9144, recall: 0.7461, f1: 0.8217, edges-pos-ontonotes_loss: 0.0211
09/16 06:56:52 AM: Update 2889: task edges-pos-ontonotes, batch 889 (2889): mcc: 0.8213, acc: 0.7113, precision: 0.9127, recall: 0.7450, f1: 0.8203, edges-pos-ontonotes_loss: 0.0214
09/16 06:57:00 AM: ***** Step 3000 / Validation 3 *****
09/16 06:57:00 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:57:00 AM: Validating...
09/16 06:57:03 AM: Evaluate: task edges-pos-ontonotes, batch 20 (157): mcc: 0.8044, acc: 0.6860, precision: 0.9127, recall: 0.7151, f1: 0.8019, edges-pos-ontonotes_loss: 0.0212
09/16 06:57:13 AM: Evaluate: task edges-pos-ontonotes, batch 89 (157): mcc: 0.8317, acc: 0.7273, precision: 0.9224, recall: 0.7555, f1: 0.8307, edges-pos-ontonotes_loss: 0.0194
09/16 06:57:23 AM: Evaluate: task edges-pos-ontonotes, batch 136 (157): mcc: 0.8315, acc: 0.7285, precision: 0.9179, recall: 0.7589, f1: 0.8309, edges-pos-ontonotes_loss: 0.0195
09/16 06:57:27 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:57:27 AM: Best result seen so far for macro.
09/16 06:57:27 AM: Updating LR scheduler:
09/16 06:57:27 AM: 	Best result seen so far for macro_avg: 0.832
09/16 06:57:27 AM: 	# validation passes without improvement: 0
09/16 06:57:27 AM: edges-pos-ontonotes_loss: training: 0.021437 validation: 0.019408
09/16 06:57:27 AM: macro_avg: validation: 0.832159
09/16 06:57:27 AM: micro_avg: validation: 0.000000
09/16 06:57:27 AM: edges-pos-ontonotes_mcc: training: 0.819995 validation: 0.832682
09/16 06:57:27 AM: edges-pos-ontonotes_acc: training: 0.709718 validation: 0.731018
09/16 06:57:27 AM: edges-pos-ontonotes_precision: training: 0.911408 validation: 0.917556
09/16 06:57:27 AM: edges-pos-ontonotes_recall: training: 0.743712 validation: 0.761305
09/16 06:57:27 AM: edges-pos-ontonotes_f1: training: 0.819065 validation: 0.832159
09/16 06:57:27 AM: Global learning rate: 0.0001
09/16 06:57:27 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 06:57:33 AM: Update 3075: task edges-pos-ontonotes, batch 75 (3075): mcc: 0.8056, acc: 0.6919, precision: 0.8954, recall: 0.7312, f1: 0.8050, edges-pos-ontonotes_loss: 0.0221
09/16 06:57:43 AM: Update 3152: task edges-pos-ontonotes, batch 152 (3152): mcc: 0.8052, acc: 0.6929, precision: 0.8883, recall: 0.7365, f1: 0.8053, edges-pos-ontonotes_loss: 0.0221
09/16 06:57:53 AM: Update 3208: task edges-pos-ontonotes, batch 208 (3208): mcc: 0.8009, acc: 0.6874, precision: 0.8846, recall: 0.7319, f1: 0.8010, edges-pos-ontonotes_loss: 0.0226
09/16 06:58:03 AM: Update 3269: task edges-pos-ontonotes, batch 269 (3269): mcc: 0.8005, acc: 0.6871, precision: 0.8837, recall: 0.7318, f1: 0.8006, edges-pos-ontonotes_loss: 0.0230
09/16 06:58:14 AM: Update 3329: task edges-pos-ontonotes, batch 329 (3329): mcc: 0.8013, acc: 0.6883, precision: 0.8840, recall: 0.7330, f1: 0.8015, edges-pos-ontonotes_loss: 0.0232
09/16 06:58:24 AM: Update 3402: task edges-pos-ontonotes, batch 402 (3402): mcc: 0.8023, acc: 0.6895, precision: 0.8854, recall: 0.7337, f1: 0.8024, edges-pos-ontonotes_loss: 0.0231
09/16 06:58:42 AM: Update 3461: task edges-pos-ontonotes, batch 461 (3461): mcc: 0.8024, acc: 0.6899, precision: 0.8853, recall: 0.7340, f1: 0.8026, edges-pos-ontonotes_loss: 0.0230
09/16 06:58:52 AM: Update 3560: task edges-pos-ontonotes, batch 560 (3560): mcc: 0.8039, acc: 0.6914, precision: 0.8877, recall: 0.7346, f1: 0.8039, edges-pos-ontonotes_loss: 0.0227
09/16 06:59:03 AM: Update 3657: task edges-pos-ontonotes, batch 657 (3657): mcc: 0.8056, acc: 0.6937, precision: 0.8891, recall: 0.7365, f1: 0.8057, edges-pos-ontonotes_loss: 0.0222
09/16 06:59:13 AM: Update 3742: task edges-pos-ontonotes, batch 742 (3742): mcc: 0.8073, acc: 0.6960, precision: 0.8902, recall: 0.7386, f1: 0.8073, edges-pos-ontonotes_loss: 0.0219
09/16 06:59:23 AM: Update 3810: task edges-pos-ontonotes, batch 810 (3810): mcc: 0.8091, acc: 0.6986, precision: 0.8913, recall: 0.7409, f1: 0.8092, edges-pos-ontonotes_loss: 0.0217
09/16 06:59:33 AM: Update 3878: task edges-pos-ontonotes, batch 878 (3878): mcc: 0.8108, acc: 0.7010, precision: 0.8920, recall: 0.7434, f1: 0.8109, edges-pos-ontonotes_loss: 0.0215
09/16 06:59:43 AM: Update 3956: task edges-pos-ontonotes, batch 956 (3956): mcc: 0.8125, acc: 0.7035, precision: 0.8928, recall: 0.7457, f1: 0.8127, edges-pos-ontonotes_loss: 0.0213
09/16 06:59:48 AM: ***** Step 4000 / Validation 4 *****
09/16 06:59:48 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:59:48 AM: Validating...
09/16 06:59:53 AM: Evaluate: task edges-pos-ontonotes, batch 34 (157): mcc: 0.8576, acc: 0.7696, precision: 0.9339, recall: 0.7925, f1: 0.8574, edges-pos-ontonotes_loss: 0.0163
09/16 07:00:03 AM: Evaluate: task edges-pos-ontonotes, batch 99 (157): mcc: 0.8685, acc: 0.7884, precision: 0.9345, recall: 0.8119, f1: 0.8689, edges-pos-ontonotes_loss: 0.0155
09/16 07:00:13 AM: Evaluate: task edges-pos-ontonotes, batch 146 (157): mcc: 0.8645, acc: 0.7839, precision: 0.9310, recall: 0.8077, f1: 0.8649, edges-pos-ontonotes_loss: 0.0159
09/16 07:00:15 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:00:15 AM: Best result seen so far for macro.
09/16 07:00:15 AM: Updating LR scheduler:
09/16 07:00:15 AM: 	Best result seen so far for macro_avg: 0.866
09/16 07:00:15 AM: 	# validation passes without improvement: 0
09/16 07:00:15 AM: edges-pos-ontonotes_loss: training: 0.021159 validation: 0.015875
09/16 07:00:15 AM: macro_avg: validation: 0.865593
09/16 07:00:15 AM: micro_avg: validation: 0.000000
09/16 07:00:15 AM: edges-pos-ontonotes_mcc: training: 0.813374 validation: 0.865184
09/16 07:00:15 AM: edges-pos-ontonotes_acc: training: 0.704778 validation: 0.785147
09/16 07:00:15 AM: edges-pos-ontonotes_precision: training: 0.893249 validation: 0.931346
09/16 07:00:15 AM: edges-pos-ontonotes_recall: training: 0.747015 validation: 0.808512
09/16 07:00:15 AM: edges-pos-ontonotes_f1: training: 0.813613 validation: 0.865593
09/16 07:00:15 AM: Global learning rate: 0.0001
09/16 07:00:15 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:00:23 AM: Update 4060: task edges-pos-ontonotes, batch 60 (4060): mcc: 0.8298, acc: 0.7288, precision: 0.8996, recall: 0.7714, f1: 0.8306, edges-pos-ontonotes_loss: 0.0197
09/16 07:00:33 AM: Update 4106: task edges-pos-ontonotes, batch 106 (4106): mcc: 0.8281, acc: 0.7278, precision: 0.8971, recall: 0.7704, f1: 0.8289, edges-pos-ontonotes_loss: 0.0201
09/16 07:00:43 AM: Update 4163: task edges-pos-ontonotes, batch 163 (4163): mcc: 0.8253, acc: 0.7238, precision: 0.8957, recall: 0.7667, f1: 0.8262, edges-pos-ontonotes_loss: 0.0207
09/16 07:00:53 AM: Update 4225: task edges-pos-ontonotes, batch 225 (4225): mcc: 0.8252, acc: 0.7237, precision: 0.8955, recall: 0.7665, f1: 0.8260, edges-pos-ontonotes_loss: 0.0207
09/16 07:01:03 AM: Update 4286: task edges-pos-ontonotes, batch 286 (4286): mcc: 0.8259, acc: 0.7249, precision: 0.8952, recall: 0.7681, f1: 0.8268, edges-pos-ontonotes_loss: 0.0207
09/16 07:01:14 AM: Update 4341: task edges-pos-ontonotes, batch 341 (4341): mcc: 0.8265, acc: 0.7260, precision: 0.8954, recall: 0.7690, f1: 0.8274, edges-pos-ontonotes_loss: 0.0206
09/16 07:01:24 AM: Update 4398: task edges-pos-ontonotes, batch 398 (4398): mcc: 0.8269, acc: 0.7265, precision: 0.8959, recall: 0.7692, f1: 0.8277, edges-pos-ontonotes_loss: 0.0205
09/16 07:01:34 AM: Update 4442: task edges-pos-ontonotes, batch 442 (4442): mcc: 0.8263, acc: 0.7255, precision: 0.8954, recall: 0.7686, f1: 0.8272, edges-pos-ontonotes_loss: 0.0205
09/16 07:01:44 AM: Update 4501: task edges-pos-ontonotes, batch 501 (4501): mcc: 0.8268, acc: 0.7263, precision: 0.8957, recall: 0.7692, f1: 0.8277, edges-pos-ontonotes_loss: 0.0204
09/16 07:01:54 AM: Update 4552: task edges-pos-ontonotes, batch 552 (4552): mcc: 0.8267, acc: 0.7262, precision: 0.8957, recall: 0.7692, f1: 0.8276, edges-pos-ontonotes_loss: 0.0204
09/16 07:02:04 AM: Update 4607: task edges-pos-ontonotes, batch 607 (4607): mcc: 0.8273, acc: 0.7271, precision: 0.8961, recall: 0.7698, f1: 0.8282, edges-pos-ontonotes_loss: 0.0203
09/16 07:02:14 AM: Update 4661: task edges-pos-ontonotes, batch 661 (4661): mcc: 0.8279, acc: 0.7282, precision: 0.8966, recall: 0.7706, f1: 0.8288, edges-pos-ontonotes_loss: 0.0202
09/16 07:02:26 AM: Update 4713: task edges-pos-ontonotes, batch 713 (4713): mcc: 0.8284, acc: 0.7290, precision: 0.8969, recall: 0.7712, f1: 0.8293, edges-pos-ontonotes_loss: 0.0201
09/16 07:02:36 AM: Update 4769: task edges-pos-ontonotes, batch 769 (4769): mcc: 0.8287, acc: 0.7294, precision: 0.8971, recall: 0.7716, f1: 0.8296, edges-pos-ontonotes_loss: 0.0201
09/16 07:02:46 AM: Update 4827: task edges-pos-ontonotes, batch 827 (4827): mcc: 0.8294, acc: 0.7304, precision: 0.8974, recall: 0.7725, f1: 0.8303, edges-pos-ontonotes_loss: 0.0200
09/16 07:02:56 AM: Update 4884: task edges-pos-ontonotes, batch 884 (4884): mcc: 0.8301, acc: 0.7315, precision: 0.8979, recall: 0.7734, f1: 0.8310, edges-pos-ontonotes_loss: 0.0199
09/16 07:03:06 AM: Update 4943: task edges-pos-ontonotes, batch 943 (4943): mcc: 0.8306, acc: 0.7325, precision: 0.8981, recall: 0.7742, f1: 0.8316, edges-pos-ontonotes_loss: 0.0198
09/16 07:03:16 AM: Update 4996: task edges-pos-ontonotes, batch 996 (4996): mcc: 0.8313, acc: 0.7336, precision: 0.8985, recall: 0.7751, f1: 0.8322, edges-pos-ontonotes_loss: 0.0197
09/16 07:03:17 AM: ***** Step 5000 / Validation 5 *****
09/16 07:03:17 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:03:17 AM: Validating...
09/16 07:03:26 AM: Evaluate: task edges-pos-ontonotes, batch 67 (157): mcc: 0.8687, acc: 0.7904, precision: 0.9303, recall: 0.8159, f1: 0.8694, edges-pos-ontonotes_loss: 0.0155
09/16 07:03:36 AM: Evaluate: task edges-pos-ontonotes, batch 121 (157): mcc: 0.8757, acc: 0.8031, precision: 0.9325, recall: 0.8269, f1: 0.8766, edges-pos-ontonotes_loss: 0.0149
09/16 07:03:44 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:03:44 AM: Best result seen so far for macro.
09/16 07:03:44 AM: Updating LR scheduler:
09/16 07:03:44 AM: 	Best result seen so far for macro_avg: 0.879
09/16 07:03:44 AM: 	# validation passes without improvement: 0
09/16 07:03:44 AM: edges-pos-ontonotes_loss: training: 0.019758 validation: 0.014614
09/16 07:03:44 AM: macro_avg: validation: 0.878798
09/16 07:03:44 AM: micro_avg: validation: 0.000000
09/16 07:03:44 AM: edges-pos-ontonotes_mcc: training: 0.831307 validation: 0.877868
09/16 07:03:44 AM: edges-pos-ontonotes_acc: training: 0.733662 validation: 0.807285
09/16 07:03:44 AM: edges-pos-ontonotes_precision: training: 0.898447 validation: 0.932165
09/16 07:03:44 AM: edges-pos-ontonotes_recall: training: 0.775154 validation: 0.831212
09/16 07:03:44 AM: edges-pos-ontonotes_f1: training: 0.832259 validation: 0.878798
09/16 07:03:44 AM: Global learning rate: 0.0001
09/16 07:03:44 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:03:46 AM: Update 5015: task edges-pos-ontonotes, batch 15 (5015): mcc: 0.8440, acc: 0.7550, precision: 0.9060, recall: 0.7917, f1: 0.8450, edges-pos-ontonotes_loss: 0.0181
09/16 07:03:58 AM: Update 5026: task edges-pos-ontonotes, batch 26 (5026): mcc: 0.8419, acc: 0.7504, precision: 0.9051, recall: 0.7888, f1: 0.8430, edges-pos-ontonotes_loss: 0.0181
09/16 07:04:08 AM: Update 5088: task edges-pos-ontonotes, batch 88 (5088): mcc: 0.8396, acc: 0.7474, precision: 0.9033, recall: 0.7861, f1: 0.8406, edges-pos-ontonotes_loss: 0.0190
09/16 07:04:18 AM: Update 5142: task edges-pos-ontonotes, batch 142 (5142): mcc: 0.8407, acc: 0.7486, precision: 0.9033, recall: 0.7881, f1: 0.8418, edges-pos-ontonotes_loss: 0.0188
09/16 07:04:28 AM: Update 5202: task edges-pos-ontonotes, batch 202 (5202): mcc: 0.8410, acc: 0.7492, precision: 0.9034, recall: 0.7887, f1: 0.8421, edges-pos-ontonotes_loss: 0.0187
09/16 07:04:38 AM: Update 5258: task edges-pos-ontonotes, batch 258 (5258): mcc: 0.8413, acc: 0.7499, precision: 0.9027, recall: 0.7897, f1: 0.8425, edges-pos-ontonotes_loss: 0.0187
09/16 07:04:48 AM: Update 5312: task edges-pos-ontonotes, batch 312 (5312): mcc: 0.8418, acc: 0.7508, precision: 0.9031, recall: 0.7904, f1: 0.8430, edges-pos-ontonotes_loss: 0.0186
09/16 07:04:58 AM: Update 5352: task edges-pos-ontonotes, batch 352 (5352): mcc: 0.8419, acc: 0.7510, precision: 0.9028, recall: 0.7909, f1: 0.8431, edges-pos-ontonotes_loss: 0.0185
09/16 07:05:08 AM: Update 5421: task edges-pos-ontonotes, batch 421 (5421): mcc: 0.8433, acc: 0.7527, precision: 0.9036, recall: 0.7927, f1: 0.8445, edges-pos-ontonotes_loss: 0.0182
09/16 07:05:18 AM: Update 5495: task edges-pos-ontonotes, batch 495 (5495): mcc: 0.8446, acc: 0.7542, precision: 0.9045, recall: 0.7943, f1: 0.8458, edges-pos-ontonotes_loss: 0.0179
09/16 07:05:29 AM: Update 5571: task edges-pos-ontonotes, batch 571 (5571): mcc: 0.8458, acc: 0.7557, precision: 0.9053, recall: 0.7958, f1: 0.8470, edges-pos-ontonotes_loss: 0.0176
09/16 07:05:39 AM: Update 5630: task edges-pos-ontonotes, batch 630 (5630): mcc: 0.8466, acc: 0.7569, precision: 0.9056, recall: 0.7970, f1: 0.8478, edges-pos-ontonotes_loss: 0.0175
09/16 07:05:49 AM: Update 5702: task edges-pos-ontonotes, batch 702 (5702): mcc: 0.8481, acc: 0.7588, precision: 0.9068, recall: 0.7988, f1: 0.8494, edges-pos-ontonotes_loss: 0.0172
09/16 07:05:59 AM: Update 5799: task edges-pos-ontonotes, batch 799 (5799): mcc: 0.8509, acc: 0.7623, precision: 0.9087, recall: 0.8021, f1: 0.8521, edges-pos-ontonotes_loss: 0.0169
09/16 07:06:09 AM: Update 5892: task edges-pos-ontonotes, batch 892 (5892): mcc: 0.8533, acc: 0.7656, precision: 0.9103, recall: 0.8052, f1: 0.8545, edges-pos-ontonotes_loss: 0.0165
09/16 07:06:19 AM: Update 5971: task edges-pos-ontonotes, batch 971 (5971): mcc: 0.8550, acc: 0.7680, precision: 0.9114, recall: 0.8074, f1: 0.8563, edges-pos-ontonotes_loss: 0.0163
09/16 07:06:21 AM: ***** Step 6000 / Validation 6 *****
09/16 07:06:21 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:06:21 AM: Validating...
09/16 07:06:29 AM: Evaluate: task edges-pos-ontonotes, batch 53 (157): mcc: 0.8705, acc: 0.7913, precision: 0.9354, recall: 0.8147, f1: 0.8709, edges-pos-ontonotes_loss: 0.0151
09/16 07:06:39 AM: Evaluate: task edges-pos-ontonotes, batch 113 (157): mcc: 0.8749, acc: 0.7997, precision: 0.9377, recall: 0.8208, f1: 0.8754, edges-pos-ontonotes_loss: 0.0146
09/16 07:06:48 AM: Updating LR scheduler:
09/16 07:06:48 AM: 	Best result seen so far for macro_avg: 0.879
09/16 07:06:48 AM: 	# validation passes without improvement: 1
09/16 07:06:48 AM: edges-pos-ontonotes_loss: training: 0.016313 validation: 0.014436
09/16 07:06:48 AM: macro_avg: validation: 0.875461
09/16 07:06:48 AM: micro_avg: validation: 0.000000
09/16 07:06:48 AM: edges-pos-ontonotes_mcc: training: 0.855307 validation: 0.875056
09/16 07:06:48 AM: edges-pos-ontonotes_acc: training: 0.768470 validation: 0.800258
09/16 07:06:48 AM: edges-pos-ontonotes_precision: training: 0.911658 validation: 0.938629
09/16 07:06:48 AM: edges-pos-ontonotes_recall: training: 0.807714 validation: 0.820259
09/16 07:06:48 AM: edges-pos-ontonotes_f1: training: 0.856544 validation: 0.875461
09/16 07:06:48 AM: Global learning rate: 0.0001
09/16 07:06:48 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:06:49 AM: Update 6012: task edges-pos-ontonotes, batch 12 (6012): mcc: 0.8700, acc: 0.7869, precision: 0.9263, recall: 0.8218, f1: 0.8709, edges-pos-ontonotes_loss: 0.0169
09/16 07:06:59 AM: Update 6126: task edges-pos-ontonotes, batch 126 (6126): mcc: 0.8748, acc: 0.7956, precision: 0.9259, recall: 0.8312, f1: 0.8760, edges-pos-ontonotes_loss: 0.0153
09/16 07:07:10 AM: Update 6231: task edges-pos-ontonotes, batch 231 (6231): mcc: 0.8762, acc: 0.7977, precision: 0.9259, recall: 0.8338, f1: 0.8774, edges-pos-ontonotes_loss: 0.0148
09/16 07:07:20 AM: Update 6342: task edges-pos-ontonotes, batch 342 (6342): mcc: 0.8701, acc: 0.7896, precision: 0.9205, recall: 0.8273, f1: 0.8714, edges-pos-ontonotes_loss: 0.0156
09/16 07:07:30 AM: Update 6469: task edges-pos-ontonotes, batch 469 (6469): mcc: 0.8616, acc: 0.7772, precision: 0.9145, recall: 0.8170, f1: 0.8630, edges-pos-ontonotes_loss: 0.0164
09/16 07:07:40 AM: Update 6592: task edges-pos-ontonotes, batch 592 (6592): mcc: 0.8581, acc: 0.7723, precision: 0.9115, recall: 0.8131, f1: 0.8595, edges-pos-ontonotes_loss: 0.0167
09/16 07:07:50 AM: Update 6666: task edges-pos-ontonotes, batch 666 (6666): mcc: 0.8524, acc: 0.7637, precision: 0.9071, recall: 0.8064, f1: 0.8538, edges-pos-ontonotes_loss: 0.0171
09/16 07:08:00 AM: Update 6721: task edges-pos-ontonotes, batch 721 (6721): mcc: 0.8492, acc: 0.7593, precision: 0.9041, recall: 0.8032, f1: 0.8507, edges-pos-ontonotes_loss: 0.0173
09/16 07:08:11 AM: Update 6776: task edges-pos-ontonotes, batch 776 (6776): mcc: 0.8475, acc: 0.7570, precision: 0.9021, recall: 0.8017, f1: 0.8490, edges-pos-ontonotes_loss: 0.0175
09/16 07:08:21 AM: Update 6837: task edges-pos-ontonotes, batch 837 (6837): mcc: 0.8460, acc: 0.7550, precision: 0.9007, recall: 0.8003, f1: 0.8475, edges-pos-ontonotes_loss: 0.0176
09/16 07:08:31 AM: Update 6904: task edges-pos-ontonotes, batch 904 (6904): mcc: 0.8453, acc: 0.7541, precision: 0.9002, recall: 0.7994, f1: 0.8468, edges-pos-ontonotes_loss: 0.0177
09/16 07:08:41 AM: Update 6977: task edges-pos-ontonotes, batch 977 (6977): mcc: 0.8444, acc: 0.7527, precision: 0.8999, recall: 0.7980, f1: 0.8459, edges-pos-ontonotes_loss: 0.0177
09/16 07:08:44 AM: ***** Step 7000 / Validation 7 *****
09/16 07:08:44 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:08:44 AM: Validating...
09/16 07:08:51 AM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.8790, acc: 0.8053, precision: 0.9385, recall: 0.8276, f1: 0.8796, edges-pos-ontonotes_loss: 0.0141
09/16 07:09:01 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8844, acc: 0.8157, precision: 0.9359, recall: 0.8400, f1: 0.8854, edges-pos-ontonotes_loss: 0.0135
09/16 07:09:11 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.8833, acc: 0.8151, precision: 0.9340, recall: 0.8397, f1: 0.8844, edges-pos-ontonotes_loss: 0.0136
09/16 07:09:11 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:09:11 AM: Best result seen so far for macro.
09/16 07:09:11 AM: Updating LR scheduler:
09/16 07:09:11 AM: 	Best result seen so far for macro_avg: 0.884
09/16 07:09:11 AM: 	# validation passes without improvement: 0
09/16 07:09:11 AM: edges-pos-ontonotes_loss: training: 0.017762 validation: 0.013530
09/16 07:09:11 AM: macro_avg: validation: 0.884452
09/16 07:09:11 AM: micro_avg: validation: 0.000000
09/16 07:09:11 AM: edges-pos-ontonotes_mcc: training: 0.844268 validation: 0.883426
09/16 07:09:11 AM: edges-pos-ontonotes_acc: training: 0.752442 validation: 0.815116
09/16 07:09:11 AM: edges-pos-ontonotes_precision: training: 0.899941 validation: 0.934087
09/16 07:09:11 AM: edges-pos-ontonotes_recall: training: 0.797723 validation: 0.839826
09/16 07:09:11 AM: edges-pos-ontonotes_f1: training: 0.845755 validation: 0.884452
09/16 07:09:11 AM: Global learning rate: 0.0001
09/16 07:09:11 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:09:21 AM: Update 7093: task edges-pos-ontonotes, batch 93 (7093): mcc: 0.8465, acc: 0.7538, precision: 0.9067, recall: 0.7959, f1: 0.8477, edges-pos-ontonotes_loss: 0.0164
09/16 07:09:31 AM: Update 7187: task edges-pos-ontonotes, batch 187 (7187): mcc: 0.8431, acc: 0.7495, precision: 0.9043, recall: 0.7917, f1: 0.8443, edges-pos-ontonotes_loss: 0.0168
09/16 07:09:41 AM: Update 7256: task edges-pos-ontonotes, batch 256 (7256): mcc: 0.8448, acc: 0.7520, precision: 0.9038, recall: 0.7953, f1: 0.8460, edges-pos-ontonotes_loss: 0.0169
09/16 07:09:51 AM: Update 7327: task edges-pos-ontonotes, batch 327 (7327): mcc: 0.8465, acc: 0.7543, precision: 0.9039, recall: 0.7982, f1: 0.8478, edges-pos-ontonotes_loss: 0.0169
09/16 07:10:01 AM: Update 7404: task edges-pos-ontonotes, batch 404 (7404): mcc: 0.8467, acc: 0.7544, precision: 0.9037, recall: 0.7988, f1: 0.8480, edges-pos-ontonotes_loss: 0.0169
09/16 07:10:11 AM: Update 7474: task edges-pos-ontonotes, batch 474 (7474): mcc: 0.8473, acc: 0.7556, precision: 0.9034, recall: 0.8002, f1: 0.8487, edges-pos-ontonotes_loss: 0.0170
09/16 07:10:31 AM: Update 7547: task edges-pos-ontonotes, batch 547 (7547): mcc: 0.8476, acc: 0.7561, precision: 0.9034, recall: 0.8007, f1: 0.8490, edges-pos-ontonotes_loss: 0.0169
09/16 07:10:41 AM: Update 7598: task edges-pos-ontonotes, batch 598 (7598): mcc: 0.8464, acc: 0.7549, precision: 0.9022, recall: 0.7996, f1: 0.8478, edges-pos-ontonotes_loss: 0.0171
09/16 07:10:51 AM: Update 7656: task edges-pos-ontonotes, batch 656 (7656): mcc: 0.8461, acc: 0.7547, precision: 0.9015, recall: 0.7998, f1: 0.8476, edges-pos-ontonotes_loss: 0.0172
09/16 07:11:01 AM: Update 7709: task edges-pos-ontonotes, batch 709 (7709): mcc: 0.8458, acc: 0.7545, precision: 0.9006, recall: 0.7999, f1: 0.8473, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:11 AM: Update 7776: task edges-pos-ontonotes, batch 776 (7776): mcc: 0.8462, acc: 0.7551, precision: 0.9011, recall: 0.8004, f1: 0.8477, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:21 AM: Update 7832: task edges-pos-ontonotes, batch 832 (7832): mcc: 0.8465, acc: 0.7556, precision: 0.9011, recall: 0.8009, f1: 0.8481, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:32 AM: Update 7875: task edges-pos-ontonotes, batch 875 (7875): mcc: 0.8461, acc: 0.7551, precision: 0.9007, recall: 0.8005, f1: 0.8477, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:42 AM: Update 7930: task edges-pos-ontonotes, batch 930 (7930): mcc: 0.8461, acc: 0.7550, precision: 0.9005, recall: 0.8005, f1: 0.8476, edges-pos-ontonotes_loss: 0.0175
09/16 07:11:52 AM: Update 7982: task edges-pos-ontonotes, batch 982 (7982): mcc: 0.8462, acc: 0.7554, precision: 0.9006, recall: 0.8008, f1: 0.8478, edges-pos-ontonotes_loss: 0.0175
09/16 07:11:55 AM: ***** Step 8000 / Validation 8 *****
09/16 07:11:55 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:11:55 AM: Validating...
09/16 07:12:02 AM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.8818, acc: 0.8106, precision: 0.9372, recall: 0.8340, f1: 0.8826, edges-pos-ontonotes_loss: 0.0137
09/16 07:12:12 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8895, acc: 0.8238, precision: 0.9382, recall: 0.8474, f1: 0.8905, edges-pos-ontonotes_loss: 0.0130
09/16 07:12:22 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.8904, acc: 0.8266, precision: 0.9369, recall: 0.8503, f1: 0.8915, edges-pos-ontonotes_loss: 0.0128
09/16 07:12:22 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:12:22 AM: Best result seen so far for macro.
09/16 07:12:22 AM: Updating LR scheduler:
09/16 07:12:22 AM: 	Best result seen so far for macro_avg: 0.892
09/16 07:12:22 AM: 	# validation passes without improvement: 0
09/16 07:12:22 AM: edges-pos-ontonotes_loss: training: 0.017458 validation: 0.012789
09/16 07:12:22 AM: macro_avg: validation: 0.891699
09/16 07:12:22 AM: micro_avg: validation: 0.000000
09/16 07:12:22 AM: edges-pos-ontonotes_mcc: training: 0.846406 validation: 0.890591
09/16 07:12:22 AM: edges-pos-ontonotes_acc: training: 0.755633 validation: 0.826873
09/16 07:12:22 AM: edges-pos-ontonotes_precision: training: 0.900767 validation: 0.936961
09/16 07:12:22 AM: edges-pos-ontonotes_recall: training: 0.800951 validation: 0.850609
09/16 07:12:22 AM: edges-pos-ontonotes_f1: training: 0.847932 validation: 0.891699
09/16 07:12:22 AM: Global learning rate: 0.0001
09/16 07:12:22 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:12:32 AM: Update 8056: task edges-pos-ontonotes, batch 56 (8056): mcc: 0.8483, acc: 0.7586, precision: 0.9013, recall: 0.8040, f1: 0.8499, edges-pos-ontonotes_loss: 0.0176
09/16 07:12:42 AM: Update 8110: task edges-pos-ontonotes, batch 110 (8110): mcc: 0.8495, acc: 0.7611, precision: 0.9024, recall: 0.8052, f1: 0.8510, edges-pos-ontonotes_loss: 0.0174
09/16 07:12:52 AM: Update 8166: task edges-pos-ontonotes, batch 166 (8166): mcc: 0.8493, acc: 0.7613, precision: 0.9021, recall: 0.8052, f1: 0.8509, edges-pos-ontonotes_loss: 0.0173
09/16 07:13:02 AM: Update 8207: task edges-pos-ontonotes, batch 207 (8207): mcc: 0.8487, acc: 0.7608, precision: 0.9013, recall: 0.8047, f1: 0.8503, edges-pos-ontonotes_loss: 0.0174
09/16 07:13:12 AM: Update 8263: task edges-pos-ontonotes, batch 263 (8263): mcc: 0.8493, acc: 0.7619, precision: 0.9016, recall: 0.8057, f1: 0.8509, edges-pos-ontonotes_loss: 0.0174
09/16 07:13:22 AM: Update 8319: task edges-pos-ontonotes, batch 319 (8319): mcc: 0.8504, acc: 0.7634, precision: 0.9025, recall: 0.8068, f1: 0.8520, edges-pos-ontonotes_loss: 0.0174
09/16 07:13:32 AM: Update 8381: task edges-pos-ontonotes, batch 381 (8381): mcc: 0.8511, acc: 0.7646, precision: 0.9032, recall: 0.8076, f1: 0.8527, edges-pos-ontonotes_loss: 0.0173
09/16 07:13:42 AM: Update 8436: task edges-pos-ontonotes, batch 436 (8436): mcc: 0.8519, acc: 0.7656, precision: 0.9037, recall: 0.8085, f1: 0.8535, edges-pos-ontonotes_loss: 0.0173
09/16 07:13:53 AM: Update 8486: task edges-pos-ontonotes, batch 486 (8486): mcc: 0.8522, acc: 0.7660, precision: 0.9038, recall: 0.8091, f1: 0.8538, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:03 AM: Update 8537: task edges-pos-ontonotes, batch 537 (8537): mcc: 0.8523, acc: 0.7663, precision: 0.9037, recall: 0.8093, f1: 0.8539, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:13 AM: Update 8596: task edges-pos-ontonotes, batch 596 (8596): mcc: 0.8526, acc: 0.7667, precision: 0.9040, recall: 0.8097, f1: 0.8542, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:23 AM: Update 8651: task edges-pos-ontonotes, batch 651 (8651): mcc: 0.8529, acc: 0.7672, precision: 0.9041, recall: 0.8101, f1: 0.8545, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:34 AM: Update 8710: task edges-pos-ontonotes, batch 710 (8710): mcc: 0.8533, acc: 0.7679, precision: 0.9042, recall: 0.8106, f1: 0.8549, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:44 AM: Update 8767: task edges-pos-ontonotes, batch 767 (8767): mcc: 0.8531, acc: 0.7677, precision: 0.9041, recall: 0.8104, f1: 0.8547, edges-pos-ontonotes_loss: 0.0172
