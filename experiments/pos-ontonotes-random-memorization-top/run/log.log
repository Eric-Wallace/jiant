09/16 06:46:18 AM: Git branch: master
09/16 06:46:18 AM: Git SHA: fb3796f035a61c062bc75b422b0939a7eeec20ff
09/16 06:46:18 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/pos-ontonotes-random-memorization-top/",
  "exp_name": "experiments/pos-ontonotes-random-memorization-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/pos-ontonotes-random-memorization-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/random-memorization",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/pos-ontonotes-random-memorization-top__run",
  "run_dir": "./experiments/pos-ontonotes-random-memorization-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-pos-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 06:46:18 AM: Saved config to ./experiments/pos-ontonotes-random-memorization-top/run/params.conf
09/16 06:46:18 AM: Using random seed 1234
09/16 06:46:19 AM: Using GPU 0
09/16 06:46:19 AM: Loading tasks...
09/16 06:46:19 AM: Writing pre-preprocessed tasks to ./experiments/pos-ontonotes-random-memorization-top/
09/16 06:46:19 AM: 	Creating task edges-pos-ontonotes from scratch.
09/16 06:46:35 AM: Read=110514, Skip=5298, Total=115812 from ./probing_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased
09/16 06:46:36 AM: Read=15060, Skip=620, Total=15680 from ./probing_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased
09/16 06:46:39 AM: Read=11462, Skip=755, Total=12217 from ./probing_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased
09/16 06:46:49 AM: 	Task 'edges-pos-ontonotes': |train|=110514 |val|=15060 |test|=11462
09/16 06:46:49 AM: 	Finished loading tasks: edges-pos-ontonotes.
09/16 06:46:49 AM: 	Building vocab from scratch.
09/16 06:46:49 AM: 	Counting units for task edges-pos-ontonotes.
09/16 06:46:50 AM: 	Task 'edges-pos-ontonotes': adding vocab namespace 'edges-pos-ontonotes_labels'
09/16 06:46:51 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:46:51 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 06:46:52 AM: 	Saved vocab to ./experiments/pos-ontonotes-random-memorization-top/vocab
09/16 06:46:52 AM: Loading token dictionary from ./experiments/pos-ontonotes-random-memorization-top/vocab.
09/16 06:46:52 AM: 	Loaded vocab from ./experiments/pos-ontonotes-random-memorization-top/vocab
09/16 06:46:52 AM: 	Vocab namespace bert_uncased: size 30524
09/16 06:46:52 AM: 	Vocab namespace tokens: size 24015
09/16 06:46:52 AM: 	Vocab namespace edges-pos-ontonotes_labels: size 48
09/16 06:46:52 AM: 	Vocab namespace chars: size 81
09/16 06:46:52 AM: 	Finished building vocab.
09/16 06:46:52 AM: 	Task edges-pos-ontonotes (train): Indexing from scratch.
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (train): Saved 110514 instances to ./experiments/pos-ontonotes-random-memorization-top/preproc/edges-pos-ontonotes__train_data
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (val): Indexing from scratch.
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (val): Saved 15060 instances to ./experiments/pos-ontonotes-random-memorization-top/preproc/edges-pos-ontonotes__val_data
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (test): Indexing from scratch.
09/16 06:47:27 AM: 	Task edges-pos-ontonotes (test): Saved 11462 instances to ./experiments/pos-ontonotes-random-memorization-top/preproc/edges-pos-ontonotes__test_data
09/16 06:47:27 AM: 	Finished indexing tasks
09/16 06:47:27 AM: 	Creating trimmed target-only version of edges-pos-ontonotes train.
09/16 06:47:27 AM: 	  Training on 
09/16 06:47:27 AM: 	  Evaluating on edges-pos-ontonotes
09/16 06:47:27 AM: 	Finished loading tasks in 68.442s
09/16 06:47:27 AM: 	 Tasks: ['edges-pos-ontonotes']
09/16 06:47:27 AM: Building model...
09/16 06:47:27 AM: Using BERT model (bert-base-uncased).
09/16 06:47:27 AM: LOADING A FUNETUNED MODEL from: 
09/16 06:47:27 AM: models/random-memorization
09/16 06:47:27 AM: loading configuration file models/random-memorization/config.json
09/16 06:47:27 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "random-memorization",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 06:47:27 AM: loading weights file models/random-memorization/pytorch_model.bin
09/16 06:47:30 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpxcfdxmxl
09/16 06:47:32 AM: copying /tmp/tmpxcfdxmxl to cache at ./experiments/pos-ontonotes-random-memorization-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: creating metadata file for ./experiments/pos-ontonotes-random-memorization-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: removing temp file /tmp/tmpxcfdxmxl
09/16 06:47:32 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/pos-ontonotes-random-memorization-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: Initializing parameters
09/16 06:47:32 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 06:47:32 AM: 	Task 'edges-pos-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-pos-ontonotes"
}
09/16 06:47:36 AM: Model specification:
09/16 06:47:36 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-pos-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=48, bias=True)
    )
  )
)
09/16 06:47:36 AM: Model parameters:
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 24576 with torch.Size([48, 512])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 48 with torch.Size([48])
09/16 06:47:36 AM: Total number of parameters: 109703728 (1.09704e+08)
09/16 06:47:36 AM: Number of trainable parameters: 221488 (221488)
09/16 06:47:36 AM: Finished building model in 8.928s
09/16 06:47:36 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-pos-ontonotes 

09/16 06:48:37 AM: patience = 9
09/16 06:48:37 AM: val_interval = 1000
09/16 06:48:37 AM: max_vals = 250
09/16 06:48:37 AM: cuda_device = 0
09/16 06:48:37 AM: grad_norm = 5.0
09/16 06:48:37 AM: grad_clipping = None
09/16 06:48:37 AM: lr_decay = 0.99
09/16 06:48:37 AM: min_lr = 1e-06
09/16 06:48:37 AM: keep_all_checkpoints = 0
09/16 06:48:37 AM: val_data_limit = 5000
09/16 06:48:37 AM: max_epochs = -1
09/16 06:48:37 AM: dec_val_scale = 250
09/16 06:48:37 AM: training_data_fraction = 1
09/16 06:48:37 AM: type = adam
09/16 06:48:37 AM: parameter_groups = None
09/16 06:48:37 AM: Number of trainable parameters: 221488
09/16 06:48:37 AM: infer_type_and_cast = True
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: lr = 0.0001
09/16 06:48:37 AM: amsgrad = True
09/16 06:48:37 AM: type = reduce_on_plateau
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: mode = max
09/16 06:48:37 AM: factor = 0.5
09/16 06:48:37 AM: patience = 3
09/16 06:48:37 AM: threshold = 0.0001
09/16 06:48:37 AM: threshold_mode = abs
09/16 06:48:37 AM: verbose = True
09/16 06:48:37 AM: type = adam
09/16 06:48:37 AM: parameter_groups = None
09/16 06:48:37 AM: Number of trainable parameters: 221488
09/16 06:48:37 AM: infer_type_and_cast = True
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: lr = 0.0001
09/16 06:48:37 AM: amsgrad = True
09/16 06:48:37 AM: type = reduce_on_plateau
09/16 06:48:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:37 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:37 AM: mode = max
09/16 06:48:37 AM: factor = 0.5
09/16 06:48:37 AM: patience = 3
09/16 06:48:37 AM: threshold = 0.0001
09/16 06:48:37 AM: threshold_mode = abs
09/16 06:48:37 AM: verbose = True
09/16 06:48:37 AM: Starting training without restoring from a checkpoint.
09/16 06:48:37 AM: Training examples per task, before any subsampling: {'edges-pos-ontonotes': 110514}
09/16 06:48:37 AM: Beginning training with stopping criteria based on metric: edges-pos-ontonotes_f1
09/16 06:48:47 AM: Update 81: task edges-pos-ontonotes, batch 81 (81): mcc: 0.0025, acc: 0.0024, precision: 0.0220, recall: 0.0898, f1: 0.0354, edges-pos-ontonotes_loss: 0.2837
09/16 06:48:57 AM: Update 173: task edges-pos-ontonotes, batch 173 (173): mcc: 0.0088, acc: 0.0109, precision: 0.0270, recall: 0.0516, f1: 0.0355, edges-pos-ontonotes_loss: 0.1763
09/16 06:49:07 AM: Update 268: task edges-pos-ontonotes, batch 268 (268): mcc: 0.0247, acc: 0.0269, precision: 0.0424, recall: 0.0533, f1: 0.0472, edges-pos-ontonotes_loss: 0.1393
09/16 06:49:17 AM: Update 337: task edges-pos-ontonotes, batch 337 (337): mcc: 0.0322, acc: 0.0311, precision: 0.0529, recall: 0.0514, f1: 0.0521, edges-pos-ontonotes_loss: 0.1247
09/16 06:49:27 AM: Update 414: task edges-pos-ontonotes, batch 414 (414): mcc: 0.0433, acc: 0.0376, precision: 0.0692, recall: 0.0535, f1: 0.0604, edges-pos-ontonotes_loss: 0.1137
09/16 06:49:37 AM: Update 488: task edges-pos-ontonotes, batch 488 (488): mcc: 0.0678, acc: 0.0547, precision: 0.1028, recall: 0.0679, f1: 0.0818, edges-pos-ontonotes_loss: 0.1053
09/16 06:49:47 AM: Update 560: task edges-pos-ontonotes, batch 560 (560): mcc: 0.1000, acc: 0.0762, precision: 0.1487, recall: 0.0879, f1: 0.1105, edges-pos-ontonotes_loss: 0.0988
09/16 06:49:58 AM: Update 627: task edges-pos-ontonotes, batch 627 (627): mcc: 0.1330, acc: 0.0980, precision: 0.1966, recall: 0.1089, f1: 0.1401, edges-pos-ontonotes_loss: 0.0935
09/16 06:50:09 AM: Update 678: task edges-pos-ontonotes, batch 678 (678): mcc: 0.1683, acc: 0.1204, precision: 0.2491, recall: 0.1311, f1: 0.1718, edges-pos-ontonotes_loss: 0.0903
09/16 06:50:19 AM: Update 733: task edges-pos-ontonotes, batch 733 (733): mcc: 0.2028, acc: 0.1426, precision: 0.3003, recall: 0.1531, f1: 0.2028, edges-pos-ontonotes_loss: 0.0872
09/16 06:50:29 AM: Update 797: task edges-pos-ontonotes, batch 797 (797): mcc: 0.2359, acc: 0.1644, precision: 0.3486, recall: 0.1750, f1: 0.2330, edges-pos-ontonotes_loss: 0.0839
09/16 06:50:39 AM: Update 862: task edges-pos-ontonotes, batch 862 (862): mcc: 0.2665, acc: 0.1848, precision: 0.3924, recall: 0.1955, f1: 0.2610, edges-pos-ontonotes_loss: 0.0809
09/16 06:50:49 AM: Update 916: task edges-pos-ontonotes, batch 916 (916): mcc: 0.2967, acc: 0.2056, precision: 0.4340, recall: 0.2168, f1: 0.2892, edges-pos-ontonotes_loss: 0.0786
09/16 06:50:59 AM: Update 963: task edges-pos-ontonotes, batch 963 (963): mcc: 0.3179, acc: 0.2201, precision: 0.4631, recall: 0.2318, f1: 0.3090, edges-pos-ontonotes_loss: 0.0767
09/16 06:51:06 AM: ***** Step 1000 / Validation 1 *****
09/16 06:51:06 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:51:06 AM: Validating...
09/16 06:51:09 AM: Evaluate: task edges-pos-ontonotes, batch 21 (157): mcc: 0.6630, acc: 0.4700, precision: 0.9246, recall: 0.4820, f1: 0.6336, edges-pos-ontonotes_loss: 0.0364
09/16 06:51:19 AM: Evaluate: task edges-pos-ontonotes, batch 89 (157): mcc: 0.6852, acc: 0.4998, precision: 0.9303, recall: 0.5112, f1: 0.6598, edges-pos-ontonotes_loss: 0.0358
09/16 06:51:29 AM: Evaluate: task edges-pos-ontonotes, batch 136 (157): mcc: 0.6935, acc: 0.5136, precision: 0.9271, recall: 0.5253, f1: 0.6707, edges-pos-ontonotes_loss: 0.0350
09/16 06:51:33 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:51:33 AM: Best result seen so far for micro.
09/16 06:51:33 AM: Best result seen so far for macro.
09/16 06:51:33 AM: Updating LR scheduler:
09/16 06:51:33 AM: 	Best result seen so far for macro_avg: 0.671
09/16 06:51:33 AM: 	# validation passes without improvement: 0
09/16 06:51:33 AM: edges-pos-ontonotes_loss: training: 0.075313 validation: 0.035075
09/16 06:51:33 AM: macro_avg: validation: 0.671220
09/16 06:51:33 AM: micro_avg: validation: 0.000000
09/16 06:51:33 AM: edges-pos-ontonotes_mcc: training: 0.334563 validation: 0.693893
09/16 06:51:33 AM: edges-pos-ontonotes_acc: training: 0.231632 validation: 0.514228
09/16 06:51:33 AM: edges-pos-ontonotes_precision: training: 0.485530 validation: 0.926531
09/16 06:51:33 AM: edges-pos-ontonotes_recall: training: 0.243850 validation: 0.526218
09/16 06:51:33 AM: edges-pos-ontonotes_f1: training: 0.324650 validation: 0.671220
09/16 06:51:33 AM: Global learning rate: 0.0001
09/16 06:51:33 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 06:51:39 AM: Update 1031: task edges-pos-ontonotes, batch 31 (1031): mcc: 0.6522, acc: 0.4616, precision: 0.8933, recall: 0.4832, f1: 0.6272, edges-pos-ontonotes_loss: 0.0389
09/16 06:51:49 AM: Update 1085: task edges-pos-ontonotes, batch 85 (1085): mcc: 0.6631, acc: 0.4771, precision: 0.8942, recall: 0.4989, f1: 0.6405, edges-pos-ontonotes_loss: 0.0378
09/16 06:51:59 AM: Update 1141: task edges-pos-ontonotes, batch 141 (1141): mcc: 0.6719, acc: 0.4892, precision: 0.8963, recall: 0.5108, f1: 0.6508, edges-pos-ontonotes_loss: 0.0370
09/16 06:52:09 AM: Update 1196: task edges-pos-ontonotes, batch 196 (1196): mcc: 0.6779, acc: 0.4980, precision: 0.8952, recall: 0.5206, f1: 0.6583, edges-pos-ontonotes_loss: 0.0364
09/16 06:52:20 AM: Update 1249: task edges-pos-ontonotes, batch 249 (1249): mcc: 0.6850, acc: 0.5081, precision: 0.8955, recall: 0.5312, f1: 0.6668, edges-pos-ontonotes_loss: 0.0357
09/16 06:52:30 AM: Update 1253: task edges-pos-ontonotes, batch 253 (1253): mcc: 0.6851, acc: 0.5084, precision: 0.8952, recall: 0.5315, f1: 0.6670, edges-pos-ontonotes_loss: 0.0357
09/16 06:52:41 AM: Update 1310: task edges-pos-ontonotes, batch 310 (1310): mcc: 0.6909, acc: 0.5167, precision: 0.8955, recall: 0.5402, f1: 0.6739, edges-pos-ontonotes_loss: 0.0350
09/16 06:52:51 AM: Update 1367: task edges-pos-ontonotes, batch 367 (1367): mcc: 0.6963, acc: 0.5245, precision: 0.8959, recall: 0.5484, f1: 0.6803, edges-pos-ontonotes_loss: 0.0344
09/16 06:53:01 AM: Update 1424: task edges-pos-ontonotes, batch 424 (1424): mcc: 0.7020, acc: 0.5325, precision: 0.8964, recall: 0.5569, f1: 0.6870, edges-pos-ontonotes_loss: 0.0338
09/16 06:53:11 AM: Update 1477: task edges-pos-ontonotes, batch 477 (1477): mcc: 0.7073, acc: 0.5403, precision: 0.8964, recall: 0.5653, f1: 0.6934, edges-pos-ontonotes_loss: 0.0334
09/16 06:53:21 AM: Update 1531: task edges-pos-ontonotes, batch 531 (1531): mcc: 0.7120, acc: 0.5472, precision: 0.8968, recall: 0.5724, f1: 0.6988, edges-pos-ontonotes_loss: 0.0330
09/16 06:53:31 AM: Update 1582: task edges-pos-ontonotes, batch 582 (1582): mcc: 0.7155, acc: 0.5523, precision: 0.8970, recall: 0.5778, f1: 0.7029, edges-pos-ontonotes_loss: 0.0326
09/16 06:53:41 AM: Update 1637: task edges-pos-ontonotes, batch 637 (1637): mcc: 0.7197, acc: 0.5587, precision: 0.8967, recall: 0.5848, f1: 0.7079, edges-pos-ontonotes_loss: 0.0322
09/16 06:53:51 AM: Update 1693: task edges-pos-ontonotes, batch 693 (1693): mcc: 0.7238, acc: 0.5647, precision: 0.8970, recall: 0.5911, f1: 0.7126, edges-pos-ontonotes_loss: 0.0317
09/16 06:54:01 AM: Update 1747: task edges-pos-ontonotes, batch 747 (1747): mcc: 0.7272, acc: 0.5699, precision: 0.8968, recall: 0.5968, f1: 0.7167, edges-pos-ontonotes_loss: 0.0314
09/16 06:54:11 AM: Update 1807: task edges-pos-ontonotes, batch 807 (1807): mcc: 0.7305, acc: 0.5749, precision: 0.8968, recall: 0.6022, f1: 0.7205, edges-pos-ontonotes_loss: 0.0311
09/16 06:54:22 AM: Update 1862: task edges-pos-ontonotes, batch 862 (1862): mcc: 0.7341, acc: 0.5803, precision: 0.8970, recall: 0.6079, f1: 0.7247, edges-pos-ontonotes_loss: 0.0307
09/16 06:54:32 AM: Update 1902: task edges-pos-ontonotes, batch 902 (1902): mcc: 0.7367, acc: 0.5842, precision: 0.8967, recall: 0.6123, f1: 0.7277, edges-pos-ontonotes_loss: 0.0305
09/16 06:54:42 AM: Update 1973: task edges-pos-ontonotes, batch 973 (1973): mcc: 0.7397, acc: 0.5887, precision: 0.8967, recall: 0.6173, f1: 0.7312, edges-pos-ontonotes_loss: 0.0301
09/16 06:54:45 AM: ***** Step 2000 / Validation 2 *****
09/16 06:54:45 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:54:45 AM: Validating...
09/16 06:54:52 AM: Evaluate: task edges-pos-ontonotes, batch 48 (157): mcc: 0.7839, acc: 0.6549, precision: 0.9170, recall: 0.6764, f1: 0.7786, edges-pos-ontonotes_loss: 0.0261
09/16 06:55:02 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8067, acc: 0.6905, precision: 0.9211, recall: 0.7124, f1: 0.8034, edges-pos-ontonotes_loss: 0.0240
09/16 06:55:12 AM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.8144, acc: 0.7024, precision: 0.9231, recall: 0.7243, f1: 0.8117, edges-pos-ontonotes_loss: 0.0230
09/16 06:55:12 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:55:12 AM: Best result seen so far for macro.
09/16 06:55:12 AM: Updating LR scheduler:
09/16 06:55:12 AM: 	Best result seen so far for macro_avg: 0.812
09/16 06:55:12 AM: 	# validation passes without improvement: 0
09/16 06:55:12 AM: edges-pos-ontonotes_loss: training: 0.029973 validation: 0.023034
09/16 06:55:12 AM: macro_avg: validation: 0.811728
09/16 06:55:12 AM: micro_avg: validation: 0.000000
09/16 06:55:12 AM: edges-pos-ontonotes_mcc: training: 0.740868 validation: 0.814387
09/16 06:55:12 AM: edges-pos-ontonotes_acc: training: 0.590390 validation: 0.702393
09/16 06:55:12 AM: edges-pos-ontonotes_precision: training: 0.896805 validation: 0.923078
09/16 06:55:12 AM: edges-pos-ontonotes_recall: training: 0.619106 validation: 0.724351
09/16 06:55:12 AM: edges-pos-ontonotes_f1: training: 0.732520 validation: 0.811728
09/16 06:55:12 AM: Global learning rate: 0.0001
09/16 06:55:12 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 06:55:22 AM: Update 2073: task edges-pos-ontonotes, batch 73 (2073): mcc: 0.7944, acc: 0.6731, precision: 0.9021, recall: 0.7060, f1: 0.7921, edges-pos-ontonotes_loss: 0.0243
09/16 06:55:32 AM: Update 2140: task edges-pos-ontonotes, batch 140 (2140): mcc: 0.7974, acc: 0.6782, precision: 0.9005, recall: 0.7126, f1: 0.7956, edges-pos-ontonotes_loss: 0.0238
09/16 06:55:42 AM: Update 2203: task edges-pos-ontonotes, batch 203 (2203): mcc: 0.8006, acc: 0.6826, precision: 0.9020, recall: 0.7169, f1: 0.7989, edges-pos-ontonotes_loss: 0.0234
09/16 06:55:52 AM: Update 2299: task edges-pos-ontonotes, batch 299 (2299): mcc: 0.8057, acc: 0.6887, precision: 0.9058, recall: 0.7229, f1: 0.8041, edges-pos-ontonotes_loss: 0.0226
09/16 06:56:02 AM: Update 2392: task edges-pos-ontonotes, batch 392 (2392): mcc: 0.8109, acc: 0.6957, precision: 0.9091, recall: 0.7295, f1: 0.8094, edges-pos-ontonotes_loss: 0.0220
09/16 06:56:12 AM: Update 2482: task edges-pos-ontonotes, batch 482 (2482): mcc: 0.8159, acc: 0.7024, precision: 0.9117, recall: 0.7362, f1: 0.8146, edges-pos-ontonotes_loss: 0.0214
09/16 06:56:22 AM: Update 2575: task edges-pos-ontonotes, batch 575 (2575): mcc: 0.8178, acc: 0.7055, precision: 0.9125, recall: 0.7390, f1: 0.8166, edges-pos-ontonotes_loss: 0.0216
09/16 06:56:32 AM: Update 2680: task edges-pos-ontonotes, batch 680 (2680): mcc: 0.8201, acc: 0.7090, precision: 0.9135, recall: 0.7421, f1: 0.8190, edges-pos-ontonotes_loss: 0.0215
09/16 06:56:42 AM: Update 2785: task edges-pos-ontonotes, batch 785 (2785): mcc: 0.8227, acc: 0.7131, precision: 0.9144, recall: 0.7461, f1: 0.8217, edges-pos-ontonotes_loss: 0.0211
09/16 06:56:52 AM: Update 2889: task edges-pos-ontonotes, batch 889 (2889): mcc: 0.8213, acc: 0.7113, precision: 0.9127, recall: 0.7450, f1: 0.8203, edges-pos-ontonotes_loss: 0.0214
09/16 06:57:00 AM: ***** Step 3000 / Validation 3 *****
09/16 06:57:00 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:57:00 AM: Validating...
09/16 06:57:03 AM: Evaluate: task edges-pos-ontonotes, batch 20 (157): mcc: 0.8044, acc: 0.6860, precision: 0.9127, recall: 0.7151, f1: 0.8019, edges-pos-ontonotes_loss: 0.0212
09/16 06:57:13 AM: Evaluate: task edges-pos-ontonotes, batch 89 (157): mcc: 0.8317, acc: 0.7273, precision: 0.9224, recall: 0.7555, f1: 0.8307, edges-pos-ontonotes_loss: 0.0194
09/16 06:57:23 AM: Evaluate: task edges-pos-ontonotes, batch 136 (157): mcc: 0.8315, acc: 0.7285, precision: 0.9179, recall: 0.7589, f1: 0.8309, edges-pos-ontonotes_loss: 0.0195
09/16 06:57:27 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:57:27 AM: Best result seen so far for macro.
09/16 06:57:27 AM: Updating LR scheduler:
09/16 06:57:27 AM: 	Best result seen so far for macro_avg: 0.832
09/16 06:57:27 AM: 	# validation passes without improvement: 0
09/16 06:57:27 AM: edges-pos-ontonotes_loss: training: 0.021437 validation: 0.019408
09/16 06:57:27 AM: macro_avg: validation: 0.832159
09/16 06:57:27 AM: micro_avg: validation: 0.000000
09/16 06:57:27 AM: edges-pos-ontonotes_mcc: training: 0.819995 validation: 0.832682
09/16 06:57:27 AM: edges-pos-ontonotes_acc: training: 0.709718 validation: 0.731018
09/16 06:57:27 AM: edges-pos-ontonotes_precision: training: 0.911408 validation: 0.917556
09/16 06:57:27 AM: edges-pos-ontonotes_recall: training: 0.743712 validation: 0.761305
09/16 06:57:27 AM: edges-pos-ontonotes_f1: training: 0.819065 validation: 0.832159
09/16 06:57:27 AM: Global learning rate: 0.0001
09/16 06:57:27 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 06:57:33 AM: Update 3075: task edges-pos-ontonotes, batch 75 (3075): mcc: 0.8056, acc: 0.6919, precision: 0.8954, recall: 0.7312, f1: 0.8050, edges-pos-ontonotes_loss: 0.0221
09/16 06:57:43 AM: Update 3152: task edges-pos-ontonotes, batch 152 (3152): mcc: 0.8052, acc: 0.6929, precision: 0.8883, recall: 0.7365, f1: 0.8053, edges-pos-ontonotes_loss: 0.0221
09/16 06:57:53 AM: Update 3208: task edges-pos-ontonotes, batch 208 (3208): mcc: 0.8009, acc: 0.6874, precision: 0.8846, recall: 0.7319, f1: 0.8010, edges-pos-ontonotes_loss: 0.0226
09/16 06:58:03 AM: Update 3269: task edges-pos-ontonotes, batch 269 (3269): mcc: 0.8005, acc: 0.6871, precision: 0.8837, recall: 0.7318, f1: 0.8006, edges-pos-ontonotes_loss: 0.0230
09/16 06:58:14 AM: Update 3329: task edges-pos-ontonotes, batch 329 (3329): mcc: 0.8013, acc: 0.6883, precision: 0.8840, recall: 0.7330, f1: 0.8015, edges-pos-ontonotes_loss: 0.0232
09/16 06:58:24 AM: Update 3402: task edges-pos-ontonotes, batch 402 (3402): mcc: 0.8023, acc: 0.6895, precision: 0.8854, recall: 0.7337, f1: 0.8024, edges-pos-ontonotes_loss: 0.0231
09/16 06:58:42 AM: Update 3461: task edges-pos-ontonotes, batch 461 (3461): mcc: 0.8024, acc: 0.6899, precision: 0.8853, recall: 0.7340, f1: 0.8026, edges-pos-ontonotes_loss: 0.0230
09/16 06:58:52 AM: Update 3560: task edges-pos-ontonotes, batch 560 (3560): mcc: 0.8039, acc: 0.6914, precision: 0.8877, recall: 0.7346, f1: 0.8039, edges-pos-ontonotes_loss: 0.0227
09/16 06:59:03 AM: Update 3657: task edges-pos-ontonotes, batch 657 (3657): mcc: 0.8056, acc: 0.6937, precision: 0.8891, recall: 0.7365, f1: 0.8057, edges-pos-ontonotes_loss: 0.0222
09/16 06:59:13 AM: Update 3742: task edges-pos-ontonotes, batch 742 (3742): mcc: 0.8073, acc: 0.6960, precision: 0.8902, recall: 0.7386, f1: 0.8073, edges-pos-ontonotes_loss: 0.0219
09/16 06:59:23 AM: Update 3810: task edges-pos-ontonotes, batch 810 (3810): mcc: 0.8091, acc: 0.6986, precision: 0.8913, recall: 0.7409, f1: 0.8092, edges-pos-ontonotes_loss: 0.0217
09/16 06:59:33 AM: Update 3878: task edges-pos-ontonotes, batch 878 (3878): mcc: 0.8108, acc: 0.7010, precision: 0.8920, recall: 0.7434, f1: 0.8109, edges-pos-ontonotes_loss: 0.0215
09/16 06:59:43 AM: Update 3956: task edges-pos-ontonotes, batch 956 (3956): mcc: 0.8125, acc: 0.7035, precision: 0.8928, recall: 0.7457, f1: 0.8127, edges-pos-ontonotes_loss: 0.0213
09/16 06:59:48 AM: ***** Step 4000 / Validation 4 *****
09/16 06:59:48 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:59:48 AM: Validating...
09/16 06:59:53 AM: Evaluate: task edges-pos-ontonotes, batch 34 (157): mcc: 0.8576, acc: 0.7696, precision: 0.9339, recall: 0.7925, f1: 0.8574, edges-pos-ontonotes_loss: 0.0163
09/16 07:00:03 AM: Evaluate: task edges-pos-ontonotes, batch 99 (157): mcc: 0.8685, acc: 0.7884, precision: 0.9345, recall: 0.8119, f1: 0.8689, edges-pos-ontonotes_loss: 0.0155
09/16 07:00:13 AM: Evaluate: task edges-pos-ontonotes, batch 146 (157): mcc: 0.8645, acc: 0.7839, precision: 0.9310, recall: 0.8077, f1: 0.8649, edges-pos-ontonotes_loss: 0.0159
09/16 07:00:15 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:00:15 AM: Best result seen so far for macro.
09/16 07:00:15 AM: Updating LR scheduler:
09/16 07:00:15 AM: 	Best result seen so far for macro_avg: 0.866
09/16 07:00:15 AM: 	# validation passes without improvement: 0
09/16 07:00:15 AM: edges-pos-ontonotes_loss: training: 0.021159 validation: 0.015875
09/16 07:00:15 AM: macro_avg: validation: 0.865593
09/16 07:00:15 AM: micro_avg: validation: 0.000000
09/16 07:00:15 AM: edges-pos-ontonotes_mcc: training: 0.813374 validation: 0.865184
09/16 07:00:15 AM: edges-pos-ontonotes_acc: training: 0.704778 validation: 0.785147
09/16 07:00:15 AM: edges-pos-ontonotes_precision: training: 0.893249 validation: 0.931346
09/16 07:00:15 AM: edges-pos-ontonotes_recall: training: 0.747015 validation: 0.808512
09/16 07:00:15 AM: edges-pos-ontonotes_f1: training: 0.813613 validation: 0.865593
09/16 07:00:15 AM: Global learning rate: 0.0001
09/16 07:00:15 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:00:23 AM: Update 4060: task edges-pos-ontonotes, batch 60 (4060): mcc: 0.8298, acc: 0.7288, precision: 0.8996, recall: 0.7714, f1: 0.8306, edges-pos-ontonotes_loss: 0.0197
09/16 07:00:33 AM: Update 4106: task edges-pos-ontonotes, batch 106 (4106): mcc: 0.8281, acc: 0.7278, precision: 0.8971, recall: 0.7704, f1: 0.8289, edges-pos-ontonotes_loss: 0.0201
09/16 07:00:43 AM: Update 4163: task edges-pos-ontonotes, batch 163 (4163): mcc: 0.8253, acc: 0.7238, precision: 0.8957, recall: 0.7667, f1: 0.8262, edges-pos-ontonotes_loss: 0.0207
09/16 07:00:53 AM: Update 4225: task edges-pos-ontonotes, batch 225 (4225): mcc: 0.8252, acc: 0.7237, precision: 0.8955, recall: 0.7665, f1: 0.8260, edges-pos-ontonotes_loss: 0.0207
09/16 07:01:03 AM: Update 4286: task edges-pos-ontonotes, batch 286 (4286): mcc: 0.8259, acc: 0.7249, precision: 0.8952, recall: 0.7681, f1: 0.8268, edges-pos-ontonotes_loss: 0.0207
09/16 07:01:14 AM: Update 4341: task edges-pos-ontonotes, batch 341 (4341): mcc: 0.8265, acc: 0.7260, precision: 0.8954, recall: 0.7690, f1: 0.8274, edges-pos-ontonotes_loss: 0.0206
09/16 07:01:24 AM: Update 4398: task edges-pos-ontonotes, batch 398 (4398): mcc: 0.8269, acc: 0.7265, precision: 0.8959, recall: 0.7692, f1: 0.8277, edges-pos-ontonotes_loss: 0.0205
09/16 07:01:34 AM: Update 4442: task edges-pos-ontonotes, batch 442 (4442): mcc: 0.8263, acc: 0.7255, precision: 0.8954, recall: 0.7686, f1: 0.8272, edges-pos-ontonotes_loss: 0.0205
09/16 07:01:44 AM: Update 4501: task edges-pos-ontonotes, batch 501 (4501): mcc: 0.8268, acc: 0.7263, precision: 0.8957, recall: 0.7692, f1: 0.8277, edges-pos-ontonotes_loss: 0.0204
09/16 07:01:54 AM: Update 4552: task edges-pos-ontonotes, batch 552 (4552): mcc: 0.8267, acc: 0.7262, precision: 0.8957, recall: 0.7692, f1: 0.8276, edges-pos-ontonotes_loss: 0.0204
09/16 07:02:04 AM: Update 4607: task edges-pos-ontonotes, batch 607 (4607): mcc: 0.8273, acc: 0.7271, precision: 0.8961, recall: 0.7698, f1: 0.8282, edges-pos-ontonotes_loss: 0.0203
09/16 07:02:14 AM: Update 4661: task edges-pos-ontonotes, batch 661 (4661): mcc: 0.8279, acc: 0.7282, precision: 0.8966, recall: 0.7706, f1: 0.8288, edges-pos-ontonotes_loss: 0.0202
09/16 07:02:26 AM: Update 4713: task edges-pos-ontonotes, batch 713 (4713): mcc: 0.8284, acc: 0.7290, precision: 0.8969, recall: 0.7712, f1: 0.8293, edges-pos-ontonotes_loss: 0.0201
09/16 07:02:36 AM: Update 4769: task edges-pos-ontonotes, batch 769 (4769): mcc: 0.8287, acc: 0.7294, precision: 0.8971, recall: 0.7716, f1: 0.8296, edges-pos-ontonotes_loss: 0.0201
09/16 07:02:46 AM: Update 4827: task edges-pos-ontonotes, batch 827 (4827): mcc: 0.8294, acc: 0.7304, precision: 0.8974, recall: 0.7725, f1: 0.8303, edges-pos-ontonotes_loss: 0.0200
09/16 07:02:56 AM: Update 4884: task edges-pos-ontonotes, batch 884 (4884): mcc: 0.8301, acc: 0.7315, precision: 0.8979, recall: 0.7734, f1: 0.8310, edges-pos-ontonotes_loss: 0.0199
09/16 07:03:06 AM: Update 4943: task edges-pos-ontonotes, batch 943 (4943): mcc: 0.8306, acc: 0.7325, precision: 0.8981, recall: 0.7742, f1: 0.8316, edges-pos-ontonotes_loss: 0.0198
09/16 07:03:16 AM: Update 4996: task edges-pos-ontonotes, batch 996 (4996): mcc: 0.8313, acc: 0.7336, precision: 0.8985, recall: 0.7751, f1: 0.8322, edges-pos-ontonotes_loss: 0.0197
09/16 07:03:17 AM: ***** Step 5000 / Validation 5 *****
09/16 07:03:17 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:03:17 AM: Validating...
09/16 07:03:26 AM: Evaluate: task edges-pos-ontonotes, batch 67 (157): mcc: 0.8687, acc: 0.7904, precision: 0.9303, recall: 0.8159, f1: 0.8694, edges-pos-ontonotes_loss: 0.0155
09/16 07:03:36 AM: Evaluate: task edges-pos-ontonotes, batch 121 (157): mcc: 0.8757, acc: 0.8031, precision: 0.9325, recall: 0.8269, f1: 0.8766, edges-pos-ontonotes_loss: 0.0149
09/16 07:03:44 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:03:44 AM: Best result seen so far for macro.
09/16 07:03:44 AM: Updating LR scheduler:
09/16 07:03:44 AM: 	Best result seen so far for macro_avg: 0.879
09/16 07:03:44 AM: 	# validation passes without improvement: 0
09/16 07:03:44 AM: edges-pos-ontonotes_loss: training: 0.019758 validation: 0.014614
09/16 07:03:44 AM: macro_avg: validation: 0.878798
09/16 07:03:44 AM: micro_avg: validation: 0.000000
09/16 07:03:44 AM: edges-pos-ontonotes_mcc: training: 0.831307 validation: 0.877868
09/16 07:03:44 AM: edges-pos-ontonotes_acc: training: 0.733662 validation: 0.807285
09/16 07:03:44 AM: edges-pos-ontonotes_precision: training: 0.898447 validation: 0.932165
09/16 07:03:44 AM: edges-pos-ontonotes_recall: training: 0.775154 validation: 0.831212
09/16 07:03:44 AM: edges-pos-ontonotes_f1: training: 0.832259 validation: 0.878798
09/16 07:03:44 AM: Global learning rate: 0.0001
09/16 07:03:44 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:03:46 AM: Update 5015: task edges-pos-ontonotes, batch 15 (5015): mcc: 0.8440, acc: 0.7550, precision: 0.9060, recall: 0.7917, f1: 0.8450, edges-pos-ontonotes_loss: 0.0181
09/16 07:03:58 AM: Update 5026: task edges-pos-ontonotes, batch 26 (5026): mcc: 0.8419, acc: 0.7504, precision: 0.9051, recall: 0.7888, f1: 0.8430, edges-pos-ontonotes_loss: 0.0181
09/16 07:04:08 AM: Update 5088: task edges-pos-ontonotes, batch 88 (5088): mcc: 0.8396, acc: 0.7474, precision: 0.9033, recall: 0.7861, f1: 0.8406, edges-pos-ontonotes_loss: 0.0190
09/16 07:04:18 AM: Update 5142: task edges-pos-ontonotes, batch 142 (5142): mcc: 0.8407, acc: 0.7486, precision: 0.9033, recall: 0.7881, f1: 0.8418, edges-pos-ontonotes_loss: 0.0188
09/16 07:04:28 AM: Update 5202: task edges-pos-ontonotes, batch 202 (5202): mcc: 0.8410, acc: 0.7492, precision: 0.9034, recall: 0.7887, f1: 0.8421, edges-pos-ontonotes_loss: 0.0187
09/16 07:04:38 AM: Update 5258: task edges-pos-ontonotes, batch 258 (5258): mcc: 0.8413, acc: 0.7499, precision: 0.9027, recall: 0.7897, f1: 0.8425, edges-pos-ontonotes_loss: 0.0187
09/16 07:04:48 AM: Update 5312: task edges-pos-ontonotes, batch 312 (5312): mcc: 0.8418, acc: 0.7508, precision: 0.9031, recall: 0.7904, f1: 0.8430, edges-pos-ontonotes_loss: 0.0186
09/16 07:04:58 AM: Update 5352: task edges-pos-ontonotes, batch 352 (5352): mcc: 0.8419, acc: 0.7510, precision: 0.9028, recall: 0.7909, f1: 0.8431, edges-pos-ontonotes_loss: 0.0185
09/16 07:05:08 AM: Update 5421: task edges-pos-ontonotes, batch 421 (5421): mcc: 0.8433, acc: 0.7527, precision: 0.9036, recall: 0.7927, f1: 0.8445, edges-pos-ontonotes_loss: 0.0182
09/16 07:05:18 AM: Update 5495: task edges-pos-ontonotes, batch 495 (5495): mcc: 0.8446, acc: 0.7542, precision: 0.9045, recall: 0.7943, f1: 0.8458, edges-pos-ontonotes_loss: 0.0179
09/16 07:05:29 AM: Update 5571: task edges-pos-ontonotes, batch 571 (5571): mcc: 0.8458, acc: 0.7557, precision: 0.9053, recall: 0.7958, f1: 0.8470, edges-pos-ontonotes_loss: 0.0176
09/16 07:05:39 AM: Update 5630: task edges-pos-ontonotes, batch 630 (5630): mcc: 0.8466, acc: 0.7569, precision: 0.9056, recall: 0.7970, f1: 0.8478, edges-pos-ontonotes_loss: 0.0175
09/16 07:05:49 AM: Update 5702: task edges-pos-ontonotes, batch 702 (5702): mcc: 0.8481, acc: 0.7588, precision: 0.9068, recall: 0.7988, f1: 0.8494, edges-pos-ontonotes_loss: 0.0172
09/16 07:05:59 AM: Update 5799: task edges-pos-ontonotes, batch 799 (5799): mcc: 0.8509, acc: 0.7623, precision: 0.9087, recall: 0.8021, f1: 0.8521, edges-pos-ontonotes_loss: 0.0169
09/16 07:06:09 AM: Update 5892: task edges-pos-ontonotes, batch 892 (5892): mcc: 0.8533, acc: 0.7656, precision: 0.9103, recall: 0.8052, f1: 0.8545, edges-pos-ontonotes_loss: 0.0165
09/16 07:06:19 AM: Update 5971: task edges-pos-ontonotes, batch 971 (5971): mcc: 0.8550, acc: 0.7680, precision: 0.9114, recall: 0.8074, f1: 0.8563, edges-pos-ontonotes_loss: 0.0163
09/16 07:06:21 AM: ***** Step 6000 / Validation 6 *****
09/16 07:06:21 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:06:21 AM: Validating...
09/16 07:06:29 AM: Evaluate: task edges-pos-ontonotes, batch 53 (157): mcc: 0.8705, acc: 0.7913, precision: 0.9354, recall: 0.8147, f1: 0.8709, edges-pos-ontonotes_loss: 0.0151
09/16 07:06:39 AM: Evaluate: task edges-pos-ontonotes, batch 113 (157): mcc: 0.8749, acc: 0.7997, precision: 0.9377, recall: 0.8208, f1: 0.8754, edges-pos-ontonotes_loss: 0.0146
09/16 07:06:48 AM: Updating LR scheduler:
09/16 07:06:48 AM: 	Best result seen so far for macro_avg: 0.879
09/16 07:06:48 AM: 	# validation passes without improvement: 1
09/16 07:06:48 AM: edges-pos-ontonotes_loss: training: 0.016313 validation: 0.014436
09/16 07:06:48 AM: macro_avg: validation: 0.875461
09/16 07:06:48 AM: micro_avg: validation: 0.000000
09/16 07:06:48 AM: edges-pos-ontonotes_mcc: training: 0.855307 validation: 0.875056
09/16 07:06:48 AM: edges-pos-ontonotes_acc: training: 0.768470 validation: 0.800258
09/16 07:06:48 AM: edges-pos-ontonotes_precision: training: 0.911658 validation: 0.938629
09/16 07:06:48 AM: edges-pos-ontonotes_recall: training: 0.807714 validation: 0.820259
09/16 07:06:48 AM: edges-pos-ontonotes_f1: training: 0.856544 validation: 0.875461
09/16 07:06:48 AM: Global learning rate: 0.0001
09/16 07:06:48 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:06:49 AM: Update 6012: task edges-pos-ontonotes, batch 12 (6012): mcc: 0.8700, acc: 0.7869, precision: 0.9263, recall: 0.8218, f1: 0.8709, edges-pos-ontonotes_loss: 0.0169
09/16 07:06:59 AM: Update 6126: task edges-pos-ontonotes, batch 126 (6126): mcc: 0.8748, acc: 0.7956, precision: 0.9259, recall: 0.8312, f1: 0.8760, edges-pos-ontonotes_loss: 0.0153
09/16 07:07:10 AM: Update 6231: task edges-pos-ontonotes, batch 231 (6231): mcc: 0.8762, acc: 0.7977, precision: 0.9259, recall: 0.8338, f1: 0.8774, edges-pos-ontonotes_loss: 0.0148
09/16 07:07:20 AM: Update 6342: task edges-pos-ontonotes, batch 342 (6342): mcc: 0.8701, acc: 0.7896, precision: 0.9205, recall: 0.8273, f1: 0.8714, edges-pos-ontonotes_loss: 0.0156
09/16 07:07:30 AM: Update 6469: task edges-pos-ontonotes, batch 469 (6469): mcc: 0.8616, acc: 0.7772, precision: 0.9145, recall: 0.8170, f1: 0.8630, edges-pos-ontonotes_loss: 0.0164
09/16 07:07:40 AM: Update 6592: task edges-pos-ontonotes, batch 592 (6592): mcc: 0.8581, acc: 0.7723, precision: 0.9115, recall: 0.8131, f1: 0.8595, edges-pos-ontonotes_loss: 0.0167
09/16 07:07:50 AM: Update 6666: task edges-pos-ontonotes, batch 666 (6666): mcc: 0.8524, acc: 0.7637, precision: 0.9071, recall: 0.8064, f1: 0.8538, edges-pos-ontonotes_loss: 0.0171
09/16 07:08:00 AM: Update 6721: task edges-pos-ontonotes, batch 721 (6721): mcc: 0.8492, acc: 0.7593, precision: 0.9041, recall: 0.8032, f1: 0.8507, edges-pos-ontonotes_loss: 0.0173
09/16 07:08:11 AM: Update 6776: task edges-pos-ontonotes, batch 776 (6776): mcc: 0.8475, acc: 0.7570, precision: 0.9021, recall: 0.8017, f1: 0.8490, edges-pos-ontonotes_loss: 0.0175
09/16 07:08:21 AM: Update 6837: task edges-pos-ontonotes, batch 837 (6837): mcc: 0.8460, acc: 0.7550, precision: 0.9007, recall: 0.8003, f1: 0.8475, edges-pos-ontonotes_loss: 0.0176
09/16 07:08:31 AM: Update 6904: task edges-pos-ontonotes, batch 904 (6904): mcc: 0.8453, acc: 0.7541, precision: 0.9002, recall: 0.7994, f1: 0.8468, edges-pos-ontonotes_loss: 0.0177
09/16 07:08:41 AM: Update 6977: task edges-pos-ontonotes, batch 977 (6977): mcc: 0.8444, acc: 0.7527, precision: 0.8999, recall: 0.7980, f1: 0.8459, edges-pos-ontonotes_loss: 0.0177
09/16 07:08:44 AM: ***** Step 7000 / Validation 7 *****
09/16 07:08:44 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:08:44 AM: Validating...
09/16 07:08:51 AM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.8790, acc: 0.8053, precision: 0.9385, recall: 0.8276, f1: 0.8796, edges-pos-ontonotes_loss: 0.0141
09/16 07:09:01 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8844, acc: 0.8157, precision: 0.9359, recall: 0.8400, f1: 0.8854, edges-pos-ontonotes_loss: 0.0135
09/16 07:09:11 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.8833, acc: 0.8151, precision: 0.9340, recall: 0.8397, f1: 0.8844, edges-pos-ontonotes_loss: 0.0136
09/16 07:09:11 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:09:11 AM: Best result seen so far for macro.
09/16 07:09:11 AM: Updating LR scheduler:
09/16 07:09:11 AM: 	Best result seen so far for macro_avg: 0.884
09/16 07:09:11 AM: 	# validation passes without improvement: 0
09/16 07:09:11 AM: edges-pos-ontonotes_loss: training: 0.017762 validation: 0.013530
09/16 07:09:11 AM: macro_avg: validation: 0.884452
09/16 07:09:11 AM: micro_avg: validation: 0.000000
09/16 07:09:11 AM: edges-pos-ontonotes_mcc: training: 0.844268 validation: 0.883426
09/16 07:09:11 AM: edges-pos-ontonotes_acc: training: 0.752442 validation: 0.815116
09/16 07:09:11 AM: edges-pos-ontonotes_precision: training: 0.899941 validation: 0.934087
09/16 07:09:11 AM: edges-pos-ontonotes_recall: training: 0.797723 validation: 0.839826
09/16 07:09:11 AM: edges-pos-ontonotes_f1: training: 0.845755 validation: 0.884452
09/16 07:09:11 AM: Global learning rate: 0.0001
09/16 07:09:11 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:09:21 AM: Update 7093: task edges-pos-ontonotes, batch 93 (7093): mcc: 0.8465, acc: 0.7538, precision: 0.9067, recall: 0.7959, f1: 0.8477, edges-pos-ontonotes_loss: 0.0164
09/16 07:09:31 AM: Update 7187: task edges-pos-ontonotes, batch 187 (7187): mcc: 0.8431, acc: 0.7495, precision: 0.9043, recall: 0.7917, f1: 0.8443, edges-pos-ontonotes_loss: 0.0168
09/16 07:09:41 AM: Update 7256: task edges-pos-ontonotes, batch 256 (7256): mcc: 0.8448, acc: 0.7520, precision: 0.9038, recall: 0.7953, f1: 0.8460, edges-pos-ontonotes_loss: 0.0169
09/16 07:09:51 AM: Update 7327: task edges-pos-ontonotes, batch 327 (7327): mcc: 0.8465, acc: 0.7543, precision: 0.9039, recall: 0.7982, f1: 0.8478, edges-pos-ontonotes_loss: 0.0169
09/16 07:10:01 AM: Update 7404: task edges-pos-ontonotes, batch 404 (7404): mcc: 0.8467, acc: 0.7544, precision: 0.9037, recall: 0.7988, f1: 0.8480, edges-pos-ontonotes_loss: 0.0169
09/16 07:10:11 AM: Update 7474: task edges-pos-ontonotes, batch 474 (7474): mcc: 0.8473, acc: 0.7556, precision: 0.9034, recall: 0.8002, f1: 0.8487, edges-pos-ontonotes_loss: 0.0170
09/16 07:10:31 AM: Update 7547: task edges-pos-ontonotes, batch 547 (7547): mcc: 0.8476, acc: 0.7561, precision: 0.9034, recall: 0.8007, f1: 0.8490, edges-pos-ontonotes_loss: 0.0169
09/16 07:10:41 AM: Update 7598: task edges-pos-ontonotes, batch 598 (7598): mcc: 0.8464, acc: 0.7549, precision: 0.9022, recall: 0.7996, f1: 0.8478, edges-pos-ontonotes_loss: 0.0171
09/16 07:10:51 AM: Update 7656: task edges-pos-ontonotes, batch 656 (7656): mcc: 0.8461, acc: 0.7547, precision: 0.9015, recall: 0.7998, f1: 0.8476, edges-pos-ontonotes_loss: 0.0172
09/16 07:11:01 AM: Update 7709: task edges-pos-ontonotes, batch 709 (7709): mcc: 0.8458, acc: 0.7545, precision: 0.9006, recall: 0.7999, f1: 0.8473, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:11 AM: Update 7776: task edges-pos-ontonotes, batch 776 (7776): mcc: 0.8462, acc: 0.7551, precision: 0.9011, recall: 0.8004, f1: 0.8477, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:21 AM: Update 7832: task edges-pos-ontonotes, batch 832 (7832): mcc: 0.8465, acc: 0.7556, precision: 0.9011, recall: 0.8009, f1: 0.8481, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:32 AM: Update 7875: task edges-pos-ontonotes, batch 875 (7875): mcc: 0.8461, acc: 0.7551, precision: 0.9007, recall: 0.8005, f1: 0.8477, edges-pos-ontonotes_loss: 0.0174
09/16 07:11:42 AM: Update 7930: task edges-pos-ontonotes, batch 930 (7930): mcc: 0.8461, acc: 0.7550, precision: 0.9005, recall: 0.8005, f1: 0.8476, edges-pos-ontonotes_loss: 0.0175
09/16 07:11:52 AM: Update 7982: task edges-pos-ontonotes, batch 982 (7982): mcc: 0.8462, acc: 0.7554, precision: 0.9006, recall: 0.8008, f1: 0.8478, edges-pos-ontonotes_loss: 0.0175
09/16 07:11:55 AM: ***** Step 8000 / Validation 8 *****
09/16 07:11:55 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:11:55 AM: Validating...
09/16 07:12:02 AM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.8818, acc: 0.8106, precision: 0.9372, recall: 0.8340, f1: 0.8826, edges-pos-ontonotes_loss: 0.0137
09/16 07:12:12 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8895, acc: 0.8238, precision: 0.9382, recall: 0.8474, f1: 0.8905, edges-pos-ontonotes_loss: 0.0130
09/16 07:12:22 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.8904, acc: 0.8266, precision: 0.9369, recall: 0.8503, f1: 0.8915, edges-pos-ontonotes_loss: 0.0128
09/16 07:12:22 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:12:22 AM: Best result seen so far for macro.
09/16 07:12:22 AM: Updating LR scheduler:
09/16 07:12:22 AM: 	Best result seen so far for macro_avg: 0.892
09/16 07:12:22 AM: 	# validation passes without improvement: 0
09/16 07:12:22 AM: edges-pos-ontonotes_loss: training: 0.017458 validation: 0.012789
09/16 07:12:22 AM: macro_avg: validation: 0.891699
09/16 07:12:22 AM: micro_avg: validation: 0.000000
09/16 07:12:22 AM: edges-pos-ontonotes_mcc: training: 0.846406 validation: 0.890591
09/16 07:12:22 AM: edges-pos-ontonotes_acc: training: 0.755633 validation: 0.826873
09/16 07:12:22 AM: edges-pos-ontonotes_precision: training: 0.900767 validation: 0.936961
09/16 07:12:22 AM: edges-pos-ontonotes_recall: training: 0.800951 validation: 0.850609
09/16 07:12:22 AM: edges-pos-ontonotes_f1: training: 0.847932 validation: 0.891699
09/16 07:12:22 AM: Global learning rate: 0.0001
09/16 07:12:22 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:12:32 AM: Update 8056: task edges-pos-ontonotes, batch 56 (8056): mcc: 0.8483, acc: 0.7586, precision: 0.9013, recall: 0.8040, f1: 0.8499, edges-pos-ontonotes_loss: 0.0176
09/16 07:12:42 AM: Update 8110: task edges-pos-ontonotes, batch 110 (8110): mcc: 0.8495, acc: 0.7611, precision: 0.9024, recall: 0.8052, f1: 0.8510, edges-pos-ontonotes_loss: 0.0174
09/16 07:12:52 AM: Update 8166: task edges-pos-ontonotes, batch 166 (8166): mcc: 0.8493, acc: 0.7613, precision: 0.9021, recall: 0.8052, f1: 0.8509, edges-pos-ontonotes_loss: 0.0173
09/16 07:13:02 AM: Update 8207: task edges-pos-ontonotes, batch 207 (8207): mcc: 0.8487, acc: 0.7608, precision: 0.9013, recall: 0.8047, f1: 0.8503, edges-pos-ontonotes_loss: 0.0174
09/16 07:13:12 AM: Update 8263: task edges-pos-ontonotes, batch 263 (8263): mcc: 0.8493, acc: 0.7619, precision: 0.9016, recall: 0.8057, f1: 0.8509, edges-pos-ontonotes_loss: 0.0174
09/16 07:13:22 AM: Update 8319: task edges-pos-ontonotes, batch 319 (8319): mcc: 0.8504, acc: 0.7634, precision: 0.9025, recall: 0.8068, f1: 0.8520, edges-pos-ontonotes_loss: 0.0174
09/16 07:13:32 AM: Update 8381: task edges-pos-ontonotes, batch 381 (8381): mcc: 0.8511, acc: 0.7646, precision: 0.9032, recall: 0.8076, f1: 0.8527, edges-pos-ontonotes_loss: 0.0173
09/16 07:13:42 AM: Update 8436: task edges-pos-ontonotes, batch 436 (8436): mcc: 0.8519, acc: 0.7656, precision: 0.9037, recall: 0.8085, f1: 0.8535, edges-pos-ontonotes_loss: 0.0173
09/16 07:13:53 AM: Update 8486: task edges-pos-ontonotes, batch 486 (8486): mcc: 0.8522, acc: 0.7660, precision: 0.9038, recall: 0.8091, f1: 0.8538, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:03 AM: Update 8537: task edges-pos-ontonotes, batch 537 (8537): mcc: 0.8523, acc: 0.7663, precision: 0.9037, recall: 0.8093, f1: 0.8539, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:13 AM: Update 8596: task edges-pos-ontonotes, batch 596 (8596): mcc: 0.8526, acc: 0.7667, precision: 0.9040, recall: 0.8097, f1: 0.8542, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:23 AM: Update 8651: task edges-pos-ontonotes, batch 651 (8651): mcc: 0.8529, acc: 0.7672, precision: 0.9041, recall: 0.8101, f1: 0.8545, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:34 AM: Update 8710: task edges-pos-ontonotes, batch 710 (8710): mcc: 0.8533, acc: 0.7679, precision: 0.9042, recall: 0.8106, f1: 0.8549, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:44 AM: Update 8767: task edges-pos-ontonotes, batch 767 (8767): mcc: 0.8531, acc: 0.7677, precision: 0.9041, recall: 0.8104, f1: 0.8547, edges-pos-ontonotes_loss: 0.0172
09/16 07:14:54 AM: Update 8812: task edges-pos-ontonotes, batch 812 (8812): mcc: 0.8533, acc: 0.7680, precision: 0.9040, recall: 0.8108, f1: 0.8549, edges-pos-ontonotes_loss: 0.0172
09/16 07:15:04 AM: Update 8874: task edges-pos-ontonotes, batch 874 (8874): mcc: 0.8540, acc: 0.7689, precision: 0.9044, recall: 0.8117, f1: 0.8556, edges-pos-ontonotes_loss: 0.0170
09/16 07:15:14 AM: Update 8946: task edges-pos-ontonotes, batch 946 (8946): mcc: 0.8545, acc: 0.7696, precision: 0.9047, recall: 0.8125, f1: 0.8561, edges-pos-ontonotes_loss: 0.0168
09/16 07:15:22 AM: ***** Step 9000 / Validation 9 *****
09/16 07:15:22 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:15:22 AM: Validating...
09/16 07:15:25 AM: Evaluate: task edges-pos-ontonotes, batch 15 (157): mcc: 0.8746, acc: 0.8032, precision: 0.9327, recall: 0.8246, f1: 0.8753, edges-pos-ontonotes_loss: 0.0139
09/16 07:15:35 AM: Evaluate: task edges-pos-ontonotes, batch 86 (157): mcc: 0.8868, acc: 0.8184, precision: 0.9392, recall: 0.8415, f1: 0.8877, edges-pos-ontonotes_loss: 0.0132
09/16 07:15:45 AM: Evaluate: task edges-pos-ontonotes, batch 133 (157): mcc: 0.8891, acc: 0.8229, precision: 0.9394, recall: 0.8457, f1: 0.8901, edges-pos-ontonotes_loss: 0.0129
09/16 07:15:49 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:15:49 AM: Best result seen so far for macro.
09/16 07:15:49 AM: Updating LR scheduler:
09/16 07:15:49 AM: 	Best result seen so far for macro_avg: 0.892
09/16 07:15:49 AM: 	# validation passes without improvement: 0
09/16 07:15:49 AM: edges-pos-ontonotes_loss: training: 0.016727 validation: 0.012708
09/16 07:15:49 AM: macro_avg: validation: 0.892097
09/16 07:15:49 AM: micro_avg: validation: 0.000000
09/16 07:15:49 AM: edges-pos-ontonotes_mcc: training: 0.855047 validation: 0.891102
09/16 07:15:49 AM: edges-pos-ontonotes_acc: training: 0.770321 validation: 0.827000
09/16 07:15:49 AM: edges-pos-ontonotes_precision: training: 0.905118 validation: 0.939530
09/16 07:15:49 AM: edges-pos-ontonotes_recall: training: 0.813116 validation: 0.849223
09/16 07:15:49 AM: edges-pos-ontonotes_f1: training: 0.856654 validation: 0.892097
09/16 07:15:49 AM: Global learning rate: 0.0001
09/16 07:15:49 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:15:55 AM: Update 9040: task edges-pos-ontonotes, batch 40 (9040): mcc: 0.8659, acc: 0.7868, precision: 0.9118, recall: 0.8273, f1: 0.8675, edges-pos-ontonotes_loss: 0.0149
09/16 07:16:14 AM: Update 9112: task edges-pos-ontonotes, batch 112 (9112): mcc: 0.8667, acc: 0.7871, precision: 0.9118, recall: 0.8290, f1: 0.8684, edges-pos-ontonotes_loss: 0.0145
09/16 07:16:24 AM: Update 9209: task edges-pos-ontonotes, batch 209 (9209): mcc: 0.8754, acc: 0.7980, precision: 0.9189, recall: 0.8387, f1: 0.8770, edges-pos-ontonotes_loss: 0.0138
09/16 07:16:34 AM: Update 9304: task edges-pos-ontonotes, batch 304 (9304): mcc: 0.8797, acc: 0.8034, precision: 0.9224, recall: 0.8435, f1: 0.8812, edges-pos-ontonotes_loss: 0.0135
09/16 07:16:44 AM: Update 9394: task edges-pos-ontonotes, batch 394 (9394): mcc: 0.8829, acc: 0.8081, precision: 0.9247, recall: 0.8473, f1: 0.8844, edges-pos-ontonotes_loss: 0.0132
09/16 07:16:54 AM: Update 9480: task edges-pos-ontonotes, batch 480 (9480): mcc: 0.8837, acc: 0.8096, precision: 0.9253, recall: 0.8484, f1: 0.8852, edges-pos-ontonotes_loss: 0.0133
09/16 07:17:04 AM: Update 9587: task edges-pos-ontonotes, batch 587 (9587): mcc: 0.8839, acc: 0.8099, precision: 0.9257, recall: 0.8484, f1: 0.8854, edges-pos-ontonotes_loss: 0.0134
09/16 07:17:15 AM: Update 9707: task edges-pos-ontonotes, batch 707 (9707): mcc: 0.8843, acc: 0.8106, precision: 0.9259, recall: 0.8488, f1: 0.8857, edges-pos-ontonotes_loss: 0.0134
09/16 07:17:25 AM: Update 9811: task edges-pos-ontonotes, batch 811 (9811): mcc: 0.8820, acc: 0.8075, precision: 0.9242, recall: 0.8462, f1: 0.8835, edges-pos-ontonotes_loss: 0.0138
09/16 07:17:35 AM: Update 9958: task edges-pos-ontonotes, batch 958 (9958): mcc: 0.8786, acc: 0.8027, precision: 0.9218, recall: 0.8421, f1: 0.8801, edges-pos-ontonotes_loss: 0.0143
09/16 07:17:39 AM: ***** Step 10000 / Validation 10 *****
09/16 07:17:39 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:17:39 AM: Validating...
09/16 07:17:45 AM: Evaluate: task edges-pos-ontonotes, batch 43 (157): mcc: 0.8782, acc: 0.8044, precision: 0.9355, recall: 0.8289, f1: 0.8790, edges-pos-ontonotes_loss: 0.0140
09/16 07:17:55 AM: Evaluate: task edges-pos-ontonotes, batch 105 (157): mcc: 0.8862, acc: 0.8191, precision: 0.9349, recall: 0.8442, f1: 0.8872, edges-pos-ontonotes_loss: 0.0132
09/16 07:18:05 AM: Evaluate: task edges-pos-ontonotes, batch 152 (157): mcc: 0.8849, acc: 0.8178, precision: 0.9326, recall: 0.8439, f1: 0.8861, edges-pos-ontonotes_loss: 0.0132
09/16 07:18:06 AM: Updating LR scheduler:
09/16 07:18:06 AM: 	Best result seen so far for macro_avg: 0.892
09/16 07:18:06 AM: 	# validation passes without improvement: 1
09/16 07:18:06 AM: edges-pos-ontonotes_loss: training: 0.014442 validation: 0.013191
09/16 07:18:06 AM: macro_avg: validation: 0.885978
09/16 07:18:06 AM: micro_avg: validation: 0.000000
09/16 07:18:06 AM: edges-pos-ontonotes_mcc: training: 0.877771 validation: 0.884823
09/16 07:18:06 AM: edges-pos-ontonotes_acc: training: 0.801459 validation: 0.817846
09/16 07:18:06 AM: edges-pos-ontonotes_precision: training: 0.921133 validation: 0.932524
09/16 07:18:06 AM: edges-pos-ontonotes_recall: training: 0.841059 validation: 0.843857
09/16 07:18:06 AM: edges-pos-ontonotes_f1: training: 0.879277 validation: 0.885978
09/16 07:18:06 AM: Global learning rate: 0.0001
09/16 07:18:06 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:18:15 AM: Update 10061: task edges-pos-ontonotes, batch 61 (10061): mcc: 0.8425, acc: 0.7521, precision: 0.8903, recall: 0.8032, f1: 0.8445, edges-pos-ontonotes_loss: 0.0177
09/16 07:18:25 AM: Update 10114: task edges-pos-ontonotes, batch 114 (10114): mcc: 0.8438, acc: 0.7521, precision: 0.8914, recall: 0.8046, f1: 0.8458, edges-pos-ontonotes_loss: 0.0178
09/16 07:18:36 AM: Update 10174: task edges-pos-ontonotes, batch 174 (10174): mcc: 0.8435, acc: 0.7519, precision: 0.8914, recall: 0.8040, f1: 0.8454, edges-pos-ontonotes_loss: 0.0180
09/16 07:18:46 AM: Update 10237: task edges-pos-ontonotes, batch 237 (10237): mcc: 0.8439, acc: 0.7525, precision: 0.8919, recall: 0.8043, f1: 0.8459, edges-pos-ontonotes_loss: 0.0182
09/16 07:18:56 AM: Update 10298: task edges-pos-ontonotes, batch 298 (10298): mcc: 0.8445, acc: 0.7533, precision: 0.8928, recall: 0.8046, f1: 0.8464, edges-pos-ontonotes_loss: 0.0181
09/16 07:19:06 AM: Update 10358: task edges-pos-ontonotes, batch 358 (10358): mcc: 0.8449, acc: 0.7539, precision: 0.8933, recall: 0.8048, f1: 0.8468, edges-pos-ontonotes_loss: 0.0182
09/16 07:19:16 AM: Update 10422: task edges-pos-ontonotes, batch 422 (10422): mcc: 0.8447, acc: 0.7536, precision: 0.8939, recall: 0.8039, f1: 0.8465, edges-pos-ontonotes_loss: 0.0181
09/16 07:19:26 AM: Update 10519: task edges-pos-ontonotes, batch 519 (10519): mcc: 0.8451, acc: 0.7542, precision: 0.8950, recall: 0.8038, f1: 0.8470, edges-pos-ontonotes_loss: 0.0178
09/16 07:19:36 AM: Update 10604: task edges-pos-ontonotes, batch 604 (10604): mcc: 0.8454, acc: 0.7546, precision: 0.8956, recall: 0.8038, f1: 0.8472, edges-pos-ontonotes_loss: 0.0176
09/16 07:19:46 AM: Update 10694: task edges-pos-ontonotes, batch 694 (10694): mcc: 0.8462, acc: 0.7556, precision: 0.8966, recall: 0.8044, f1: 0.8480, edges-pos-ontonotes_loss: 0.0174
09/16 07:19:57 AM: Update 10761: task edges-pos-ontonotes, batch 761 (10761): mcc: 0.8470, acc: 0.7568, precision: 0.8973, recall: 0.8053, f1: 0.8488, edges-pos-ontonotes_loss: 0.0173
09/16 07:20:07 AM: Update 10833: task edges-pos-ontonotes, batch 833 (10833): mcc: 0.8479, acc: 0.7578, precision: 0.8980, recall: 0.8062, f1: 0.8496, edges-pos-ontonotes_loss: 0.0172
09/16 07:20:17 AM: Update 10906: task edges-pos-ontonotes, batch 906 (10906): mcc: 0.8489, acc: 0.7592, precision: 0.8989, recall: 0.8073, f1: 0.8506, edges-pos-ontonotes_loss: 0.0171
09/16 07:20:27 AM: Update 10981: task edges-pos-ontonotes, batch 981 (10981): mcc: 0.8497, acc: 0.7603, precision: 0.8995, recall: 0.8081, f1: 0.8514, edges-pos-ontonotes_loss: 0.0170
09/16 07:20:29 AM: ***** Step 11000 / Validation 11 *****
09/16 07:20:29 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:20:29 AM: Validating...
09/16 07:20:37 AM: Evaluate: task edges-pos-ontonotes, batch 53 (157): mcc: 0.8916, acc: 0.8274, precision: 0.9346, recall: 0.8546, f1: 0.8928, edges-pos-ontonotes_loss: 0.0125
09/16 07:20:47 AM: Evaluate: task edges-pos-ontonotes, batch 111 (157): mcc: 0.8955, acc: 0.8347, precision: 0.9336, recall: 0.8629, f1: 0.8968, edges-pos-ontonotes_loss: 0.0122
09/16 07:20:57 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.8943, acc: 0.8341, precision: 0.9307, recall: 0.8634, f1: 0.8958, edges-pos-ontonotes_loss: 0.0123
09/16 07:20:57 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:20:57 AM: Best result seen so far for macro.
09/16 07:20:57 AM: Updating LR scheduler:
09/16 07:20:57 AM: 	Best result seen so far for macro_avg: 0.896
09/16 07:20:57 AM: 	# validation passes without improvement: 0
09/16 07:20:57 AM: edges-pos-ontonotes_loss: training: 0.016981 validation: 0.012238
09/16 07:20:57 AM: macro_avg: validation: 0.895918
09/16 07:20:57 AM: micro_avg: validation: 0.000000
09/16 07:20:57 AM: edges-pos-ontonotes_mcc: training: 0.849738 validation: 0.894439
09/16 07:20:57 AM: edges-pos-ontonotes_acc: training: 0.760401 validation: 0.834228
09/16 07:20:57 AM: edges-pos-ontonotes_precision: training: 0.899620 validation: 0.930708
09/16 07:20:57 AM: edges-pos-ontonotes_recall: training: 0.808190 validation: 0.863636
09/16 07:20:57 AM: edges-pos-ontonotes_f1: training: 0.851458 validation: 0.895918
09/16 07:20:57 AM: Global learning rate: 0.0001
09/16 07:20:57 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:21:07 AM: Update 11049: task edges-pos-ontonotes, batch 49 (11049): mcc: 0.8470, acc: 0.7572, precision: 0.8958, recall: 0.8066, f1: 0.8489, edges-pos-ontonotes_loss: 0.0179
09/16 07:21:18 AM: Update 11106: task edges-pos-ontonotes, batch 106 (11106): mcc: 0.8502, acc: 0.7619, precision: 0.8979, recall: 0.8106, f1: 0.8520, edges-pos-ontonotes_loss: 0.0178
09/16 07:21:28 AM: Update 11160: task edges-pos-ontonotes, batch 160 (11160): mcc: 0.8502, acc: 0.7624, precision: 0.8980, recall: 0.8105, f1: 0.8520, edges-pos-ontonotes_loss: 0.0176
09/16 07:21:38 AM: Update 11215: task edges-pos-ontonotes, batch 215 (11215): mcc: 0.8507, acc: 0.7632, precision: 0.8978, recall: 0.8115, f1: 0.8525, edges-pos-ontonotes_loss: 0.0176
09/16 07:21:48 AM: Update 11273: task edges-pos-ontonotes, batch 273 (11273): mcc: 0.8507, acc: 0.7634, precision: 0.8978, recall: 0.8117, f1: 0.8526, edges-pos-ontonotes_loss: 0.0175
09/16 07:22:06 AM: Update 11320: task edges-pos-ontonotes, batch 320 (11320): mcc: 0.8507, acc: 0.7635, precision: 0.8980, recall: 0.8116, f1: 0.8526, edges-pos-ontonotes_loss: 0.0174
09/16 07:22:16 AM: Update 11375: task edges-pos-ontonotes, batch 375 (11375): mcc: 0.8509, acc: 0.7637, precision: 0.8982, recall: 0.8117, f1: 0.8528, edges-pos-ontonotes_loss: 0.0174
09/16 07:22:26 AM: Update 11427: task edges-pos-ontonotes, batch 427 (11427): mcc: 0.8514, acc: 0.7643, precision: 0.8991, recall: 0.8118, f1: 0.8532, edges-pos-ontonotes_loss: 0.0173
09/16 07:22:36 AM: Update 11476: task edges-pos-ontonotes, batch 476 (11476): mcc: 0.8518, acc: 0.7649, precision: 0.8993, recall: 0.8123, f1: 0.8536, edges-pos-ontonotes_loss: 0.0172
09/16 07:22:46 AM: Update 11531: task edges-pos-ontonotes, batch 531 (11531): mcc: 0.8519, acc: 0.7652, precision: 0.8995, recall: 0.8124, f1: 0.8537, edges-pos-ontonotes_loss: 0.0172
09/16 07:22:56 AM: Update 11584: task edges-pos-ontonotes, batch 584 (11584): mcc: 0.8520, acc: 0.7655, precision: 0.8997, recall: 0.8124, f1: 0.8538, edges-pos-ontonotes_loss: 0.0171
09/16 07:23:07 AM: Update 11633: task edges-pos-ontonotes, batch 633 (11633): mcc: 0.8526, acc: 0.7662, precision: 0.9001, recall: 0.8130, f1: 0.8544, edges-pos-ontonotes_loss: 0.0171
09/16 07:23:17 AM: Update 11692: task edges-pos-ontonotes, batch 692 (11692): mcc: 0.8527, acc: 0.7665, precision: 0.9004, recall: 0.8131, f1: 0.8545, edges-pos-ontonotes_loss: 0.0171
09/16 07:23:28 AM: Update 11746: task edges-pos-ontonotes, batch 746 (11746): mcc: 0.8533, acc: 0.7674, precision: 0.9008, recall: 0.8137, f1: 0.8551, edges-pos-ontonotes_loss: 0.0170
09/16 07:23:38 AM: Update 11804: task edges-pos-ontonotes, batch 804 (11804): mcc: 0.8539, acc: 0.7682, precision: 0.9014, recall: 0.8143, f1: 0.8556, edges-pos-ontonotes_loss: 0.0169
09/16 07:23:48 AM: Update 11857: task edges-pos-ontonotes, batch 857 (11857): mcc: 0.8541, acc: 0.7687, precision: 0.9017, recall: 0.8146, f1: 0.8559, edges-pos-ontonotes_loss: 0.0169
09/16 07:23:58 AM: Update 11913: task edges-pos-ontonotes, batch 913 (11913): mcc: 0.8547, acc: 0.7695, precision: 0.9021, recall: 0.8152, f1: 0.8565, edges-pos-ontonotes_loss: 0.0169
09/16 07:24:08 AM: Update 11956: task edges-pos-ontonotes, batch 956 (11956): mcc: 0.8551, acc: 0.7701, precision: 0.9023, recall: 0.8157, f1: 0.8569, edges-pos-ontonotes_loss: 0.0168
09/16 07:24:16 AM: ***** Step 12000 / Validation 12 *****
09/16 07:24:16 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:24:16 AM: Validating...
09/16 07:24:18 AM: Evaluate: task edges-pos-ontonotes, batch 13 (157): mcc: 0.8832, acc: 0.8186, precision: 0.9325, recall: 0.8408, f1: 0.8843, edges-pos-ontonotes_loss: 0.0130
09/16 07:24:28 AM: Evaluate: task edges-pos-ontonotes, batch 84 (157): mcc: 0.8937, acc: 0.8307, precision: 0.9378, recall: 0.8557, f1: 0.8948, edges-pos-ontonotes_loss: 0.0125
09/16 07:24:38 AM: Evaluate: task edges-pos-ontonotes, batch 132 (157): mcc: 0.8956, acc: 0.8350, precision: 0.9371, recall: 0.8598, f1: 0.8968, edges-pos-ontonotes_loss: 0.0122
09/16 07:24:43 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:24:43 AM: Best result seen so far for macro.
09/16 07:24:43 AM: Updating LR scheduler:
09/16 07:24:43 AM: 	Best result seen so far for macro_avg: 0.899
09/16 07:24:43 AM: 	# validation passes without improvement: 0
09/16 07:24:43 AM: edges-pos-ontonotes_loss: training: 0.016816 validation: 0.012052
09/16 07:24:43 AM: macro_avg: validation: 0.898707
09/16 07:24:43 AM: micro_avg: validation: 0.000000
09/16 07:24:43 AM: edges-pos-ontonotes_mcc: training: 0.855387 validation: 0.897441
09/16 07:24:43 AM: edges-pos-ontonotes_acc: training: 0.770580 validation: 0.838841
09/16 07:24:43 AM: edges-pos-ontonotes_precision: training: 0.902514 validation: 0.937446
09/16 07:24:43 AM: edges-pos-ontonotes_recall: training: 0.816118 validation: 0.863043
09/16 07:24:43 AM: edges-pos-ontonotes_f1: training: 0.857145 validation: 0.898707
09/16 07:24:43 AM: Global learning rate: 0.0001
09/16 07:24:43 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:24:48 AM: Update 12029: task edges-pos-ontonotes, batch 29 (12029): mcc: 0.8601, acc: 0.7773, precision: 0.9059, recall: 0.8219, f1: 0.8619, edges-pos-ontonotes_loss: 0.0163
09/16 07:24:58 AM: Update 12089: task edges-pos-ontonotes, batch 89 (12089): mcc: 0.8588, acc: 0.7765, precision: 0.9041, recall: 0.8211, f1: 0.8606, edges-pos-ontonotes_loss: 0.0172
09/16 07:25:09 AM: Update 12147: task edges-pos-ontonotes, batch 147 (12147): mcc: 0.8588, acc: 0.7761, precision: 0.9041, recall: 0.8210, f1: 0.8606, edges-pos-ontonotes_loss: 0.0169
09/16 07:25:19 AM: Update 12206: task edges-pos-ontonotes, batch 206 (12206): mcc: 0.8589, acc: 0.7765, precision: 0.9042, recall: 0.8211, f1: 0.8607, edges-pos-ontonotes_loss: 0.0168
09/16 07:25:31 AM: Update 12259: task edges-pos-ontonotes, batch 259 (12259): mcc: 0.8584, acc: 0.7763, precision: 0.9038, recall: 0.8206, f1: 0.8602, edges-pos-ontonotes_loss: 0.0167
09/16 07:25:41 AM: Update 12321: task edges-pos-ontonotes, batch 321 (12321): mcc: 0.8591, acc: 0.7772, precision: 0.9044, recall: 0.8213, f1: 0.8609, edges-pos-ontonotes_loss: 0.0163
09/16 07:25:51 AM: Update 12398: task edges-pos-ontonotes, batch 398 (12398): mcc: 0.8616, acc: 0.7803, precision: 0.9064, recall: 0.8241, f1: 0.8633, edges-pos-ontonotes_loss: 0.0158
09/16 07:26:01 AM: Update 12467: task edges-pos-ontonotes, batch 467 (12467): mcc: 0.8625, acc: 0.7817, precision: 0.9071, recall: 0.8253, f1: 0.8643, edges-pos-ontonotes_loss: 0.0156
09/16 07:26:11 AM: Update 12531: task edges-pos-ontonotes, batch 531 (12531): mcc: 0.8632, acc: 0.7827, precision: 0.9075, recall: 0.8263, f1: 0.8650, edges-pos-ontonotes_loss: 0.0154
09/16 07:26:21 AM: Update 12593: task edges-pos-ontonotes, batch 593 (12593): mcc: 0.8646, acc: 0.7845, precision: 0.9084, recall: 0.8280, f1: 0.8663, edges-pos-ontonotes_loss: 0.0152
09/16 07:26:31 AM: Update 12688: task edges-pos-ontonotes, batch 688 (12688): mcc: 0.8673, acc: 0.7880, precision: 0.9106, recall: 0.8310, f1: 0.8690, edges-pos-ontonotes_loss: 0.0148
09/16 07:26:41 AM: Update 12778: task edges-pos-ontonotes, batch 778 (12778): mcc: 0.8698, acc: 0.7913, precision: 0.9126, recall: 0.8339, f1: 0.8715, edges-pos-ontonotes_loss: 0.0145
09/16 07:26:51 AM: Update 12874: task edges-pos-ontonotes, batch 874 (12874): mcc: 0.8719, acc: 0.7942, precision: 0.9141, recall: 0.8365, f1: 0.8736, edges-pos-ontonotes_loss: 0.0142
09/16 07:27:02 AM: Update 12968: task edges-pos-ontonotes, batch 968 (12968): mcc: 0.8730, acc: 0.7958, precision: 0.9151, recall: 0.8376, f1: 0.8747, edges-pos-ontonotes_loss: 0.0141
09/16 07:27:04 AM: ***** Step 13000 / Validation 13 *****
09/16 07:27:04 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:27:04 AM: Validating...
09/16 07:27:12 AM: Evaluate: task edges-pos-ontonotes, batch 50 (157): mcc: 0.8862, acc: 0.8161, precision: 0.9429, recall: 0.8370, f1: 0.8868, edges-pos-ontonotes_loss: 0.0132
09/16 07:27:22 AM: Evaluate: task edges-pos-ontonotes, batch 111 (157): mcc: 0.8902, acc: 0.8245, precision: 0.9426, recall: 0.8448, f1: 0.8910, edges-pos-ontonotes_loss: 0.0126
09/16 07:27:32 AM: Updating LR scheduler:
09/16 07:27:32 AM: 	Best result seen so far for macro_avg: 0.899
09/16 07:27:32 AM: 	# validation passes without improvement: 1
09/16 07:27:32 AM: edges-pos-ontonotes_loss: training: 0.014079 validation: 0.012480
09/16 07:27:32 AM: macro_avg: validation: 0.891903
09/16 07:27:32 AM: micro_avg: validation: 0.000000
09/16 07:27:32 AM: edges-pos-ontonotes_mcc: training: 0.873352 validation: 0.891057
09/16 07:27:32 AM: edges-pos-ontonotes_acc: training: 0.796223 validation: 0.826545
09/16 07:27:32 AM: edges-pos-ontonotes_precision: training: 0.915336 validation: 0.942330
09/16 07:27:32 AM: edges-pos-ontonotes_recall: training: 0.838085 validation: 0.846598
09/16 07:27:32 AM: edges-pos-ontonotes_f1: training: 0.875009 validation: 0.891903
09/16 07:27:32 AM: Global learning rate: 0.0001
09/16 07:27:32 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:27:32 AM: Update 13002: task edges-pos-ontonotes, batch 2 (13002): mcc: 0.8693, acc: 0.7883, precision: 0.9171, recall: 0.8288, f1: 0.8707, edges-pos-ontonotes_loss: 0.0144
09/16 07:27:42 AM: Update 13108: task edges-pos-ontonotes, batch 108 (13108): mcc: 0.8886, acc: 0.8179, precision: 0.9283, recall: 0.8549, f1: 0.8901, edges-pos-ontonotes_loss: 0.0135
09/16 07:27:52 AM: Update 13209: task edges-pos-ontonotes, batch 209 (13209): mcc: 0.8882, acc: 0.8176, precision: 0.9273, recall: 0.8550, f1: 0.8897, edges-pos-ontonotes_loss: 0.0132
09/16 07:28:02 AM: Update 13349: task edges-pos-ontonotes, batch 349 (13349): mcc: 0.8759, acc: 0.7995, precision: 0.9179, recall: 0.8406, f1: 0.8775, edges-pos-ontonotes_loss: 0.0148
09/16 07:28:12 AM: Update 13482: task edges-pos-ontonotes, batch 482 (13482): mcc: 0.8702, acc: 0.7913, precision: 0.9137, recall: 0.8337, f1: 0.8719, edges-pos-ontonotes_loss: 0.0154
09/16 07:28:24 AM: Update 13511: task edges-pos-ontonotes, batch 511 (13511): mcc: 0.8694, acc: 0.7902, precision: 0.9129, recall: 0.8328, f1: 0.8710, edges-pos-ontonotes_loss: 0.0154
09/16 07:28:34 AM: Update 13572: task edges-pos-ontonotes, batch 572 (13572): mcc: 0.8647, acc: 0.7829, precision: 0.9090, recall: 0.8277, f1: 0.8664, edges-pos-ontonotes_loss: 0.0157
09/16 07:28:44 AM: Update 13634: task edges-pos-ontonotes, batch 634 (13634): mcc: 0.8616, acc: 0.7787, precision: 0.9057, recall: 0.8248, f1: 0.8634, edges-pos-ontonotes_loss: 0.0159
09/16 07:28:54 AM: Update 13693: task edges-pos-ontonotes, batch 693 (13693): mcc: 0.8603, acc: 0.7769, precision: 0.9045, recall: 0.8235, f1: 0.8621, edges-pos-ontonotes_loss: 0.0160
09/16 07:29:05 AM: Update 13754: task edges-pos-ontonotes, batch 754 (13754): mcc: 0.8589, acc: 0.7750, precision: 0.9030, recall: 0.8223, f1: 0.8608, edges-pos-ontonotes_loss: 0.0162
09/16 07:29:15 AM: Update 13818: task edges-pos-ontonotes, batch 818 (13818): mcc: 0.8579, acc: 0.7735, precision: 0.9020, recall: 0.8213, f1: 0.8598, edges-pos-ontonotes_loss: 0.0163
09/16 07:29:25 AM: Update 13880: task edges-pos-ontonotes, batch 880 (13880): mcc: 0.8570, acc: 0.7723, precision: 0.9016, recall: 0.8200, f1: 0.8589, edges-pos-ontonotes_loss: 0.0164
09/16 07:29:35 AM: Update 13972: task edges-pos-ontonotes, batch 972 (13972): mcc: 0.8567, acc: 0.7716, precision: 0.9020, recall: 0.8191, f1: 0.8586, edges-pos-ontonotes_loss: 0.0164
09/16 07:29:38 AM: ***** Step 14000 / Validation 14 *****
09/16 07:29:38 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:29:38 AM: Validating...
09/16 07:29:45 AM: Evaluate: task edges-pos-ontonotes, batch 48 (157): mcc: 0.8920, acc: 0.8270, precision: 0.9400, recall: 0.8504, f1: 0.8930, edges-pos-ontonotes_loss: 0.0124
09/16 07:29:55 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8977, acc: 0.8379, precision: 0.9381, recall: 0.8629, f1: 0.8989, edges-pos-ontonotes_loss: 0.0120
09/16 07:30:05 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.8971, acc: 0.8378, precision: 0.9355, recall: 0.8641, f1: 0.8984, edges-pos-ontonotes_loss: 0.0120
09/16 07:30:06 AM: Updating LR scheduler:
09/16 07:30:06 AM: 	Best result seen so far for macro_avg: 0.899
09/16 07:30:06 AM: 	# validation passes without improvement: 2
09/16 07:30:06 AM: edges-pos-ontonotes_loss: training: 0.016353 validation: 0.011943
09/16 07:30:06 AM: macro_avg: validation: 0.898551
09/16 07:30:06 AM: micro_avg: validation: 0.000000
09/16 07:30:06 AM: edges-pos-ontonotes_mcc: training: 0.856622 validation: 0.897210
09/16 07:30:06 AM: edges-pos-ontonotes_acc: training: 0.771441 validation: 0.837984
09/16 07:30:06 AM: edges-pos-ontonotes_precision: training: 0.902136 validation: 0.935539
09/16 07:30:06 AM: edges-pos-ontonotes_recall: training: 0.818779 validation: 0.864377
09/16 07:30:06 AM: edges-pos-ontonotes_f1: training: 0.858439 validation: 0.898551
09/16 07:30:06 AM: Global learning rate: 0.0001
09/16 07:30:06 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:30:15 AM: Update 14103: task edges-pos-ontonotes, batch 103 (14103): mcc: 0.8536, acc: 0.7651, precision: 0.9040, recall: 0.8114, f1: 0.8552, edges-pos-ontonotes_loss: 0.0159
09/16 07:30:26 AM: Update 14171: task edges-pos-ontonotes, batch 171 (14171): mcc: 0.8559, acc: 0.7682, precision: 0.9050, recall: 0.8149, f1: 0.8576, edges-pos-ontonotes_loss: 0.0158
09/16 07:30:36 AM: Update 14252: task edges-pos-ontonotes, batch 252 (14252): mcc: 0.8571, acc: 0.7705, precision: 0.9046, recall: 0.8174, f1: 0.8588, edges-pos-ontonotes_loss: 0.0159
09/16 07:30:46 AM: Update 14324: task edges-pos-ontonotes, batch 324 (14324): mcc: 0.8574, acc: 0.7709, precision: 0.9047, recall: 0.8180, f1: 0.8592, edges-pos-ontonotes_loss: 0.0159
09/16 07:30:56 AM: Update 14394: task edges-pos-ontonotes, batch 394 (14394): mcc: 0.8583, acc: 0.7721, precision: 0.9052, recall: 0.8191, f1: 0.8600, edges-pos-ontonotes_loss: 0.0158
09/16 07:31:06 AM: Update 14463: task edges-pos-ontonotes, batch 463 (14463): mcc: 0.8588, acc: 0.7730, precision: 0.9055, recall: 0.8198, f1: 0.8605, edges-pos-ontonotes_loss: 0.0158
09/16 07:31:16 AM: Update 14509: task edges-pos-ontonotes, batch 509 (14509): mcc: 0.8574, acc: 0.7712, precision: 0.9044, recall: 0.8181, f1: 0.8591, edges-pos-ontonotes_loss: 0.0159
09/16 07:31:26 AM: Update 14570: task edges-pos-ontonotes, batch 570 (14570): mcc: 0.8565, acc: 0.7703, precision: 0.9032, recall: 0.8177, f1: 0.8583, edges-pos-ontonotes_loss: 0.0162
09/16 07:31:36 AM: Update 14632: task edges-pos-ontonotes, batch 632 (14632): mcc: 0.8563, acc: 0.7702, precision: 0.9027, recall: 0.8177, f1: 0.8581, edges-pos-ontonotes_loss: 0.0162
09/16 07:31:46 AM: Update 14686: task edges-pos-ontonotes, batch 686 (14686): mcc: 0.8563, acc: 0.7703, precision: 0.9025, recall: 0.8178, f1: 0.8581, edges-pos-ontonotes_loss: 0.0163
09/16 07:31:56 AM: Update 14742: task edges-pos-ontonotes, batch 742 (14742): mcc: 0.8566, acc: 0.7708, precision: 0.9025, recall: 0.8183, f1: 0.8584, edges-pos-ontonotes_loss: 0.0163
09/16 07:32:07 AM: Update 14786: task edges-pos-ontonotes, batch 786 (14786): mcc: 0.8565, acc: 0.7708, precision: 0.9025, recall: 0.8183, f1: 0.8583, edges-pos-ontonotes_loss: 0.0163
09/16 07:32:17 AM: Update 14841: task edges-pos-ontonotes, batch 841 (14841): mcc: 0.8564, acc: 0.7707, precision: 0.9024, recall: 0.8182, f1: 0.8582, edges-pos-ontonotes_loss: 0.0163
09/16 07:32:27 AM: Update 14898: task edges-pos-ontonotes, batch 898 (14898): mcc: 0.8564, acc: 0.7708, precision: 0.9024, recall: 0.8182, f1: 0.8582, edges-pos-ontonotes_loss: 0.0163
09/16 07:32:37 AM: Update 14949: task edges-pos-ontonotes, batch 949 (14949): mcc: 0.8566, acc: 0.7711, precision: 0.9025, recall: 0.8184, f1: 0.8584, edges-pos-ontonotes_loss: 0.0163
09/16 07:32:46 AM: ***** Step 15000 / Validation 15 *****
09/16 07:32:46 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:32:46 AM: Validating...
09/16 07:32:47 AM: Evaluate: task edges-pos-ontonotes, batch 7 (157): mcc: 0.8900, acc: 0.8308, precision: 0.9349, recall: 0.8513, f1: 0.8911, edges-pos-ontonotes_loss: 0.0123
09/16 07:32:57 AM: Evaluate: task edges-pos-ontonotes, batch 78 (157): mcc: 0.8975, acc: 0.8360, precision: 0.9404, recall: 0.8604, f1: 0.8986, edges-pos-ontonotes_loss: 0.0120
09/16 07:33:07 AM: Evaluate: task edges-pos-ontonotes, batch 129 (157): mcc: 0.8990, acc: 0.8396, precision: 0.9398, recall: 0.8638, f1: 0.9002, edges-pos-ontonotes_loss: 0.0118
09/16 07:33:13 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:33:13 AM: Best result seen so far for macro.
09/16 07:33:13 AM: Updating LR scheduler:
09/16 07:33:13 AM: 	Best result seen so far for macro_avg: 0.902
09/16 07:33:13 AM: 	# validation passes without improvement: 0
09/16 07:33:13 AM: edges-pos-ontonotes_loss: training: 0.016269 validation: 0.011648
09/16 07:33:13 AM: macro_avg: validation: 0.901554
09/16 07:33:13 AM: micro_avg: validation: 0.000000
09/16 07:33:13 AM: edges-pos-ontonotes_mcc: training: 0.856844 validation: 0.900316
09/16 07:33:13 AM: edges-pos-ontonotes_acc: training: 0.771427 validation: 0.842704
09/16 07:33:13 AM: edges-pos-ontonotes_precision: training: 0.902727 validation: 0.939635
09/16 07:33:13 AM: edges-pos-ontonotes_recall: training: 0.818653 validation: 0.866440
09/16 07:33:13 AM: edges-pos-ontonotes_f1: training: 0.858637 validation: 0.901554
09/16 07:33:13 AM: Global learning rate: 0.0001
09/16 07:33:13 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:33:18 AM: Update 15024: task edges-pos-ontonotes, batch 24 (15024): mcc: 0.8616, acc: 0.7805, precision: 0.9037, recall: 0.8266, f1: 0.8634, edges-pos-ontonotes_loss: 0.0166
09/16 07:33:28 AM: Update 15079: task edges-pos-ontonotes, batch 79 (15079): mcc: 0.8596, acc: 0.7768, precision: 0.9041, recall: 0.8225, f1: 0.8614, edges-pos-ontonotes_loss: 0.0166
09/16 07:33:40 AM: Update 15093: task edges-pos-ontonotes, batch 93 (15093): mcc: 0.8598, acc: 0.7768, precision: 0.9039, recall: 0.8230, f1: 0.8616, edges-pos-ontonotes_loss: 0.0165
09/16 07:33:50 AM: Update 15151: task edges-pos-ontonotes, batch 151 (15151): mcc: 0.8602, acc: 0.7774, precision: 0.9049, recall: 0.8229, f1: 0.8619, edges-pos-ontonotes_loss: 0.0165
09/16 07:34:00 AM: Update 15208: task edges-pos-ontonotes, batch 208 (15208): mcc: 0.8612, acc: 0.7790, precision: 0.9059, recall: 0.8239, f1: 0.8629, edges-pos-ontonotes_loss: 0.0163
09/16 07:34:11 AM: Update 15261: task edges-pos-ontonotes, batch 261 (15261): mcc: 0.8616, acc: 0.7797, precision: 0.9057, recall: 0.8248, f1: 0.8633, edges-pos-ontonotes_loss: 0.0162
09/16 07:34:21 AM: Update 15319: task edges-pos-ontonotes, batch 319 (15319): mcc: 0.8619, acc: 0.7805, precision: 0.9060, recall: 0.8251, f1: 0.8637, edges-pos-ontonotes_loss: 0.0161
09/16 07:34:31 AM: Update 15376: task edges-pos-ontonotes, batch 376 (15376): mcc: 0.8622, acc: 0.7810, precision: 0.9063, recall: 0.8255, f1: 0.8640, edges-pos-ontonotes_loss: 0.0161
09/16 07:34:41 AM: Update 15423: task edges-pos-ontonotes, batch 423 (15423): mcc: 0.8622, acc: 0.7812, precision: 0.9065, recall: 0.8253, f1: 0.8640, edges-pos-ontonotes_loss: 0.0161
09/16 07:34:51 AM: Update 15475: task edges-pos-ontonotes, batch 475 (15475): mcc: 0.8623, acc: 0.7814, precision: 0.9065, recall: 0.8254, f1: 0.8641, edges-pos-ontonotes_loss: 0.0161
09/16 07:35:01 AM: Update 15529: task edges-pos-ontonotes, batch 529 (15529): mcc: 0.8623, acc: 0.7814, precision: 0.9064, recall: 0.8255, f1: 0.8641, edges-pos-ontonotes_loss: 0.0161
09/16 07:35:11 AM: Update 15581: task edges-pos-ontonotes, batch 581 (15581): mcc: 0.8625, acc: 0.7817, precision: 0.9065, recall: 0.8257, f1: 0.8642, edges-pos-ontonotes_loss: 0.0161
09/16 07:35:21 AM: Update 15635: task edges-pos-ontonotes, batch 635 (15635): mcc: 0.8626, acc: 0.7820, precision: 0.9066, recall: 0.8260, f1: 0.8644, edges-pos-ontonotes_loss: 0.0160
09/16 07:35:31 AM: Update 15695: task edges-pos-ontonotes, batch 695 (15695): mcc: 0.8627, acc: 0.7821, precision: 0.9066, recall: 0.8261, f1: 0.8644, edges-pos-ontonotes_loss: 0.0161
09/16 07:35:42 AM: Update 15745: task edges-pos-ontonotes, batch 745 (15745): mcc: 0.8624, acc: 0.7820, precision: 0.9063, recall: 0.8259, f1: 0.8642, edges-pos-ontonotes_loss: 0.0161
09/16 07:35:52 AM: Update 15808: task edges-pos-ontonotes, batch 808 (15808): mcc: 0.8629, acc: 0.7825, precision: 0.9067, recall: 0.8263, f1: 0.8646, edges-pos-ontonotes_loss: 0.0159
09/16 07:36:02 AM: Update 15875: task edges-pos-ontonotes, batch 875 (15875): mcc: 0.8633, acc: 0.7831, precision: 0.9068, recall: 0.8269, f1: 0.8650, edges-pos-ontonotes_loss: 0.0158
09/16 07:36:12 AM: Update 15948: task edges-pos-ontonotes, batch 948 (15948): mcc: 0.8640, acc: 0.7841, precision: 0.9074, recall: 0.8279, f1: 0.8658, edges-pos-ontonotes_loss: 0.0156
09/16 07:36:19 AM: ***** Step 16000 / Validation 16 *****
09/16 07:36:19 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:36:19 AM: Validating...
09/16 07:36:22 AM: Evaluate: task edges-pos-ontonotes, batch 17 (157): mcc: 0.8827, acc: 0.8164, precision: 0.9290, recall: 0.8431, f1: 0.8840, edges-pos-ontonotes_loss: 0.0129
09/16 07:36:32 AM: Evaluate: task edges-pos-ontonotes, batch 84 (157): mcc: 0.8966, acc: 0.8355, precision: 0.9374, recall: 0.8615, f1: 0.8978, edges-pos-ontonotes_loss: 0.0122
09/16 07:36:42 AM: Evaluate: task edges-pos-ontonotes, batch 128 (157): mcc: 0.8979, acc: 0.8388, precision: 0.9371, recall: 0.8643, f1: 0.8992, edges-pos-ontonotes_loss: 0.0119
09/16 07:36:48 AM: Updating LR scheduler:
09/16 07:36:48 AM: 	Best result seen so far for macro_avg: 0.902
09/16 07:36:48 AM: 	# validation passes without improvement: 1
09/16 07:36:48 AM: edges-pos-ontonotes_loss: training: 0.015505 validation: 0.011690
09/16 07:36:48 AM: macro_avg: validation: 0.901428
09/16 07:36:48 AM: micro_avg: validation: 0.000000
09/16 07:36:48 AM: edges-pos-ontonotes_mcc: training: 0.864313 validation: 0.900099
09/16 07:36:48 AM: edges-pos-ontonotes_acc: training: 0.784428 validation: 0.843000
09/16 07:36:48 AM: edges-pos-ontonotes_precision: training: 0.907660 validation: 0.937292
09/16 07:36:48 AM: edges-pos-ontonotes_recall: training: 0.828152 validation: 0.868207
09/16 07:36:48 AM: edges-pos-ontonotes_f1: training: 0.866085 validation: 0.901428
09/16 07:36:48 AM: Global learning rate: 0.0001
09/16 07:36:48 AM: Saving checkpoints to: ./experiments/pos-ontonotes-random-memorization-top/run
09/16 07:36:52 AM: Update 16024: task edges-pos-ontonotes, batch 24 (16024): mcc: 0.8708, acc: 0.7926, precision: 0.9093, recall: 0.8389, f1: 0.8727, edges-pos-ontonotes_loss: 0.0137
09/16 07:37:02 AM: Update 16097: task edges-pos-ontonotes, batch 97 (16097): mcc: 0.8846, acc: 0.8103, precision: 0.9228, recall: 0.8524, f1: 0.8862, edges-pos-ontonotes_loss: 0.0127
09/16 07:37:12 AM: Update 16191: task edges-pos-ontonotes, batch 191 (16191): mcc: 0.8906, acc: 0.8185, precision: 0.9272, recall: 0.8596, f1: 0.8921, edges-pos-ontonotes_loss: 0.0123
09/16 07:37:22 AM: Update 16288: task edges-pos-ontonotes, batch 288 (16288): mcc: 0.8928, acc: 0.8218, precision: 0.9288, recall: 0.8624, f1: 0.8943, edges-pos-ontonotes_loss: 0.0123
09/16 07:37:32 AM: Update 16368: task edges-pos-ontonotes, batch 368 (16368): mcc: 0.8934, acc: 0.8229, precision: 0.9290, recall: 0.8631, f1: 0.8949, edges-pos-ontonotes_loss: 0.0123
09/16 07:37:42 AM: Update 16472: task edges-pos-ontonotes, batch 472 (16472): mcc: 0.8928, acc: 0.8226, precision: 0.9290, recall: 0.8622, f1: 0.8943, edges-pos-ontonotes_loss: 0.0124
