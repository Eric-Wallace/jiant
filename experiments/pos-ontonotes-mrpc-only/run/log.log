09/16 12:22:33 PM: Git branch: master
09/16 12:22:33 PM: Git SHA: ce97551376ebcff91ec7c178ddad0ca53f8fcb03
09/16 12:22:33 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/pos-ontonotes-mrpc-only/",
  "exp_name": "experiments/pos-ontonotes-mrpc-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/pos-ontonotes-mrpc-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/mrpc",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/pos-ontonotes-mrpc-only__run",
  "run_dir": "./experiments/pos-ontonotes-mrpc-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-pos-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 12:22:33 PM: Saved config to ./experiments/pos-ontonotes-mrpc-only/run/params.conf
09/16 12:22:33 PM: Using random seed 1234
09/16 12:22:34 PM: Using GPU 0
09/16 12:22:34 PM: Loading tasks...
09/16 12:22:34 PM: Writing pre-preprocessed tasks to ./experiments/pos-ontonotes-mrpc-only/
09/16 12:22:34 PM: 	Creating task edges-pos-ontonotes from scratch.
09/16 12:22:58 PM: Read=110514, Skip=5298, Total=115812 from ./probing_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased
09/16 12:22:59 PM: Read=15060, Skip=620, Total=15680 from ./probing_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased
09/16 12:23:04 PM: Read=11462, Skip=755, Total=12217 from ./probing_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased
09/16 12:23:17 PM: 	Task 'edges-pos-ontonotes': |train|=110514 |val|=15060 |test|=11462
09/16 12:23:17 PM: 	Finished loading tasks: edges-pos-ontonotes.
09/16 12:23:17 PM: 	Building vocab from scratch.
09/16 12:23:17 PM: 	Counting units for task edges-pos-ontonotes.
09/16 12:23:19 PM: 	Task 'edges-pos-ontonotes': adding vocab namespace 'edges-pos-ontonotes_labels'
09/16 12:23:21 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:23:21 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 12:23:21 PM: 	Saved vocab to ./experiments/pos-ontonotes-mrpc-only/vocab
09/16 12:23:21 PM: Loading token dictionary from ./experiments/pos-ontonotes-mrpc-only/vocab.
09/16 12:23:21 PM: 	Loaded vocab from ./experiments/pos-ontonotes-mrpc-only/vocab
09/16 12:23:21 PM: 	Vocab namespace bert_uncased: size 30524
09/16 12:23:21 PM: 	Vocab namespace tokens: size 24015
09/16 12:23:21 PM: 	Vocab namespace edges-pos-ontonotes_labels: size 48
09/16 12:23:21 PM: 	Vocab namespace chars: size 81
09/16 12:23:21 PM: 	Finished building vocab.
09/16 12:23:21 PM: 	Task edges-pos-ontonotes (train): Indexing from scratch.
09/16 12:23:50 PM: 	Task edges-pos-ontonotes (train): Saved 110514 instances to ./experiments/pos-ontonotes-mrpc-only/preproc/edges-pos-ontonotes__train_data
09/16 12:23:50 PM: 	Task edges-pos-ontonotes (val): Indexing from scratch.
09/16 12:23:54 PM: 	Task edges-pos-ontonotes (val): Saved 15060 instances to ./experiments/pos-ontonotes-mrpc-only/preproc/edges-pos-ontonotes__val_data
09/16 12:23:54 PM: 	Task edges-pos-ontonotes (test): Indexing from scratch.
09/16 12:23:58 PM: 	Task edges-pos-ontonotes (test): Saved 11462 instances to ./experiments/pos-ontonotes-mrpc-only/preproc/edges-pos-ontonotes__test_data
09/16 12:23:58 PM: 	Finished indexing tasks
09/16 12:23:58 PM: 	Creating trimmed target-only version of edges-pos-ontonotes train.
09/16 12:23:58 PM: 	  Training on 
09/16 12:23:58 PM: 	  Evaluating on edges-pos-ontonotes
09/16 12:23:58 PM: 	Finished loading tasks in 83.913s
09/16 12:23:58 PM: 	 Tasks: ['edges-pos-ontonotes']
09/16 12:23:58 PM: Building model...
09/16 12:23:58 PM: Using BERT model (bert-base-uncased).
09/16 12:23:58 PM: LOADING A FUNETUNED MODEL from: 
09/16 12:23:58 PM: models/mrpc
09/16 12:23:58 PM: loading configuration file models/mrpc/config.json
09/16 12:23:58 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 12:23:58 PM: loading weights file models/mrpc/pytorch_model.bin
09/16 12:24:01 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpqoxew2qe
09/16 12:24:03 PM: copying /tmp/tmpqoxew2qe to cache at ./experiments/pos-ontonotes-mrpc-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:24:03 PM: creating metadata file for ./experiments/pos-ontonotes-mrpc-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:24:03 PM: removing temp file /tmp/tmpqoxew2qe
09/16 12:24:03 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/pos-ontonotes-mrpc-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:24:03 PM: Initializing parameters
09/16 12:24:03 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 12:24:03 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 12:24:03 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 12:24:03 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 12:24:03 PM:    _text_field_embedder.model.pooler.dense.bias
09/16 12:24:03 PM:    _text_field_embedder.model.pooler.dense.weight
09/16 12:24:03 PM: 	Task 'edges-pos-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-pos-ontonotes"
}
09/16 12:24:08 PM: Model specification:
09/16 12:24:08 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-pos-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=48, bias=True)
    )
  )
)
09/16 12:24:08 PM: Model parameters:
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:08 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:08 PM: 	edges-pos-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 12:24:08 PM: 	edges-pos-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 12:24:08 PM: 	edges-pos-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 24576 with torch.Size([48, 512])
09/16 12:24:08 PM: 	edges-pos-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 48 with torch.Size([48])
09/16 12:24:08 PM: Total number of parameters: 109703728 (1.09704e+08)
09/16 12:24:08 PM: Number of trainable parameters: 221488 (221488)
09/16 12:24:08 PM: Finished building model in 10.949s
09/16 12:24:08 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-pos-ontonotes 

09/16 12:25:25 PM: patience = 9
09/16 12:25:25 PM: val_interval = 1000
09/16 12:25:25 PM: max_vals = 250
09/16 12:25:25 PM: cuda_device = 0
09/16 12:25:25 PM: grad_norm = 5.0
09/16 12:25:25 PM: grad_clipping = None
09/16 12:25:25 PM: lr_decay = 0.99
09/16 12:25:25 PM: min_lr = 1e-06
09/16 12:25:25 PM: keep_all_checkpoints = 0
09/16 12:25:25 PM: val_data_limit = 5000
09/16 12:25:25 PM: max_epochs = -1
09/16 12:25:25 PM: dec_val_scale = 250
09/16 12:25:25 PM: training_data_fraction = 1
09/16 12:25:25 PM: type = adam
09/16 12:25:25 PM: parameter_groups = None
09/16 12:25:25 PM: Number of trainable parameters: 221488
09/16 12:25:25 PM: infer_type_and_cast = True
09/16 12:25:25 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:25 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:25 PM: lr = 0.0001
09/16 12:25:25 PM: amsgrad = True
09/16 12:25:25 PM: type = reduce_on_plateau
09/16 12:25:25 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:25 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:25 PM: mode = max
09/16 12:25:25 PM: factor = 0.5
09/16 12:25:25 PM: patience = 3
09/16 12:25:25 PM: threshold = 0.0001
09/16 12:25:25 PM: threshold_mode = abs
09/16 12:25:25 PM: verbose = True
09/16 12:25:25 PM: type = adam
09/16 12:25:25 PM: parameter_groups = None
09/16 12:25:25 PM: Number of trainable parameters: 221488
09/16 12:25:25 PM: infer_type_and_cast = True
09/16 12:25:25 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:25 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:25 PM: lr = 0.0001
09/16 12:25:25 PM: amsgrad = True
09/16 12:25:25 PM: type = reduce_on_plateau
09/16 12:25:25 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:25 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:25 PM: mode = max
09/16 12:25:25 PM: factor = 0.5
09/16 12:25:25 PM: patience = 3
09/16 12:25:25 PM: threshold = 0.0001
09/16 12:25:25 PM: threshold_mode = abs
09/16 12:25:25 PM: verbose = True
09/16 12:25:25 PM: Starting training without restoring from a checkpoint.
09/16 12:25:25 PM: Training examples per task, before any subsampling: {'edges-pos-ontonotes': 110514}
09/16 12:25:25 PM: Beginning training with stopping criteria based on metric: edges-pos-ontonotes_f1
09/16 12:25:35 PM: Update 82: task edges-pos-ontonotes, batch 82 (82): mcc: 0.0659, acc: 0.0797, precision: 0.0430, recall: 0.3156, f1: 0.0757, edges-pos-ontonotes_loss: 0.3576
09/16 12:25:45 PM: Update 175: task edges-pos-ontonotes, batch 175 (175): mcc: 0.0991, acc: 0.1403, precision: 0.0714, recall: 0.2493, f1: 0.1110, edges-pos-ontonotes_loss: 0.2099
09/16 12:25:55 PM: Update 277: task edges-pos-ontonotes, batch 277 (277): mcc: 0.1574, acc: 0.2127, precision: 0.1195, recall: 0.2831, f1: 0.1680, edges-pos-ontonotes_loss: 0.1510
09/16 12:26:05 PM: Update 355: task edges-pos-ontonotes, batch 355 (355): mcc: 0.2175, acc: 0.2745, precision: 0.1748, recall: 0.3281, f1: 0.2281, edges-pos-ontonotes_loss: 0.1271
09/16 12:26:15 PM: Update 422: task edges-pos-ontonotes, batch 422 (422): mcc: 0.2634, acc: 0.3156, precision: 0.2210, recall: 0.3619, f1: 0.2744, edges-pos-ontonotes_loss: 0.1128
09/16 12:26:25 PM: Update 501: task edges-pos-ontonotes, batch 501 (501): mcc: 0.3165, acc: 0.3605, precision: 0.2770, recall: 0.4014, f1: 0.3278, edges-pos-ontonotes_loss: 0.1002
09/16 12:26:35 PM: Update 585: task edges-pos-ontonotes, batch 585 (585): mcc: 0.3675, acc: 0.4028, precision: 0.3329, recall: 0.4398, f1: 0.3789, edges-pos-ontonotes_loss: 0.0898
09/16 12:26:46 PM: Update 628: task edges-pos-ontonotes, batch 628 (628): mcc: 0.3910, acc: 0.4214, precision: 0.3594, recall: 0.4570, f1: 0.4024, edges-pos-ontonotes_loss: 0.0855
09/16 12:26:56 PM: Update 689: task edges-pos-ontonotes, batch 689 (689): mcc: 0.4269, acc: 0.4473, precision: 0.4016, recall: 0.4818, f1: 0.4381, edges-pos-ontonotes_loss: 0.0805
09/16 12:27:06 PM: Update 742: task edges-pos-ontonotes, batch 742 (742): mcc: 0.4550, acc: 0.4686, precision: 0.4347, recall: 0.5020, f1: 0.4660, edges-pos-ontonotes_loss: 0.0767
09/16 12:27:16 PM: Update 796: task edges-pos-ontonotes, batch 796 (796): mcc: 0.4758, acc: 0.4845, precision: 0.4591, recall: 0.5174, f1: 0.4865, edges-pos-ontonotes_loss: 0.0732
09/16 12:27:26 PM: Update 847: task edges-pos-ontonotes, batch 847 (847): mcc: 0.4951, acc: 0.4994, precision: 0.4817, recall: 0.5319, f1: 0.5056, edges-pos-ontonotes_loss: 0.0703
09/16 12:27:36 PM: Update 896: task edges-pos-ontonotes, batch 896 (896): mcc: 0.5134, acc: 0.5133, precision: 0.5029, recall: 0.5458, f1: 0.5235, edges-pos-ontonotes_loss: 0.0677
09/16 12:27:46 PM: Update 936: task edges-pos-ontonotes, batch 936 (936): mcc: 0.5277, acc: 0.5243, precision: 0.5195, recall: 0.5569, f1: 0.5376, edges-pos-ontonotes_loss: 0.0657
09/16 12:27:57 PM: Update 974: task edges-pos-ontonotes, batch 974 (974): mcc: 0.5394, acc: 0.5325, precision: 0.5338, recall: 0.5651, f1: 0.5490, edges-pos-ontonotes_loss: 0.0641
09/16 12:28:02 PM: ***** Step 1000 / Validation 1 *****
09/16 12:28:02 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:28:02 PM: Validating...
09/16 12:28:07 PM: Evaluate: task edges-pos-ontonotes, batch 27 (157): mcc: 0.8166, acc: 0.7438, precision: 0.8755, recall: 0.7683, f1: 0.8184, edges-pos-ontonotes_loss: 0.0200
09/16 12:28:17 PM: Evaluate: task edges-pos-ontonotes, batch 113 (157): mcc: 0.8271, acc: 0.7546, precision: 0.8855, recall: 0.7787, f1: 0.8287, edges-pos-ontonotes_loss: 0.0194
09/16 12:28:24 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:28:24 PM: Best result seen so far for micro.
09/16 12:28:24 PM: Best result seen so far for macro.
09/16 12:28:24 PM: Updating LR scheduler:
09/16 12:28:24 PM: 	Best result seen so far for macro_avg: 0.825
09/16 12:28:24 PM: 	# validation passes without improvement: 0
09/16 12:28:24 PM: edges-pos-ontonotes_loss: training: 0.063015 validation: 0.019725
09/16 12:28:24 PM: macro_avg: validation: 0.824988
09/16 12:28:24 PM: micro_avg: validation: 0.000000
09/16 12:28:24 PM: edges-pos-ontonotes_mcc: training: 0.547061 validation: 0.823467
09/16 12:28:24 PM: edges-pos-ontonotes_acc: training: 0.538269 validation: 0.749262
09/16 12:28:24 PM: edges-pos-ontonotes_precision: training: 0.543074 validation: 0.884600
09/16 12:28:24 PM: edges-pos-ontonotes_recall: training: 0.570731 validation: 0.772903
09/16 12:28:24 PM: edges-pos-ontonotes_f1: training: 0.556559 validation: 0.824988
09/16 12:28:24 PM: Global learning rate: 0.0001
09/16 12:28:24 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:28:27 PM: Update 1021: task edges-pos-ontonotes, batch 21 (1021): mcc: 0.8014, acc: 0.7078, precision: 0.8761, recall: 0.7400, f1: 0.8023, edges-pos-ontonotes_loss: 0.0231
09/16 12:28:37 PM: Update 1086: task edges-pos-ontonotes, batch 86 (1086): mcc: 0.8110, acc: 0.7213, precision: 0.8814, recall: 0.7528, f1: 0.8120, edges-pos-ontonotes_loss: 0.0221
09/16 12:28:47 PM: Update 1152: task edges-pos-ontonotes, batch 152 (1152): mcc: 0.8149, acc: 0.7274, precision: 0.8814, recall: 0.7600, f1: 0.8162, edges-pos-ontonotes_loss: 0.0215
09/16 12:28:57 PM: Update 1213: task edges-pos-ontonotes, batch 213 (1213): mcc: 0.8162, acc: 0.7300, precision: 0.8801, recall: 0.7635, f1: 0.8177, edges-pos-ontonotes_loss: 0.0213
09/16 12:29:16 PM: Update 1253: task edges-pos-ontonotes, batch 253 (1253): mcc: 0.8165, acc: 0.7314, precision: 0.8788, recall: 0.7653, f1: 0.8181, edges-pos-ontonotes_loss: 0.0211
09/16 12:29:26 PM: Update 1305: task edges-pos-ontonotes, batch 305 (1305): mcc: 0.8184, acc: 0.7344, precision: 0.8793, recall: 0.7682, f1: 0.8200, edges-pos-ontonotes_loss: 0.0209
09/16 12:29:36 PM: Update 1351: task edges-pos-ontonotes, batch 351 (1351): mcc: 0.8195, acc: 0.7362, precision: 0.8793, recall: 0.7702, f1: 0.8212, edges-pos-ontonotes_loss: 0.0207
09/16 12:29:46 PM: Update 1420: task edges-pos-ontonotes, batch 420 (1420): mcc: 0.8220, acc: 0.7402, precision: 0.8803, recall: 0.7741, f1: 0.8238, edges-pos-ontonotes_loss: 0.0204
09/16 12:29:56 PM: Update 1484: task edges-pos-ontonotes, batch 484 (1484): mcc: 0.8241, acc: 0.7435, precision: 0.8807, recall: 0.7775, f1: 0.8259, edges-pos-ontonotes_loss: 0.0202
09/16 12:30:06 PM: Update 1543: task edges-pos-ontonotes, batch 543 (1543): mcc: 0.8258, acc: 0.7464, precision: 0.8811, recall: 0.7804, f1: 0.8277, edges-pos-ontonotes_loss: 0.0200
09/16 12:30:16 PM: Update 1595: task edges-pos-ontonotes, batch 595 (1595): mcc: 0.8271, acc: 0.7485, precision: 0.8814, recall: 0.7825, f1: 0.8290, edges-pos-ontonotes_loss: 0.0198
09/16 12:30:26 PM: Update 1656: task edges-pos-ontonotes, batch 656 (1656): mcc: 0.8285, acc: 0.7506, precision: 0.8816, recall: 0.7850, f1: 0.8305, edges-pos-ontonotes_loss: 0.0196
09/16 12:30:36 PM: Update 1724: task edges-pos-ontonotes, batch 724 (1724): mcc: 0.8299, acc: 0.7529, precision: 0.8817, recall: 0.7874, f1: 0.8319, edges-pos-ontonotes_loss: 0.0194
09/16 12:30:47 PM: Update 1793: task edges-pos-ontonotes, batch 793 (1793): mcc: 0.8312, acc: 0.7550, precision: 0.8820, recall: 0.7896, f1: 0.8332, edges-pos-ontonotes_loss: 0.0192
09/16 12:30:57 PM: Update 1846: task edges-pos-ontonotes, batch 846 (1846): mcc: 0.8320, acc: 0.7564, precision: 0.8820, recall: 0.7910, f1: 0.8340, edges-pos-ontonotes_loss: 0.0191
09/16 12:31:07 PM: Update 1886: task edges-pos-ontonotes, batch 886 (1886): mcc: 0.8324, acc: 0.7572, precision: 0.8819, recall: 0.7919, f1: 0.8345, edges-pos-ontonotes_loss: 0.0190
09/16 12:31:17 PM: Update 1960: task edges-pos-ontonotes, batch 960 (1960): mcc: 0.8339, acc: 0.7594, precision: 0.8822, recall: 0.7945, f1: 0.8360, edges-pos-ontonotes_loss: 0.0187
09/16 12:31:21 PM: ***** Step 2000 / Validation 2 *****
09/16 12:31:21 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:31:21 PM: Validating...
09/16 12:31:27 PM: Evaluate: task edges-pos-ontonotes, batch 52 (157): mcc: 0.8319, acc: 0.7817, precision: 0.8771, recall: 0.7954, f1: 0.8343, edges-pos-ontonotes_loss: 0.0176
09/16 12:31:37 PM: Evaluate: task edges-pos-ontonotes, batch 129 (157): mcc: 0.8417, acc: 0.7921, precision: 0.8840, recall: 0.8074, f1: 0.8439, edges-pos-ontonotes_loss: 0.0167
09/16 12:31:41 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:31:41 PM: Best result seen so far for macro.
09/16 12:31:41 PM: Updating LR scheduler:
09/16 12:31:41 PM: 	Best result seen so far for macro_avg: 0.846
09/16 12:31:41 PM: 	# validation passes without improvement: 0
09/16 12:31:41 PM: edges-pos-ontonotes_loss: training: 0.018534 validation: 0.016539
09/16 12:31:41 PM: macro_avg: validation: 0.845893
09/16 12:31:41 PM: micro_avg: validation: 0.000000
09/16 12:31:41 PM: edges-pos-ontonotes_mcc: training: 0.834735 validation: 0.843693
09/16 12:31:41 PM: edges-pos-ontonotes_acc: training: 0.760505 validation: 0.793782
09/16 12:31:41 PM: edges-pos-ontonotes_precision: training: 0.882505 validation: 0.886592
09/16 12:31:41 PM: edges-pos-ontonotes_recall: training: 0.795705 validation: 0.808766
09/16 12:31:41 PM: edges-pos-ontonotes_f1: training: 0.836860 validation: 0.845893
09/16 12:31:41 PM: Global learning rate: 0.0001
09/16 12:31:41 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:31:47 PM: Update 2039: task edges-pos-ontonotes, batch 39 (2039): mcc: 0.8615, acc: 0.7946, precision: 0.8966, recall: 0.8332, f1: 0.8637, edges-pos-ontonotes_loss: 0.0145
09/16 12:31:57 PM: Update 2091: task edges-pos-ontonotes, batch 91 (2091): mcc: 0.8640, acc: 0.7992, precision: 0.8967, recall: 0.8378, f1: 0.8662, edges-pos-ontonotes_loss: 0.0140
09/16 12:32:07 PM: Update 2148: task edges-pos-ontonotes, batch 148 (2148): mcc: 0.8603, acc: 0.7951, precision: 0.8934, recall: 0.8339, f1: 0.8626, edges-pos-ontonotes_loss: 0.0140
09/16 12:32:17 PM: Update 2226: task edges-pos-ontonotes, batch 226 (2226): mcc: 0.8623, acc: 0.7982, precision: 0.8949, recall: 0.8363, f1: 0.8646, edges-pos-ontonotes_loss: 0.0137
09/16 12:32:27 PM: Update 2328: task edges-pos-ontonotes, batch 328 (2328): mcc: 0.8678, acc: 0.8057, precision: 0.8978, recall: 0.8439, f1: 0.8700, edges-pos-ontonotes_loss: 0.0130
09/16 12:32:37 PM: Update 2440: task edges-pos-ontonotes, batch 440 (2440): mcc: 0.8726, acc: 0.8124, precision: 0.9010, recall: 0.8501, f1: 0.8748, edges-pos-ontonotes_loss: 0.0126
09/16 12:32:47 PM: Update 2539: task edges-pos-ontonotes, batch 539 (2539): mcc: 0.8745, acc: 0.8152, precision: 0.9020, recall: 0.8527, f1: 0.8767, edges-pos-ontonotes_loss: 0.0125
09/16 12:32:57 PM: Update 2674: task edges-pos-ontonotes, batch 674 (2674): mcc: 0.8755, acc: 0.8169, precision: 0.9028, recall: 0.8540, f1: 0.8777, edges-pos-ontonotes_loss: 0.0125
09/16 12:33:07 PM: Update 2803: task edges-pos-ontonotes, batch 803 (2803): mcc: 0.8772, acc: 0.8197, precision: 0.9038, recall: 0.8563, f1: 0.8794, edges-pos-ontonotes_loss: 0.0124
09/16 12:33:17 PM: Update 2867: task edges-pos-ontonotes, batch 867 (2867): mcc: 0.8757, acc: 0.8183, precision: 0.9023, recall: 0.8547, f1: 0.8779, edges-pos-ontonotes_loss: 0.0125
09/16 12:33:27 PM: Update 2986: task edges-pos-ontonotes, batch 986 (2986): mcc: 0.8751, acc: 0.8183, precision: 0.9017, recall: 0.8543, f1: 0.8773, edges-pos-ontonotes_loss: 0.0125
09/16 12:33:29 PM: ***** Step 3000 / Validation 3 *****
09/16 12:33:29 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:33:29 PM: Validating...
09/16 12:33:37 PM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.8501, acc: 0.8112, precision: 0.8838, recall: 0.8235, f1: 0.8526, edges-pos-ontonotes_loss: 0.0160
09/16 12:33:47 PM: Evaluate: task edges-pos-ontonotes, batch 98 (157): mcc: 0.8527, acc: 0.8135, precision: 0.8854, recall: 0.8270, f1: 0.8552, edges-pos-ontonotes_loss: 0.0158
09/16 12:33:58 PM: Evaluate: task edges-pos-ontonotes, batch 139 (157): mcc: 0.8411, acc: 0.7959, precision: 0.8788, recall: 0.8111, f1: 0.8436, edges-pos-ontonotes_loss: 0.0165
09/16 12:34:00 PM: Updating LR scheduler:
09/16 12:34:00 PM: 	Best result seen so far for macro_avg: 0.846
09/16 12:34:00 PM: 	# validation passes without improvement: 1
09/16 12:34:00 PM: edges-pos-ontonotes_loss: training: 0.012527 validation: 0.016817
09/16 12:34:00 PM: macro_avg: validation: 0.841154
09/16 12:34:00 PM: micro_avg: validation: 0.000000
09/16 12:34:00 PM: edges-pos-ontonotes_mcc: training: 0.875061 validation: 0.838633
09/16 12:34:00 PM: edges-pos-ontonotes_acc: training: 0.818323 validation: 0.792702
09/16 12:34:00 PM: edges-pos-ontonotes_precision: training: 0.901563 validation: 0.876740
09/16 12:34:00 PM: edges-pos-ontonotes_recall: training: 0.854251 validation: 0.808343
09/16 12:34:00 PM: edges-pos-ontonotes_f1: training: 0.877269 validation: 0.841154
09/16 12:34:00 PM: Global learning rate: 0.0001
09/16 12:34:00 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:34:08 PM: Update 3104: task edges-pos-ontonotes, batch 104 (3104): mcc: 0.8606, acc: 0.8088, precision: 0.8867, recall: 0.8408, f1: 0.8631, edges-pos-ontonotes_loss: 0.0129
09/16 12:34:18 PM: Update 3170: task edges-pos-ontonotes, batch 170 (3170): mcc: 0.8429, acc: 0.7837, precision: 0.8716, recall: 0.8212, f1: 0.8457, edges-pos-ontonotes_loss: 0.0143
09/16 12:34:28 PM: Update 3225: task edges-pos-ontonotes, batch 225 (3225): mcc: 0.8369, acc: 0.7738, precision: 0.8697, recall: 0.8115, f1: 0.8396, edges-pos-ontonotes_loss: 0.0153
09/16 12:34:38 PM: Update 3276: task edges-pos-ontonotes, batch 276 (3276): mcc: 0.8354, acc: 0.7706, precision: 0.8705, recall: 0.8079, f1: 0.8381, edges-pos-ontonotes_loss: 0.0160
09/16 12:34:48 PM: Update 3346: task edges-pos-ontonotes, batch 346 (3346): mcc: 0.8354, acc: 0.7700, precision: 0.8712, recall: 0.8073, f1: 0.8380, edges-pos-ontonotes_loss: 0.0165
09/16 12:34:58 PM: Update 3429: task edges-pos-ontonotes, batch 429 (3429): mcc: 0.8355, acc: 0.7698, precision: 0.8717, recall: 0.8072, f1: 0.8382, edges-pos-ontonotes_loss: 0.0167
09/16 12:35:14 PM: Update 3461: task edges-pos-ontonotes, batch 461 (3461): mcc: 0.8348, acc: 0.7687, precision: 0.8712, recall: 0.8063, f1: 0.8375, edges-pos-ontonotes_loss: 0.0168
09/16 12:35:25 PM: Update 3525: task edges-pos-ontonotes, batch 525 (3525): mcc: 0.8367, acc: 0.7714, precision: 0.8724, recall: 0.8087, f1: 0.8394, edges-pos-ontonotes_loss: 0.0166
09/16 12:35:35 PM: Update 3622: task edges-pos-ontonotes, batch 622 (3622): mcc: 0.8393, acc: 0.7753, precision: 0.8740, recall: 0.8122, f1: 0.8420, edges-pos-ontonotes_loss: 0.0161
09/16 12:35:45 PM: Update 3704: task edges-pos-ontonotes, batch 704 (3704): mcc: 0.8407, acc: 0.7774, precision: 0.8745, recall: 0.8143, f1: 0.8434, edges-pos-ontonotes_loss: 0.0158
09/16 12:35:55 PM: Update 3781: task edges-pos-ontonotes, batch 781 (3781): mcc: 0.8422, acc: 0.7798, precision: 0.8753, recall: 0.8165, f1: 0.8449, edges-pos-ontonotes_loss: 0.0156
09/16 12:36:05 PM: Update 3856: task edges-pos-ontonotes, batch 856 (3856): mcc: 0.8433, acc: 0.7809, precision: 0.8758, recall: 0.8181, f1: 0.8459, edges-pos-ontonotes_loss: 0.0155
09/16 12:36:15 PM: Update 3930: task edges-pos-ontonotes, batch 930 (3930): mcc: 0.8444, acc: 0.7821, precision: 0.8764, recall: 0.8196, f1: 0.8471, edges-pos-ontonotes_loss: 0.0153
09/16 12:36:23 PM: ***** Step 4000 / Validation 4 *****
09/16 12:36:23 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:36:23 PM: Validating...
09/16 12:36:25 PM: Evaluate: task edges-pos-ontonotes, batch 11 (157): mcc: 0.8606, acc: 0.8316, precision: 0.8797, recall: 0.8474, f1: 0.8633, edges-pos-ontonotes_loss: 0.0138
09/16 12:36:35 PM: Evaluate: task edges-pos-ontonotes, batch 102 (157): mcc: 0.8697, acc: 0.8404, precision: 0.8919, recall: 0.8533, f1: 0.8722, edges-pos-ontonotes_loss: 0.0134
09/16 12:36:44 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:36:44 PM: Best result seen so far for macro.
09/16 12:36:44 PM: Updating LR scheduler:
09/16 12:36:44 PM: 	Best result seen so far for macro_avg: 0.859
09/16 12:36:44 PM: 	# validation passes without improvement: 0
09/16 12:36:44 PM: edges-pos-ontonotes_loss: training: 0.015240 validation: 0.014186
09/16 12:36:44 PM: macro_avg: validation: 0.859090
09/16 12:36:44 PM: micro_avg: validation: 0.000000
09/16 12:36:44 PM: edges-pos-ontonotes_mcc: training: 0.845392 validation: 0.856444
09/16 12:36:44 PM: edges-pos-ontonotes_acc: training: 0.783072 validation: 0.822735
09/16 12:36:44 PM: edges-pos-ontonotes_precision: training: 0.877119 validation: 0.880995
09/16 12:36:44 PM: edges-pos-ontonotes_recall: training: 0.820811 validation: 0.838249
09/16 12:36:44 PM: edges-pos-ontonotes_f1: training: 0.848031 validation: 0.859090
09/16 12:36:44 PM: Global learning rate: 0.0001
09/16 12:36:44 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:36:45 PM: Update 4013: task edges-pos-ontonotes, batch 13 (4013): mcc: 0.8609, acc: 0.8052, precision: 0.8844, recall: 0.8436, f1: 0.8635, edges-pos-ontonotes_loss: 0.0140
09/16 12:36:56 PM: Update 4087: task edges-pos-ontonotes, batch 87 (4087): mcc: 0.8577, acc: 0.7993, precision: 0.8844, recall: 0.8374, f1: 0.8603, edges-pos-ontonotes_loss: 0.0141
09/16 12:37:06 PM: Update 4145: task edges-pos-ontonotes, batch 145 (4145): mcc: 0.8514, acc: 0.7908, precision: 0.8787, recall: 0.8308, f1: 0.8541, edges-pos-ontonotes_loss: 0.0151
09/16 12:37:16 PM: Update 4201: task edges-pos-ontonotes, batch 201 (4201): mcc: 0.8500, acc: 0.7890, precision: 0.8770, recall: 0.8296, f1: 0.8527, edges-pos-ontonotes_loss: 0.0154
09/16 12:37:26 PM: Update 4274: task edges-pos-ontonotes, batch 274 (4274): mcc: 0.8508, acc: 0.7897, precision: 0.8776, recall: 0.8306, f1: 0.8535, edges-pos-ontonotes_loss: 0.0155
09/16 12:37:36 PM: Update 4341: task edges-pos-ontonotes, batch 341 (4341): mcc: 0.8508, acc: 0.7898, precision: 0.8775, recall: 0.8307, f1: 0.8535, edges-pos-ontonotes_loss: 0.0155
09/16 12:37:47 PM: Update 4400: task edges-pos-ontonotes, batch 400 (4400): mcc: 0.8506, acc: 0.7896, precision: 0.8775, recall: 0.8305, f1: 0.8533, edges-pos-ontonotes_loss: 0.0154
09/16 12:37:57 PM: Update 4466: task edges-pos-ontonotes, batch 466 (4466): mcc: 0.8502, acc: 0.7896, precision: 0.8773, recall: 0.8297, f1: 0.8529, edges-pos-ontonotes_loss: 0.0155
09/16 12:38:07 PM: Update 4527: task edges-pos-ontonotes, batch 527 (4527): mcc: 0.8505, acc: 0.7904, precision: 0.8776, recall: 0.8301, f1: 0.8532, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:17 PM: Update 4592: task edges-pos-ontonotes, batch 592 (4592): mcc: 0.8504, acc: 0.7907, precision: 0.8775, recall: 0.8301, f1: 0.8531, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:27 PM: Update 4659: task edges-pos-ontonotes, batch 659 (4659): mcc: 0.8511, acc: 0.7917, precision: 0.8782, recall: 0.8307, f1: 0.8538, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:37 PM: Update 4713: task edges-pos-ontonotes, batch 713 (4713): mcc: 0.8511, acc: 0.7920, precision: 0.8783, recall: 0.8306, f1: 0.8538, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:47 PM: Update 4783: task edges-pos-ontonotes, batch 783 (4783): mcc: 0.8516, acc: 0.7929, precision: 0.8787, recall: 0.8311, f1: 0.8543, edges-pos-ontonotes_loss: 0.0155
09/16 12:38:57 PM: Update 4853: task edges-pos-ontonotes, batch 853 (4853): mcc: 0.8519, acc: 0.7934, precision: 0.8790, recall: 0.8314, f1: 0.8545, edges-pos-ontonotes_loss: 0.0155
09/16 12:39:07 PM: Update 4907: task edges-pos-ontonotes, batch 907 (4907): mcc: 0.8520, acc: 0.7936, precision: 0.8791, recall: 0.8315, f1: 0.8546, edges-pos-ontonotes_loss: 0.0155
09/16 12:39:17 PM: Update 4973: task edges-pos-ontonotes, batch 973 (4973): mcc: 0.8524, acc: 0.7942, precision: 0.8795, recall: 0.8319, f1: 0.8550, edges-pos-ontonotes_loss: 0.0155
09/16 12:39:22 PM: ***** Step 5000 / Validation 5 *****
09/16 12:39:22 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:39:22 PM: Validating...
09/16 12:39:28 PM: Evaluate: task edges-pos-ontonotes, batch 55 (157): mcc: 0.8598, acc: 0.8286, precision: 0.8806, recall: 0.8450, f1: 0.8624, edges-pos-ontonotes_loss: 0.0148
09/16 12:39:38 PM: Evaluate: task edges-pos-ontonotes, batch 130 (157): mcc: 0.8621, acc: 0.8315, precision: 0.8829, recall: 0.8472, f1: 0.8647, edges-pos-ontonotes_loss: 0.0143
09/16 12:39:42 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:39:42 PM: Best result seen so far for macro.
09/16 12:39:42 PM: Updating LR scheduler:
09/16 12:39:42 PM: 	Best result seen so far for macro_avg: 0.866
09/16 12:39:42 PM: 	# validation passes without improvement: 0
09/16 12:39:42 PM: edges-pos-ontonotes_loss: training: 0.015530 validation: 0.014211
09/16 12:39:42 PM: macro_avg: validation: 0.866249
09/16 12:39:42 PM: micro_avg: validation: 0.000000
09/16 12:39:42 PM: edges-pos-ontonotes_mcc: training: 0.852465 validation: 0.863671
09/16 12:39:42 PM: edges-pos-ontonotes_acc: training: 0.794382 validation: 0.832672
09/16 12:39:42 PM: edges-pos-ontonotes_precision: training: 0.879563 validation: 0.885134
09/16 12:39:42 PM: edges-pos-ontonotes_recall: training: 0.831993 validation: 0.848154
09/16 12:39:42 PM: edges-pos-ontonotes_f1: training: 0.855117 validation: 0.866249
09/16 12:39:42 PM: Global learning rate: 0.0001
09/16 12:39:42 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:40:00 PM: Update 5026: task edges-pos-ontonotes, batch 26 (5026): mcc: 0.8628, acc: 0.8068, precision: 0.8908, recall: 0.8411, f1: 0.8652, edges-pos-ontonotes_loss: 0.0144
09/16 12:40:10 PM: Update 5092: task edges-pos-ontonotes, batch 92 (5092): mcc: 0.8577, acc: 0.8014, precision: 0.8848, recall: 0.8371, f1: 0.8603, edges-pos-ontonotes_loss: 0.0151
09/16 12:40:20 PM: Update 5152: task edges-pos-ontonotes, batch 152 (5152): mcc: 0.8593, acc: 0.8036, precision: 0.8858, recall: 0.8390, f1: 0.8618, edges-pos-ontonotes_loss: 0.0150
09/16 12:40:30 PM: Update 5214: task edges-pos-ontonotes, batch 214 (5214): mcc: 0.8593, acc: 0.8040, precision: 0.8857, recall: 0.8393, f1: 0.8619, edges-pos-ontonotes_loss: 0.0150
09/16 12:40:40 PM: Update 5279: task edges-pos-ontonotes, batch 279 (5279): mcc: 0.8591, acc: 0.8037, precision: 0.8850, recall: 0.8394, f1: 0.8616, edges-pos-ontonotes_loss: 0.0150
09/16 12:40:50 PM: Update 5336: task edges-pos-ontonotes, batch 336 (5336): mcc: 0.8590, acc: 0.8035, precision: 0.8850, recall: 0.8393, f1: 0.8615, edges-pos-ontonotes_loss: 0.0150
09/16 12:41:00 PM: Update 5376: task edges-pos-ontonotes, batch 376 (5376): mcc: 0.8583, acc: 0.8030, precision: 0.8839, recall: 0.8390, f1: 0.8609, edges-pos-ontonotes_loss: 0.0149
09/16 12:41:10 PM: Update 5457: task edges-pos-ontonotes, batch 457 (5457): mcc: 0.8597, acc: 0.8042, precision: 0.8854, recall: 0.8404, f1: 0.8623, edges-pos-ontonotes_loss: 0.0145
09/16 12:41:20 PM: Update 5548: task edges-pos-ontonotes, batch 548 (5548): mcc: 0.8615, acc: 0.8058, precision: 0.8868, recall: 0.8423, f1: 0.8640, edges-pos-ontonotes_loss: 0.0141
09/16 12:41:30 PM: Update 5622: task edges-pos-ontonotes, batch 622 (5622): mcc: 0.8622, acc: 0.8065, precision: 0.8872, recall: 0.8434, f1: 0.8647, edges-pos-ontonotes_loss: 0.0139
09/16 12:41:41 PM: Update 5695: task edges-pos-ontonotes, batch 695 (5695): mcc: 0.8638, acc: 0.8080, precision: 0.8885, recall: 0.8451, f1: 0.8663, edges-pos-ontonotes_loss: 0.0136
09/16 12:41:51 PM: Update 5797: task edges-pos-ontonotes, batch 797 (5797): mcc: 0.8663, acc: 0.8110, precision: 0.8904, recall: 0.8480, f1: 0.8687, edges-pos-ontonotes_loss: 0.0132
09/16 12:42:01 PM: Update 5894: task edges-pos-ontonotes, batch 894 (5894): mcc: 0.8686, acc: 0.8138, precision: 0.8922, recall: 0.8508, f1: 0.8710, edges-pos-ontonotes_loss: 0.0129
09/16 12:42:11 PM: Update 5991: task edges-pos-ontonotes, batch 991 (5991): mcc: 0.8702, acc: 0.8157, precision: 0.8935, recall: 0.8527, f1: 0.8726, edges-pos-ontonotes_loss: 0.0126
09/16 12:42:11 PM: ***** Step 6000 / Validation 6 *****
09/16 12:42:11 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:42:11 PM: Validating...
09/16 12:42:21 PM: Evaluate: task edges-pos-ontonotes, batch 92 (157): mcc: 0.8545, acc: 0.8098, precision: 0.8894, recall: 0.8265, f1: 0.8568, edges-pos-ontonotes_loss: 0.0146
09/16 12:42:31 PM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.8511, acc: 0.8043, precision: 0.8884, recall: 0.8210, f1: 0.8534, edges-pos-ontonotes_loss: 0.0149
09/16 12:42:31 PM: Updating LR scheduler:
09/16 12:42:31 PM: 	Best result seen so far for macro_avg: 0.866
09/16 12:42:31 PM: 	# validation passes without improvement: 1
09/16 12:42:31 PM: edges-pos-ontonotes_loss: training: 0.012637 validation: 0.014949
09/16 12:42:31 PM: macro_avg: validation: 0.853398
09/16 12:42:31 PM: micro_avg: validation: 0.000000
09/16 12:42:31 PM: edges-pos-ontonotes_mcc: training: 0.870244 validation: 0.851092
09/16 12:42:31 PM: edges-pos-ontonotes_acc: training: 0.815744 validation: 0.804332
09/16 12:42:31 PM: edges-pos-ontonotes_precision: training: 0.893542 validation: 0.888385
09/16 12:42:31 PM: edges-pos-ontonotes_recall: training: 0.852695 validation: 0.821063
09/16 12:42:31 PM: edges-pos-ontonotes_f1: training: 0.872641 validation: 0.853398
09/16 12:42:31 PM: Global learning rate: 0.0001
09/16 12:42:31 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:42:41 PM: Update 6128: task edges-pos-ontonotes, batch 128 (6128): mcc: 0.8887, acc: 0.8406, precision: 0.9069, recall: 0.8753, f1: 0.8908, edges-pos-ontonotes_loss: 0.0111
09/16 12:42:51 PM: Update 6218: task edges-pos-ontonotes, batch 218 (6218): mcc: 0.8912, acc: 0.8452, precision: 0.9081, recall: 0.8789, f1: 0.8933, edges-pos-ontonotes_loss: 0.0108
09/16 12:43:01 PM: Update 6339: task edges-pos-ontonotes, batch 339 (6339): mcc: 0.8856, acc: 0.8393, precision: 0.9030, recall: 0.8731, f1: 0.8878, edges-pos-ontonotes_loss: 0.0111
09/16 12:43:11 PM: Update 6441: task edges-pos-ontonotes, batch 441 (6441): mcc: 0.8819, acc: 0.8357, precision: 0.9000, recall: 0.8689, f1: 0.8842, edges-pos-ontonotes_loss: 0.0114
09/16 12:43:21 PM: Update 6577: task edges-pos-ontonotes, batch 577 (6577): mcc: 0.8792, acc: 0.8332, precision: 0.8976, recall: 0.8660, f1: 0.8815, edges-pos-ontonotes_loss: 0.0114
09/16 12:43:32 PM: Update 6644: task edges-pos-ontonotes, batch 644 (6644): mcc: 0.8708, acc: 0.8218, precision: 0.8913, recall: 0.8558, f1: 0.8732, edges-pos-ontonotes_loss: 0.0121
09/16 12:43:42 PM: Update 6719: task edges-pos-ontonotes, batch 719 (6719): mcc: 0.8650, acc: 0.8132, precision: 0.8877, recall: 0.8483, f1: 0.8676, edges-pos-ontonotes_loss: 0.0126
09/16 12:43:52 PM: Update 6786: task edges-pos-ontonotes, batch 786 (6786): mcc: 0.8606, acc: 0.8069, precision: 0.8850, recall: 0.8425, f1: 0.8632, edges-pos-ontonotes_loss: 0.0130
09/16 12:44:02 PM: Update 6851: task edges-pos-ontonotes, batch 851 (6851): mcc: 0.8585, acc: 0.8039, precision: 0.8837, recall: 0.8396, f1: 0.8611, edges-pos-ontonotes_loss: 0.0133
09/16 12:44:12 PM: Update 6918: task edges-pos-ontonotes, batch 918 (6918): mcc: 0.8567, acc: 0.8014, precision: 0.8826, recall: 0.8372, f1: 0.8593, edges-pos-ontonotes_loss: 0.0135
09/16 12:44:21 PM: ***** Step 7000 / Validation 7 *****
09/16 12:44:21 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:44:21 PM: Validating...
09/16 12:44:22 PM: Evaluate: task edges-pos-ontonotes, batch 6 (157): mcc: 0.8683, acc: 0.8442, precision: 0.8845, recall: 0.8576, f1: 0.8708, edges-pos-ontonotes_loss: 0.0134
09/16 12:44:32 PM: Evaluate: task edges-pos-ontonotes, batch 101 (157): mcc: 0.8696, acc: 0.8409, precision: 0.8915, recall: 0.8534, f1: 0.8720, edges-pos-ontonotes_loss: 0.0132
09/16 12:44:41 PM: Updating LR scheduler:
09/16 12:44:41 PM: 	Best result seen so far for macro_avg: 0.866
09/16 12:44:41 PM: 	# validation passes without improvement: 2
09/16 12:44:41 PM: edges-pos-ontonotes_loss: training: 0.013528 validation: 0.013921
09/16 12:44:41 PM: macro_avg: validation: 0.860346
09/16 12:44:41 PM: micro_avg: validation: 0.000000
09/16 12:44:41 PM: edges-pos-ontonotes_mcc: training: 0.856683 validation: 0.857723
09/16 12:44:41 PM: edges-pos-ontonotes_acc: training: 0.801501 validation: 0.824714
09/16 12:44:41 PM: edges-pos-ontonotes_precision: training: 0.882416 validation: 0.882152
09/16 12:44:41 PM: edges-pos-ontonotes_recall: training: 0.837346 validation: 0.839593
09/16 12:44:41 PM: edges-pos-ontonotes_f1: training: 0.859290 validation: 0.860346
09/16 12:44:41 PM: Global learning rate: 0.0001
09/16 12:44:41 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:44:42 PM: Update 7013: task edges-pos-ontonotes, batch 13 (7013): mcc: 0.8623, acc: 0.8140, precision: 0.8843, recall: 0.8464, f1: 0.8649, edges-pos-ontonotes_loss: 0.0134
09/16 12:44:52 PM: Update 7131: task edges-pos-ontonotes, batch 131 (7131): mcc: 0.8618, acc: 0.8113, precision: 0.8848, recall: 0.8449, f1: 0.8644, edges-pos-ontonotes_loss: 0.0126
09/16 12:45:04 PM: Update 7234: task edges-pos-ontonotes, batch 234 (7234): mcc: 0.8631, acc: 0.8126, precision: 0.8856, recall: 0.8466, f1: 0.8657, edges-pos-ontonotes_loss: 0.0126
09/16 12:45:14 PM: Update 7299: task edges-pos-ontonotes, batch 299 (7299): mcc: 0.8627, acc: 0.8106, precision: 0.8856, recall: 0.8458, f1: 0.8653, edges-pos-ontonotes_loss: 0.0129
09/16 12:45:24 PM: Update 7386: task edges-pos-ontonotes, batch 386 (7386): mcc: 0.8626, acc: 0.8092, precision: 0.8856, recall: 0.8456, f1: 0.8652, edges-pos-ontonotes_loss: 0.0130
09/16 12:45:34 PM: Update 7463: task edges-pos-ontonotes, batch 463 (7463): mcc: 0.8626, acc: 0.8083, precision: 0.8856, recall: 0.8456, f1: 0.8651, edges-pos-ontonotes_loss: 0.0130
09/16 12:45:44 PM: Update 7543: task edges-pos-ontonotes, batch 543 (7543): mcc: 0.8629, acc: 0.8087, precision: 0.8858, recall: 0.8461, f1: 0.8655, edges-pos-ontonotes_loss: 0.0130
09/16 12:45:59 PM: Update 7547: task edges-pos-ontonotes, batch 547 (7547): mcc: 0.8625, acc: 0.8081, precision: 0.8855, recall: 0.8455, f1: 0.8650, edges-pos-ontonotes_loss: 0.0130
09/16 12:46:09 PM: Update 7598: task edges-pos-ontonotes, batch 598 (7598): mcc: 0.8600, acc: 0.8045, precision: 0.8832, recall: 0.8430, f1: 0.8626, edges-pos-ontonotes_loss: 0.0133
09/16 12:46:19 PM: Update 7662: task edges-pos-ontonotes, batch 662 (7662): mcc: 0.8589, acc: 0.8030, precision: 0.8822, recall: 0.8418, f1: 0.8615, edges-pos-ontonotes_loss: 0.0135
09/16 12:46:29 PM: Update 7729: task edges-pos-ontonotes, batch 729 (7729): mcc: 0.8584, acc: 0.8023, precision: 0.8818, recall: 0.8412, f1: 0.8610, edges-pos-ontonotes_loss: 0.0136
09/16 12:46:40 PM: Update 7782: task edges-pos-ontonotes, batch 782 (7782): mcc: 0.8584, acc: 0.8023, precision: 0.8818, recall: 0.8413, f1: 0.8611, edges-pos-ontonotes_loss: 0.0137
09/16 12:46:50 PM: Update 7830: task edges-pos-ontonotes, batch 830 (7830): mcc: 0.8584, acc: 0.8024, precision: 0.8816, recall: 0.8414, f1: 0.8610, edges-pos-ontonotes_loss: 0.0137
09/16 12:47:00 PM: Update 7869: task edges-pos-ontonotes, batch 869 (7869): mcc: 0.8581, acc: 0.8022, precision: 0.8814, recall: 0.8410, f1: 0.8608, edges-pos-ontonotes_loss: 0.0138
09/16 12:47:10 PM: Update 7935: task edges-pos-ontonotes, batch 935 (7935): mcc: 0.8578, acc: 0.8019, precision: 0.8813, recall: 0.8405, f1: 0.8604, edges-pos-ontonotes_loss: 0.0139
09/16 12:47:20 PM: Update 8000: task edges-pos-ontonotes, batch 1000 (8000): mcc: 0.8575, acc: 0.8018, precision: 0.8811, recall: 0.8403, f1: 0.8602, edges-pos-ontonotes_loss: 0.0140
09/16 12:47:20 PM: ***** Step 8000 / Validation 8 *****
09/16 12:47:20 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:47:20 PM: Validating...
09/16 12:47:30 PM: Evaluate: task edges-pos-ontonotes, batch 95 (157): mcc: 0.8676, acc: 0.8398, precision: 0.8868, recall: 0.8540, f1: 0.8701, edges-pos-ontonotes_loss: 0.0134
09/16 12:47:39 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:47:39 PM: Best result seen so far for macro.
09/16 12:47:39 PM: Updating LR scheduler:
09/16 12:47:39 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:47:39 PM: 	# validation passes without improvement: 0
09/16 12:47:39 PM: edges-pos-ontonotes_loss: training: 0.013975 validation: 0.013380
09/16 12:47:39 PM: macro_avg: validation: 0.870672
09/16 12:47:39 PM: micro_avg: validation: 0.000000
09/16 12:47:39 PM: edges-pos-ontonotes_mcc: training: 0.857547 validation: 0.868143
09/16 12:47:39 PM: edges-pos-ontonotes_acc: training: 0.801833 validation: 0.838101
09/16 12:47:39 PM: edges-pos-ontonotes_precision: training: 0.881071 validation: 0.887784
09/16 12:47:39 PM: edges-pos-ontonotes_recall: training: 0.840293 validation: 0.854207
09/16 12:47:39 PM: edges-pos-ontonotes_f1: training: 0.860199 validation: 0.870672
09/16 12:47:39 PM: Global learning rate: 0.0001
09/16 12:47:39 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:47:40 PM: Update 8003: task edges-pos-ontonotes, batch 3 (8003): mcc: 0.8452, acc: 0.7918, precision: 0.8673, recall: 0.8298, f1: 0.8481, edges-pos-ontonotes_loss: 0.0159
09/16 12:47:50 PM: Update 8072: task edges-pos-ontonotes, batch 72 (8072): mcc: 0.8595, acc: 0.8059, precision: 0.8836, recall: 0.8416, f1: 0.8621, edges-pos-ontonotes_loss: 0.0148
09/16 12:48:00 PM: Update 8135: task edges-pos-ontonotes, batch 135 (8135): mcc: 0.8585, acc: 0.8052, precision: 0.8826, recall: 0.8406, f1: 0.8611, edges-pos-ontonotes_loss: 0.0148
09/16 12:48:10 PM: Update 8190: task edges-pos-ontonotes, batch 190 (8190): mcc: 0.8574, acc: 0.8042, precision: 0.8823, recall: 0.8388, f1: 0.8600, edges-pos-ontonotes_loss: 0.0149
09/16 12:48:21 PM: Update 8255: task edges-pos-ontonotes, batch 255 (8255): mcc: 0.8574, acc: 0.8042, precision: 0.8821, recall: 0.8390, f1: 0.8600, edges-pos-ontonotes_loss: 0.0150
09/16 12:48:31 PM: Update 8326: task edges-pos-ontonotes, batch 326 (8326): mcc: 0.8580, acc: 0.8050, precision: 0.8827, recall: 0.8396, f1: 0.8606, edges-pos-ontonotes_loss: 0.0149
09/16 12:48:41 PM: Update 8397: task edges-pos-ontonotes, batch 397 (8397): mcc: 0.8586, acc: 0.8057, precision: 0.8834, recall: 0.8401, f1: 0.8612, edges-pos-ontonotes_loss: 0.0149
09/16 12:48:51 PM: Update 8465: task edges-pos-ontonotes, batch 465 (8465): mcc: 0.8591, acc: 0.8064, precision: 0.8837, recall: 0.8408, f1: 0.8617, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:01 PM: Update 8515: task edges-pos-ontonotes, batch 515 (8515): mcc: 0.8591, acc: 0.8065, precision: 0.8837, recall: 0.8408, f1: 0.8617, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:11 PM: Update 8585: task edges-pos-ontonotes, batch 585 (8585): mcc: 0.8593, acc: 0.8068, precision: 0.8838, recall: 0.8410, f1: 0.8619, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:21 PM: Update 8635: task edges-pos-ontonotes, batch 635 (8635): mcc: 0.8596, acc: 0.8070, precision: 0.8841, recall: 0.8414, f1: 0.8622, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:31 PM: Update 8687: task edges-pos-ontonotes, batch 687 (8687): mcc: 0.8597, acc: 0.8069, precision: 0.8840, recall: 0.8415, f1: 0.8622, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:41 PM: Update 8754: task edges-pos-ontonotes, batch 754 (8754): mcc: 0.8599, acc: 0.8071, precision: 0.8843, recall: 0.8418, f1: 0.8625, edges-pos-ontonotes_loss: 0.0147
09/16 12:49:51 PM: Update 8803: task edges-pos-ontonotes, batch 803 (8803): mcc: 0.8598, acc: 0.8071, precision: 0.8841, recall: 0.8418, f1: 0.8624, edges-pos-ontonotes_loss: 0.0147
09/16 12:50:02 PM: Update 8874: task edges-pos-ontonotes, batch 874 (8874): mcc: 0.8604, acc: 0.8077, precision: 0.8843, recall: 0.8426, f1: 0.8629, edges-pos-ontonotes_loss: 0.0145
09/16 12:50:12 PM: Update 8948: task edges-pos-ontonotes, batch 948 (8948): mcc: 0.8610, acc: 0.8080, precision: 0.8849, recall: 0.8433, f1: 0.8636, edges-pos-ontonotes_loss: 0.0144
09/16 12:50:18 PM: ***** Step 9000 / Validation 9 *****
09/16 12:50:18 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:50:18 PM: Validating...
09/16 12:50:22 PM: Evaluate: task edges-pos-ontonotes, batch 35 (157): mcc: 0.8518, acc: 0.8217, precision: 0.8772, recall: 0.8330, f1: 0.8545, edges-pos-ontonotes_loss: 0.0149
09/16 12:50:32 PM: Evaluate: task edges-pos-ontonotes, batch 118 (157): mcc: 0.8612, acc: 0.8306, precision: 0.8853, recall: 0.8432, f1: 0.8637, edges-pos-ontonotes_loss: 0.0141
09/16 12:50:38 PM: Updating LR scheduler:
09/16 12:50:38 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:50:38 PM: 	# validation passes without improvement: 1
09/16 12:50:38 PM: edges-pos-ontonotes_loss: training: 0.014236 validation: 0.013981
09/16 12:50:38 PM: macro_avg: validation: 0.864708
09/16 12:50:38 PM: micro_avg: validation: 0.000000
09/16 12:50:38 PM: edges-pos-ontonotes_mcc: training: 0.861433 validation: 0.862224
09/16 12:50:38 PM: edges-pos-ontonotes_acc: training: 0.808397 validation: 0.829836
09/16 12:50:38 PM: edges-pos-ontonotes_precision: training: 0.885264 validation: 0.888292
09/16 12:50:38 PM: edges-pos-ontonotes_recall: training: 0.843728 validation: 0.842344
09/16 12:50:38 PM: edges-pos-ontonotes_f1: training: 0.863997 validation: 0.864708
09/16 12:50:38 PM: Global learning rate: 0.0001
09/16 12:50:38 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:50:42 PM: Update 9034: task edges-pos-ontonotes, batch 34 (9034): mcc: 0.8788, acc: 0.8250, precision: 0.9000, recall: 0.8629, f1: 0.8811, edges-pos-ontonotes_loss: 0.0119
09/16 12:51:03 PM: Update 9112: task edges-pos-ontonotes, batch 112 (9112): mcc: 0.8743, acc: 0.8194, precision: 0.8960, recall: 0.8581, f1: 0.8767, edges-pos-ontonotes_loss: 0.0118
09/16 12:51:13 PM: Update 9221: task edges-pos-ontonotes, batch 221 (9221): mcc: 0.8822, acc: 0.8301, precision: 0.9014, recall: 0.8680, f1: 0.8844, edges-pos-ontonotes_loss: 0.0110
09/16 12:51:23 PM: Update 9333: task edges-pos-ontonotes, batch 333 (9333): mcc: 0.8868, acc: 0.8364, precision: 0.9051, recall: 0.8734, f1: 0.8889, edges-pos-ontonotes_loss: 0.0106
09/16 12:51:33 PM: Update 9426: task edges-pos-ontonotes, batch 426 (9426): mcc: 0.8886, acc: 0.8385, precision: 0.9063, recall: 0.8757, f1: 0.8908, edges-pos-ontonotes_loss: 0.0104
09/16 12:51:43 PM: Update 9554: task edges-pos-ontonotes, batch 554 (9554): mcc: 0.8886, acc: 0.8389, precision: 0.9063, recall: 0.8757, f1: 0.8908, edges-pos-ontonotes_loss: 0.0106
09/16 12:51:53 PM: Update 9696: task edges-pos-ontonotes, batch 696 (9696): mcc: 0.8898, acc: 0.8411, precision: 0.9070, recall: 0.8774, f1: 0.8919, edges-pos-ontonotes_loss: 0.0105
09/16 12:52:03 PM: Update 9839: task edges-pos-ontonotes, batch 839 (9839): mcc: 0.8881, acc: 0.8399, precision: 0.9055, recall: 0.8756, f1: 0.8903, edges-pos-ontonotes_loss: 0.0107
09/16 12:52:13 PM: Update 9982: task edges-pos-ontonotes, batch 982 (9982): mcc: 0.8864, acc: 0.8386, precision: 0.9038, recall: 0.8738, f1: 0.8886, edges-pos-ontonotes_loss: 0.0108
09/16 12:52:15 PM: ***** Step 10000 / Validation 10 *****
09/16 12:52:15 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:52:15 PM: Validating...
09/16 12:52:23 PM: Evaluate: task edges-pos-ontonotes, batch 68 (157): mcc: 0.8634, acc: 0.8362, precision: 0.8902, recall: 0.8427, f1: 0.8658, edges-pos-ontonotes_loss: 0.0144
09/16 12:52:33 PM: Evaluate: task edges-pos-ontonotes, batch 138 (157): mcc: 0.8511, acc: 0.8180, precision: 0.8814, recall: 0.8276, f1: 0.8536, edges-pos-ontonotes_loss: 0.0152
09/16 12:52:36 PM: Updating LR scheduler:
09/16 12:52:36 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:52:36 PM: 	# validation passes without improvement: 2
09/16 12:52:36 PM: edges-pos-ontonotes_loss: training: 0.010767 validation: 0.015479
09/16 12:52:36 PM: macro_avg: validation: 0.851542
09/16 12:52:36 PM: micro_avg: validation: 0.000000
09/16 12:52:36 PM: edges-pos-ontonotes_mcc: training: 0.886296 validation: 0.848957
09/16 12:52:36 PM: edges-pos-ontonotes_acc: training: 0.838607 validation: 0.814767
09/16 12:52:36 PM: edges-pos-ontonotes_precision: training: 0.903736 validation: 0.880198
09/16 12:52:36 PM: edges-pos-ontonotes_recall: training: 0.873761 validation: 0.824693
09/16 12:52:36 PM: edges-pos-ontonotes_f1: training: 0.888496 validation: 0.851542
09/16 12:52:36 PM: Global learning rate: 0.0001
09/16 12:52:36 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:52:44 PM: Update 10051: task edges-pos-ontonotes, batch 51 (10051): mcc: 0.8540, acc: 0.8061, precision: 0.8750, recall: 0.8393, f1: 0.8567, edges-pos-ontonotes_loss: 0.0131
09/16 12:52:54 PM: Update 10114: task edges-pos-ontonotes, batch 114 (10114): mcc: 0.8414, acc: 0.7851, precision: 0.8672, recall: 0.8226, f1: 0.8443, edges-pos-ontonotes_loss: 0.0153
09/16 12:53:04 PM: Update 10183: task edges-pos-ontonotes, batch 183 (10183): mcc: 0.8408, acc: 0.7821, precision: 0.8681, recall: 0.8206, f1: 0.8437, edges-pos-ontonotes_loss: 0.0159
09/16 12:53:14 PM: Update 10251: task edges-pos-ontonotes, batch 251 (10251): mcc: 0.8421, acc: 0.7835, precision: 0.8699, recall: 0.8213, f1: 0.8449, edges-pos-ontonotes_loss: 0.0161
09/16 12:53:24 PM: Update 10326: task edges-pos-ontonotes, batch 326 (10326): mcc: 0.8427, acc: 0.7837, precision: 0.8711, recall: 0.8214, f1: 0.8455, edges-pos-ontonotes_loss: 0.0162
09/16 12:53:34 PM: Update 10391: task edges-pos-ontonotes, batch 391 (10391): mcc: 0.8429, acc: 0.7836, precision: 0.8714, recall: 0.8214, f1: 0.8457, edges-pos-ontonotes_loss: 0.0162
09/16 12:53:44 PM: Update 10508: task edges-pos-ontonotes, batch 508 (10508): mcc: 0.8464, acc: 0.7885, precision: 0.8739, recall: 0.8257, f1: 0.8491, edges-pos-ontonotes_loss: 0.0154
09/16 12:53:54 PM: Update 10618: task edges-pos-ontonotes, batch 618 (10618): mcc: 0.8486, acc: 0.7920, precision: 0.8754, recall: 0.8285, f1: 0.8513, edges-pos-ontonotes_loss: 0.0149
09/16 12:54:05 PM: Update 10694: task edges-pos-ontonotes, batch 694 (10694): mcc: 0.8500, acc: 0.7940, precision: 0.8763, recall: 0.8304, f1: 0.8527, edges-pos-ontonotes_loss: 0.0146
09/16 12:54:15 PM: Update 10774: task edges-pos-ontonotes, batch 774 (10774): mcc: 0.8514, acc: 0.7957, precision: 0.8773, recall: 0.8322, f1: 0.8542, edges-pos-ontonotes_loss: 0.0145
09/16 12:54:25 PM: Update 10863: task edges-pos-ontonotes, batch 863 (10863): mcc: 0.8530, acc: 0.7972, precision: 0.8784, recall: 0.8340, f1: 0.8556, edges-pos-ontonotes_loss: 0.0143
09/16 12:54:36 PM: Update 10936: task edges-pos-ontonotes, batch 936 (10936): mcc: 0.8540, acc: 0.7985, precision: 0.8792, recall: 0.8354, f1: 0.8567, edges-pos-ontonotes_loss: 0.0142
09/16 12:54:42 PM: ***** Step 11000 / Validation 11 *****
09/16 12:54:42 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:54:42 PM: Validating...
09/16 12:54:46 PM: Evaluate: task edges-pos-ontonotes, batch 34 (157): mcc: 0.8691, acc: 0.8345, precision: 0.9014, recall: 0.8430, f1: 0.8712, edges-pos-ontonotes_loss: 0.0131
09/16 12:54:56 PM: Evaluate: task edges-pos-ontonotes, batch 117 (157): mcc: 0.8732, acc: 0.8399, precision: 0.9022, recall: 0.8502, f1: 0.8754, edges-pos-ontonotes_loss: 0.0127
09/16 12:55:02 PM: Updating LR scheduler:
09/16 12:55:02 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:55:02 PM: 	# validation passes without improvement: 3
09/16 12:55:02 PM: edges-pos-ontonotes_loss: training: 0.014134 validation: 0.013139
09/16 12:55:02 PM: macro_avg: validation: 0.870559
09/16 12:55:02 PM: micro_avg: validation: 0.000000
09/16 12:55:02 PM: edges-pos-ontonotes_mcc: training: 0.854772 validation: 0.868300
09/16 12:55:02 PM: edges-pos-ontonotes_acc: training: 0.799361 validation: 0.833497
09/16 12:55:02 PM: edges-pos-ontonotes_precision: training: 0.879680 validation: 0.897605
09/16 12:55:02 PM: edges-pos-ontonotes_recall: training: 0.836302 validation: 0.845096
09/16 12:55:02 PM: edges-pos-ontonotes_f1: training: 0.857442 validation: 0.870559
09/16 12:55:02 PM: Global learning rate: 0.0001
09/16 12:55:02 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:55:06 PM: Update 11013: task edges-pos-ontonotes, batch 13 (11013): mcc: 0.8357, acc: 0.7779, precision: 0.8593, recall: 0.8193, f1: 0.8388, edges-pos-ontonotes_loss: 0.0148
09/16 12:55:16 PM: Update 11085: task edges-pos-ontonotes, batch 85 (11085): mcc: 0.8526, acc: 0.7950, precision: 0.8750, recall: 0.8366, f1: 0.8554, edges-pos-ontonotes_loss: 0.0151
09/16 12:55:26 PM: Update 11151: task edges-pos-ontonotes, batch 151 (11151): mcc: 0.8541, acc: 0.7973, precision: 0.8764, recall: 0.8381, f1: 0.8569, edges-pos-ontonotes_loss: 0.0150
09/16 12:55:36 PM: Update 11221: task edges-pos-ontonotes, batch 221 (11221): mcc: 0.8548, acc: 0.7985, precision: 0.8768, recall: 0.8390, f1: 0.8575, edges-pos-ontonotes_loss: 0.0149
09/16 12:55:46 PM: Update 11287: task edges-pos-ontonotes, batch 287 (11287): mcc: 0.8550, acc: 0.7987, precision: 0.8769, recall: 0.8393, f1: 0.8577, edges-pos-ontonotes_loss: 0.0148
09/16 12:56:04 PM: Update 11320: task edges-pos-ontonotes, batch 320 (11320): mcc: 0.8553, acc: 0.7992, precision: 0.8774, recall: 0.8395, f1: 0.8581, edges-pos-ontonotes_loss: 0.0148
09/16 12:56:14 PM: Update 11385: task edges-pos-ontonotes, batch 385 (11385): mcc: 0.8554, acc: 0.7996, precision: 0.8780, recall: 0.8392, f1: 0.8582, edges-pos-ontonotes_loss: 0.0149
09/16 12:56:24 PM: Update 11447: task edges-pos-ontonotes, batch 447 (11447): mcc: 0.8557, acc: 0.8000, precision: 0.8786, recall: 0.8391, f1: 0.8584, edges-pos-ontonotes_loss: 0.0149
09/16 12:56:34 PM: Update 11511: task edges-pos-ontonotes, batch 511 (11511): mcc: 0.8559, acc: 0.8008, precision: 0.8789, recall: 0.8393, f1: 0.8586, edges-pos-ontonotes_loss: 0.0149
09/16 12:56:44 PM: Update 11580: task edges-pos-ontonotes, batch 580 (11580): mcc: 0.8565, acc: 0.8018, precision: 0.8795, recall: 0.8398, f1: 0.8592, edges-pos-ontonotes_loss: 0.0148
09/16 12:56:54 PM: Update 11634: task edges-pos-ontonotes, batch 634 (11634): mcc: 0.8569, acc: 0.8026, precision: 0.8799, recall: 0.8402, f1: 0.8596, edges-pos-ontonotes_loss: 0.0148
09/16 12:57:04 PM: Update 11707: task edges-pos-ontonotes, batch 707 (11707): mcc: 0.8572, acc: 0.8031, precision: 0.8803, recall: 0.8404, f1: 0.8599, edges-pos-ontonotes_loss: 0.0148
09/16 12:57:15 PM: Update 11777: task edges-pos-ontonotes, batch 777 (11777): mcc: 0.8576, acc: 0.8038, precision: 0.8807, recall: 0.8407, f1: 0.8603, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:25 PM: Update 11846: task edges-pos-ontonotes, batch 846 (11846): mcc: 0.8581, acc: 0.8046, precision: 0.8812, recall: 0.8413, f1: 0.8608, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:35 PM: Update 11911: task edges-pos-ontonotes, batch 911 (11911): mcc: 0.8582, acc: 0.8048, precision: 0.8813, recall: 0.8413, f1: 0.8608, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:45 PM: Update 11961: task edges-pos-ontonotes, batch 961 (11961): mcc: 0.8584, acc: 0.8052, precision: 0.8815, recall: 0.8414, f1: 0.8610, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:51 PM: ***** Step 12000 / Validation 12 *****
09/16 12:57:51 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:57:51 PM: Validating...
09/16 12:57:55 PM: Evaluate: task edges-pos-ontonotes, batch 44 (157): mcc: 0.8587, acc: 0.8313, precision: 0.8775, recall: 0.8459, f1: 0.8614, edges-pos-ontonotes_loss: 0.0143
09/16 12:58:05 PM: Evaluate: task edges-pos-ontonotes, batch 117 (157): mcc: 0.8673, acc: 0.8408, precision: 0.8855, recall: 0.8548, f1: 0.8699, edges-pos-ontonotes_loss: 0.0135
09/16 12:58:11 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:58:11 PM: Best result seen so far for macro.
09/16 12:58:11 PM: Updating LR scheduler:
09/16 12:58:11 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:58:11 PM: 	# validation passes without improvement: 0
09/16 12:58:11 PM: edges-pos-ontonotes_loss: training: 0.014680 validation: 0.013454
09/16 12:58:11 PM: macro_avg: validation: 0.870698
09/16 12:58:11 PM: micro_avg: validation: 0.000000
09/16 12:58:11 PM: edges-pos-ontonotes_mcc: training: 0.858410 validation: 0.868155
09/16 12:58:11 PM: edges-pos-ontonotes_acc: training: 0.805285 validation: 0.841021
09/16 12:58:11 PM: edges-pos-ontonotes_precision: training: 0.881574 validation: 0.887165
09/16 12:58:11 PM: edges-pos-ontonotes_recall: training: 0.841468 validation: 0.854831
09/16 12:58:11 PM: edges-pos-ontonotes_f1: training: 0.861054 validation: 0.870698
09/16 12:58:11 PM: Global learning rate: 5e-05
09/16 12:58:11 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 12:58:15 PM: Update 12022: task edges-pos-ontonotes, batch 22 (12022): mcc: 0.8644, acc: 0.8125, precision: 0.8867, recall: 0.8480, f1: 0.8669, edges-pos-ontonotes_loss: 0.0136
09/16 12:58:26 PM: Update 12068: task edges-pos-ontonotes, batch 68 (12068): mcc: 0.8629, acc: 0.8107, precision: 0.8859, recall: 0.8459, f1: 0.8654, edges-pos-ontonotes_loss: 0.0143
09/16 12:58:36 PM: Update 12129: task edges-pos-ontonotes, batch 129 (12129): mcc: 0.8619, acc: 0.8098, precision: 0.8850, recall: 0.8449, f1: 0.8645, edges-pos-ontonotes_loss: 0.0144
09/16 12:58:46 PM: Update 12184: task edges-pos-ontonotes, batch 184 (12184): mcc: 0.8634, acc: 0.8120, precision: 0.8868, recall: 0.8461, f1: 0.8660, edges-pos-ontonotes_loss: 0.0143
09/16 12:58:56 PM: Update 12239: task edges-pos-ontonotes, batch 239 (12239): mcc: 0.8636, acc: 0.8123, precision: 0.8866, recall: 0.8466, f1: 0.8661, edges-pos-ontonotes_loss: 0.0142
09/16 12:59:06 PM: Update 12285: task edges-pos-ontonotes, batch 285 (12285): mcc: 0.8629, acc: 0.8119, precision: 0.8855, recall: 0.8463, f1: 0.8654, edges-pos-ontonotes_loss: 0.0141
09/16 12:59:16 PM: Update 12365: task edges-pos-ontonotes, batch 365 (12365): mcc: 0.8640, acc: 0.8126, precision: 0.8864, recall: 0.8475, f1: 0.8665, edges-pos-ontonotes_loss: 0.0138
09/16 12:59:27 PM: Update 12430: task edges-pos-ontonotes, batch 430 (12430): mcc: 0.8653, acc: 0.8137, precision: 0.8878, recall: 0.8487, f1: 0.8678, edges-pos-ontonotes_loss: 0.0135
09/16 12:59:37 PM: Update 12498: task edges-pos-ontonotes, batch 498 (12498): mcc: 0.8658, acc: 0.8137, precision: 0.8883, recall: 0.8491, f1: 0.8683, edges-pos-ontonotes_loss: 0.0133
09/16 12:59:48 PM: Update 12572: task edges-pos-ontonotes, batch 572 (12572): mcc: 0.8666, acc: 0.8141, precision: 0.8892, recall: 0.8499, f1: 0.8691, edges-pos-ontonotes_loss: 0.0131
09/16 12:59:58 PM: Update 12684: task edges-pos-ontonotes, batch 684 (12684): mcc: 0.8693, acc: 0.8170, precision: 0.8911, recall: 0.8532, f1: 0.8717, edges-pos-ontonotes_loss: 0.0127
09/16 01:00:08 PM: Update 12775: task edges-pos-ontonotes, batch 775 (12775): mcc: 0.8714, acc: 0.8194, precision: 0.8926, recall: 0.8557, f1: 0.8738, edges-pos-ontonotes_loss: 0.0124
09/16 01:00:19 PM: Update 12839: task edges-pos-ontonotes, batch 839 (12839): mcc: 0.8726, acc: 0.8208, precision: 0.8937, recall: 0.8571, f1: 0.8750, edges-pos-ontonotes_loss: 0.0122
09/16 01:00:29 PM: Update 12918: task edges-pos-ontonotes, batch 918 (12918): mcc: 0.8736, acc: 0.8220, precision: 0.8945, recall: 0.8582, f1: 0.8760, edges-pos-ontonotes_loss: 0.0121
09/16 01:00:35 PM: ***** Step 13000 / Validation 13 *****
09/16 01:00:35 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:00:35 PM: Validating...
09/16 01:00:39 PM: Evaluate: task edges-pos-ontonotes, batch 31 (157): mcc: 0.8437, acc: 0.8174, precision: 0.8697, recall: 0.8246, f1: 0.8466, edges-pos-ontonotes_loss: 0.0145
09/16 01:00:49 PM: Evaluate: task edges-pos-ontonotes, batch 105 (157): mcc: 0.8525, acc: 0.8261, precision: 0.8764, recall: 0.8351, f1: 0.8552, edges-pos-ontonotes_loss: 0.0139
09/16 01:00:57 PM: Updating LR scheduler:
09/16 01:00:57 PM: 	Best result seen so far for macro_avg: 0.871
09/16 01:00:57 PM: 	# validation passes without improvement: 1
09/16 01:00:57 PM: edges-pos-ontonotes_loss: training: 0.011994 validation: 0.014169
09/16 01:00:57 PM: macro_avg: validation: 0.851325
09/16 01:00:57 PM: micro_avg: validation: 0.000000
09/16 01:00:57 PM: edges-pos-ontonotes_mcc: training: 0.874406 validation: 0.848635
09/16 01:00:57 PM: edges-pos-ontonotes_acc: training: 0.822968 validation: 0.817719
09/16 01:00:57 PM: edges-pos-ontonotes_precision: training: 0.895314 validation: 0.877017
09/16 01:00:57 PM: edges-pos-ontonotes_recall: training: 0.858992 validation: 0.827095
09/16 01:00:57 PM: edges-pos-ontonotes_f1: training: 0.876777 validation: 0.851325
09/16 01:00:57 PM: Global learning rate: 5e-05
09/16 01:00:57 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:00:59 PM: Update 13021: task edges-pos-ontonotes, batch 21 (13021): mcc: 0.8848, acc: 0.8369, precision: 0.9006, recall: 0.8740, f1: 0.8871, edges-pos-ontonotes_loss: 0.0110
09/16 01:01:09 PM: Update 13140: task edges-pos-ontonotes, batch 140 (13140): mcc: 0.8902, acc: 0.8434, precision: 0.9051, recall: 0.8799, f1: 0.8923, edges-pos-ontonotes_loss: 0.0106
09/16 01:01:19 PM: Update 13216: task edges-pos-ontonotes, batch 216 (13216): mcc: 0.8870, acc: 0.8395, precision: 0.9035, recall: 0.8754, f1: 0.8892, edges-pos-ontonotes_loss: 0.0106
09/16 01:01:29 PM: Update 13320: task edges-pos-ontonotes, batch 320 (13320): mcc: 0.8827, acc: 0.8357, precision: 0.9004, recall: 0.8701, f1: 0.8850, edges-pos-ontonotes_loss: 0.0111
09/16 01:01:39 PM: Update 13421: task edges-pos-ontonotes, batch 421 (13421): mcc: 0.8797, acc: 0.8334, precision: 0.8979, recall: 0.8666, f1: 0.8820, edges-pos-ontonotes_loss: 0.0113
09/16 01:01:58 PM: Update 13511: task edges-pos-ontonotes, batch 511 (13511): mcc: 0.8776, acc: 0.8316, precision: 0.8960, recall: 0.8644, f1: 0.8800, edges-pos-ontonotes_loss: 0.0114
09/16 01:02:09 PM: Update 13575: task edges-pos-ontonotes, batch 575 (13575): mcc: 0.8686, acc: 0.8198, precision: 0.8889, recall: 0.8539, f1: 0.8711, edges-pos-ontonotes_loss: 0.0120
09/16 01:02:19 PM: Update 13656: task edges-pos-ontonotes, batch 656 (13656): mcc: 0.8632, acc: 0.8129, precision: 0.8852, recall: 0.8471, f1: 0.8658, edges-pos-ontonotes_loss: 0.0125
09/16 01:02:29 PM: Update 13732: task edges-pos-ontonotes, batch 732 (13732): mcc: 0.8597, acc: 0.8077, precision: 0.8827, recall: 0.8428, f1: 0.8623, edges-pos-ontonotes_loss: 0.0129
09/16 01:02:39 PM: Update 13801: task edges-pos-ontonotes, batch 801 (13801): mcc: 0.8572, acc: 0.8040, precision: 0.8814, recall: 0.8393, f1: 0.8599, edges-pos-ontonotes_loss: 0.0132
09/16 01:02:49 PM: Update 13866: task edges-pos-ontonotes, batch 866 (13866): mcc: 0.8561, acc: 0.8022, precision: 0.8808, recall: 0.8379, f1: 0.8588, edges-pos-ontonotes_loss: 0.0134
09/16 01:02:59 PM: Update 13973: task edges-pos-ontonotes, batch 973 (13973): mcc: 0.8566, acc: 0.8030, precision: 0.8809, recall: 0.8387, f1: 0.8592, edges-pos-ontonotes_loss: 0.0134
09/16 01:03:02 PM: ***** Step 14000 / Validation 14 *****
09/16 01:03:02 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:03:02 PM: Validating...
09/16 01:03:09 PM: Evaluate: task edges-pos-ontonotes, batch 74 (157): mcc: 0.8729, acc: 0.8398, precision: 0.9020, recall: 0.8496, f1: 0.8750, edges-pos-ontonotes_loss: 0.0126
09/16 01:03:19 PM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.8639, acc: 0.8277, precision: 0.8933, recall: 0.8407, f1: 0.8662, edges-pos-ontonotes_loss: 0.0131
09/16 01:03:21 PM: Updating LR scheduler:
09/16 01:03:21 PM: 	Best result seen so far for macro_avg: 0.871
09/16 01:03:21 PM: 	# validation passes without improvement: 2
09/16 01:03:21 PM: edges-pos-ontonotes_loss: training: 0.013344 validation: 0.013237
09/16 01:03:21 PM: macro_avg: validation: 0.865351
09/16 01:03:21 PM: micro_avg: validation: 0.000000
09/16 01:03:21 PM: edges-pos-ontonotes_mcc: training: 0.856727 validation: 0.863000
09/16 01:03:21 PM: edges-pos-ontonotes_acc: training: 0.803178 validation: 0.826439
09/16 01:03:21 PM: edges-pos-ontonotes_precision: training: 0.880946 validation: 0.892752
09/16 01:03:21 PM: edges-pos-ontonotes_recall: training: 0.838839 validation: 0.839582
09/16 01:03:21 PM: edges-pos-ontonotes_f1: training: 0.859377 validation: 0.865351
09/16 01:03:21 PM: Global learning rate: 5e-05
09/16 01:03:21 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:03:29 PM: Update 14105: task edges-pos-ontonotes, batch 105 (14105): mcc: 0.8668, acc: 0.8194, precision: 0.8854, recall: 0.8539, f1: 0.8693, edges-pos-ontonotes_loss: 0.0122
09/16 01:03:39 PM: Update 14185: task edges-pos-ontonotes, batch 185 (14185): mcc: 0.8659, acc: 0.8178, precision: 0.8860, recall: 0.8517, f1: 0.8685, edges-pos-ontonotes_loss: 0.0125
09/16 01:03:49 PM: Update 14271: task edges-pos-ontonotes, batch 271 (14271): mcc: 0.8663, acc: 0.8167, precision: 0.8871, recall: 0.8513, f1: 0.8688, edges-pos-ontonotes_loss: 0.0126
09/16 01:03:59 PM: Update 14339: task edges-pos-ontonotes, batch 339 (14339): mcc: 0.8656, acc: 0.8143, precision: 0.8870, recall: 0.8500, f1: 0.8681, edges-pos-ontonotes_loss: 0.0127
09/16 01:04:09 PM: Update 14427: task edges-pos-ontonotes, batch 427 (14427): mcc: 0.8654, acc: 0.8135, precision: 0.8870, recall: 0.8497, f1: 0.8680, edges-pos-ontonotes_loss: 0.0128
09/16 01:04:20 PM: Update 14477: task edges-pos-ontonotes, batch 477 (14477): mcc: 0.8639, acc: 0.8111, precision: 0.8859, recall: 0.8479, f1: 0.8665, edges-pos-ontonotes_loss: 0.0129
09/16 01:04:30 PM: Update 14551: task edges-pos-ontonotes, batch 551 (14551): mcc: 0.8619, acc: 0.8081, precision: 0.8842, recall: 0.8457, f1: 0.8645, edges-pos-ontonotes_loss: 0.0131
09/16 01:04:40 PM: Update 14625: task edges-pos-ontonotes, batch 625 (14625): mcc: 0.8606, acc: 0.8063, precision: 0.8829, recall: 0.8443, f1: 0.8632, edges-pos-ontonotes_loss: 0.0133
09/16 01:04:50 PM: Update 14691: task edges-pos-ontonotes, batch 691 (14691): mcc: 0.8603, acc: 0.8058, precision: 0.8825, recall: 0.8442, f1: 0.8629, edges-pos-ontonotes_loss: 0.0135
09/16 01:05:00 PM: Update 14758: task edges-pos-ontonotes, batch 758 (14758): mcc: 0.8600, acc: 0.8055, precision: 0.8822, recall: 0.8439, f1: 0.8626, edges-pos-ontonotes_loss: 0.0135
09/16 01:05:10 PM: Update 14812: task edges-pos-ontonotes, batch 812 (14812): mcc: 0.8599, acc: 0.8055, precision: 0.8823, recall: 0.8437, f1: 0.8626, edges-pos-ontonotes_loss: 0.0136
09/16 01:05:20 PM: Update 14879: task edges-pos-ontonotes, batch 879 (14879): mcc: 0.8597, acc: 0.8055, precision: 0.8822, recall: 0.8434, f1: 0.8624, edges-pos-ontonotes_loss: 0.0137
09/16 01:05:31 PM: Update 14941: task edges-pos-ontonotes, batch 941 (14941): mcc: 0.8597, acc: 0.8057, precision: 0.8823, recall: 0.8432, f1: 0.8623, edges-pos-ontonotes_loss: 0.0137
09/16 01:05:39 PM: ***** Step 15000 / Validation 15 *****
09/16 01:05:39 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:05:39 PM: Validating...
09/16 01:05:41 PM: Evaluate: task edges-pos-ontonotes, batch 14 (157): mcc: 0.8666, acc: 0.8432, precision: 0.8852, recall: 0.8536, f1: 0.8691, edges-pos-ontonotes_loss: 0.0131
09/16 01:05:51 PM: Evaluate: task edges-pos-ontonotes, batch 106 (157): mcc: 0.8734, acc: 0.8489, precision: 0.8909, recall: 0.8613, f1: 0.8759, edges-pos-ontonotes_loss: 0.0128
09/16 01:05:59 PM: Best result seen so far for edges-pos-ontonotes.
09/16 01:05:59 PM: Best result seen so far for macro.
09/16 01:05:59 PM: Updating LR scheduler:
09/16 01:05:59 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:05:59 PM: 	# validation passes without improvement: 0
09/16 01:05:59 PM: edges-pos-ontonotes_loss: training: 0.013793 validation: 0.012941
09/16 01:05:59 PM: macro_avg: validation: 0.874239
09/16 01:05:59 PM: micro_avg: validation: 0.000000
09/16 01:05:59 PM: edges-pos-ontonotes_mcc: training: 0.859695 validation: 0.871760
09/16 01:05:59 PM: edges-pos-ontonotes_acc: training: 0.805845 validation: 0.845836
09/16 01:05:59 PM: edges-pos-ontonotes_precision: training: 0.882154 validation: 0.890299
09/16 01:05:59 PM: edges-pos-ontonotes_recall: training: 0.843379 validation: 0.858747
09/16 01:05:59 PM: edges-pos-ontonotes_f1: training: 0.862331 validation: 0.874239
09/16 01:05:59 PM: Global learning rate: 5e-05
09/16 01:05:59 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:06:01 PM: Update 15013: task edges-pos-ontonotes, batch 13 (15013): mcc: 0.8612, acc: 0.8081, precision: 0.8835, recall: 0.8449, f1: 0.8638, edges-pos-ontonotes_loss: 0.0152
09/16 01:06:11 PM: Update 15079: task edges-pos-ontonotes, batch 79 (15079): mcc: 0.8599, acc: 0.8084, precision: 0.8839, recall: 0.8422, f1: 0.8625, edges-pos-ontonotes_loss: 0.0147
09/16 01:06:27 PM: Update 15093: task edges-pos-ontonotes, batch 93 (15093): mcc: 0.8587, acc: 0.8075, precision: 0.8826, recall: 0.8410, f1: 0.8613, edges-pos-ontonotes_loss: 0.0148
09/16 01:06:37 PM: Update 15159: task edges-pos-ontonotes, batch 159 (15159): mcc: 0.8590, acc: 0.8081, precision: 0.8826, recall: 0.8415, f1: 0.8616, edges-pos-ontonotes_loss: 0.0148
09/16 01:06:47 PM: Update 15226: task edges-pos-ontonotes, batch 226 (15226): mcc: 0.8599, acc: 0.8092, precision: 0.8836, recall: 0.8424, f1: 0.8625, edges-pos-ontonotes_loss: 0.0147
09/16 01:06:57 PM: Update 15292: task edges-pos-ontonotes, batch 292 (15292): mcc: 0.8604, acc: 0.8098, precision: 0.8840, recall: 0.8430, f1: 0.8630, edges-pos-ontonotes_loss: 0.0145
09/16 01:07:07 PM: Update 15365: task edges-pos-ontonotes, batch 365 (15365): mcc: 0.8611, acc: 0.8105, precision: 0.8846, recall: 0.8436, f1: 0.8636, edges-pos-ontonotes_loss: 0.0145
09/16 01:07:17 PM: Update 15418: task edges-pos-ontonotes, batch 418 (15418): mcc: 0.8612, acc: 0.8106, precision: 0.8848, recall: 0.8437, f1: 0.8638, edges-pos-ontonotes_loss: 0.0145
09/16 01:07:27 PM: Update 15487: task edges-pos-ontonotes, batch 487 (15487): mcc: 0.8611, acc: 0.8104, precision: 0.8847, recall: 0.8436, f1: 0.8637, edges-pos-ontonotes_loss: 0.0145
09/16 01:07:37 PM: Update 15554: task edges-pos-ontonotes, batch 554 (15554): mcc: 0.8615, acc: 0.8108, precision: 0.8849, recall: 0.8442, f1: 0.8641, edges-pos-ontonotes_loss: 0.0144
09/16 01:07:47 PM: Update 15618: task edges-pos-ontonotes, batch 618 (15618): mcc: 0.8618, acc: 0.8111, precision: 0.8851, recall: 0.8445, f1: 0.8643, edges-pos-ontonotes_loss: 0.0144
09/16 01:07:57 PM: Update 15682: task edges-pos-ontonotes, batch 682 (15682): mcc: 0.8618, acc: 0.8111, precision: 0.8850, recall: 0.8446, f1: 0.8643, edges-pos-ontonotes_loss: 0.0144
09/16 01:08:07 PM: Update 15743: task edges-pos-ontonotes, batch 743 (15743): mcc: 0.8616, acc: 0.8110, precision: 0.8849, recall: 0.8445, f1: 0.8642, edges-pos-ontonotes_loss: 0.0144
09/16 01:08:18 PM: Update 15817: task edges-pos-ontonotes, batch 817 (15817): mcc: 0.8622, acc: 0.8114, precision: 0.8853, recall: 0.8452, f1: 0.8648, edges-pos-ontonotes_loss: 0.0142
09/16 01:08:28 PM: Update 15897: task edges-pos-ontonotes, batch 897 (15897): mcc: 0.8629, acc: 0.8119, precision: 0.8858, recall: 0.8460, f1: 0.8654, edges-pos-ontonotes_loss: 0.0140
09/16 01:08:38 PM: Update 15984: task edges-pos-ontonotes, batch 984 (15984): mcc: 0.8638, acc: 0.8127, precision: 0.8866, recall: 0.8470, f1: 0.8664, edges-pos-ontonotes_loss: 0.0138
09/16 01:08:40 PM: ***** Step 16000 / Validation 16 *****
09/16 01:08:40 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:08:40 PM: Validating...
09/16 01:08:48 PM: Evaluate: task edges-pos-ontonotes, batch 79 (157): mcc: 0.8656, acc: 0.8251, precision: 0.8946, recall: 0.8428, f1: 0.8679, edges-pos-ontonotes_loss: 0.0137
09/16 01:08:58 PM: Evaluate: task edges-pos-ontonotes, batch 146 (157): mcc: 0.8657, acc: 0.8261, precision: 0.8966, recall: 0.8411, f1: 0.8680, edges-pos-ontonotes_loss: 0.0135
09/16 01:08:59 PM: Updating LR scheduler:
09/16 01:08:59 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:08:59 PM: 	# validation passes without improvement: 1
09/16 01:08:59 PM: edges-pos-ontonotes_loss: training: 0.013743 validation: 0.013494
09/16 01:08:59 PM: macro_avg: validation: 0.868016
09/16 01:08:59 PM: micro_avg: validation: 0.000000
09/16 01:08:59 PM: edges-pos-ontonotes_mcc: training: 0.863882 validation: 0.865769
09/16 01:08:59 PM: edges-pos-ontonotes_acc: training: 0.812783 validation: 0.825931
09/16 01:08:59 PM: edges-pos-ontonotes_precision: training: 0.886636 validation: 0.896934
09/16 01:08:59 PM: edges-pos-ontonotes_recall: training: 0.847113 validation: 0.840905
09/16 01:08:59 PM: edges-pos-ontonotes_f1: training: 0.866424 validation: 0.868016
09/16 01:08:59 PM: Global learning rate: 5e-05
09/16 01:08:59 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:09:08 PM: Update 16063: task edges-pos-ontonotes, batch 63 (16063): mcc: 0.8790, acc: 0.8261, precision: 0.8993, recall: 0.8640, f1: 0.8813, edges-pos-ontonotes_loss: 0.0111
09/16 01:09:18 PM: Update 16175: task edges-pos-ontonotes, batch 175 (16175): mcc: 0.8879, acc: 0.8377, precision: 0.9063, recall: 0.8744, f1: 0.8901, edges-pos-ontonotes_loss: 0.0104
09/16 01:09:28 PM: Update 16287: task edges-pos-ontonotes, batch 287 (16287): mcc: 0.8905, acc: 0.8410, precision: 0.9079, recall: 0.8778, f1: 0.8926, edges-pos-ontonotes_loss: 0.0102
09/16 01:09:38 PM: Update 16390: task edges-pos-ontonotes, batch 390 (16390): mcc: 0.8904, acc: 0.8414, precision: 0.9079, recall: 0.8777, f1: 0.8925, edges-pos-ontonotes_loss: 0.0103
09/16 01:09:48 PM: Update 16510: task edges-pos-ontonotes, batch 510 (16510): mcc: 0.8901, acc: 0.8412, precision: 0.9079, recall: 0.8772, f1: 0.8923, edges-pos-ontonotes_loss: 0.0104
09/16 01:09:58 PM: Update 16633: task edges-pos-ontonotes, batch 633 (16633): mcc: 0.8912, acc: 0.8430, precision: 0.9085, recall: 0.8786, f1: 0.8933, edges-pos-ontonotes_loss: 0.0103
09/16 01:10:08 PM: Update 16738: task edges-pos-ontonotes, batch 738 (16738): mcc: 0.8888, acc: 0.8408, precision: 0.9066, recall: 0.8759, f1: 0.8910, edges-pos-ontonotes_loss: 0.0106
09/16 01:10:18 PM: Update 16884: task edges-pos-ontonotes, batch 884 (16884): mcc: 0.8868, acc: 0.8394, precision: 0.9047, recall: 0.8737, f1: 0.8890, edges-pos-ontonotes_loss: 0.0108
09/16 01:10:28 PM: Update 16983: task edges-pos-ontonotes, batch 983 (16983): mcc: 0.8835, acc: 0.8359, precision: 0.9021, recall: 0.8701, f1: 0.8858, edges-pos-ontonotes_loss: 0.0109
09/16 01:10:31 PM: ***** Step 17000 / Validation 17 *****
09/16 01:10:31 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:10:31 PM: Validating...
09/16 01:10:38 PM: Evaluate: task edges-pos-ontonotes, batch 75 (157): mcc: 0.8693, acc: 0.8429, precision: 0.8911, recall: 0.8532, f1: 0.8717, edges-pos-ontonotes_loss: 0.0134
09/16 01:10:48 PM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.8590, acc: 0.8270, precision: 0.8819, recall: 0.8422, f1: 0.8616, edges-pos-ontonotes_loss: 0.0140
09/16 01:10:50 PM: Updating LR scheduler:
09/16 01:10:50 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:10:50 PM: 	# validation passes without improvement: 2
09/16 01:10:50 PM: edges-pos-ontonotes_loss: training: 0.010982 validation: 0.014096
09/16 01:10:50 PM: macro_avg: validation: 0.860254
09/16 01:10:50 PM: micro_avg: validation: 0.000000
09/16 01:10:50 PM: edges-pos-ontonotes_mcc: training: 0.881932 validation: 0.857606
09/16 01:10:50 PM: edges-pos-ontonotes_acc: training: 0.833854 validation: 0.825095
09/16 01:10:50 PM: edges-pos-ontonotes_precision: training: 0.900645 validation: 0.881235
09/16 01:10:50 PM: edges-pos-ontonotes_recall: training: 0.868338 validation: 0.840249
09/16 01:10:50 PM: edges-pos-ontonotes_f1: training: 0.884197 validation: 0.860254
09/16 01:10:50 PM: Global learning rate: 5e-05
09/16 01:10:50 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:10:58 PM: Update 17057: task edges-pos-ontonotes, batch 57 (17057): mcc: 0.8434, acc: 0.7872, precision: 0.8678, recall: 0.8259, f1: 0.8464, edges-pos-ontonotes_loss: 0.0162
09/16 01:11:08 PM: Update 17123: task edges-pos-ontonotes, batch 123 (17123): mcc: 0.8411, acc: 0.7832, precision: 0.8679, recall: 0.8214, f1: 0.8440, edges-pos-ontonotes_loss: 0.0166
09/16 01:11:19 PM: Update 17195: task edges-pos-ontonotes, batch 195 (17195): mcc: 0.8430, acc: 0.7849, precision: 0.8708, recall: 0.8222, f1: 0.8458, edges-pos-ontonotes_loss: 0.0163
09/16 01:11:29 PM: Update 17268: task edges-pos-ontonotes, batch 268 (17268): mcc: 0.8431, acc: 0.7848, precision: 0.8714, recall: 0.8219, f1: 0.8459, edges-pos-ontonotes_loss: 0.0165
09/16 01:11:39 PM: Update 17338: task edges-pos-ontonotes, batch 338 (17338): mcc: 0.8434, acc: 0.7852, precision: 0.8716, recall: 0.8223, f1: 0.8462, edges-pos-ontonotes_loss: 0.0162
09/16 01:11:49 PM: Update 17425: task edges-pos-ontonotes, batch 425 (17425): mcc: 0.8467, acc: 0.7898, precision: 0.8737, recall: 0.8265, f1: 0.8495, edges-pos-ontonotes_loss: 0.0154
09/16 01:11:59 PM: Update 17518: task edges-pos-ontonotes, batch 518 (17518): mcc: 0.8492, acc: 0.7936, precision: 0.8754, recall: 0.8298, f1: 0.8520, edges-pos-ontonotes_loss: 0.0149
09/16 01:12:18 PM: Update 17614: task edges-pos-ontonotes, batch 614 (17614): mcc: 0.8511, acc: 0.7964, precision: 0.8766, recall: 0.8323, f1: 0.8539, edges-pos-ontonotes_loss: 0.0144
09/16 01:12:28 PM: Update 17703: task edges-pos-ontonotes, batch 703 (17703): mcc: 0.8527, acc: 0.7983, precision: 0.8775, recall: 0.8344, f1: 0.8554, edges-pos-ontonotes_loss: 0.0143
09/16 01:12:38 PM: Update 17792: task edges-pos-ontonotes, batch 792 (17792): mcc: 0.8542, acc: 0.8000, precision: 0.8786, recall: 0.8363, f1: 0.8569, edges-pos-ontonotes_loss: 0.0142
09/16 01:12:48 PM: Update 17873: task edges-pos-ontonotes, batch 873 (17873): mcc: 0.8556, acc: 0.8015, precision: 0.8796, recall: 0.8379, f1: 0.8583, edges-pos-ontonotes_loss: 0.0140
09/16 01:12:58 PM: Update 17928: task edges-pos-ontonotes, batch 928 (17928): mcc: 0.8559, acc: 0.8016, precision: 0.8800, recall: 0.8381, f1: 0.8586, edges-pos-ontonotes_loss: 0.0140
09/16 01:13:08 PM: Update 17979: task edges-pos-ontonotes, batch 979 (17979): mcc: 0.8553, acc: 0.8005, precision: 0.8793, recall: 0.8376, f1: 0.8580, edges-pos-ontonotes_loss: 0.0140
09/16 01:13:11 PM: ***** Step 18000 / Validation 18 *****
09/16 01:13:11 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:13:11 PM: Validating...
09/16 01:13:18 PM: Evaluate: task edges-pos-ontonotes, batch 73 (157): mcc: 0.8743, acc: 0.8522, precision: 0.8927, recall: 0.8614, f1: 0.8768, edges-pos-ontonotes_loss: 0.0125
09/16 01:13:28 PM: Evaluate: task edges-pos-ontonotes, batch 142 (157): mcc: 0.8720, acc: 0.8480, precision: 0.8912, recall: 0.8583, f1: 0.8744, edges-pos-ontonotes_loss: 0.0127
09/16 01:13:30 PM: Updating LR scheduler:
09/16 01:13:30 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:13:30 PM: 	# validation passes without improvement: 3
09/16 01:13:30 PM: edges-pos-ontonotes_loss: training: 0.014040 validation: 0.012849
09/16 01:13:30 PM: macro_avg: validation: 0.874007
09/16 01:13:30 PM: micro_avg: validation: 0.000000
09/16 01:13:30 PM: edges-pos-ontonotes_mcc: training: 0.855240 validation: 0.871542
09/16 01:13:30 PM: edges-pos-ontonotes_acc: training: 0.800447 validation: 0.847032
09/16 01:13:30 PM: edges-pos-ontonotes_precision: training: 0.879243 validation: 0.890935
09/16 01:13:30 PM: edges-pos-ontonotes_recall: training: 0.837619 validation: 0.857710
09/16 01:13:30 PM: edges-pos-ontonotes_f1: training: 0.857926 validation: 0.874007
09/16 01:13:30 PM: Global learning rate: 5e-05
09/16 01:13:30 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:13:38 PM: Update 18053: task edges-pos-ontonotes, batch 53 (18053): mcc: 0.8539, acc: 0.7957, precision: 0.8768, recall: 0.8375, f1: 0.8567, edges-pos-ontonotes_loss: 0.0146
09/16 01:13:48 PM: Update 18124: task edges-pos-ontonotes, batch 124 (18124): mcc: 0.8563, acc: 0.8006, precision: 0.8789, recall: 0.8401, f1: 0.8590, edges-pos-ontonotes_loss: 0.0145
09/16 01:13:58 PM: Update 18195: task edges-pos-ontonotes, batch 195 (18195): mcc: 0.8566, acc: 0.8012, precision: 0.8784, recall: 0.8410, f1: 0.8593, edges-pos-ontonotes_loss: 0.0145
09/16 01:14:08 PM: Update 18253: task edges-pos-ontonotes, batch 253 (18253): mcc: 0.8572, acc: 0.8022, precision: 0.8790, recall: 0.8416, f1: 0.8599, edges-pos-ontonotes_loss: 0.0145
09/16 01:14:19 PM: Update 18321: task edges-pos-ontonotes, batch 321 (18321): mcc: 0.8572, acc: 0.8029, precision: 0.8793, recall: 0.8413, f1: 0.8599, edges-pos-ontonotes_loss: 0.0145
09/16 01:14:29 PM: Update 18385: task edges-pos-ontonotes, batch 385 (18385): mcc: 0.8574, acc: 0.8033, precision: 0.8795, recall: 0.8415, f1: 0.8601, edges-pos-ontonotes_loss: 0.0146
09/16 01:14:39 PM: Update 18452: task edges-pos-ontonotes, batch 452 (18452): mcc: 0.8575, acc: 0.8039, precision: 0.8799, recall: 0.8414, f1: 0.8602, edges-pos-ontonotes_loss: 0.0146
09/16 01:14:49 PM: Update 18520: task edges-pos-ontonotes, batch 520 (18520): mcc: 0.8582, acc: 0.8049, precision: 0.8804, recall: 0.8421, f1: 0.8608, edges-pos-ontonotes_loss: 0.0146
09/16 01:14:59 PM: Update 18568: task edges-pos-ontonotes, batch 568 (18568): mcc: 0.8583, acc: 0.8054, precision: 0.8806, recall: 0.8423, f1: 0.8610, edges-pos-ontonotes_loss: 0.0146
09/16 01:15:09 PM: Update 18618: task edges-pos-ontonotes, batch 618 (18618): mcc: 0.8587, acc: 0.8060, precision: 0.8809, recall: 0.8427, f1: 0.8614, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:19 PM: Update 18669: task edges-pos-ontonotes, batch 669 (18669): mcc: 0.8587, acc: 0.8063, precision: 0.8810, recall: 0.8426, f1: 0.8614, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:29 PM: Update 18723: task edges-pos-ontonotes, batch 723 (18723): mcc: 0.8590, acc: 0.8067, precision: 0.8814, recall: 0.8429, f1: 0.8617, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:39 PM: Update 18763: task edges-pos-ontonotes, batch 763 (18763): mcc: 0.8591, acc: 0.8068, precision: 0.8814, recall: 0.8429, f1: 0.8617, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:49 PM: Update 18827: task edges-pos-ontonotes, batch 827 (18827): mcc: 0.8593, acc: 0.8073, precision: 0.8818, recall: 0.8430, f1: 0.8620, edges-pos-ontonotes_loss: 0.0145
09/16 01:16:00 PM: Update 18879: task edges-pos-ontonotes, batch 879 (18879): mcc: 0.8593, acc: 0.8074, precision: 0.8818, recall: 0.8429, f1: 0.8619, edges-pos-ontonotes_loss: 0.0145
09/16 01:16:10 PM: Update 18948: task edges-pos-ontonotes, batch 948 (18948): mcc: 0.8596, acc: 0.8078, precision: 0.8820, recall: 0.8432, f1: 0.8622, edges-pos-ontonotes_loss: 0.0145
09/16 01:16:17 PM: ***** Step 19000 / Validation 19 *****
09/16 01:16:17 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:16:17 PM: Validating...
09/16 01:16:20 PM: Evaluate: task edges-pos-ontonotes, batch 27 (157): mcc: 0.8631, acc: 0.8385, precision: 0.8821, recall: 0.8500, f1: 0.8657, edges-pos-ontonotes_loss: 0.0138
09/16 01:16:30 PM: Evaluate: task edges-pos-ontonotes, batch 114 (157): mcc: 0.8703, acc: 0.8458, precision: 0.8898, recall: 0.8565, f1: 0.8728, edges-pos-ontonotes_loss: 0.0131
09/16 01:16:37 PM: Updating LR scheduler:
09/16 01:16:37 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:16:37 PM: 	# validation passes without improvement: 0
09/16 01:16:37 PM: edges-pos-ontonotes_loss: training: 0.014474 validation: 0.013088
09/16 01:16:37 PM: macro_avg: validation: 0.873188
09/16 01:16:37 PM: micro_avg: validation: 0.000000
09/16 01:16:37 PM: edges-pos-ontonotes_mcc: training: 0.859796 validation: 0.870727
09/16 01:16:37 PM: edges-pos-ontonotes_acc: training: 0.808124 validation: 0.845043
09/16 01:16:37 PM: edges-pos-ontonotes_precision: training: 0.882287 validation: 0.891039
09/16 01:16:37 PM: edges-pos-ontonotes_recall: training: 0.843446 validation: 0.856038
09/16 01:16:37 PM: edges-pos-ontonotes_f1: training: 0.862429 validation: 0.873188
09/16 01:16:37 PM: Global learning rate: 2.5e-05
09/16 01:16:37 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:16:40 PM: Update 19020: task edges-pos-ontonotes, batch 20 (19020): mcc: 0.8642, acc: 0.8137, precision: 0.8884, recall: 0.8460, f1: 0.8667, edges-pos-ontonotes_loss: 0.0141
09/16 01:16:50 PM: Update 19090: task edges-pos-ontonotes, batch 90 (19090): mcc: 0.8618, acc: 0.8103, precision: 0.8849, recall: 0.8448, f1: 0.8644, edges-pos-ontonotes_loss: 0.0145
09/16 01:17:00 PM: Update 19159: task edges-pos-ontonotes, batch 159 (19159): mcc: 0.8627, acc: 0.8112, precision: 0.8854, recall: 0.8460, f1: 0.8653, edges-pos-ontonotes_loss: 0.0143
09/16 01:17:15 PM: Update 19179: task edges-pos-ontonotes, batch 179 (19179): mcc: 0.8622, acc: 0.8109, precision: 0.8848, recall: 0.8457, f1: 0.8648, edges-pos-ontonotes_loss: 0.0143
09/16 01:17:25 PM: Update 19249: task edges-pos-ontonotes, batch 249 (19249): mcc: 0.8630, acc: 0.8121, precision: 0.8845, recall: 0.8475, f1: 0.8656, edges-pos-ontonotes_loss: 0.0139
09/16 01:17:35 PM: Update 19333: task edges-pos-ontonotes, batch 333 (19333): mcc: 0.8652, acc: 0.8145, precision: 0.8864, recall: 0.8499, f1: 0.8677, edges-pos-ontonotes_loss: 0.0134
09/16 01:17:45 PM: Update 19419: task edges-pos-ontonotes, batch 419 (19419): mcc: 0.8665, acc: 0.8152, precision: 0.8878, recall: 0.8511, f1: 0.8691, edges-pos-ontonotes_loss: 0.0131
09/16 01:17:56 PM: Update 19492: task edges-pos-ontonotes, batch 492 (19492): mcc: 0.8674, acc: 0.8158, precision: 0.8889, recall: 0.8516, f1: 0.8699, edges-pos-ontonotes_loss: 0.0129
09/16 01:18:06 PM: Update 19594: task edges-pos-ontonotes, batch 594 (19594): mcc: 0.8699, acc: 0.8185, precision: 0.8908, recall: 0.8546, f1: 0.8724, edges-pos-ontonotes_loss: 0.0125
09/16 01:18:16 PM: Update 19704: task edges-pos-ontonotes, batch 704 (19704): mcc: 0.8727, acc: 0.8217, precision: 0.8931, recall: 0.8579, f1: 0.8751, edges-pos-ontonotes_loss: 0.0121
09/16 01:18:26 PM: Update 19805: task edges-pos-ontonotes, batch 805 (19805): mcc: 0.8745, acc: 0.8237, precision: 0.8945, recall: 0.8600, f1: 0.8769, edges-pos-ontonotes_loss: 0.0118
09/16 01:18:36 PM: Update 19937: task edges-pos-ontonotes, batch 937 (19937): mcc: 0.8755, acc: 0.8249, precision: 0.8953, recall: 0.8611, f1: 0.8779, edges-pos-ontonotes_loss: 0.0118
09/16 01:18:40 PM: ***** Step 20000 / Validation 20 *****
09/16 01:18:40 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:18:40 PM: Validating...
09/16 01:18:46 PM: Evaluate: task edges-pos-ontonotes, batch 65 (157): mcc: 0.8631, acc: 0.8245, precision: 0.9029, recall: 0.8303, f1: 0.8651, edges-pos-ontonotes_loss: 0.0135
09/16 01:18:56 PM: Evaluate: task edges-pos-ontonotes, batch 135 (157): mcc: 0.8533, acc: 0.8109, precision: 0.8964, recall: 0.8178, f1: 0.8553, edges-pos-ontonotes_loss: 0.0136
09/16 01:19:00 PM: Updating LR scheduler:
09/16 01:19:00 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:19:00 PM: 	# validation passes without improvement: 1
09/16 01:19:00 PM: edges-pos-ontonotes_loss: training: 0.011703 validation: 0.013749
09/16 01:19:00 PM: macro_avg: validation: 0.853318
09/16 01:19:00 PM: micro_avg: validation: 0.000000
09/16 01:19:00 PM: edges-pos-ontonotes_mcc: training: 0.875902 validation: 0.851316
09/16 01:19:00 PM: edges-pos-ontonotes_acc: training: 0.825348 validation: 0.808142
09/16 01:19:00 PM: edges-pos-ontonotes_precision: training: 0.895677 validation: 0.895200
09/16 01:19:00 PM: edges-pos-ontonotes_recall: training: 0.861523 validation: 0.815179
09/16 01:19:00 PM: edges-pos-ontonotes_f1: training: 0.878268 validation: 0.853318
09/16 01:19:00 PM: Global learning rate: 2.5e-05
09/16 01:19:00 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:19:06 PM: Update 20091: task edges-pos-ontonotes, batch 91 (20091): mcc: 0.8944, acc: 0.8467, precision: 0.9126, recall: 0.8808, f1: 0.8964, edges-pos-ontonotes_loss: 0.0103
09/16 01:19:16 PM: Update 20239: task edges-pos-ontonotes, batch 239 (20239): mcc: 0.8807, acc: 0.8321, precision: 0.9025, recall: 0.8642, f1: 0.8829, edges-pos-ontonotes_loss: 0.0111
09/16 01:19:27 PM: Update 20408: task edges-pos-ontonotes, batch 408 (20408): mcc: 0.8760, acc: 0.8291, precision: 0.8977, recall: 0.8597, f1: 0.8783, edges-pos-ontonotes_loss: 0.0115
09/16 01:19:37 PM: Update 20479: task edges-pos-ontonotes, batch 479 (20479): mcc: 0.8672, acc: 0.8185, precision: 0.8904, recall: 0.8500, f1: 0.8697, edges-pos-ontonotes_loss: 0.0121
09/16 01:19:47 PM: Update 20558: task edges-pos-ontonotes, batch 558 (20558): mcc: 0.8604, acc: 0.8095, precision: 0.8843, recall: 0.8426, f1: 0.8630, edges-pos-ontonotes_loss: 0.0128
09/16 01:19:57 PM: Update 20639: task edges-pos-ontonotes, batch 639 (20639): mcc: 0.8567, acc: 0.8046, precision: 0.8815, recall: 0.8384, f1: 0.8594, edges-pos-ontonotes_loss: 0.0133
09/16 01:20:07 PM: Update 20709: task edges-pos-ontonotes, batch 709 (20709): mcc: 0.8541, acc: 0.8008, precision: 0.8796, recall: 0.8352, f1: 0.8568, edges-pos-ontonotes_loss: 0.0135
09/16 01:20:17 PM: Update 20770: task edges-pos-ontonotes, batch 770 (20770): mcc: 0.8526, acc: 0.7985, precision: 0.8782, recall: 0.8335, f1: 0.8553, edges-pos-ontonotes_loss: 0.0137
09/16 01:20:27 PM: Update 20886: task edges-pos-ontonotes, batch 886 (20886): mcc: 0.8538, acc: 0.8003, precision: 0.8790, recall: 0.8351, f1: 0.8565, edges-pos-ontonotes_loss: 0.0136
09/16 01:20:37 PM: Update 21000: task edges-pos-ontonotes, batch 1000 (21000): mcc: 0.8551, acc: 0.8021, precision: 0.8797, recall: 0.8369, f1: 0.8578, edges-pos-ontonotes_loss: 0.0134
09/16 01:20:37 PM: ***** Step 21000 / Validation 21 *****
09/16 01:20:37 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:20:37 PM: Validating...
09/16 01:20:47 PM: Evaluate: task edges-pos-ontonotes, batch 96 (157): mcc: 0.8725, acc: 0.8471, precision: 0.8946, recall: 0.8560, f1: 0.8748, edges-pos-ontonotes_loss: 0.0126
09/16 01:20:56 PM: Updating LR scheduler:
09/16 01:20:56 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:20:56 PM: 	# validation passes without improvement: 2
09/16 01:20:56 PM: edges-pos-ontonotes_loss: training: 0.013436 validation: 0.013116
09/16 01:20:56 PM: macro_avg: validation: 0.865296
09/16 01:20:56 PM: micro_avg: validation: 0.000000
09/16 01:20:56 PM: edges-pos-ontonotes_mcc: training: 0.855083 validation: 0.862773
09/16 01:20:56 PM: edges-pos-ontonotes_acc: training: 0.802117 validation: 0.833032
09/16 01:20:56 PM: edges-pos-ontonotes_precision: training: 0.879697 validation: 0.887081
09/16 01:20:56 PM: edges-pos-ontonotes_recall: training: 0.836880 validation: 0.844556
09/16 01:20:56 PM: edges-pos-ontonotes_f1: training: 0.857755 validation: 0.865296
09/16 01:20:56 PM: Global learning rate: 2.5e-05
09/16 01:20:56 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:20:57 PM: Update 21007: task edges-pos-ontonotes, batch 7 (21007): mcc: 0.8600, acc: 0.8101, precision: 0.8833, recall: 0.8429, f1: 0.8626, edges-pos-ontonotes_loss: 0.0139
09/16 01:21:07 PM: Update 21077: task edges-pos-ontonotes, batch 77 (21077): mcc: 0.8618, acc: 0.8138, precision: 0.8830, recall: 0.8467, f1: 0.8644, edges-pos-ontonotes_loss: 0.0128
09/16 01:21:18 PM: Update 21143: task edges-pos-ontonotes, batch 143 (21143): mcc: 0.8617, acc: 0.8120, precision: 0.8829, recall: 0.8465, f1: 0.8643, edges-pos-ontonotes_loss: 0.0132
09/16 01:21:28 PM: Update 21224: task edges-pos-ontonotes, batch 224 (21224): mcc: 0.8619, acc: 0.8107, precision: 0.8832, recall: 0.8467, f1: 0.8645, edges-pos-ontonotes_loss: 0.0132
09/16 01:21:38 PM: Update 21316: task edges-pos-ontonotes, batch 316 (21316): mcc: 0.8636, acc: 0.8119, precision: 0.8849, recall: 0.8483, f1: 0.8662, edges-pos-ontonotes_loss: 0.0131
09/16 01:21:57 PM: Update 21387: task edges-pos-ontonotes, batch 387 (21387): mcc: 0.8644, acc: 0.8120, precision: 0.8857, recall: 0.8489, f1: 0.8669, edges-pos-ontonotes_loss: 0.0130
09/16 01:22:07 PM: Update 21453: task edges-pos-ontonotes, batch 453 (21453): mcc: 0.8613, acc: 0.8067, precision: 0.8833, recall: 0.8453, f1: 0.8639, edges-pos-ontonotes_loss: 0.0133
09/16 01:22:17 PM: Update 21517: task edges-pos-ontonotes, batch 517 (21517): mcc: 0.8595, acc: 0.8041, precision: 0.8818, recall: 0.8434, f1: 0.8622, edges-pos-ontonotes_loss: 0.0135
09/16 01:22:28 PM: Update 21589: task edges-pos-ontonotes, batch 589 (21589): mcc: 0.8591, acc: 0.8034, precision: 0.8813, recall: 0.8431, f1: 0.8617, edges-pos-ontonotes_loss: 0.0136
09/16 01:22:38 PM: Update 21663: task edges-pos-ontonotes, batch 663 (21663): mcc: 0.8588, acc: 0.8031, precision: 0.8810, recall: 0.8428, f1: 0.8614, edges-pos-ontonotes_loss: 0.0137
09/16 01:22:48 PM: Update 21708: task edges-pos-ontonotes, batch 708 (21708): mcc: 0.8588, acc: 0.8031, precision: 0.8810, recall: 0.8427, f1: 0.8614, edges-pos-ontonotes_loss: 0.0138
09/16 01:22:58 PM: Update 21758: task edges-pos-ontonotes, batch 758 (21758): mcc: 0.8583, acc: 0.8028, precision: 0.8808, recall: 0.8421, f1: 0.8610, edges-pos-ontonotes_loss: 0.0139
09/16 01:23:08 PM: Update 21819: task edges-pos-ontonotes, batch 819 (21819): mcc: 0.8584, acc: 0.8032, precision: 0.8808, recall: 0.8421, f1: 0.8610, edges-pos-ontonotes_loss: 0.0139
09/16 01:23:18 PM: Update 21880: task edges-pos-ontonotes, batch 880 (21880): mcc: 0.8584, acc: 0.8034, precision: 0.8808, recall: 0.8422, f1: 0.8611, edges-pos-ontonotes_loss: 0.0140
09/16 01:23:28 PM: Update 21946: task edges-pos-ontonotes, batch 946 (21946): mcc: 0.8587, acc: 0.8040, precision: 0.8811, recall: 0.8424, f1: 0.8613, edges-pos-ontonotes_loss: 0.0140
09/16 01:23:37 PM: ***** Step 22000 / Validation 22 *****
09/16 01:23:37 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:23:37 PM: Validating...
09/16 01:23:38 PM: Evaluate: task edges-pos-ontonotes, batch 12 (157): mcc: 0.8702, acc: 0.8454, precision: 0.8880, recall: 0.8580, f1: 0.8727, edges-pos-ontonotes_loss: 0.0130
09/16 01:23:48 PM: Evaluate: task edges-pos-ontonotes, batch 105 (157): mcc: 0.8759, acc: 0.8518, precision: 0.8953, recall: 0.8619, f1: 0.8783, edges-pos-ontonotes_loss: 0.0127
09/16 01:23:57 PM: Best result seen so far for edges-pos-ontonotes.
09/16 01:23:57 PM: Best result seen so far for macro.
09/16 01:23:57 PM: Updating LR scheduler:
09/16 01:23:57 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:23:57 PM: 	# validation passes without improvement: 0
09/16 01:23:57 PM: edges-pos-ontonotes_loss: training: 0.014014 validation: 0.012826
09/16 01:23:57 PM: macro_avg: validation: 0.875509
09/16 01:23:57 PM: micro_avg: validation: 0.000000
09/16 01:23:57 PM: edges-pos-ontonotes_mcc: training: 0.858883 validation: 0.873085
09/16 01:23:57 PM: edges-pos-ontonotes_acc: training: 0.804409 validation: 0.847551
09/16 01:23:57 PM: edges-pos-ontonotes_precision: training: 0.881322 validation: 0.892882
09/16 01:23:57 PM: edges-pos-ontonotes_recall: training: 0.842619 validation: 0.858800
09/16 01:23:57 PM: edges-pos-ontonotes_f1: training: 0.861536 validation: 0.875509
09/16 01:23:57 PM: Global learning rate: 2.5e-05
09/16 01:23:57 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:23:59 PM: Update 22011: task edges-pos-ontonotes, batch 11 (22011): mcc: 0.8561, acc: 0.8040, precision: 0.8800, recall: 0.8386, f1: 0.8588, edges-pos-ontonotes_loss: 0.0142
09/16 01:24:09 PM: Update 22061: task edges-pos-ontonotes, batch 61 (22061): mcc: 0.8573, acc: 0.8062, precision: 0.8806, recall: 0.8403, f1: 0.8600, edges-pos-ontonotes_loss: 0.0152
09/16 01:24:19 PM: Update 22130: task edges-pos-ontonotes, batch 130 (22130): mcc: 0.8598, acc: 0.8087, precision: 0.8831, recall: 0.8427, f1: 0.8624, edges-pos-ontonotes_loss: 0.0147
09/16 01:24:29 PM: Update 22199: task edges-pos-ontonotes, batch 199 (22199): mcc: 0.8596, acc: 0.8080, precision: 0.8832, recall: 0.8421, f1: 0.8622, edges-pos-ontonotes_loss: 0.0145
09/16 01:24:39 PM: Update 22266: task edges-pos-ontonotes, batch 266 (22266): mcc: 0.8607, acc: 0.8097, precision: 0.8841, recall: 0.8434, f1: 0.8633, edges-pos-ontonotes_loss: 0.0144
09/16 01:24:50 PM: Update 22326: task edges-pos-ontonotes, batch 326 (22326): mcc: 0.8608, acc: 0.8099, precision: 0.8841, recall: 0.8437, f1: 0.8634, edges-pos-ontonotes_loss: 0.0144
09/16 01:25:00 PM: Update 22395: task edges-pos-ontonotes, batch 395 (22395): mcc: 0.8612, acc: 0.8103, precision: 0.8844, recall: 0.8441, f1: 0.8638, edges-pos-ontonotes_loss: 0.0144
09/16 01:25:10 PM: Update 22462: task edges-pos-ontonotes, batch 462 (22462): mcc: 0.8615, acc: 0.8106, precision: 0.8846, recall: 0.8444, f1: 0.8641, edges-pos-ontonotes_loss: 0.0144
09/16 01:25:20 PM: Update 22535: task edges-pos-ontonotes, batch 535 (22535): mcc: 0.8619, acc: 0.8110, precision: 0.8849, recall: 0.8448, f1: 0.8644, edges-pos-ontonotes_loss: 0.0144
09/16 01:25:30 PM: Update 22599: task edges-pos-ontonotes, batch 599 (22599): mcc: 0.8619, acc: 0.8109, precision: 0.8848, recall: 0.8450, f1: 0.8644, edges-pos-ontonotes_loss: 0.0144
09/16 01:25:40 PM: Update 22648: task edges-pos-ontonotes, batch 648 (22648): mcc: 0.8611, acc: 0.8101, precision: 0.8839, recall: 0.8443, f1: 0.8637, edges-pos-ontonotes_loss: 0.0144
09/16 01:25:50 PM: Update 22724: task edges-pos-ontonotes, batch 724 (22724): mcc: 0.8620, acc: 0.8111, precision: 0.8845, recall: 0.8456, f1: 0.8646, edges-pos-ontonotes_loss: 0.0142
09/16 01:26:00 PM: Update 22811: task edges-pos-ontonotes, batch 811 (22811): mcc: 0.8633, acc: 0.8124, precision: 0.8856, recall: 0.8470, f1: 0.8659, edges-pos-ontonotes_loss: 0.0139
09/16 01:26:11 PM: Update 22898: task edges-pos-ontonotes, batch 898 (22898): mcc: 0.8641, acc: 0.8130, precision: 0.8862, recall: 0.8479, f1: 0.8666, edges-pos-ontonotes_loss: 0.0137
09/16 01:26:21 PM: Update 22970: task edges-pos-ontonotes, batch 970 (22970): mcc: 0.8646, acc: 0.8134, precision: 0.8868, recall: 0.8483, f1: 0.8671, edges-pos-ontonotes_loss: 0.0135
09/16 01:26:23 PM: ***** Step 23000 / Validation 23 *****
09/16 01:26:23 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:26:23 PM: Validating...
09/16 01:26:31 PM: Evaluate: task edges-pos-ontonotes, batch 74 (157): mcc: 0.8707, acc: 0.8365, precision: 0.8994, recall: 0.8480, f1: 0.8729, edges-pos-ontonotes_loss: 0.0133
09/16 01:26:41 PM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.8683, acc: 0.8334, precision: 0.8987, recall: 0.8441, f1: 0.8705, edges-pos-ontonotes_loss: 0.0132
09/16 01:26:43 PM: Updating LR scheduler:
09/16 01:26:43 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:26:43 PM: 	# validation passes without improvement: 1
09/16 01:26:43 PM: edges-pos-ontonotes_loss: training: 0.013459 validation: 0.013183
09/16 01:26:43 PM: macro_avg: validation: 0.870292
09/16 01:26:43 PM: micro_avg: validation: 0.000000
09/16 01:26:43 PM: edges-pos-ontonotes_mcc: training: 0.865015 validation: 0.868083
09/16 01:26:43 PM: edges-pos-ontonotes_acc: training: 0.813897 validation: 0.832767
09/16 01:26:43 PM: edges-pos-ontonotes_precision: training: 0.887109 validation: 0.898977
09/16 01:26:43 PM: edges-pos-ontonotes_recall: training: 0.848837 validation: 0.843381
09/16 01:26:43 PM: edges-pos-ontonotes_f1: training: 0.867551 validation: 0.870292
09/16 01:26:43 PM: Global learning rate: 2.5e-05
09/16 01:26:43 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:26:51 PM: Update 23065: task edges-pos-ontonotes, batch 65 (23065): mcc: 0.8910, acc: 0.8446, precision: 0.9069, recall: 0.8797, f1: 0.8931, edges-pos-ontonotes_loss: 0.0103
09/16 01:27:01 PM: Update 23173: task edges-pos-ontonotes, batch 173 (23173): mcc: 0.8930, acc: 0.8459, precision: 0.9092, recall: 0.8814, f1: 0.8951, edges-pos-ontonotes_loss: 0.0100
09/16 01:27:19 PM: Update 23265: task edges-pos-ontonotes, batch 265 (23265): mcc: 0.8933, acc: 0.8458, precision: 0.9098, recall: 0.8814, f1: 0.8954, edges-pos-ontonotes_loss: 0.0099
09/16 01:27:29 PM: Update 23405: task edges-pos-ontonotes, batch 405 (23405): mcc: 0.8917, acc: 0.8442, precision: 0.9088, recall: 0.8793, f1: 0.8938, edges-pos-ontonotes_loss: 0.0104
09/16 01:27:39 PM: Update 23539: task edges-pos-ontonotes, batch 539 (23539): mcc: 0.8919, acc: 0.8446, precision: 0.9088, recall: 0.8796, f1: 0.8940, edges-pos-ontonotes_loss: 0.0104
09/16 01:27:49 PM: Update 23681: task edges-pos-ontonotes, batch 681 (23681): mcc: 0.8886, acc: 0.8411, precision: 0.9063, recall: 0.8758, f1: 0.8908, edges-pos-ontonotes_loss: 0.0107
09/16 01:27:59 PM: Update 23851: task edges-pos-ontonotes, batch 851 (23851): mcc: 0.8856, acc: 0.8386, precision: 0.9040, recall: 0.8722, f1: 0.8878, edges-pos-ontonotes_loss: 0.0109
09/16 01:28:09 PM: Update 23934: task edges-pos-ontonotes, batch 934 (23934): mcc: 0.8805, acc: 0.8326, precision: 0.9001, recall: 0.8660, f1: 0.8828, edges-pos-ontonotes_loss: 0.0112
09/16 01:28:18 PM: ***** Step 24000 / Validation 24 *****
09/16 01:28:18 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:28:18 PM: Validating...
09/16 01:28:19 PM: Evaluate: task edges-pos-ontonotes, batch 12 (157): mcc: 0.8637, acc: 0.8328, precision: 0.8905, recall: 0.8430, f1: 0.8661, edges-pos-ontonotes_loss: 0.0137
09/16 01:28:29 PM: Evaluate: task edges-pos-ontonotes, batch 105 (157): mcc: 0.8693, acc: 0.8347, precision: 0.8973, recall: 0.8472, f1: 0.8716, edges-pos-ontonotes_loss: 0.0129
09/16 01:28:38 PM: Updating LR scheduler:
09/16 01:28:38 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:28:38 PM: 	# validation passes without improvement: 2
09/16 01:28:38 PM: edges-pos-ontonotes_loss: training: 0.011598 validation: 0.013360
09/16 01:28:38 PM: macro_avg: validation: 0.864976
09/16 01:28:38 PM: micro_avg: validation: 0.000000
09/16 01:28:38 PM: edges-pos-ontonotes_mcc: training: 0.875586 validation: 0.862624
09/16 01:28:38 PM: edges-pos-ontonotes_acc: training: 0.826431 validation: 0.825857
09/16 01:28:38 PM: edges-pos-ontonotes_precision: training: 0.896027 validation: 0.892588
09/16 01:28:38 PM: edges-pos-ontonotes_recall: training: 0.860576 validation: 0.839021
09/16 01:28:38 PM: edges-pos-ontonotes_f1: training: 0.877944 validation: 0.864976
09/16 01:28:38 PM: Global learning rate: 2.5e-05
09/16 01:28:38 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:28:40 PM: Update 24013: task edges-pos-ontonotes, batch 13 (24013): mcc: 0.8376, acc: 0.7801, precision: 0.8721, recall: 0.8107, f1: 0.8403, edges-pos-ontonotes_loss: 0.0164
09/16 01:28:50 PM: Update 24078: task edges-pos-ontonotes, batch 78 (24078): mcc: 0.8407, acc: 0.7821, precision: 0.8710, recall: 0.8177, f1: 0.8435, edges-pos-ontonotes_loss: 0.0162
09/16 01:29:00 PM: Update 24136: task edges-pos-ontonotes, batch 136 (24136): mcc: 0.8421, acc: 0.7839, precision: 0.8711, recall: 0.8202, f1: 0.8449, edges-pos-ontonotes_loss: 0.0161
09/16 01:29:10 PM: Update 24192: task edges-pos-ontonotes, batch 192 (24192): mcc: 0.8431, acc: 0.7851, precision: 0.8719, recall: 0.8214, f1: 0.8459, edges-pos-ontonotes_loss: 0.0160
09/16 01:29:20 PM: Update 24273: task edges-pos-ontonotes, batch 273 (24273): mcc: 0.8446, acc: 0.7870, precision: 0.8726, recall: 0.8236, f1: 0.8474, edges-pos-ontonotes_loss: 0.0154
09/16 01:29:30 PM: Update 24389: task edges-pos-ontonotes, batch 389 (24389): mcc: 0.8484, acc: 0.7929, precision: 0.8749, recall: 0.8287, f1: 0.8512, edges-pos-ontonotes_loss: 0.0146
09/16 01:29:40 PM: Update 24473: task edges-pos-ontonotes, batch 473 (24473): mcc: 0.8511, acc: 0.7966, precision: 0.8765, recall: 0.8322, f1: 0.8538, edges-pos-ontonotes_loss: 0.0142
09/16 01:29:50 PM: Update 24540: task edges-pos-ontonotes, batch 540 (24540): mcc: 0.8527, acc: 0.7990, precision: 0.8776, recall: 0.8344, f1: 0.8555, edges-pos-ontonotes_loss: 0.0140
09/16 01:30:00 PM: Update 24614: task edges-pos-ontonotes, batch 614 (24614): mcc: 0.8540, acc: 0.8004, precision: 0.8784, recall: 0.8361, f1: 0.8567, edges-pos-ontonotes_loss: 0.0139
09/16 01:30:10 PM: Update 24712: task edges-pos-ontonotes, batch 712 (24712): mcc: 0.8556, acc: 0.8021, precision: 0.8796, recall: 0.8380, f1: 0.8583, edges-pos-ontonotes_loss: 0.0138
09/16 01:30:20 PM: Update 24798: task edges-pos-ontonotes, batch 798 (24798): mcc: 0.8570, acc: 0.8035, precision: 0.8805, recall: 0.8397, f1: 0.8596, edges-pos-ontonotes_loss: 0.0137
09/16 01:30:31 PM: Update 24847: task edges-pos-ontonotes, batch 847 (24847): mcc: 0.8575, acc: 0.8040, precision: 0.8809, recall: 0.8403, f1: 0.8601, edges-pos-ontonotes_loss: 0.0136
09/16 01:30:41 PM: Update 24901: task edges-pos-ontonotes, batch 901 (24901): mcc: 0.8567, acc: 0.8027, precision: 0.8801, recall: 0.8395, f1: 0.8593, edges-pos-ontonotes_loss: 0.0137
09/16 01:30:51 PM: Update 24969: task edges-pos-ontonotes, batch 969 (24969): mcc: 0.8561, acc: 0.8016, precision: 0.8796, recall: 0.8389, f1: 0.8588, edges-pos-ontonotes_loss: 0.0138
09/16 01:30:55 PM: ***** Step 25000 / Validation 25 *****
09/16 01:30:55 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:30:55 PM: Validating...
09/16 01:31:01 PM: Evaluate: task edges-pos-ontonotes, batch 59 (157): mcc: 0.8728, acc: 0.8512, precision: 0.8921, recall: 0.8591, f1: 0.8753, edges-pos-ontonotes_loss: 0.0127
09/16 01:31:11 PM: Evaluate: task edges-pos-ontonotes, batch 132 (157): mcc: 0.8736, acc: 0.8504, precision: 0.8931, recall: 0.8596, f1: 0.8760, edges-pos-ontonotes_loss: 0.0126
09/16 01:31:15 PM: Updating LR scheduler:
09/16 01:31:15 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:31:15 PM: 	# validation passes without improvement: 3
09/16 01:31:15 PM: edges-pos-ontonotes_loss: training: 0.013784 validation: 0.012747
09/16 01:31:15 PM: macro_avg: validation: 0.874962
09/16 01:31:15 PM: micro_avg: validation: 0.000000
09/16 01:31:15 PM: edges-pos-ontonotes_mcc: training: 0.856090 validation: 0.872525
09/16 01:31:15 PM: edges-pos-ontonotes_acc: training: 0.801572 validation: 0.848535
09/16 01:31:15 PM: edges-pos-ontonotes_precision: training: 0.879527 validation: 0.892259
09/16 01:31:15 PM: edges-pos-ontonotes_recall: training: 0.838979 validation: 0.858324
09/16 01:31:15 PM: edges-pos-ontonotes_f1: training: 0.858774 validation: 0.874962
09/16 01:31:15 PM: Global learning rate: 2.5e-05
09/16 01:31:15 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:31:21 PM: Update 25042: task edges-pos-ontonotes, batch 42 (25042): mcc: 0.8579, acc: 0.8023, precision: 0.8802, recall: 0.8419, f1: 0.8606, edges-pos-ontonotes_loss: 0.0143
09/16 01:31:32 PM: Update 25109: task edges-pos-ontonotes, batch 109 (25109): mcc: 0.8558, acc: 0.8004, precision: 0.8769, recall: 0.8409, f1: 0.8586, edges-pos-ontonotes_loss: 0.0145
09/16 01:31:49 PM: Update 25160: task edges-pos-ontonotes, batch 160 (25160): mcc: 0.8564, acc: 0.8011, precision: 0.8777, recall: 0.8414, f1: 0.8591, edges-pos-ontonotes_loss: 0.0145
09/16 01:31:59 PM: Update 25228: task edges-pos-ontonotes, batch 228 (25228): mcc: 0.8566, acc: 0.8026, precision: 0.8787, recall: 0.8408, f1: 0.8593, edges-pos-ontonotes_loss: 0.0145
09/16 01:32:09 PM: Update 25284: task edges-pos-ontonotes, batch 284 (25284): mcc: 0.8567, acc: 0.8033, precision: 0.8788, recall: 0.8408, f1: 0.8594, edges-pos-ontonotes_loss: 0.0146
09/16 01:32:19 PM: Update 25336: task edges-pos-ontonotes, batch 336 (25336): mcc: 0.8571, acc: 0.8037, precision: 0.8790, recall: 0.8414, f1: 0.8598, edges-pos-ontonotes_loss: 0.0146
09/16 01:32:29 PM: Update 25396: task edges-pos-ontonotes, batch 396 (25396): mcc: 0.8574, acc: 0.8042, precision: 0.8794, recall: 0.8416, f1: 0.8601, edges-pos-ontonotes_loss: 0.0146
09/16 01:32:39 PM: Update 25464: task edges-pos-ontonotes, batch 464 (25464): mcc: 0.8579, acc: 0.8050, precision: 0.8801, recall: 0.8419, f1: 0.8606, edges-pos-ontonotes_loss: 0.0145
09/16 01:32:49 PM: Update 25517: task edges-pos-ontonotes, batch 517 (25517): mcc: 0.8582, acc: 0.8054, precision: 0.8805, recall: 0.8420, f1: 0.8608, edges-pos-ontonotes_loss: 0.0146
09/16 01:32:59 PM: Update 25588: task edges-pos-ontonotes, batch 588 (25588): mcc: 0.8582, acc: 0.8059, precision: 0.8807, recall: 0.8420, f1: 0.8609, edges-pos-ontonotes_loss: 0.0146
09/16 01:33:09 PM: Update 25657: task edges-pos-ontonotes, batch 657 (25657): mcc: 0.8587, acc: 0.8064, precision: 0.8812, recall: 0.8423, f1: 0.8613, edges-pos-ontonotes_loss: 0.0146
09/16 01:33:19 PM: Update 25719: task edges-pos-ontonotes, batch 719 (25719): mcc: 0.8590, acc: 0.8070, precision: 0.8815, recall: 0.8427, f1: 0.8617, edges-pos-ontonotes_loss: 0.0145
09/16 01:33:31 PM: Update 25786: task edges-pos-ontonotes, batch 786 (25786): mcc: 0.8593, acc: 0.8076, precision: 0.8819, recall: 0.8430, f1: 0.8620, edges-pos-ontonotes_loss: 0.0145
09/16 01:33:41 PM: Update 25853: task edges-pos-ontonotes, batch 853 (25853): mcc: 0.8597, acc: 0.8080, precision: 0.8823, recall: 0.8433, f1: 0.8624, edges-pos-ontonotes_loss: 0.0145
09/16 01:33:52 PM: Update 25918: task edges-pos-ontonotes, batch 918 (25918): mcc: 0.8600, acc: 0.8083, precision: 0.8825, recall: 0.8436, f1: 0.8626, edges-pos-ontonotes_loss: 0.0145
09/16 01:34:02 PM: Update 25985: task edges-pos-ontonotes, batch 985 (25985): mcc: 0.8599, acc: 0.8082, precision: 0.8825, recall: 0.8435, f1: 0.8625, edges-pos-ontonotes_loss: 0.0145
09/16 01:34:04 PM: ***** Step 26000 / Validation 26 *****
09/16 01:34:04 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:34:04 PM: Validating...
09/16 01:34:12 PM: Evaluate: task edges-pos-ontonotes, batch 78 (157): mcc: 0.8715, acc: 0.8485, precision: 0.8902, recall: 0.8582, f1: 0.8739, edges-pos-ontonotes_loss: 0.0130
09/16 01:34:22 PM: Evaluate: task edges-pos-ontonotes, batch 146 (157): mcc: 0.8719, acc: 0.8470, precision: 0.8911, recall: 0.8581, f1: 0.8743, edges-pos-ontonotes_loss: 0.0129
09/16 01:34:23 PM: Updating LR scheduler:
09/16 01:34:23 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:34:23 PM: 	# validation passes without improvement: 0
09/16 01:34:23 PM: edges-pos-ontonotes_loss: training: 0.014462 validation: 0.012913
09/16 01:34:23 PM: macro_avg: validation: 0.874811
09/16 01:34:23 PM: micro_avg: validation: 0.000000
09/16 01:34:23 PM: edges-pos-ontonotes_mcc: training: 0.860052 validation: 0.872366
09/16 01:34:23 PM: edges-pos-ontonotes_acc: training: 0.808354 validation: 0.847413
09/16 01:34:23 PM: edges-pos-ontonotes_precision: training: 0.882602 validation: 0.891865
09/16 01:34:23 PM: edges-pos-ontonotes_recall: training: 0.843634 validation: 0.858398
09/16 01:34:23 PM: edges-pos-ontonotes_f1: training: 0.862678 validation: 0.874811
09/16 01:34:23 PM: Global learning rate: 1.25e-05
09/16 01:34:23 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:34:32 PM: Update 26060: task edges-pos-ontonotes, batch 60 (26060): mcc: 0.8613, acc: 0.8098, precision: 0.8826, recall: 0.8460, f1: 0.8639, edges-pos-ontonotes_loss: 0.0143
09/16 01:34:42 PM: Update 26121: task edges-pos-ontonotes, batch 121 (26121): mcc: 0.8615, acc: 0.8109, precision: 0.8822, recall: 0.8467, f1: 0.8641, edges-pos-ontonotes_loss: 0.0140
09/16 01:34:52 PM: Update 26206: task edges-pos-ontonotes, batch 206 (26206): mcc: 0.8650, acc: 0.8149, precision: 0.8850, recall: 0.8508, f1: 0.8676, edges-pos-ontonotes_loss: 0.0133
09/16 01:35:02 PM: Update 26289: task edges-pos-ontonotes, batch 289 (26289): mcc: 0.8665, acc: 0.8164, precision: 0.8863, recall: 0.8525, f1: 0.8691, edges-pos-ontonotes_loss: 0.0129
09/16 01:35:13 PM: Update 26373: task edges-pos-ontonotes, batch 373 (26373): mcc: 0.8683, acc: 0.8181, precision: 0.8880, recall: 0.8543, f1: 0.8708, edges-pos-ontonotes_loss: 0.0127
09/16 01:35:23 PM: Update 26456: task edges-pos-ontonotes, batch 456 (26456): mcc: 0.8699, acc: 0.8195, precision: 0.8893, recall: 0.8561, f1: 0.8724, edges-pos-ontonotes_loss: 0.0124
09/16 01:35:33 PM: Update 26568: task edges-pos-ontonotes, batch 568 (26568): mcc: 0.8730, acc: 0.8230, precision: 0.8920, recall: 0.8596, f1: 0.8755, edges-pos-ontonotes_loss: 0.0120
09/16 01:35:43 PM: Update 26679: task edges-pos-ontonotes, batch 679 (26679): mcc: 0.8754, acc: 0.8258, precision: 0.8940, recall: 0.8623, f1: 0.8778, edges-pos-ontonotes_loss: 0.0117
09/16 01:35:53 PM: Update 26773: task edges-pos-ontonotes, batch 773 (26773): mcc: 0.8766, acc: 0.8270, precision: 0.8950, recall: 0.8634, f1: 0.8790, edges-pos-ontonotes_loss: 0.0116
09/16 01:36:03 PM: Update 26912: task edges-pos-ontonotes, batch 912 (26912): mcc: 0.8776, acc: 0.8281, precision: 0.8960, recall: 0.8645, f1: 0.8799, edges-pos-ontonotes_loss: 0.0116
09/16 01:36:09 PM: ***** Step 27000 / Validation 27 *****
09/16 01:36:09 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:36:09 PM: Validating...
09/16 01:36:13 PM: Evaluate: task edges-pos-ontonotes, batch 40 (157): mcc: 0.8570, acc: 0.8310, precision: 0.8836, recall: 0.8369, f1: 0.8596, edges-pos-ontonotes_loss: 0.0136
09/16 01:36:23 PM: Evaluate: task edges-pos-ontonotes, batch 121 (157): mcc: 0.8634, acc: 0.8358, precision: 0.8898, recall: 0.8430, f1: 0.8658, edges-pos-ontonotes_loss: 0.0132
09/16 01:36:28 PM: Updating LR scheduler:
09/16 01:36:28 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:36:28 PM: 	# validation passes without improvement: 1
09/16 01:36:28 PM: edges-pos-ontonotes_loss: training: 0.011482 validation: 0.013357
09/16 01:36:28 PM: macro_avg: validation: 0.863211
09/16 01:36:28 PM: micro_avg: validation: 0.000000
09/16 01:36:28 PM: edges-pos-ontonotes_mcc: training: 0.878246 validation: 0.860807
09/16 01:36:28 PM: edges-pos-ontonotes_acc: training: 0.828844 validation: 0.830566
09/16 01:36:29 PM: edges-pos-ontonotes_precision: training: 0.896654 validation: 0.890337
09/16 01:36:29 PM: edges-pos-ontonotes_recall: training: 0.865098 validation: 0.837688
09/16 01:36:29 PM: edges-pos-ontonotes_f1: training: 0.880593 validation: 0.863211
09/16 01:36:29 PM: Global learning rate: 1.25e-05
09/16 01:36:29 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:36:34 PM: Update 27038: task edges-pos-ontonotes, batch 38 (27038): mcc: 0.8887, acc: 0.8397, precision: 0.9066, recall: 0.8755, f1: 0.8908, edges-pos-ontonotes_loss: 0.0099
09/16 01:36:44 PM: Update 27172: task edges-pos-ontonotes, batch 172 (27172): mcc: 0.8657, acc: 0.8134, precision: 0.8903, recall: 0.8471, f1: 0.8681, edges-pos-ontonotes_loss: 0.0121
09/16 01:36:54 PM: Update 27330: task edges-pos-ontonotes, batch 330 (27330): mcc: 0.8683, acc: 0.8194, precision: 0.8927, recall: 0.8497, f1: 0.8707, edges-pos-ontonotes_loss: 0.0119
09/16 01:37:05 PM: Update 27351: task edges-pos-ontonotes, batch 351 (27351): mcc: 0.8672, acc: 0.8185, precision: 0.8916, recall: 0.8487, f1: 0.8697, edges-pos-ontonotes_loss: 0.0119
09/16 01:37:15 PM: Update 27420: task edges-pos-ontonotes, batch 420 (27420): mcc: 0.8568, acc: 0.8055, precision: 0.8839, recall: 0.8361, f1: 0.8593, edges-pos-ontonotes_loss: 0.0127
09/16 01:37:25 PM: Update 27497: task edges-pos-ontonotes, batch 497 (27497): mcc: 0.8522, acc: 0.7996, precision: 0.8796, recall: 0.8314, f1: 0.8548, edges-pos-ontonotes_loss: 0.0133
09/16 01:37:35 PM: Update 27571: task edges-pos-ontonotes, batch 571 (27571): mcc: 0.8507, acc: 0.7974, precision: 0.8780, recall: 0.8300, f1: 0.8534, edges-pos-ontonotes_loss: 0.0137
09/16 01:37:45 PM: Update 27643: task edges-pos-ontonotes, batch 643 (27643): mcc: 0.8489, acc: 0.7947, precision: 0.8763, recall: 0.8283, f1: 0.8517, edges-pos-ontonotes_loss: 0.0140
09/16 01:37:55 PM: Update 27719: task edges-pos-ontonotes, batch 719 (27719): mcc: 0.8483, acc: 0.7936, precision: 0.8757, recall: 0.8277, f1: 0.8510, edges-pos-ontonotes_loss: 0.0141
09/16 01:38:06 PM: Update 27834: task edges-pos-ontonotes, batch 834 (27834): mcc: 0.8500, acc: 0.7960, precision: 0.8767, recall: 0.8300, f1: 0.8527, edges-pos-ontonotes_loss: 0.0139
09/16 01:38:16 PM: Update 27948: task edges-pos-ontonotes, batch 948 (27948): mcc: 0.8516, acc: 0.7983, precision: 0.8775, recall: 0.8323, f1: 0.8543, edges-pos-ontonotes_loss: 0.0137
09/16 01:38:23 PM: ***** Step 28000 / Validation 28 *****
09/16 01:38:23 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:38:23 PM: Validating...
09/16 01:38:26 PM: Evaluate: task edges-pos-ontonotes, batch 29 (157): mcc: 0.8632, acc: 0.8380, precision: 0.8860, recall: 0.8464, f1: 0.8657, edges-pos-ontonotes_loss: 0.0133
09/16 01:38:36 PM: Evaluate: task edges-pos-ontonotes, batch 115 (157): mcc: 0.8674, acc: 0.8415, precision: 0.8903, recall: 0.8503, f1: 0.8698, edges-pos-ontonotes_loss: 0.0127
09/16 01:38:42 PM: Updating LR scheduler:
09/16 01:38:42 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:38:42 PM: 	# validation passes without improvement: 2
09/16 01:38:42 PM: edges-pos-ontonotes_loss: training: 0.013621 validation: 0.013003
09/16 01:38:42 PM: macro_avg: validation: 0.865540
09/16 01:38:42 PM: micro_avg: validation: 0.000000
09/16 01:38:42 PM: edges-pos-ontonotes_mcc: training: 0.852443 validation: 0.863064
09/16 01:38:42 PM: edges-pos-ontonotes_acc: training: 0.799490 validation: 0.834206
09/16 01:38:42 PM: edges-pos-ontonotes_precision: training: 0.877984 validation: 0.888801
09/16 01:38:42 PM: edges-pos-ontonotes_recall: training: 0.833460 validation: 0.843466
09/16 01:38:42 PM: edges-pos-ontonotes_f1: training: 0.855143 validation: 0.865540
09/16 01:38:42 PM: Global learning rate: 1.25e-05
09/16 01:38:42 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:38:46 PM: Update 28033: task edges-pos-ontonotes, batch 33 (28033): mcc: 0.8629, acc: 0.8119, precision: 0.8851, recall: 0.8467, f1: 0.8655, edges-pos-ontonotes_loss: 0.0137
09/16 01:38:56 PM: Update 28125: task edges-pos-ontonotes, batch 125 (28125): mcc: 0.8633, acc: 0.8113, precision: 0.8852, recall: 0.8474, f1: 0.8659, edges-pos-ontonotes_loss: 0.0132
09/16 01:39:06 PM: Update 28214: task edges-pos-ontonotes, batch 214 (28214): mcc: 0.8631, acc: 0.8106, precision: 0.8844, recall: 0.8478, f1: 0.8657, edges-pos-ontonotes_loss: 0.0133
09/16 01:39:16 PM: Update 28301: task edges-pos-ontonotes, batch 301 (28301): mcc: 0.8643, acc: 0.8117, precision: 0.8857, recall: 0.8488, f1: 0.8669, edges-pos-ontonotes_loss: 0.0132
09/16 01:39:26 PM: Update 28353: task edges-pos-ontonotes, batch 353 (28353): mcc: 0.8601, acc: 0.8057, precision: 0.8824, recall: 0.8440, f1: 0.8628, edges-pos-ontonotes_loss: 0.0134
09/16 01:39:36 PM: Update 28424: task edges-pos-ontonotes, batch 424 (28424): mcc: 0.8582, acc: 0.8023, precision: 0.8807, recall: 0.8419, f1: 0.8609, edges-pos-ontonotes_loss: 0.0136
09/16 01:39:46 PM: Update 28494: task edges-pos-ontonotes, batch 494 (28494): mcc: 0.8572, acc: 0.8005, precision: 0.8798, recall: 0.8409, f1: 0.8599, edges-pos-ontonotes_loss: 0.0138
09/16 01:39:56 PM: Update 28562: task edges-pos-ontonotes, batch 562 (28562): mcc: 0.8565, acc: 0.7996, precision: 0.8790, recall: 0.8403, f1: 0.8592, edges-pos-ontonotes_loss: 0.0139
09/16 01:40:07 PM: Update 28621: task edges-pos-ontonotes, batch 621 (28621): mcc: 0.8564, acc: 0.7993, precision: 0.8787, recall: 0.8403, f1: 0.8591, edges-pos-ontonotes_loss: 0.0140
09/16 01:40:17 PM: Update 28684: task edges-pos-ontonotes, batch 684 (28684): mcc: 0.8563, acc: 0.7996, precision: 0.8789, recall: 0.8399, f1: 0.8590, edges-pos-ontonotes_loss: 0.0140
09/16 01:40:27 PM: Update 28751: task edges-pos-ontonotes, batch 751 (28751): mcc: 0.8567, acc: 0.8005, precision: 0.8794, recall: 0.8402, f1: 0.8593, edges-pos-ontonotes_loss: 0.0141
09/16 01:40:37 PM: Update 28816: task edges-pos-ontonotes, batch 816 (28816): mcc: 0.8568, acc: 0.8011, precision: 0.8795, recall: 0.8404, f1: 0.8595, edges-pos-ontonotes_loss: 0.0141
09/16 01:40:47 PM: Update 28871: task edges-pos-ontonotes, batch 871 (28871): mcc: 0.8570, acc: 0.8016, precision: 0.8797, recall: 0.8406, f1: 0.8597, edges-pos-ontonotes_loss: 0.0141
09/16 01:40:58 PM: Update 28916: task edges-pos-ontonotes, batch 916 (28916): mcc: 0.8572, acc: 0.8020, precision: 0.8799, recall: 0.8409, f1: 0.8599, edges-pos-ontonotes_loss: 0.0141
09/16 01:41:08 PM: Update 28951: task edges-pos-ontonotes, batch 951 (28951): mcc: 0.8572, acc: 0.8022, precision: 0.8798, recall: 0.8408, f1: 0.8599, edges-pos-ontonotes_loss: 0.0142
09/16 01:41:15 PM: ***** Step 29000 / Validation 29 *****
09/16 01:41:15 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:41:15 PM: Validating...
09/16 01:41:18 PM: Evaluate: task edges-pos-ontonotes, batch 23 (157): mcc: 0.8664, acc: 0.8420, precision: 0.8843, recall: 0.8542, f1: 0.8690, edges-pos-ontonotes_loss: 0.0133
09/16 01:41:28 PM: Evaluate: task edges-pos-ontonotes, batch 110 (157): mcc: 0.8747, acc: 0.8504, precision: 0.8923, recall: 0.8624, f1: 0.8771, edges-pos-ontonotes_loss: 0.0126
09/16 01:41:35 PM: Updating LR scheduler:
09/16 01:41:35 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:41:35 PM: 	# validation passes without improvement: 3
09/16 01:41:35 PM: edges-pos-ontonotes_loss: training: 0.014165 validation: 0.012761
09/16 01:41:35 PM: macro_avg: validation: 0.875437
09/16 01:41:35 PM: micro_avg: validation: 0.000000
09/16 01:41:35 PM: edges-pos-ontonotes_mcc: training: 0.857229 validation: 0.872976
09/16 01:41:35 PM: edges-pos-ontonotes_acc: training: 0.802510 validation: 0.847149
09/16 01:41:35 PM: edges-pos-ontonotes_precision: training: 0.879898 validation: 0.891137
09/16 01:41:35 PM: edges-pos-ontonotes_recall: training: 0.840811 validation: 0.860281
09/16 01:41:35 PM: edges-pos-ontonotes_f1: training: 0.859910 validation: 0.875437
09/16 01:41:35 PM: Global learning rate: 1.25e-05
09/16 01:41:35 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:41:38 PM: Update 29020: task edges-pos-ontonotes, batch 20 (29020): mcc: 0.8615, acc: 0.8097, precision: 0.8827, recall: 0.8463, f1: 0.8641, edges-pos-ontonotes_loss: 0.0141
09/16 01:41:48 PM: Update 29094: task edges-pos-ontonotes, batch 94 (29094): mcc: 0.8629, acc: 0.8115, precision: 0.8861, recall: 0.8459, f1: 0.8655, edges-pos-ontonotes_loss: 0.0146
09/16 01:41:58 PM: Update 29165: task edges-pos-ontonotes, batch 165 (29165): mcc: 0.8624, acc: 0.8109, precision: 0.8856, recall: 0.8453, f1: 0.8650, edges-pos-ontonotes_loss: 0.0146
09/16 01:42:08 PM: Update 29232: task edges-pos-ontonotes, batch 232 (29232): mcc: 0.8615, acc: 0.8103, precision: 0.8848, recall: 0.8443, f1: 0.8641, edges-pos-ontonotes_loss: 0.0145
09/16 01:42:21 PM: Update 29246: task edges-pos-ontonotes, batch 246 (29246): mcc: 0.8614, acc: 0.8103, precision: 0.8848, recall: 0.8442, f1: 0.8640, edges-pos-ontonotes_loss: 0.0145
09/16 01:42:31 PM: Update 29308: task edges-pos-ontonotes, batch 308 (29308): mcc: 0.8611, acc: 0.8100, precision: 0.8844, recall: 0.8440, f1: 0.8637, edges-pos-ontonotes_loss: 0.0145
09/16 01:42:41 PM: Update 29378: task edges-pos-ontonotes, batch 378 (29378): mcc: 0.8606, acc: 0.8093, precision: 0.8837, recall: 0.8436, f1: 0.8632, edges-pos-ontonotes_loss: 0.0146
09/16 01:42:51 PM: Update 29444: task edges-pos-ontonotes, batch 444 (29444): mcc: 0.8606, acc: 0.8092, precision: 0.8837, recall: 0.8436, f1: 0.8632, edges-pos-ontonotes_loss: 0.0145
09/16 01:43:01 PM: Update 29516: task edges-pos-ontonotes, batch 516 (29516): mcc: 0.8611, acc: 0.8097, precision: 0.8842, recall: 0.8441, f1: 0.8637, edges-pos-ontonotes_loss: 0.0145
09/16 01:43:12 PM: Update 29570: task edges-pos-ontonotes, batch 570 (29570): mcc: 0.8608, acc: 0.8094, precision: 0.8838, recall: 0.8440, f1: 0.8634, edges-pos-ontonotes_loss: 0.0144
09/16 01:43:22 PM: Update 29648: task edges-pos-ontonotes, batch 648 (29648): mcc: 0.8619, acc: 0.8107, precision: 0.8844, recall: 0.8455, f1: 0.8645, edges-pos-ontonotes_loss: 0.0142
09/16 01:43:32 PM: Update 29736: task edges-pos-ontonotes, batch 736 (29736): mcc: 0.8630, acc: 0.8119, precision: 0.8851, recall: 0.8469, f1: 0.8655, edges-pos-ontonotes_loss: 0.0139
09/16 01:43:42 PM: Update 29813: task edges-pos-ontonotes, batch 813 (29813): mcc: 0.8639, acc: 0.8127, precision: 0.8858, recall: 0.8478, f1: 0.8664, edges-pos-ontonotes_loss: 0.0137
09/16 01:43:52 PM: Update 29890: task edges-pos-ontonotes, batch 890 (29890): mcc: 0.8647, acc: 0.8135, precision: 0.8865, recall: 0.8489, f1: 0.8673, edges-pos-ontonotes_loss: 0.0135
09/16 01:44:02 PM: Update 29985: task edges-pos-ontonotes, batch 985 (29985): mcc: 0.8662, acc: 0.8151, precision: 0.8876, recall: 0.8506, f1: 0.8687, edges-pos-ontonotes_loss: 0.0132
09/16 01:44:03 PM: ***** Step 30000 / Validation 30 *****
09/16 01:44:03 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:44:03 PM: Validating...
09/16 01:44:12 PM: Evaluate: task edges-pos-ontonotes, batch 87 (157): mcc: 0.8718, acc: 0.8393, precision: 0.9019, recall: 0.8478, f1: 0.8740, edges-pos-ontonotes_loss: 0.0130
09/16 01:44:22 PM: Evaluate: task edges-pos-ontonotes, batch 152 (157): mcc: 0.8692, acc: 0.8343, precision: 0.9012, recall: 0.8434, f1: 0.8713, edges-pos-ontonotes_loss: 0.0130
09/16 01:44:23 PM: Updating LR scheduler:
09/16 01:44:23 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:44:23 PM: 	# validation passes without improvement: 0
09/16 01:44:23 PM: edges-pos-ontonotes_loss: training: 0.013186 validation: 0.013042
09/16 01:44:23 PM: macro_avg: validation: 0.870498
09/16 01:44:23 PM: micro_avg: validation: 0.000000
09/16 01:44:23 PM: edges-pos-ontonotes_mcc: training: 0.866414 validation: 0.868344
09/16 01:44:23 PM: edges-pos-ontonotes_acc: training: 0.815351 validation: 0.833233
09/16 01:44:23 PM: edges-pos-ontonotes_precision: training: 0.887788 validation: 0.900660
09/16 01:44:23 PM: edges-pos-ontonotes_recall: training: 0.850872 validation: 0.842291
09/16 01:44:23 PM: edges-pos-ontonotes_f1: training: 0.868938 validation: 0.870498
09/16 01:44:23 PM: Global learning rate: 6.25e-06
09/16 01:44:23 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:44:32 PM: Update 30105: task edges-pos-ontonotes, batch 105 (30105): mcc: 0.8924, acc: 0.8450, precision: 0.9069, recall: 0.8826, f1: 0.8946, edges-pos-ontonotes_loss: 0.0101
09/16 01:44:42 PM: Update 30200: task edges-pos-ontonotes, batch 200 (30200): mcc: 0.8921, acc: 0.8447, precision: 0.9076, recall: 0.8813, f1: 0.8942, edges-pos-ontonotes_loss: 0.0101
09/16 01:44:52 PM: Update 30329: task edges-pos-ontonotes, batch 329 (30329): mcc: 0.8901, acc: 0.8426, precision: 0.9063, recall: 0.8787, f1: 0.8922, edges-pos-ontonotes_loss: 0.0105
09/16 01:45:02 PM: Update 30475: task edges-pos-ontonotes, batch 475 (30475): mcc: 0.8897, acc: 0.8421, precision: 0.9066, recall: 0.8775, f1: 0.8918, edges-pos-ontonotes_loss: 0.0107
09/16 01:45:12 PM: Update 30615: task edges-pos-ontonotes, batch 615 (30615): mcc: 0.8841, acc: 0.8350, precision: 0.9026, recall: 0.8706, f1: 0.8863, edges-pos-ontonotes_loss: 0.0111
09/16 01:45:23 PM: Update 30761: task edges-pos-ontonotes, batch 761 (30761): mcc: 0.8802, acc: 0.8308, precision: 0.8999, recall: 0.8658, f1: 0.8825, edges-pos-ontonotes_loss: 0.0114
09/16 01:45:33 PM: Update 30855: task edges-pos-ontonotes, batch 855 (30855): mcc: 0.8755, acc: 0.8253, precision: 0.8969, recall: 0.8596, f1: 0.8778, edges-pos-ontonotes_loss: 0.0117
09/16 01:45:43 PM: Update 30934: task edges-pos-ontonotes, batch 934 (30934): mcc: 0.8704, acc: 0.8193, precision: 0.8933, recall: 0.8533, f1: 0.8728, edges-pos-ontonotes_loss: 0.0121
09/16 01:45:53 PM: ***** Step 31000 / Validation 31 *****
09/16 01:45:53 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:45:53 PM: Validating...
09/16 01:45:53 PM: Evaluate: task edges-pos-ontonotes, batch 2 (157): mcc: 0.8751, acc: 0.8492, precision: 0.8898, recall: 0.8657, f1: 0.8776, edges-pos-ontonotes_loss: 0.0122
09/16 01:46:03 PM: Evaluate: task edges-pos-ontonotes, batch 98 (157): mcc: 0.8721, acc: 0.8466, precision: 0.8941, recall: 0.8558, f1: 0.8745, edges-pos-ontonotes_loss: 0.0127
09/16 01:46:12 PM: Updating LR scheduler:
09/16 01:46:12 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:46:12 PM: 	# validation passes without improvement: 1
09/16 01:46:12 PM: edges-pos-ontonotes_loss: training: 0.012391 validation: 0.013113
09/16 01:46:12 PM: macro_avg: validation: 0.864564
09/16 01:46:12 PM: micro_avg: validation: 0.000000
09/16 01:46:12 PM: edges-pos-ontonotes_mcc: training: 0.866523 validation: 0.862077
09/16 01:46:12 PM: edges-pos-ontonotes_acc: training: 0.814483 validation: 0.832661
09/16 01:46:12 PM: edges-pos-ontonotes_precision: training: 0.890337 validation: 0.888129
09/16 01:46:12 PM: edges-pos-ontonotes_recall: training: 0.848628 validation: 0.842217
09/16 01:46:12 PM: edges-pos-ontonotes_f1: training: 0.868982 validation: 0.864564
09/16 01:46:12 PM: Global learning rate: 6.25e-06
09/16 01:46:12 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:46:13 PM: Update 31007: task edges-pos-ontonotes, batch 7 (31007): mcc: 0.8549, acc: 0.8044, precision: 0.8799, recall: 0.8365, f1: 0.8576, edges-pos-ontonotes_loss: 0.0144
09/16 01:46:23 PM: Update 31076: task edges-pos-ontonotes, batch 76 (31076): mcc: 0.8431, acc: 0.7872, precision: 0.8707, recall: 0.8225, f1: 0.8459, edges-pos-ontonotes_loss: 0.0159
09/16 01:46:34 PM: Update 31141: task edges-pos-ontonotes, batch 141 (31141): mcc: 0.8407, acc: 0.7828, precision: 0.8692, recall: 0.8194, f1: 0.8436, edges-pos-ontonotes_loss: 0.0164
09/16 01:46:44 PM: Update 31258: task edges-pos-ontonotes, batch 258 (31258): mcc: 0.8481, acc: 0.7929, precision: 0.8740, recall: 0.8290, f1: 0.8509, edges-pos-ontonotes_loss: 0.0147
09/16 01:46:54 PM: Update 31353: task edges-pos-ontonotes, batch 353 (31353): mcc: 0.8515, acc: 0.7977, precision: 0.8765, recall: 0.8331, f1: 0.8543, edges-pos-ontonotes_loss: 0.0140
09/16 01:47:13 PM: Update 31454: task edges-pos-ontonotes, batch 454 (31454): mcc: 0.8539, acc: 0.8014, precision: 0.8783, recall: 0.8360, f1: 0.8566, edges-pos-ontonotes_loss: 0.0137
09/16 01:47:23 PM: Update 31531: task edges-pos-ontonotes, batch 531 (31531): mcc: 0.8556, acc: 0.8034, precision: 0.8796, recall: 0.8379, f1: 0.8583, edges-pos-ontonotes_loss: 0.0136
09/16 01:47:33 PM: Update 31591: task edges-pos-ontonotes, batch 591 (31591): mcc: 0.8566, acc: 0.8046, precision: 0.8803, recall: 0.8393, f1: 0.8593, edges-pos-ontonotes_loss: 0.0136
09/16 01:47:44 PM: Update 31669: task edges-pos-ontonotes, batch 669 (31669): mcc: 0.8576, acc: 0.8055, precision: 0.8812, recall: 0.8403, f1: 0.8602, edges-pos-ontonotes_loss: 0.0135
09/16 01:47:54 PM: Update 31740: task edges-pos-ontonotes, batch 740 (31740): mcc: 0.8582, acc: 0.8060, precision: 0.8816, recall: 0.8410, f1: 0.8609, edges-pos-ontonotes_loss: 0.0135
09/16 01:48:04 PM: Update 31802: task edges-pos-ontonotes, batch 802 (31802): mcc: 0.8572, acc: 0.8044, precision: 0.8808, recall: 0.8398, f1: 0.8598, edges-pos-ontonotes_loss: 0.0136
09/16 01:48:14 PM: Update 31864: task edges-pos-ontonotes, batch 864 (31864): mcc: 0.8560, acc: 0.8024, precision: 0.8798, recall: 0.8386, f1: 0.8587, edges-pos-ontonotes_loss: 0.0137
09/16 01:48:24 PM: Update 31928: task edges-pos-ontonotes, batch 928 (31928): mcc: 0.8557, acc: 0.8014, precision: 0.8795, recall: 0.8383, f1: 0.8584, edges-pos-ontonotes_loss: 0.0137
09/16 01:48:34 PM: Update 31994: task edges-pos-ontonotes, batch 994 (31994): mcc: 0.8552, acc: 0.8002, precision: 0.8791, recall: 0.8377, f1: 0.8579, edges-pos-ontonotes_loss: 0.0138
09/16 01:48:35 PM: ***** Step 32000 / Validation 32 *****
09/16 01:48:35 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:48:35 PM: Validating...
09/16 01:48:44 PM: Evaluate: task edges-pos-ontonotes, batch 88 (157): mcc: 0.8741, acc: 0.8404, precision: 0.9047, recall: 0.8494, f1: 0.8762, edges-pos-ontonotes_loss: 0.0124
09/16 01:48:54 PM: Evaluate: task edges-pos-ontonotes, batch 153 (157): mcc: 0.8698, acc: 0.8352, precision: 0.8988, recall: 0.8468, f1: 0.8720, edges-pos-ontonotes_loss: 0.0127
09/16 01:48:55 PM: Updating LR scheduler:
09/16 01:48:55 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:48:55 PM: 	# validation passes without improvement: 2
09/16 01:48:55 PM: Ran out of early stopping patience. Stopping training.
09/16 01:48:55 PM: edges-pos-ontonotes_loss: training: 0.013819 validation: 0.012734
09/16 01:48:55 PM: macro_avg: validation: 0.871342
09/16 01:48:55 PM: micro_avg: validation: 0.000000
09/16 01:48:55 PM: edges-pos-ontonotes_mcc: training: 0.855103 validation: 0.869091
09/16 01:48:55 PM: edges-pos-ontonotes_acc: training: 0.799990 validation: 0.834471
09/16 01:48:55 PM: edges-pos-ontonotes_precision: training: 0.878981 validation: 0.898147
09/16 01:48:55 PM: edges-pos-ontonotes_recall: training: 0.837608 validation: 0.846090
09/16 01:48:55 PM: edges-pos-ontonotes_f1: training: 0.857796 validation: 0.871342
09/16 01:48:55 PM: Global learning rate: 6.25e-06
09/16 01:48:55 PM: Saving checkpoints to: ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:48:55 PM: Stopped training after 32 validation checks
09/16 01:48:55 PM: Trained edges-pos-ontonotes for 32000 batches or 9.265 epochs
09/16 01:48:55 PM: ***** VALIDATION RESULTS *****
09/16 01:48:55 PM: edges-pos-ontonotes_f1 (for best val pass 22): edges-pos-ontonotes_loss: 0.01283, macro_avg: 0.87551, micro_avg: 0.00000, edges-pos-ontonotes_mcc: 0.87309, edges-pos-ontonotes_acc: 0.84755, edges-pos-ontonotes_precision: 0.89288, edges-pos-ontonotes_recall: 0.85880, edges-pos-ontonotes_f1: 0.87551
09/16 01:48:55 PM: micro_avg (for best val pass 1): edges-pos-ontonotes_loss: 0.01973, macro_avg: 0.82499, micro_avg: 0.00000, edges-pos-ontonotes_mcc: 0.82347, edges-pos-ontonotes_acc: 0.74926, edges-pos-ontonotes_precision: 0.88460, edges-pos-ontonotes_recall: 0.77290, edges-pos-ontonotes_f1: 0.82499
09/16 01:48:55 PM: macro_avg (for best val pass 22): edges-pos-ontonotes_loss: 0.01283, macro_avg: 0.87551, micro_avg: 0.00000, edges-pos-ontonotes_mcc: 0.87309, edges-pos-ontonotes_acc: 0.84755, edges-pos-ontonotes_precision: 0.89288, edges-pos-ontonotes_recall: 0.85880, edges-pos-ontonotes_f1: 0.87551
09/16 01:48:55 PM: Evaluating...
09/16 01:48:55 PM: Loaded model state from ./experiments/pos-ontonotes-mrpc-only/run/edges-pos-ontonotes/model_state_target_train_val_22.best.th
09/16 01:48:55 PM: Evaluating on: edges-pos-ontonotes, split: val
09/16 01:49:25 PM: 	Task edges-pos-ontonotes: batch 194
09/16 01:49:55 PM: 	Task edges-pos-ontonotes: batch 418
09/16 01:50:03 PM: Task 'edges-pos-ontonotes': sorting predictions by 'idx'
09/16 01:50:03 PM: Finished evaluating on: edges-pos-ontonotes
09/16 01:50:04 PM: Task 'edges-pos-ontonotes': joining predictions with input split 'val'
09/16 01:50:15 PM: Task 'edges-pos-ontonotes': Wrote predictions to ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:50:15 PM: Wrote all preds for split 'val' to ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:50:15 PM: Evaluating on: edges-pos-ontonotes, split: test
09/16 01:50:46 PM: 	Task edges-pos-ontonotes: batch 195
09/16 01:51:05 PM: Task 'edges-pos-ontonotes': sorting predictions by 'idx'
09/16 01:51:05 PM: Finished evaluating on: edges-pos-ontonotes
09/16 01:51:05 PM: Task 'edges-pos-ontonotes': joining predictions with input split 'test'
09/16 01:51:14 PM: Task 'edges-pos-ontonotes': Wrote predictions to ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:51:14 PM: Wrote all preds for split 'test' to ./experiments/pos-ontonotes-mrpc-only/run
09/16 01:51:14 PM: Writing results for split 'val' to ./experiments/pos-ontonotes-mrpc-only/results.tsv
09/16 01:51:14 PM: micro_avg: 0.000, macro_avg: 0.876, edges-pos-ontonotes_mcc: 0.874, edges-pos-ontonotes_acc: 0.849, edges-pos-ontonotes_precision: 0.893, edges-pos-ontonotes_recall: 0.860, edges-pos-ontonotes_f1: 0.876
09/16 01:51:15 PM: Done!
09/16 01:51:15 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
