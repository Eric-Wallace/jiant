09/16 12:14:42 PM: Git branch: master
09/16 12:14:42 PM: Git SHA: 93c1dfd555f3458ddbb66d458dfeca984f2d8527
09/16 12:14:42 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-RANDOM-only/",
  "exp_name": "experiments/ner-ontonotes-RANDOM-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-RANDOM-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/RANDOM",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/ner-ontonotes-RANDOM-only__run",
  "run_dir": "./experiments/ner-ontonotes-RANDOM-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 12:14:42 PM: Saved config to ./experiments/ner-ontonotes-RANDOM-only/run/params.conf
09/16 12:14:42 PM: Using random seed 1234
09/16 12:15:22 PM: Using GPU 0
09/16 12:15:22 PM: Loading tasks...
09/16 12:15:22 PM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-RANDOM-only/
09/16 12:15:22 PM: 	Creating task edges-ner-ontonotes from scratch.
09/16 12:15:24 PM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 12:15:24 PM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 12:15:24 PM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 12:15:25 PM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 12:15:25 PM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 12:15:25 PM: 	Building vocab from scratch.
09/16 12:15:25 PM: 	Counting units for task edges-ner-ontonotes.
09/16 12:15:27 PM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 12:15:28 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:15:28 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 12:15:29 PM: 	Saved vocab to ./experiments/ner-ontonotes-RANDOM-only/vocab
09/16 12:15:29 PM: Loading token dictionary from ./experiments/ner-ontonotes-RANDOM-only/vocab.
09/16 12:15:29 PM: 	Loaded vocab from ./experiments/ner-ontonotes-RANDOM-only/vocab
09/16 12:15:29 PM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 12:15:29 PM: 	Vocab namespace tokens: size 22840
09/16 12:15:29 PM: 	Vocab namespace bert_uncased: size 30524
09/16 12:15:29 PM: 	Vocab namespace chars: size 77
09/16 12:15:29 PM: 	Finished building vocab.
09/16 12:15:29 PM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 12:15:43 PM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-RANDOM-only/preproc/edges-ner-ontonotes__train_data
09/16 12:15:43 PM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 12:15:45 PM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-RANDOM-only/preproc/edges-ner-ontonotes__val_data
09/16 12:15:45 PM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 12:15:46 PM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-RANDOM-only/preproc/edges-ner-ontonotes__test_data
09/16 12:15:46 PM: 	Finished indexing tasks
09/16 12:15:46 PM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 12:15:46 PM: 	  Training on 
09/16 12:15:46 PM: 	  Evaluating on edges-ner-ontonotes
09/16 12:15:46 PM: 	Finished loading tasks in 24.188s
09/16 12:15:46 PM: 	 Tasks: ['edges-ner-ontonotes']
09/16 12:15:46 PM: Building model...
09/16 12:15:46 PM: Using BERT model (bert-base-uncased).
09/16 12:15:46 PM: LOADING A RANDOMLY WEIGHTS BERT
09/16 12:15:52 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmppdiheema
09/16 12:15:54 PM: copying /tmp/tmppdiheema to cache at ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:54 PM: creating metadata file for ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:54 PM: removing temp file /tmp/tmppdiheema
09/16 12:15:54 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:54 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 12:15:55 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpgncifgk0
09/16 12:28:05 PM: copying /tmp/tmpgncifgk0 to cache at ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:28:07 PM: creating metadata file for ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:28:07 PM: removing temp file /tmp/tmpgncifgk0
09/16 12:28:07 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:28:17 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpx4j6rtfw
09/16 12:28:22 PM: copying /tmp/tmpx4j6rtfw to cache at ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:28:22 PM: creating metadata file for ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:28:22 PM: removing temp file /tmp/tmpx4j6rtfw
09/16 12:28:22 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-RANDOM-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:28:22 PM: Initializing parameters
09/16 12:28:22 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 12:28:22 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 12:28:22 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 12:28:22 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 12:28:22 PM:    _text_field_embedder.model.pooler.dense.bias
09/16 12:28:22 PM:    _text_field_embedder.model.pooler.dense.weight
09/16 12:28:22 PM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 12:29:00 PM: Model specification:
09/16 12:29:00 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 12:29:00 PM: Model parameters:
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:29:01 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:29:01 PM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 12:29:01 PM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 12:29:01 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 12:29:01 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 12:29:01 PM: Total number of parameters: 109688338 (1.09688e+08)
09/16 12:29:01 PM: Number of trainable parameters: 206098 (206098)
09/16 12:29:01 PM: Finished building model in 794.649s
09/16 12:29:01 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 12:29:09 PM: patience = 9
09/16 12:29:09 PM: val_interval = 1000
09/16 12:29:09 PM: max_vals = 250
09/16 12:29:09 PM: cuda_device = 0
09/16 12:29:09 PM: grad_norm = 5.0
09/16 12:29:09 PM: grad_clipping = None
09/16 12:29:09 PM: lr_decay = 0.99
09/16 12:29:09 PM: min_lr = 1e-06
09/16 12:29:09 PM: keep_all_checkpoints = 0
09/16 12:29:09 PM: val_data_limit = 5000
09/16 12:29:09 PM: max_epochs = -1
09/16 12:29:09 PM: dec_val_scale = 250
09/16 12:29:09 PM: training_data_fraction = 1
09/16 12:29:09 PM: type = adam
09/16 12:29:09 PM: parameter_groups = None
09/16 12:29:09 PM: Number of trainable parameters: 206098
09/16 12:29:09 PM: infer_type_and_cast = True
09/16 12:29:09 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:09 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:09 PM: lr = 0.0001
09/16 12:29:09 PM: amsgrad = True
09/16 12:29:09 PM: type = reduce_on_plateau
09/16 12:29:09 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:09 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:09 PM: mode = max
09/16 12:29:09 PM: factor = 0.5
09/16 12:29:09 PM: patience = 3
09/16 12:29:09 PM: threshold = 0.0001
09/16 12:29:09 PM: threshold_mode = abs
09/16 12:29:09 PM: verbose = True
09/16 12:29:09 PM: type = adam
09/16 12:29:09 PM: parameter_groups = None
09/16 12:29:09 PM: Number of trainable parameters: 206098
09/16 12:29:09 PM: infer_type_and_cast = True
09/16 12:29:09 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:09 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:09 PM: lr = 0.0001
09/16 12:29:09 PM: amsgrad = True
09/16 12:29:09 PM: type = reduce_on_plateau
09/16 12:29:09 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:29:09 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:29:09 PM: mode = max
09/16 12:29:09 PM: factor = 0.5
09/16 12:29:09 PM: patience = 3
09/16 12:29:09 PM: threshold = 0.0001
09/16 12:29:09 PM: threshold_mode = abs
09/16 12:29:09 PM: verbose = True
09/16 12:29:09 PM: Starting training without restoring from a checkpoint.
09/16 12:29:09 PM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 12:29:09 PM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 12:29:19 PM: Update 95: task edges-ner-ontonotes, batch 95 (95): mcc: 0.1218, acc: 0.1066, precision: 0.0959, recall: 0.5587, f1: 0.1637, edges-ner-ontonotes_loss: 0.5932
09/16 12:29:29 PM: Update 214: task edges-ner-ontonotes, batch 214 (214): mcc: 0.1615, acc: 0.1995, precision: 0.1232, recall: 0.5105, f1: 0.1985, edges-ner-ontonotes_loss: 0.4876
09/16 12:29:43 PM: Update 314: task edges-ner-ontonotes, batch 314 (314): mcc: 0.1822, acc: 0.2357, precision: 0.1407, recall: 0.4903, f1: 0.2187, edges-ner-ontonotes_loss: 0.4299
09/16 12:29:53 PM: Update 396: task edges-ner-ontonotes, batch 396 (396): mcc: 0.1838, acc: 0.2415, precision: 0.1457, recall: 0.4699, f1: 0.2224, edges-ner-ontonotes_loss: 0.4098
09/16 12:30:03 PM: Update 466: task edges-ner-ontonotes, batch 466 (466): mcc: 0.1951, acc: 0.2575, precision: 0.1557, recall: 0.4656, f1: 0.2333, edges-ner-ontonotes_loss: 0.3894
09/16 12:30:13 PM: Update 545: task edges-ner-ontonotes, batch 545 (545): mcc: 0.2105, acc: 0.2767, precision: 0.1693, recall: 0.4641, f1: 0.2481, edges-ner-ontonotes_loss: 0.3673
09/16 12:30:23 PM: Update 656: task edges-ner-ontonotes, batch 656 (656): mcc: 0.2286, acc: 0.2985, precision: 0.1856, recall: 0.4660, f1: 0.2655, edges-ner-ontonotes_loss: 0.3442
09/16 12:30:33 PM: Update 747: task edges-ner-ontonotes, batch 747 (747): mcc: 0.2396, acc: 0.3093, precision: 0.1963, recall: 0.4665, f1: 0.2763, edges-ner-ontonotes_loss: 0.3312
09/16 12:30:43 PM: Update 863: task edges-ner-ontonotes, batch 863 (863): mcc: 0.2548, acc: 0.3229, precision: 0.2112, recall: 0.4687, f1: 0.2911, edges-ner-ontonotes_loss: 0.3135
09/16 12:30:53 PM: Update 939: task edges-ner-ontonotes, batch 939 (939): mcc: 0.2669, acc: 0.3335, precision: 0.2233, recall: 0.4716, f1: 0.3030, edges-ner-ontonotes_loss: 0.3021
09/16 12:31:04 PM: Update 959: task edges-ner-ontonotes, batch 959 (959): mcc: 0.2697, acc: 0.3360, precision: 0.2260, recall: 0.4724, f1: 0.3057, edges-ner-ontonotes_loss: 0.2997
09/16 12:31:11 PM: ***** Step 1000 / Validation 1 *****
09/16 12:31:11 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:31:11 PM: Validating...
09/16 12:31:14 PM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.4170, acc: 0.3900, precision: 0.4477, recall: 0.4513, f1: 0.4495, edges-ner-ontonotes_loss: 0.1926
09/16 12:31:24 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.4641, acc: 0.4359, precision: 0.5021, recall: 0.4849, f1: 0.4933, edges-ner-ontonotes_loss: 0.1736
09/16 12:31:30 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:31:31 PM: Best result seen so far for micro.
09/16 12:31:31 PM: Best result seen so far for macro.
09/16 12:31:31 PM: Updating LR scheduler:
09/16 12:31:31 PM: 	Best result seen so far for macro_avg: 0.509
09/16 12:31:31 PM: 	# validation passes without improvement: 0
09/16 12:31:31 PM: edges-ner-ontonotes_loss: training: 0.294418 validation: 0.166540
09/16 12:31:31 PM: macro_avg: validation: 0.509321
09/16 12:31:31 PM: micro_avg: validation: 0.000000
09/16 12:31:31 PM: edges-ner-ontonotes_mcc: training: 0.275094 validation: 0.481654
09/16 12:31:31 PM: edges-ner-ontonotes_acc: training: 0.340502 validation: 0.448135
09/16 12:31:31 PM: edges-ner-ontonotes_precision: training: 0.231728 validation: 0.525526
09/16 12:31:31 PM: edges-ner-ontonotes_recall: training: 0.473104 validation: 0.494086
09/16 12:31:31 PM: edges-ner-ontonotes_f1: training: 0.311086 validation: 0.509321
09/16 12:31:31 PM: Global learning rate: 0.0001
09/16 12:31:31 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:31:34 PM: Update 1024: task edges-ner-ontonotes, batch 24 (1024): mcc: 0.4756, acc: 0.4473, precision: 0.5098, recall: 0.4991, f1: 0.5044, edges-ner-ontonotes_loss: 0.1632
09/16 12:31:44 PM: Update 1097: task edges-ner-ontonotes, batch 97 (1097): mcc: 0.4957, acc: 0.4521, precision: 0.5461, recall: 0.4999, f1: 0.5220, edges-ner-ontonotes_loss: 0.1559
09/16 12:31:54 PM: Update 1172: task edges-ner-ontonotes, batch 172 (1172): mcc: 0.5180, acc: 0.4663, precision: 0.5782, recall: 0.5102, f1: 0.5421, edges-ner-ontonotes_loss: 0.1480
09/16 12:32:04 PM: Update 1251: task edges-ner-ontonotes, batch 251 (1251): mcc: 0.5298, acc: 0.4695, precision: 0.5995, recall: 0.5117, f1: 0.5521, edges-ner-ontonotes_loss: 0.1438
09/16 12:32:15 PM: Update 1283: task edges-ner-ontonotes, batch 283 (1283): mcc: 0.5203, acc: 0.4588, precision: 0.5919, recall: 0.5013, f1: 0.5429, edges-ner-ontonotes_loss: 0.1467
09/16 12:32:25 PM: Update 1368: task edges-ner-ontonotes, batch 368 (1368): mcc: 0.5161, acc: 0.4469, precision: 0.5999, recall: 0.4864, f1: 0.5372, edges-ner-ontonotes_loss: 0.1481
09/16 12:32:35 PM: Update 1437: task edges-ner-ontonotes, batch 437 (1437): mcc: 0.5211, acc: 0.4454, precision: 0.6149, recall: 0.4821, f1: 0.5405, edges-ner-ontonotes_loss: 0.1456
09/16 12:32:45 PM: Update 1528: task edges-ner-ontonotes, batch 528 (1528): mcc: 0.5294, acc: 0.4462, precision: 0.6349, recall: 0.4797, f1: 0.5465, edges-ner-ontonotes_loss: 0.1425
09/16 12:32:55 PM: Update 1584: task edges-ner-ontonotes, batch 584 (1584): mcc: 0.5361, acc: 0.4491, precision: 0.6476, recall: 0.4809, f1: 0.5519, edges-ner-ontonotes_loss: 0.1399
09/16 12:33:05 PM: Update 1691: task edges-ner-ontonotes, batch 691 (1691): mcc: 0.5501, acc: 0.4593, precision: 0.6674, recall: 0.4888, f1: 0.5643, edges-ner-ontonotes_loss: 0.1350
09/16 12:33:15 PM: Update 1782: task edges-ner-ontonotes, batch 782 (1782): mcc: 0.5632, acc: 0.4697, precision: 0.6852, recall: 0.4969, f1: 0.5761, edges-ner-ontonotes_loss: 0.1307
09/16 12:33:25 PM: Update 1856: task edges-ner-ontonotes, batch 856 (1856): mcc: 0.5741, acc: 0.4791, precision: 0.6987, recall: 0.5048, f1: 0.5861, edges-ner-ontonotes_loss: 0.1273
09/16 12:33:35 PM: Update 1888: task edges-ner-ontonotes, batch 888 (1888): mcc: 0.5771, acc: 0.4810, precision: 0.7036, recall: 0.5060, f1: 0.5887, edges-ner-ontonotes_loss: 0.1262
09/16 12:33:44 PM: ***** Step 2000 / Validation 2 *****
09/16 12:33:44 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:33:44 PM: Validating...
09/16 12:33:45 PM: Evaluate: task edges-ner-ontonotes, batch 18 (157): mcc: 0.6124, acc: 0.4955, precision: 0.7881, recall: 0.5018, f1: 0.6132, edges-ner-ontonotes_loss: 0.1158
09/16 12:33:55 PM: Evaluate: task edges-ner-ontonotes, batch 114 (157): mcc: 0.6585, acc: 0.5344, precision: 0.8386, recall: 0.5400, f1: 0.6570, edges-ner-ontonotes_loss: 0.1002
09/16 12:33:58 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:33:58 PM: Best result seen so far for macro.
09/16 12:33:58 PM: Updating LR scheduler:
09/16 12:33:58 PM: 	Best result seen so far for macro_avg: 0.654
09/16 12:33:58 PM: 	# validation passes without improvement: 0
09/16 12:33:58 PM: edges-ner-ontonotes_loss: training: 0.123032 validation: 0.099782
09/16 12:33:58 PM: macro_avg: validation: 0.654109
09/16 12:33:58 PM: micro_avg: validation: 0.000000
09/16 12:33:58 PM: edges-ner-ontonotes_mcc: training: 0.587941 validation: 0.656731
09/16 12:33:58 PM: edges-ner-ontonotes_acc: training: 0.489410 validation: 0.527980
09/16 12:33:58 PM: edges-ner-ontonotes_precision: training: 0.718242 validation: 0.842678
09/16 12:33:58 PM: edges-ner-ontonotes_recall: training: 0.512770 validation: 0.534501
09/16 12:33:58 PM: edges-ner-ontonotes_f1: training: 0.598358 validation: 0.654109
09/16 12:33:58 PM: Global learning rate: 0.0001
09/16 12:33:58 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:34:05 PM: Update 2047: task edges-ner-ontonotes, batch 47 (2047): mcc: 0.6897, acc: 0.5688, precision: 0.8534, recall: 0.5795, f1: 0.6903, edges-ner-ontonotes_loss: 0.0919
09/16 12:34:15 PM: Update 2125: task edges-ner-ontonotes, batch 125 (2125): mcc: 0.7018, acc: 0.5828, precision: 0.8621, recall: 0.5929, f1: 0.7026, edges-ner-ontonotes_loss: 0.0890
09/16 12:34:29 PM: Update 2183: task edges-ner-ontonotes, batch 183 (2183): mcc: 0.7037, acc: 0.5865, precision: 0.8619, recall: 0.5961, f1: 0.7048, edges-ner-ontonotes_loss: 0.0882
09/16 12:34:39 PM: Update 2293: task edges-ner-ontonotes, batch 293 (2293): mcc: 0.6871, acc: 0.5652, precision: 0.8501, recall: 0.5777, f1: 0.6879, edges-ner-ontonotes_loss: 0.0911
09/16 12:34:49 PM: Update 2390: task edges-ner-ontonotes, batch 390 (2390): mcc: 0.6841, acc: 0.5609, precision: 0.8472, recall: 0.5749, f1: 0.6850, edges-ner-ontonotes_loss: 0.0909
09/16 12:35:06 PM: Update 2496: task edges-ner-ontonotes, batch 496 (2496): mcc: 0.6889, acc: 0.5668, precision: 0.8487, recall: 0.5817, f1: 0.6903, edges-ner-ontonotes_loss: 0.0894
09/16 12:35:16 PM: Update 2624: task edges-ner-ontonotes, batch 624 (2624): mcc: 0.6882, acc: 0.5666, precision: 0.8460, recall: 0.5825, f1: 0.6900, edges-ner-ontonotes_loss: 0.0892
09/16 12:35:26 PM: Update 2766: task edges-ner-ontonotes, batch 766 (2766): mcc: 0.6912, acc: 0.5702, precision: 0.8471, recall: 0.5865, f1: 0.6932, edges-ner-ontonotes_loss: 0.0880
09/16 12:35:36 PM: Update 2859: task edges-ner-ontonotes, batch 859 (2859): mcc: 0.6886, acc: 0.5680, precision: 0.8444, recall: 0.5844, f1: 0.6907, edges-ner-ontonotes_loss: 0.0886
09/16 12:35:45 PM: ***** Step 3000 / Validation 3 *****
09/16 12:35:45 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:35:45 PM: Validating...
09/16 12:35:47 PM: Evaluate: task edges-ner-ontonotes, batch 48 (157): mcc: 0.6726, acc: 0.5517, precision: 0.8501, recall: 0.5545, f1: 0.6712, edges-ner-ontonotes_loss: 0.0961
09/16 12:35:53 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:35:53 PM: Best result seen so far for macro.
09/16 12:35:53 PM: Updating LR scheduler:
09/16 12:35:53 PM: 	Best result seen so far for macro_avg: 0.689
09/16 12:35:53 PM: 	# validation passes without improvement: 0
09/16 12:35:53 PM: edges-ner-ontonotes_loss: training: 0.089862 validation: 0.088371
09/16 12:35:53 PM: macro_avg: validation: 0.689122
09/16 12:35:53 PM: micro_avg: validation: 0.000000
09/16 12:35:53 PM: edges-ner-ontonotes_mcc: training: 0.685629 validation: 0.691209
09/16 12:35:53 PM: edges-ner-ontonotes_acc: training: 0.564619 validation: 0.565211
09/16 12:35:53 PM: edges-ner-ontonotes_precision: training: 0.843586 validation: 0.869098
09/16 12:35:53 PM: edges-ner-ontonotes_recall: training: 0.580073 validation: 0.570898
09/16 12:35:53 PM: edges-ner-ontonotes_f1: training: 0.687442 validation: 0.689122
09/16 12:35:53 PM: Global learning rate: 0.0001
09/16 12:35:53 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:35:57 PM: Update 3051: task edges-ner-ontonotes, batch 51 (3051): mcc: 0.6717, acc: 0.5502, precision: 0.8393, recall: 0.5606, f1: 0.6722, edges-ner-ontonotes_loss: 0.0959
09/16 12:36:07 PM: Update 3211: task edges-ner-ontonotes, batch 211 (3211): mcc: 0.6982, acc: 0.5825, precision: 0.8532, recall: 0.5936, f1: 0.7001, edges-ner-ontonotes_loss: 0.0891
09/16 12:36:17 PM: Update 3337: task edges-ner-ontonotes, batch 337 (3337): mcc: 0.7139, acc: 0.6019, precision: 0.8630, recall: 0.6121, f1: 0.7162, edges-ner-ontonotes_loss: 0.0847
09/16 12:36:27 PM: Update 3456: task edges-ner-ontonotes, batch 456 (3456): mcc: 0.7207, acc: 0.6108, precision: 0.8658, recall: 0.6212, f1: 0.7234, edges-ner-ontonotes_loss: 0.0827
09/16 12:36:37 PM: Update 3611: task edges-ner-ontonotes, batch 611 (3611): mcc: 0.7209, acc: 0.6138, precision: 0.8624, recall: 0.6240, f1: 0.7241, edges-ner-ontonotes_loss: 0.0821
09/16 12:36:48 PM: Update 3733: task edges-ner-ontonotes, batch 733 (3733): mcc: 0.7255, acc: 0.6200, precision: 0.8641, recall: 0.6304, f1: 0.7290, edges-ner-ontonotes_loss: 0.0809
09/16 12:36:58 PM: Update 3777: task edges-ner-ontonotes, batch 777 (3777): mcc: 0.7233, acc: 0.6173, precision: 0.8621, recall: 0.6284, f1: 0.7269, edges-ner-ontonotes_loss: 0.0811
09/16 12:37:08 PM: Update 3972: task edges-ner-ontonotes, batch 972 (3972): mcc: 0.7219, acc: 0.6142, precision: 0.8604, recall: 0.6273, f1: 0.7255, edges-ner-ontonotes_loss: 0.0804
09/16 12:37:09 PM: ***** Step 4000 / Validation 4 *****
09/16 12:37:09 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:37:09 PM: Validating...
09/16 12:37:18 PM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.6951, acc: 0.5980, precision: 0.8251, recall: 0.6097, f1: 0.7012, edges-ner-ontonotes_loss: 0.0875
09/16 12:37:20 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:37:20 PM: Best result seen so far for macro.
09/16 12:37:20 PM: Updating LR scheduler:
09/16 12:37:20 PM: 	Best result seen so far for macro_avg: 0.713
09/16 12:37:20 PM: 	# validation passes without improvement: 0
09/16 12:37:20 PM: edges-ner-ontonotes_loss: training: 0.080280 validation: 0.083919
09/16 12:37:20 PM: macro_avg: validation: 0.712659
09/16 12:37:20 PM: micro_avg: validation: 0.000000
09/16 12:37:20 PM: edges-ner-ontonotes_mcc: training: 0.722293 validation: 0.707365
09/16 12:37:20 PM: edges-ner-ontonotes_acc: training: 0.614734 validation: 0.607749
09/16 12:37:20 PM: edges-ner-ontonotes_precision: training: 0.860204 validation: 0.839346
09/16 12:37:20 PM: edges-ner-ontonotes_recall: training: 0.628071 validation: 0.619199
09/16 12:37:20 PM: edges-ner-ontonotes_f1: training: 0.726034 validation: 0.712659
09/16 12:37:20 PM: Global learning rate: 0.0001
09/16 12:37:20 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:37:31 PM: Update 4052: task edges-ner-ontonotes, batch 52 (4052): mcc: 0.7219, acc: 0.6143, precision: 0.8442, recall: 0.6399, f1: 0.7280, edges-ner-ontonotes_loss: 0.0764
09/16 12:37:41 PM: Update 4166: task edges-ner-ontonotes, batch 166 (4166): mcc: 0.7239, acc: 0.6186, precision: 0.8471, recall: 0.6409, f1: 0.7297, edges-ner-ontonotes_loss: 0.0755
09/16 12:37:51 PM: Update 4269: task edges-ner-ontonotes, batch 269 (4269): mcc: 0.7238, acc: 0.6177, precision: 0.8473, recall: 0.6407, f1: 0.7296, edges-ner-ontonotes_loss: 0.0760
09/16 12:38:01 PM: Update 4381: task edges-ner-ontonotes, batch 381 (4381): mcc: 0.7232, acc: 0.6174, precision: 0.8469, recall: 0.6401, f1: 0.7291, edges-ner-ontonotes_loss: 0.0767
09/16 12:38:11 PM: Update 4491: task edges-ner-ontonotes, batch 491 (4491): mcc: 0.7147, acc: 0.6083, precision: 0.8420, recall: 0.6295, f1: 0.7204, edges-ner-ontonotes_loss: 0.0801
09/16 12:38:21 PM: Update 4585: task edges-ner-ontonotes, batch 585 (4585): mcc: 0.7110, acc: 0.6040, precision: 0.8411, recall: 0.6240, f1: 0.7165, edges-ner-ontonotes_loss: 0.0819
09/16 12:38:31 PM: Update 4689: task edges-ner-ontonotes, batch 689 (4689): mcc: 0.7106, acc: 0.6040, precision: 0.8414, recall: 0.6231, f1: 0.7160, edges-ner-ontonotes_loss: 0.0824
09/16 12:38:41 PM: Update 4813: task edges-ner-ontonotes, batch 813 (4813): mcc: 0.7154, acc: 0.6098, precision: 0.8452, recall: 0.6282, f1: 0.7207, edges-ner-ontonotes_loss: 0.0811
09/16 12:38:51 PM: Update 4951: task edges-ner-ontonotes, batch 951 (4951): mcc: 0.7222, acc: 0.6183, precision: 0.8502, recall: 0.6357, f1: 0.7274, edges-ner-ontonotes_loss: 0.0796
09/16 12:39:00 PM: ***** Step 5000 / Validation 5 *****
09/16 12:39:00 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:39:00 PM: Validating...
09/16 12:39:01 PM: Evaluate: task edges-ner-ontonotes, batch 10 (157): mcc: 0.6510, acc: 0.5606, precision: 0.7891, recall: 0.5637, f1: 0.6576, edges-ner-ontonotes_loss: 0.0999
09/16 12:39:11 PM: Evaluate: task edges-ner-ontonotes, batch 121 (157): mcc: 0.7066, acc: 0.6006, precision: 0.8550, recall: 0.6060, f1: 0.7093, edges-ner-ontonotes_loss: 0.0830
09/16 12:39:14 PM: Updating LR scheduler:
09/16 12:39:14 PM: 	Best result seen so far for macro_avg: 0.713
09/16 12:39:14 PM: 	# validation passes without improvement: 1
09/16 12:39:14 PM: edges-ner-ontonotes_loss: training: 0.079338 validation: 0.082215
09/16 12:39:14 PM: macro_avg: validation: 0.708268
09/16 12:39:14 PM: micro_avg: validation: 0.000000
09/16 12:39:14 PM: edges-ner-ontonotes_mcc: training: 0.723080 validation: 0.706339
09/16 12:39:14 PM: edges-ner-ontonotes_acc: training: 0.619425 validation: 0.597589
09/16 12:39:14 PM: edges-ner-ontonotes_precision: training: 0.851035 validation: 0.859198
09/16 12:39:14 PM: edges-ner-ontonotes_recall: training: 0.636513 validation: 0.602442
09/16 12:39:14 PM: edges-ner-ontonotes_f1: training: 0.728306 validation: 0.708268
09/16 12:39:14 PM: Global learning rate: 0.0001
09/16 12:39:14 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:39:22 PM: Update 5129: task edges-ner-ontonotes, batch 129 (5129): mcc: 0.7271, acc: 0.6300, precision: 0.8469, recall: 0.6466, f1: 0.7333, edges-ner-ontonotes_loss: 0.0774
09/16 12:39:32 PM: Update 5253: task edges-ner-ontonotes, batch 253 (5253): mcc: 0.7430, acc: 0.6485, precision: 0.8594, recall: 0.6637, f1: 0.7490, edges-ner-ontonotes_loss: 0.0740
09/16 12:39:42 PM: Update 5349: task edges-ner-ontonotes, batch 349 (5349): mcc: 0.7384, acc: 0.6420, precision: 0.8570, recall: 0.6577, f1: 0.7442, edges-ner-ontonotes_loss: 0.0748
09/16 12:39:52 PM: Update 5461: task edges-ner-ontonotes, batch 461 (5461): mcc: 0.7363, acc: 0.6378, precision: 0.8562, recall: 0.6547, f1: 0.7421, edges-ner-ontonotes_loss: 0.0749
09/16 12:40:02 PM: Update 5581: task edges-ner-ontonotes, batch 581 (5581): mcc: 0.7375, acc: 0.6385, precision: 0.8560, recall: 0.6570, f1: 0.7434, edges-ner-ontonotes_loss: 0.0741
09/16 12:40:12 PM: Update 5662: task edges-ner-ontonotes, batch 662 (5662): mcc: 0.7355, acc: 0.6365, precision: 0.8529, recall: 0.6561, f1: 0.7416, edges-ner-ontonotes_loss: 0.0741
09/16 12:40:22 PM: Update 5767: task edges-ner-ontonotes, batch 767 (5767): mcc: 0.7362, acc: 0.6366, precision: 0.8523, recall: 0.6577, f1: 0.7425, edges-ner-ontonotes_loss: 0.0738
09/16 12:40:32 PM: Update 5881: task edges-ner-ontonotes, batch 881 (5881): mcc: 0.7375, acc: 0.6378, precision: 0.8527, recall: 0.6596, f1: 0.7438, edges-ner-ontonotes_loss: 0.0735
09/16 12:40:41 PM: ***** Step 6000 / Validation 6 *****
09/16 12:40:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:40:41 PM: Validating...
09/16 12:40:42 PM: Evaluate: task edges-ner-ontonotes, batch 25 (157): mcc: 0.6305, acc: 0.5416, precision: 0.7620, recall: 0.5502, f1: 0.6390, edges-ner-ontonotes_loss: 0.1038
09/16 12:40:49 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:40:49 PM: Best result seen so far for macro.
09/16 12:40:49 PM: Updating LR scheduler:
09/16 12:40:49 PM: 	Best result seen so far for macro_avg: 0.729
09/16 12:40:49 PM: 	# validation passes without improvement: 0
09/16 12:40:49 PM: edges-ner-ontonotes_loss: training: 0.074808 validation: 0.079796
09/16 12:40:49 PM: macro_avg: validation: 0.728570
09/16 12:40:49 PM: micro_avg: validation: 0.000000
09/16 12:40:49 PM: edges-ner-ontonotes_mcc: training: 0.733707 validation: 0.724730
09/16 12:40:49 PM: edges-ner-ontonotes_acc: training: 0.633602 validation: 0.623294
09/16 12:40:49 PM: edges-ner-ontonotes_precision: training: 0.850285 validation: 0.861355
09/16 12:40:49 PM: edges-ner-ontonotes_recall: training: 0.655159 validation: 0.631256
09/16 12:40:49 PM: edges-ner-ontonotes_f1: training: 0.740077 validation: 0.728570
09/16 12:40:49 PM: Global learning rate: 0.0001
09/16 12:40:49 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:40:52 PM: Update 6048: task edges-ner-ontonotes, batch 48 (6048): mcc: 0.6935, acc: 0.5901, precision: 0.8341, recall: 0.6002, f1: 0.6980, edges-ner-ontonotes_loss: 0.0871
09/16 12:41:06 PM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.7036, acc: 0.6024, precision: 0.8358, recall: 0.6157, f1: 0.7090, edges-ner-ontonotes_loss: 0.0862
09/16 12:41:16 PM: Update 6507: task edges-ner-ontonotes, batch 507 (6507): mcc: 0.7340, acc: 0.6367, precision: 0.8565, recall: 0.6508, f1: 0.7396, edges-ner-ontonotes_loss: 0.0773
09/16 12:41:26 PM: Update 6570: task edges-ner-ontonotes, batch 570 (6570): mcc: 0.7358, acc: 0.6385, precision: 0.8582, recall: 0.6523, f1: 0.7412, edges-ner-ontonotes_loss: 0.0765
09/16 12:41:36 PM: Update 6711: task edges-ner-ontonotes, batch 711 (6711): mcc: 0.7394, acc: 0.6436, precision: 0.8589, recall: 0.6579, f1: 0.7451, edges-ner-ontonotes_loss: 0.0756
09/16 12:41:46 PM: Update 6842: task edges-ner-ontonotes, batch 842 (6842): mcc: 0.7436, acc: 0.6492, precision: 0.8605, recall: 0.6637, f1: 0.7494, edges-ner-ontonotes_loss: 0.0747
09/16 12:41:56 PM: Update 6897: task edges-ner-ontonotes, batch 897 (6897): mcc: 0.7419, acc: 0.6471, precision: 0.8591, recall: 0.6620, f1: 0.7478, edges-ner-ontonotes_loss: 0.0748
09/16 12:42:05 PM: ***** Step 7000 / Validation 7 *****
09/16 12:42:05 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:42:05 PM: Validating...
09/16 12:42:06 PM: Evaluate: task edges-ner-ontonotes, batch 14 (157): mcc: 0.6373, acc: 0.5630, precision: 0.7561, recall: 0.5664, f1: 0.6476, edges-ner-ontonotes_loss: 0.1036
09/16 12:42:14 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:42:14 PM: Best result seen so far for macro.
09/16 12:42:14 PM: Updating LR scheduler:
09/16 12:42:14 PM: 	Best result seen so far for macro_avg: 0.733
09/16 12:42:14 PM: 	# validation passes without improvement: 0
09/16 12:42:14 PM: edges-ner-ontonotes_loss: training: 0.074630 validation: 0.078482
09/16 12:42:14 PM: macro_avg: validation: 0.733376
09/16 12:42:14 PM: micro_avg: validation: 0.000000
09/16 12:42:14 PM: edges-ner-ontonotes_mcc: training: 0.740596 validation: 0.726924
09/16 12:42:14 PM: edges-ner-ontonotes_acc: training: 0.644921 validation: 0.633076
09/16 12:42:14 PM: edges-ner-ontonotes_precision: training: 0.857913 validation: 0.845157
09/16 12:42:14 PM: edges-ner-ontonotes_recall: training: 0.660776 validation: 0.647710
09/16 12:42:14 PM: edges-ner-ontonotes_f1: training: 0.746550 validation: 0.733376
09/16 12:42:14 PM: Global learning rate: 0.0001
09/16 12:42:14 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:42:16 PM: Update 7038: task edges-ner-ontonotes, batch 38 (7038): mcc: 0.7336, acc: 0.6329, precision: 0.8491, recall: 0.6560, f1: 0.7402, edges-ner-ontonotes_loss: 0.0734
09/16 12:42:27 PM: Update 7164: task edges-ner-ontonotes, batch 164 (7164): mcc: 0.7458, acc: 0.6483, precision: 0.8538, recall: 0.6731, f1: 0.7527, edges-ner-ontonotes_loss: 0.0700
09/16 12:42:37 PM: Update 7285: task edges-ner-ontonotes, batch 285 (7285): mcc: 0.7431, acc: 0.6467, precision: 0.8500, recall: 0.6715, f1: 0.7503, edges-ner-ontonotes_loss: 0.0703
09/16 12:42:47 PM: Update 7466: task edges-ner-ontonotes, batch 466 (7466): mcc: 0.7451, acc: 0.6478, precision: 0.8514, recall: 0.6738, f1: 0.7522, edges-ner-ontonotes_loss: 0.0700
09/16 12:42:57 PM: Update 7510: task edges-ner-ontonotes, batch 510 (7510): mcc: 0.7413, acc: 0.6440, precision: 0.8482, recall: 0.6699, f1: 0.7486, edges-ner-ontonotes_loss: 0.0715
09/16 12:43:07 PM: Update 7618: task edges-ner-ontonotes, batch 618 (7618): mcc: 0.7350, acc: 0.6371, precision: 0.8449, recall: 0.6618, f1: 0.7422, edges-ner-ontonotes_loss: 0.0740
09/16 12:43:17 PM: Update 7741: task edges-ner-ontonotes, batch 741 (7741): mcc: 0.7323, acc: 0.6341, precision: 0.8444, recall: 0.6575, f1: 0.7393, edges-ner-ontonotes_loss: 0.0757
09/16 12:43:27 PM: Update 7786: task edges-ner-ontonotes, batch 786 (7786): mcc: 0.7316, acc: 0.6333, precision: 0.8440, recall: 0.6566, f1: 0.7386, edges-ner-ontonotes_loss: 0.0760
09/16 12:43:38 PM: Update 7912: task edges-ner-ontonotes, batch 912 (7912): mcc: 0.7346, acc: 0.6373, precision: 0.8468, recall: 0.6595, f1: 0.7415, edges-ner-ontonotes_loss: 0.0754
09/16 12:43:44 PM: ***** Step 8000 / Validation 8 *****
09/16 12:43:44 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:43:44 PM: Validating...
09/16 12:43:48 PM: Evaluate: task edges-ner-ontonotes, batch 51 (157): mcc: 0.7306, acc: 0.6426, precision: 0.8561, recall: 0.6452, f1: 0.7359, edges-ner-ontonotes_loss: 0.0778
09/16 12:43:58 PM: Evaluate: task edges-ner-ontonotes, batch 148 (157): mcc: 0.7165, acc: 0.6158, precision: 0.8570, recall: 0.6208, f1: 0.7200, edges-ner-ontonotes_loss: 0.0791
09/16 12:43:58 PM: Updating LR scheduler:
09/16 12:43:58 PM: 	Best result seen so far for macro_avg: 0.733
09/16 12:43:58 PM: 	# validation passes without improvement: 1
09/16 12:43:58 PM: edges-ner-ontonotes_loss: training: 0.074370 validation: 0.079101
09/16 12:43:58 PM: macro_avg: validation: 0.718256
09/16 12:43:58 PM: micro_avg: validation: 0.000000
09/16 12:43:58 PM: edges-ner-ontonotes_mcc: training: 0.738202 validation: 0.714847
09/16 12:43:58 PM: edges-ner-ontonotes_acc: training: 0.641597 validation: 0.613436
09/16 12:43:58 PM: edges-ner-ontonotes_precision: training: 0.849935 validation: 0.856783
09/16 12:43:58 PM: edges-ner-ontonotes_recall: training: 0.663150 validation: 0.618289
09/16 12:43:58 PM: edges-ner-ontonotes_f1: training: 0.745013 validation: 0.718256
09/16 12:43:58 PM: Global learning rate: 0.0001
09/16 12:43:58 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:44:08 PM: Update 8096: task edges-ner-ontonotes, batch 96 (8096): mcc: 0.7812, acc: 0.6978, precision: 0.8794, recall: 0.7130, f1: 0.7875, edges-ner-ontonotes_loss: 0.0643
09/16 12:44:18 PM: Update 8208: task edges-ner-ontonotes, batch 208 (8208): mcc: 0.7619, acc: 0.6734, precision: 0.8660, recall: 0.6908, f1: 0.7685, edges-ner-ontonotes_loss: 0.0680
09/16 12:44:28 PM: Update 8319: task edges-ner-ontonotes, batch 319 (8319): mcc: 0.7583, acc: 0.6706, precision: 0.8607, recall: 0.6890, f1: 0.7653, edges-ner-ontonotes_loss: 0.0694
09/16 12:44:41 PM: Update 8407: task edges-ner-ontonotes, batch 407 (8407): mcc: 0.7610, acc: 0.6744, precision: 0.8621, recall: 0.6925, f1: 0.7680, edges-ner-ontonotes_loss: 0.0690
09/16 12:44:51 PM: Update 8533: task edges-ner-ontonotes, batch 533 (8533): mcc: 0.7527, acc: 0.6628, precision: 0.8566, recall: 0.6827, f1: 0.7598, edges-ner-ontonotes_loss: 0.0702
09/16 12:45:01 PM: Update 8645: task edges-ner-ontonotes, batch 645 (8645): mcc: 0.7520, acc: 0.6604, precision: 0.8559, recall: 0.6821, f1: 0.7592, edges-ner-ontonotes_loss: 0.0699
09/16 12:45:11 PM: Update 8724: task edges-ner-ontonotes, batch 724 (8724): mcc: 0.7521, acc: 0.6602, precision: 0.8561, recall: 0.6821, f1: 0.7593, edges-ner-ontonotes_loss: 0.0698
09/16 12:45:21 PM: Update 8850: task edges-ner-ontonotes, batch 850 (8850): mcc: 0.7511, acc: 0.6584, precision: 0.8546, recall: 0.6816, f1: 0.7584, edges-ner-ontonotes_loss: 0.0699
09/16 12:45:31 PM: Update 8976: task edges-ner-ontonotes, batch 976 (8976): mcc: 0.7516, acc: 0.6588, precision: 0.8544, recall: 0.6826, f1: 0.7589, edges-ner-ontonotes_loss: 0.0697
09/16 12:45:33 PM: ***** Step 9000 / Validation 9 *****
09/16 12:45:33 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:45:33 PM: Validating...
09/16 12:45:41 PM: Evaluate: task edges-ner-ontonotes, batch 107 (157): mcc: 0.7181, acc: 0.6333, precision: 0.8302, recall: 0.6448, f1: 0.7258, edges-ner-ontonotes_loss: 0.0835
09/16 12:45:46 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:45:46 PM: Best result seen so far for macro.
09/16 12:45:46 PM: Updating LR scheduler:
09/16 12:45:46 PM: 	Best result seen so far for macro_avg: 0.740
09/16 12:45:46 PM: 	# validation passes without improvement: 0
09/16 12:45:46 PM: edges-ner-ontonotes_loss: training: 0.069742 validation: 0.077990
09/16 12:45:46 PM: macro_avg: validation: 0.740495
09/16 12:45:46 PM: micro_avg: validation: 0.000000
09/16 12:45:46 PM: edges-ner-ontonotes_mcc: training: 0.751631 validation: 0.733063
09/16 12:45:46 PM: edges-ner-ontonotes_acc: training: 0.658741 validation: 0.644904
09/16 12:45:46 PM: edges-ner-ontonotes_precision: training: 0.854514 validation: 0.841882
09/16 12:45:46 PM: edges-ner-ontonotes_recall: training: 0.682564 validation: 0.660904
09/16 12:45:46 PM: edges-ner-ontonotes_f1: training: 0.758921 validation: 0.740495
09/16 12:45:46 PM: Global learning rate: 0.0001
09/16 12:45:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:45:54 PM: Update 9033: task edges-ner-ontonotes, batch 33 (9033): mcc: 0.7553, acc: 0.6636, precision: 0.8469, recall: 0.6955, f1: 0.7638, edges-ner-ontonotes_loss: 0.0678
09/16 12:46:04 PM: Update 9202: task edges-ner-ontonotes, batch 202 (9202): mcc: 0.7183, acc: 0.6208, precision: 0.8303, recall: 0.6450, f1: 0.7260, edges-ner-ontonotes_loss: 0.0817
09/16 12:46:14 PM: Update 9342: task edges-ner-ontonotes, batch 342 (9342): mcc: 0.7188, acc: 0.6220, precision: 0.8330, recall: 0.6436, f1: 0.7262, edges-ner-ontonotes_loss: 0.0821
09/16 12:46:24 PM: Update 9531: task edges-ner-ontonotes, batch 531 (9531): mcc: 0.7350, acc: 0.6411, precision: 0.8463, recall: 0.6606, f1: 0.7420, edges-ner-ontonotes_loss: 0.0767
09/16 12:46:34 PM: Update 9678: task edges-ner-ontonotes, batch 678 (9678): mcc: 0.7429, acc: 0.6512, precision: 0.8515, recall: 0.6700, f1: 0.7499, edges-ner-ontonotes_loss: 0.0745
09/16 12:46:44 PM: Update 9816: task edges-ner-ontonotes, batch 816 (9816): mcc: 0.7436, acc: 0.6522, precision: 0.8514, recall: 0.6712, f1: 0.7506, edges-ner-ontonotes_loss: 0.0740
09/16 12:46:54 PM: Update 9943: task edges-ner-ontonotes, batch 943 (9943): mcc: 0.7485, acc: 0.6586, precision: 0.8543, recall: 0.6772, f1: 0.7555, edges-ner-ontonotes_loss: 0.0729
09/16 12:47:04 PM: ***** Step 10000 / Validation 10 *****
09/16 12:47:04 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:47:04 PM: Validating...
09/16 12:47:04 PM: Evaluate: task edges-ner-ontonotes, batch 4 (157): mcc: 0.5799, acc: 0.5145, precision: 0.6942, recall: 0.5181, f1: 0.5934, edges-ner-ontonotes_loss: 0.1252
09/16 12:47:14 PM: Evaluate: task edges-ner-ontonotes, batch 134 (157): mcc: 0.7374, acc: 0.6507, precision: 0.8528, recall: 0.6595, f1: 0.7438, edges-ner-ontonotes_loss: 0.0768
09/16 12:47:15 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:47:15 PM: Best result seen so far for macro.
09/16 12:47:15 PM: Updating LR scheduler:
09/16 12:47:15 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:47:15 PM: 	# validation passes without improvement: 0
09/16 12:47:15 PM: edges-ner-ontonotes_loss: training: 0.072759 validation: 0.076398
09/16 12:47:15 PM: macro_avg: validation: 0.743131
09/16 12:47:15 PM: micro_avg: validation: 0.000000
09/16 12:47:15 PM: edges-ner-ontonotes_mcc: training: 0.748174 validation: 0.737057
09/16 12:47:15 PM: edges-ner-ontonotes_acc: training: 0.657717 validation: 0.648544
09/16 12:47:15 PM: edges-ner-ontonotes_precision: training: 0.854219 validation: 0.854678
09/16 12:47:15 PM: edges-ner-ontonotes_recall: training: 0.676820 validation: 0.657340
09/16 12:47:15 PM: edges-ner-ontonotes_f1: training: 0.755242 validation: 0.743131
09/16 12:47:15 PM: Global learning rate: 0.0001
09/16 12:47:15 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:47:24 PM: Update 10124: task edges-ner-ontonotes, batch 124 (10124): mcc: 0.7401, acc: 0.6426, precision: 0.8458, recall: 0.6698, f1: 0.7476, edges-ner-ontonotes_loss: 0.0715
09/16 12:47:39 PM: Update 10276: task edges-ner-ontonotes, batch 276 (10276): mcc: 0.7468, acc: 0.6504, precision: 0.8501, recall: 0.6778, f1: 0.7542, edges-ner-ontonotes_loss: 0.0696
09/16 12:47:49 PM: Update 10399: task edges-ner-ontonotes, batch 399 (10399): mcc: 0.7474, acc: 0.6516, precision: 0.8494, recall: 0.6795, f1: 0.7550, edges-ner-ontonotes_loss: 0.0692
09/16 12:47:59 PM: Update 10506: task edges-ner-ontonotes, batch 506 (10506): mcc: 0.7487, acc: 0.6528, precision: 0.8493, recall: 0.6818, f1: 0.7564, edges-ner-ontonotes_loss: 0.0692
09/16 12:48:10 PM: Update 10589: task edges-ner-ontonotes, batch 589 (10589): mcc: 0.7502, acc: 0.6543, precision: 0.8509, recall: 0.6832, f1: 0.7579, edges-ner-ontonotes_loss: 0.0688
09/16 12:48:20 PM: Update 10685: task edges-ner-ontonotes, batch 685 (10685): mcc: 0.7427, acc: 0.6464, precision: 0.8453, recall: 0.6748, f1: 0.7505, edges-ner-ontonotes_loss: 0.0714
09/16 12:48:30 PM: Update 10804: task edges-ner-ontonotes, batch 804 (10804): mcc: 0.7402, acc: 0.6439, precision: 0.8442, recall: 0.6713, f1: 0.7479, edges-ner-ontonotes_loss: 0.0729
09/16 12:48:43 PM: Update 10893: task edges-ner-ontonotes, batch 893 (10893): mcc: 0.7388, acc: 0.6430, precision: 0.8437, recall: 0.6693, f1: 0.7464, edges-ner-ontonotes_loss: 0.0737
09/16 12:48:49 PM: ***** Step 11000 / Validation 11 *****
09/16 12:48:49 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:48:49 PM: Validating...
09/16 12:48:53 PM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.7455, acc: 0.6575, precision: 0.8639, recall: 0.6643, f1: 0.7511, edges-ner-ontonotes_loss: 0.0766
09/16 12:48:59 PM: Updating LR scheduler:
09/16 12:48:59 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:48:59 PM: 	# validation passes without improvement: 1
09/16 12:48:59 PM: edges-ner-ontonotes_loss: training: 0.073318 validation: 0.076993
09/16 12:48:59 PM: macro_avg: validation: 0.733048
09/16 12:48:59 PM: micro_avg: validation: 0.000000
09/16 12:48:59 PM: edges-ner-ontonotes_mcc: training: 0.741206 validation: 0.729242
09/16 12:48:59 PM: edges-ner-ontonotes_acc: training: 0.646045 validation: 0.630497
09/16 12:48:59 PM: edges-ner-ontonotes_precision: training: 0.846131 validation: 0.864845
09/16 12:48:59 PM: edges-ner-ontonotes_recall: training: 0.671466 validation: 0.636109
09/16 12:48:59 PM: edges-ner-ontonotes_f1: training: 0.748747 validation: 0.733048
09/16 12:48:59 PM: Global learning rate: 0.0001
09/16 12:48:59 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:49:03 PM: Update 11061: task edges-ner-ontonotes, batch 61 (11061): mcc: 0.7831, acc: 0.6943, precision: 0.8847, recall: 0.7121, f1: 0.7890, edges-ner-ontonotes_loss: 0.0642
09/16 12:49:14 PM: Update 11184: task edges-ner-ontonotes, batch 184 (11184): mcc: 0.7850, acc: 0.7001, precision: 0.8809, recall: 0.7185, f1: 0.7914, edges-ner-ontonotes_loss: 0.0633
09/16 12:49:24 PM: Update 11248: task edges-ner-ontonotes, batch 248 (11248): mcc: 0.7761, acc: 0.6900, precision: 0.8737, recall: 0.7090, f1: 0.7828, edges-ner-ontonotes_loss: 0.0644
09/16 12:49:34 PM: Update 11382: task edges-ner-ontonotes, batch 382 (11382): mcc: 0.7643, acc: 0.6780, precision: 0.8641, recall: 0.6966, f1: 0.7714, edges-ner-ontonotes_loss: 0.0671
09/16 12:49:45 PM: Update 11519: task edges-ner-ontonotes, batch 519 (11519): mcc: 0.7673, acc: 0.6810, precision: 0.8660, recall: 0.7001, f1: 0.7743, edges-ner-ontonotes_loss: 0.0665
09/16 12:49:55 PM: Update 11661: task edges-ner-ontonotes, batch 661 (11661): mcc: 0.7599, acc: 0.6706, precision: 0.8610, recall: 0.6916, f1: 0.7670, edges-ner-ontonotes_loss: 0.0677
09/16 12:50:05 PM: Update 11794: task edges-ner-ontonotes, batch 794 (11794): mcc: 0.7592, acc: 0.6690, precision: 0.8595, recall: 0.6915, f1: 0.7664, edges-ner-ontonotes_loss: 0.0676
09/16 12:50:15 PM: Update 11931: task edges-ner-ontonotes, batch 931 (11931): mcc: 0.7575, acc: 0.6666, precision: 0.8575, recall: 0.6902, f1: 0.7648, edges-ner-ontonotes_loss: 0.0677
09/16 12:50:23 PM: ***** Step 12000 / Validation 12 *****
09/16 12:50:23 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:50:23 PM: Validating...
09/16 12:50:25 PM: Evaluate: task edges-ner-ontonotes, batch 39 (157): mcc: 0.7024, acc: 0.6201, precision: 0.8123, recall: 0.6324, f1: 0.7112, edges-ner-ontonotes_loss: 0.0880
09/16 12:50:35 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:50:35 PM: Best result seen so far for macro.
09/16 12:50:35 PM: Updating LR scheduler:
09/16 12:50:35 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:50:35 PM: 	# validation passes without improvement: 2
09/16 12:50:35 PM: edges-ner-ontonotes_loss: training: 0.067611 validation: 0.076713
09/16 12:50:35 PM: macro_avg: validation: 0.743172
09/16 12:50:35 PM: micro_avg: validation: 0.000000
09/16 12:50:35 PM: edges-ner-ontonotes_mcc: training: 0.757539 validation: 0.735502
09/16 12:50:35 PM: edges-ner-ontonotes_acc: training: 0.666849 validation: 0.648923
09/16 12:50:35 PM: edges-ner-ontonotes_precision: training: 0.856719 validation: 0.841565
09/16 12:50:35 PM: edges-ner-ontonotes_recall: training: 0.690991 validation: 0.665378
09/16 12:50:35 PM: edges-ner-ontonotes_f1: training: 0.764982 validation: 0.743172
09/16 12:50:35 PM: Global learning rate: 0.0001
09/16 12:50:35 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:50:35 PM: Update 12001: task edges-ner-ontonotes, batch 1 (12001): mcc: 0.7468, acc: 0.6484, precision: 0.8592, recall: 0.6703, f1: 0.7531, edges-ner-ontonotes_loss: 0.0683
09/16 12:50:45 PM: Update 12121: task edges-ner-ontonotes, batch 121 (12121): mcc: 0.7583, acc: 0.6678, precision: 0.8502, recall: 0.6978, f1: 0.7665, edges-ner-ontonotes_loss: 0.0659
09/16 12:50:55 PM: Update 12211: task edges-ner-ontonotes, batch 211 (12211): mcc: 0.7382, acc: 0.6453, precision: 0.8374, recall: 0.6735, f1: 0.7465, edges-ner-ontonotes_loss: 0.0734
09/16 12:51:05 PM: Update 12411: task edges-ner-ontonotes, batch 411 (12411): mcc: 0.7312, acc: 0.6378, precision: 0.8364, recall: 0.6623, f1: 0.7392, edges-ner-ontonotes_loss: 0.0770
09/16 12:51:15 PM: Update 12565: task edges-ner-ontonotes, batch 565 (12565): mcc: 0.7371, acc: 0.6446, precision: 0.8419, recall: 0.6680, f1: 0.7449, edges-ner-ontonotes_loss: 0.0752
09/16 12:51:25 PM: Update 12748: task edges-ner-ontonotes, batch 748 (12748): mcc: 0.7491, acc: 0.6583, precision: 0.8514, recall: 0.6807, f1: 0.7565, edges-ner-ontonotes_loss: 0.0721
09/16 12:51:35 PM: Update 12804: task edges-ner-ontonotes, batch 804 (12804): mcc: 0.7491, acc: 0.6584, precision: 0.8518, recall: 0.6805, f1: 0.7566, edges-ner-ontonotes_loss: 0.0720
09/16 12:51:45 PM: Update 12914: task edges-ner-ontonotes, batch 914 (12914): mcc: 0.7498, acc: 0.6596, precision: 0.8518, recall: 0.6817, f1: 0.7573, edges-ner-ontonotes_loss: 0.0718
09/16 12:51:54 PM: ***** Step 13000 / Validation 13 *****
09/16 12:51:54 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:51:54 PM: Validating...
09/16 12:51:55 PM: Evaluate: task edges-ner-ontonotes, batch 25 (157): mcc: 0.6773, acc: 0.5915, precision: 0.7991, recall: 0.6000, f1: 0.6854, edges-ner-ontonotes_loss: 0.0946
09/16 12:52:05 PM: Updating LR scheduler:
09/16 12:52:05 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:52:05 PM: 	# validation passes without improvement: 3
09/16 12:52:05 PM: edges-ner-ontonotes_loss: training: 0.071305 validation: 0.075564
09/16 12:52:05 PM: macro_avg: validation: 0.741586
09/16 12:52:05 PM: micro_avg: validation: 0.000000
09/16 12:52:05 PM: edges-ner-ontonotes_mcc: training: 0.751482 validation: 0.736008
09/16 12:52:05 PM: edges-ner-ontonotes_acc: training: 0.661732 validation: 0.645511
09/16 12:52:05 PM: edges-ner-ontonotes_precision: training: 0.852959 validation: 0.857555
09/16 12:52:05 PM: edges-ner-ontonotes_recall: training: 0.683605 validation: 0.653245
09/16 12:52:05 PM: edges-ner-ontonotes_f1: training: 0.758949 validation: 0.741586
09/16 12:52:05 PM: Global learning rate: 0.0001
09/16 12:52:05 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:52:05 PM: Update 13015: task edges-ner-ontonotes, batch 15 (13015): mcc: 0.7794, acc: 0.6920, precision: 0.8770, recall: 0.7120, f1: 0.7859, edges-ner-ontonotes_loss: 0.0639
09/16 12:52:16 PM: Update 13075: task edges-ner-ontonotes, batch 75 (13075): mcc: 0.7751, acc: 0.6914, precision: 0.8681, recall: 0.7121, f1: 0.7824, edges-ner-ontonotes_loss: 0.0655
09/16 12:52:26 PM: Update 13193: task edges-ner-ontonotes, batch 193 (13193): mcc: 0.7544, acc: 0.6624, precision: 0.8544, recall: 0.6875, f1: 0.7619, edges-ner-ontonotes_loss: 0.0679
09/16 12:52:36 PM: Update 13355: task edges-ner-ontonotes, batch 355 (13355): mcc: 0.7534, acc: 0.6597, precision: 0.8534, recall: 0.6866, f1: 0.7610, edges-ner-ontonotes_loss: 0.0682
09/16 12:52:46 PM: Update 13420: task edges-ner-ontonotes, batch 420 (13420): mcc: 0.7538, acc: 0.6601, precision: 0.8531, recall: 0.6874, f1: 0.7614, edges-ner-ontonotes_loss: 0.0681
09/16 12:52:57 PM: Update 13544: task edges-ner-ontonotes, batch 544 (13544): mcc: 0.7548, acc: 0.6614, precision: 0.8524, recall: 0.6898, f1: 0.7625, edges-ner-ontonotes_loss: 0.0678
09/16 12:53:07 PM: Update 13656: task edges-ner-ontonotes, batch 656 (13656): mcc: 0.7558, acc: 0.6628, precision: 0.8525, recall: 0.6916, f1: 0.7637, edges-ner-ontonotes_loss: 0.0676
09/16 12:53:17 PM: Update 13717: task edges-ner-ontonotes, batch 717 (13717): mcc: 0.7542, acc: 0.6612, precision: 0.8508, recall: 0.6901, f1: 0.7621, edges-ner-ontonotes_loss: 0.0680
09/16 12:53:27 PM: Update 13819: task edges-ner-ontonotes, batch 819 (13819): mcc: 0.7494, acc: 0.6556, precision: 0.8482, recall: 0.6840, f1: 0.7573, edges-ner-ontonotes_loss: 0.0699
09/16 12:53:36 PM: ***** Step 14000 / Validation 14 *****
09/16 12:53:36 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:53:36 PM: Validating...
09/16 12:53:37 PM: Evaluate: task edges-ner-ontonotes, batch 12 (157): mcc: 0.6745, acc: 0.5995, precision: 0.7873, recall: 0.6047, f1: 0.6841, edges-ner-ontonotes_loss: 0.0957
09/16 12:53:47 PM: Evaluate: task edges-ner-ontonotes, batch 139 (157): mcc: 0.7297, acc: 0.6384, precision: 0.8535, recall: 0.6457, f1: 0.7352, edges-ner-ontonotes_loss: 0.0769
09/16 12:53:48 PM: Updating LR scheduler:
09/16 12:53:48 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:53:48 PM: 	# validation passes without improvement: 0
09/16 12:53:48 PM: edges-ner-ontonotes_loss: training: 0.071759 validation: 0.076403
09/16 12:53:48 PM: macro_avg: validation: 0.736401
09/16 12:53:48 PM: micro_avg: validation: 0.000000
09/16 12:53:48 PM: edges-ner-ontonotes_mcc: training: 0.744816 validation: 0.731068
09/16 12:53:48 PM: edges-ner-ontonotes_acc: training: 0.651130 validation: 0.638838
09/16 12:53:48 PM: edges-ner-ontonotes_precision: training: 0.845606 validation: 0.855880
09/16 12:53:48 PM: edges-ner-ontonotes_recall: training: 0.678171 validation: 0.646194
09/16 12:53:48 PM: edges-ner-ontonotes_f1: training: 0.752689 validation: 0.736401
09/16 12:53:48 PM: Global learning rate: 5e-05
09/16 12:53:48 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:53:57 PM: Update 14039: task edges-ner-ontonotes, batch 39 (14039): mcc: 0.7651, acc: 0.6720, precision: 0.8742, recall: 0.6895, f1: 0.7710, edges-ner-ontonotes_loss: 0.0677
09/16 12:54:09 PM: Update 14178: task edges-ner-ontonotes, batch 178 (14178): mcc: 0.7683, acc: 0.6788, precision: 0.8695, recall: 0.6990, f1: 0.7750, edges-ner-ontonotes_loss: 0.0673
09/16 12:54:24 PM: Update 14318: task edges-ner-ontonotes, batch 318 (14318): mcc: 0.7752, acc: 0.6885, precision: 0.8740, recall: 0.7073, f1: 0.7819, edges-ner-ontonotes_loss: 0.0655
09/16 12:54:34 PM: Update 14479: task edges-ner-ontonotes, batch 479 (14479): mcc: 0.7657, acc: 0.6776, precision: 0.8663, recall: 0.6972, f1: 0.7726, edges-ner-ontonotes_loss: 0.0667
09/16 12:54:44 PM: Update 14592: task edges-ner-ontonotes, batch 592 (14592): mcc: 0.7666, acc: 0.6793, precision: 0.8653, recall: 0.6995, f1: 0.7736, edges-ner-ontonotes_loss: 0.0668
09/16 12:54:54 PM: Update 14685: task edges-ner-ontonotes, batch 685 (14685): mcc: 0.7633, acc: 0.6748, precision: 0.8631, recall: 0.6957, f1: 0.7704, edges-ner-ontonotes_loss: 0.0670
09/16 12:55:04 PM: Update 14843: task edges-ner-ontonotes, batch 843 (14843): mcc: 0.7598, acc: 0.6692, precision: 0.8605, recall: 0.6917, f1: 0.7670, edges-ner-ontonotes_loss: 0.0675
09/16 12:55:18 PM: Update 14944: task edges-ner-ontonotes, batch 944 (14944): mcc: 0.7594, acc: 0.6682, precision: 0.8600, recall: 0.6915, f1: 0.7666, edges-ner-ontonotes_loss: 0.0675
09/16 12:55:23 PM: ***** Step 15000 / Validation 15 *****
09/16 12:55:23 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:55:23 PM: Validating...
09/16 12:55:28 PM: Evaluate: task edges-ner-ontonotes, batch 59 (157): mcc: 0.7348, acc: 0.6555, precision: 0.8418, recall: 0.6639, f1: 0.7423, edges-ner-ontonotes_loss: 0.0782
09/16 12:55:38 PM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.7422, acc: 0.6560, precision: 0.8500, recall: 0.6699, f1: 0.7493, edges-ner-ontonotes_loss: 0.0753
09/16 12:55:39 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:55:39 PM: Best result seen so far for macro.
09/16 12:55:39 PM: Updating LR scheduler:
09/16 12:55:39 PM: 	Best result seen so far for macro_avg: 0.749
09/16 12:55:39 PM: 	# validation passes without improvement: 0
09/16 12:55:39 PM: edges-ner-ontonotes_loss: training: 0.067405 validation: 0.075027
09/16 12:55:39 PM: macro_avg: validation: 0.749427
09/16 12:55:39 PM: micro_avg: validation: 0.000000
09/16 12:55:39 PM: edges-ner-ontonotes_mcc: training: 0.758682 validation: 0.742458
09/16 12:55:39 PM: edges-ner-ontonotes_acc: training: 0.667198 validation: 0.655823
09/16 12:55:39 PM: edges-ner-ontonotes_precision: training: 0.859244 validation: 0.851437
09/16 12:55:39 PM: edges-ner-ontonotes_recall: training: 0.690862 validation: 0.669245
09/16 12:55:39 PM: edges-ner-ontonotes_f1: training: 0.765907 validation: 0.749427
09/16 12:55:39 PM: Global learning rate: 5e-05
09/16 12:55:39 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:55:48 PM: Update 15111: task edges-ner-ontonotes, batch 111 (15111): mcc: 0.7551, acc: 0.6605, precision: 0.8511, recall: 0.6915, f1: 0.7630, edges-ner-ontonotes_loss: 0.0678
09/16 12:56:01 PM: Update 15257: task edges-ner-ontonotes, batch 257 (15257): mcc: 0.7601, acc: 0.6666, precision: 0.8549, recall: 0.6971, f1: 0.7680, edges-ner-ontonotes_loss: 0.0664
09/16 12:56:11 PM: Update 15438: task edges-ner-ontonotes, batch 438 (15438): mcc: 0.7470, acc: 0.6545, precision: 0.8463, recall: 0.6813, f1: 0.7549, edges-ner-ontonotes_loss: 0.0724
09/16 12:56:22 PM: Update 15561: task edges-ner-ontonotes, batch 561 (15561): mcc: 0.7427, acc: 0.6494, precision: 0.8449, recall: 0.6751, f1: 0.7505, edges-ner-ontonotes_loss: 0.0739
09/16 12:56:32 PM: Update 15680: task edges-ner-ontonotes, batch 680 (15680): mcc: 0.7467, acc: 0.6544, precision: 0.8480, recall: 0.6794, f1: 0.7544, edges-ner-ontonotes_loss: 0.0725
09/16 12:56:42 PM: Update 15791: task edges-ner-ontonotes, batch 791 (15791): mcc: 0.7506, acc: 0.6592, precision: 0.8511, recall: 0.6836, f1: 0.7582, edges-ner-ontonotes_loss: 0.0714
09/16 12:56:52 PM: Update 15944: task edges-ner-ontonotes, batch 944 (15944): mcc: 0.7538, acc: 0.6633, precision: 0.8535, recall: 0.6872, f1: 0.7614, edges-ner-ontonotes_loss: 0.0703
09/16 12:56:58 PM: ***** Step 16000 / Validation 16 *****
09/16 12:56:58 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:56:58 PM: Validating...
09/16 12:57:02 PM: Evaluate: task edges-ner-ontonotes, batch 65 (157): mcc: 0.7437, acc: 0.6590, precision: 0.8566, recall: 0.6672, f1: 0.7501, edges-ner-ontonotes_loss: 0.0746
09/16 12:57:11 PM: Updating LR scheduler:
09/16 12:57:11 PM: 	Best result seen so far for macro_avg: 0.749
09/16 12:57:11 PM: 	# validation passes without improvement: 1
09/16 12:57:11 PM: edges-ner-ontonotes_loss: training: 0.070262 validation: 0.074854
09/16 12:57:11 PM: macro_avg: validation: 0.742361
09/16 12:57:11 PM: micro_avg: validation: 0.000000
09/16 12:57:11 PM: edges-ner-ontonotes_mcc: training: 0.754307 validation: 0.736615
09/16 12:57:11 PM: edges-ner-ontonotes_acc: training: 0.663718 validation: 0.644980
09/16 12:57:11 PM: edges-ner-ontonotes_precision: training: 0.853922 validation: 0.856760
09/16 12:57:11 PM: edges-ner-ontonotes_recall: training: 0.687712 validation: 0.654914
09/16 12:57:11 PM: edges-ner-ontonotes_f1: training: 0.761858 validation: 0.742361
09/16 12:57:11 PM: Global learning rate: 5e-05
09/16 12:57:11 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:57:12 PM: Update 16011: task edges-ner-ontonotes, batch 11 (16011): mcc: 0.7487, acc: 0.6596, precision: 0.8531, recall: 0.6787, f1: 0.7559, edges-ner-ontonotes_loss: 0.0755
09/16 12:57:22 PM: Update 16140: task edges-ner-ontonotes, batch 140 (16140): mcc: 0.7602, acc: 0.6733, precision: 0.8537, recall: 0.6983, f1: 0.7682, edges-ner-ontonotes_loss: 0.0674
09/16 12:57:33 PM: Update 16233: task edges-ner-ontonotes, batch 233 (16233): mcc: 0.7579, acc: 0.6697, precision: 0.8536, recall: 0.6943, f1: 0.7658, edges-ner-ontonotes_loss: 0.0671
09/16 12:57:43 PM: Update 16343: task edges-ner-ontonotes, batch 343 (16343): mcc: 0.7530, acc: 0.6618, precision: 0.8509, recall: 0.6879, f1: 0.7608, edges-ner-ontonotes_loss: 0.0679
09/16 12:57:53 PM: Update 16473: task edges-ner-ontonotes, batch 473 (16473): mcc: 0.7538, acc: 0.6621, precision: 0.8513, recall: 0.6890, f1: 0.7616, edges-ner-ontonotes_loss: 0.0674
09/16 12:58:03 PM: Update 16570: task edges-ner-ontonotes, batch 570 (16570): mcc: 0.7537, acc: 0.6613, precision: 0.8515, recall: 0.6888, f1: 0.7615, edges-ner-ontonotes_loss: 0.0674
09/16 12:58:13 PM: Update 16724: task edges-ner-ontonotes, batch 724 (16724): mcc: 0.7529, acc: 0.6597, precision: 0.8504, recall: 0.6883, f1: 0.7608, edges-ner-ontonotes_loss: 0.0675
09/16 12:58:26 PM: Update 16813: task edges-ner-ontonotes, batch 813 (16813): mcc: 0.7545, acc: 0.6613, precision: 0.8516, recall: 0.6900, f1: 0.7623, edges-ner-ontonotes_loss: 0.0671
09/16 12:58:36 PM: Update 16944: task edges-ner-ontonotes, batch 944 (16944): mcc: 0.7492, acc: 0.6558, precision: 0.8475, recall: 0.6843, f1: 0.7572, edges-ner-ontonotes_loss: 0.0691
09/16 12:58:41 PM: ***** Step 17000 / Validation 17 *****
09/16 12:58:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:58:41 PM: Validating...
09/16 12:58:46 PM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.7355, acc: 0.6406, precision: 0.8600, recall: 0.6504, f1: 0.7407, edges-ner-ontonotes_loss: 0.0789
09/16 12:58:55 PM: Updating LR scheduler:
09/16 12:58:55 PM: 	Best result seen so far for macro_avg: 0.749
09/16 12:58:55 PM: 	# validation passes without improvement: 2
09/16 12:58:55 PM: edges-ner-ontonotes_loss: training: 0.069632 validation: 0.074521
09/16 12:58:55 PM: macro_avg: validation: 0.748049
09/16 12:58:55 PM: micro_avg: validation: 0.000000
09/16 12:58:55 PM: edges-ner-ontonotes_mcc: training: 0.748595 validation: 0.742328
09/16 12:58:55 PM: edges-ner-ontonotes_acc: training: 0.655265 validation: 0.649075
09/16 12:58:55 PM: edges-ner-ontonotes_precision: training: 0.847138 validation: 0.860766
09/16 12:58:55 PM: edges-ner-ontonotes_recall: training: 0.683465 validation: 0.661435
09/16 12:58:55 PM: edges-ner-ontonotes_f1: training: 0.756551 validation: 0.748049
09/16 12:58:55 PM: Global learning rate: 5e-05
09/16 12:58:55 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 12:58:58 PM: Update 17039: task edges-ner-ontonotes, batch 39 (17039): mcc: 0.7334, acc: 0.6380, precision: 0.8409, recall: 0.6622, f1: 0.7409, edges-ner-ontonotes_loss: 0.0777
09/16 12:59:09 PM: Update 17131: task edges-ner-ontonotes, batch 131 (17131): mcc: 0.7315, acc: 0.6353, precision: 0.8418, recall: 0.6584, f1: 0.7389, edges-ner-ontonotes_loss: 0.0773
09/16 12:59:19 PM: Update 17271: task edges-ner-ontonotes, batch 271 (17271): mcc: 0.7559, acc: 0.6665, precision: 0.8573, recall: 0.6877, f1: 0.7632, edges-ner-ontonotes_loss: 0.0706
09/16 12:59:29 PM: Update 17424: task edges-ner-ontonotes, batch 424 (17424): mcc: 0.7644, acc: 0.6777, precision: 0.8619, recall: 0.6986, f1: 0.7717, edges-ner-ontonotes_loss: 0.0681
09/16 12:59:39 PM: Update 17475: task edges-ner-ontonotes, batch 475 (17475): mcc: 0.7622, acc: 0.6754, precision: 0.8604, recall: 0.6960, f1: 0.7695, edges-ner-ontonotes_loss: 0.0684
09/16 12:59:49 PM: Update 17610: task edges-ner-ontonotes, batch 610 (17610): mcc: 0.7615, acc: 0.6748, precision: 0.8590, recall: 0.6960, f1: 0.7689, edges-ner-ontonotes_loss: 0.0681
09/16 01:00:02 PM: Update 17743: task edges-ner-ontonotes, batch 743 (17743): mcc: 0.7633, acc: 0.6773, precision: 0.8592, recall: 0.6990, f1: 0.7708, edges-ner-ontonotes_loss: 0.0677
09/16 01:00:12 PM: Update 17853: task edges-ner-ontonotes, batch 853 (17853): mcc: 0.7596, acc: 0.6721, precision: 0.8564, recall: 0.6948, f1: 0.7672, edges-ner-ontonotes_loss: 0.0680
09/16 01:00:22 PM: Update 17977: task edges-ner-ontonotes, batch 977 (17977): mcc: 0.7590, acc: 0.6706, precision: 0.8558, recall: 0.6943, f1: 0.7666, edges-ner-ontonotes_loss: 0.0679
09/16 01:00:24 PM: ***** Step 18000 / Validation 18 *****
09/16 01:00:24 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:00:24 PM: Validating...
09/16 01:00:32 PM: Evaluate: task edges-ner-ontonotes, batch 130 (157): mcc: 0.7369, acc: 0.6510, precision: 0.8433, recall: 0.6663, f1: 0.7444, edges-ner-ontonotes_loss: 0.0768
09/16 01:00:34 PM: Best result seen so far for edges-ner-ontonotes.
09/16 01:00:34 PM: Best result seen so far for macro.
09/16 01:00:34 PM: Updating LR scheduler:
09/16 01:00:34 PM: 	Best result seen so far for macro_avg: 0.751
09/16 01:00:34 PM: 	# validation passes without improvement: 0
09/16 01:00:34 PM: edges-ner-ontonotes_loss: training: 0.067680 validation: 0.074656
09/16 01:00:34 PM: macro_avg: validation: 0.751270
09/16 01:00:34 PM: micro_avg: validation: 0.000000
09/16 01:00:34 PM: edges-ner-ontonotes_mcc: training: 0.759605 validation: 0.744078
09/16 01:00:34 PM: edges-ner-ontonotes_acc: training: 0.670962 validation: 0.658022
09/16 01:00:34 PM: edges-ner-ontonotes_precision: training: 0.856275 validation: 0.850690
09/16 01:00:34 PM: edges-ner-ontonotes_recall: training: 0.694972 validation: 0.672657
09/16 01:00:34 PM: edges-ner-ontonotes_f1: training: 0.767237 validation: 0.751270
09/16 01:00:34 PM: Global learning rate: 5e-05
09/16 01:00:34 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:00:45 PM: Update 18056: task edges-ner-ontonotes, batch 56 (18056): mcc: 0.7581, acc: 0.6686, precision: 0.8503, recall: 0.6974, f1: 0.7663, edges-ner-ontonotes_loss: 0.0669
09/16 01:00:55 PM: Update 18202: task edges-ner-ontonotes, batch 202 (18202): mcc: 0.7518, acc: 0.6586, precision: 0.8464, recall: 0.6897, f1: 0.7601, edges-ner-ontonotes_loss: 0.0670
09/16 01:01:05 PM: Update 18341: task edges-ner-ontonotes, batch 341 (18341): mcc: 0.7555, acc: 0.6619, precision: 0.8488, recall: 0.6941, f1: 0.7637, edges-ner-ontonotes_loss: 0.0666
09/16 01:01:15 PM: Update 18478: task edges-ner-ontonotes, batch 478 (18478): mcc: 0.7466, acc: 0.6529, precision: 0.8431, recall: 0.6834, f1: 0.7549, edges-ner-ontonotes_loss: 0.0703
09/16 01:01:28 PM: Update 18673: task edges-ner-ontonotes, batch 673 (18673): mcc: 0.7433, acc: 0.6504, precision: 0.8424, recall: 0.6783, f1: 0.7515, edges-ner-ontonotes_loss: 0.0724
09/16 01:01:38 PM: Update 18817: task edges-ner-ontonotes, batch 817 (18817): mcc: 0.7476, acc: 0.6558, precision: 0.8461, recall: 0.6826, f1: 0.7556, edges-ner-ontonotes_loss: 0.0713
09/16 01:01:52 PM: Update 18986: task edges-ner-ontonotes, batch 986 (18986): mcc: 0.7538, acc: 0.6636, precision: 0.8512, recall: 0.6891, f1: 0.7616, edges-ner-ontonotes_loss: 0.0697
09/16 01:01:54 PM: ***** Step 19000 / Validation 19 *****
09/16 01:01:54 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:01:54 PM: Validating...
09/16 01:02:02 PM: Evaluate: task edges-ner-ontonotes, batch 134 (157): mcc: 0.7348, acc: 0.6436, precision: 0.8572, recall: 0.6515, f1: 0.7403, edges-ner-ontonotes_loss: 0.0754
09/16 01:02:04 PM: Updating LR scheduler:
09/16 01:02:04 PM: 	Best result seen so far for macro_avg: 0.751
09/16 01:02:04 PM: 	# validation passes without improvement: 1
09/16 01:02:04 PM: edges-ner-ontonotes_loss: training: 0.069724 validation: 0.074959
09/16 01:02:04 PM: macro_avg: validation: 0.740111
09/16 01:02:04 PM: micro_avg: validation: 0.000000
09/16 01:02:04 PM: edges-ner-ontonotes_mcc: training: 0.753724 validation: 0.734764
09/16 01:02:04 PM: edges-ner-ontonotes_acc: training: 0.663372 validation: 0.642251
09/16 01:02:04 PM: edges-ner-ontonotes_precision: training: 0.851391 validation: 0.858329
09/16 01:02:04 PM: edges-ner-ontonotes_recall: training: 0.688827 validation: 0.650516
09/16 01:02:04 PM: edges-ner-ontonotes_f1: training: 0.761530 validation: 0.740111
09/16 01:02:04 PM: Global learning rate: 5e-05
09/16 01:02:04 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:02:12 PM: Update 19100: task edges-ner-ontonotes, batch 100 (19100): mcc: 0.7631, acc: 0.6745, precision: 0.8591, recall: 0.6987, f1: 0.7706, edges-ner-ontonotes_loss: 0.0664
09/16 01:02:22 PM: Update 19205: task edges-ner-ontonotes, batch 205 (19205): mcc: 0.7620, acc: 0.6741, precision: 0.8575, recall: 0.6982, f1: 0.7697, edges-ner-ontonotes_loss: 0.0669
09/16 01:02:35 PM: Update 19299: task edges-ner-ontonotes, batch 299 (19299): mcc: 0.7654, acc: 0.6792, precision: 0.8587, recall: 0.7031, f1: 0.7731, edges-ner-ontonotes_loss: 0.0663
09/16 01:02:46 PM: Update 19456: task edges-ner-ontonotes, batch 456 (19456): mcc: 0.7594, acc: 0.6693, precision: 0.8567, recall: 0.6943, f1: 0.7670, edges-ner-ontonotes_loss: 0.0670
09/16 01:02:56 PM: Update 19581: task edges-ner-ontonotes, batch 581 (19581): mcc: 0.7585, acc: 0.6682, precision: 0.8551, recall: 0.6940, f1: 0.7662, edges-ner-ontonotes_loss: 0.0669
09/16 01:03:06 PM: Update 19680: task edges-ner-ontonotes, batch 680 (19680): mcc: 0.7581, acc: 0.6676, precision: 0.8537, recall: 0.6945, f1: 0.7659, edges-ner-ontonotes_loss: 0.0670
09/16 01:03:16 PM: Update 19807: task edges-ner-ontonotes, batch 807 (19807): mcc: 0.7578, acc: 0.6665, precision: 0.8532, recall: 0.6944, f1: 0.7657, edges-ner-ontonotes_loss: 0.0669
09/16 01:03:26 PM: Update 19920: task edges-ner-ontonotes, batch 920 (19920): mcc: 0.7586, acc: 0.6669, precision: 0.8535, recall: 0.6955, f1: 0.7664, edges-ner-ontonotes_loss: 0.0667
09/16 01:03:36 PM: Update 19972: task edges-ner-ontonotes, batch 972 (19972): mcc: 0.7569, acc: 0.6652, precision: 0.8520, recall: 0.6938, f1: 0.7648, edges-ner-ontonotes_loss: 0.0675
09/16 01:03:38 PM: ***** Step 20000 / Validation 20 *****
09/16 01:03:38 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:03:38 PM: Validating...
09/16 01:03:46 PM: Evaluate: task edges-ner-ontonotes, batch 130 (157): mcc: 0.7348, acc: 0.6452, precision: 0.8496, recall: 0.6576, f1: 0.7414, edges-ner-ontonotes_loss: 0.0765
09/16 01:03:47 PM: Updating LR scheduler:
09/16 01:03:49 PM: 	Best result seen so far for macro_avg: 0.751
09/16 01:03:49 PM: 	# validation passes without improvement: 2
09/16 01:03:49 PM: edges-ner-ontonotes_loss: training: 0.068072 validation: 0.074297
09/16 01:03:49 PM: macro_avg: validation: 0.748569
09/16 01:03:49 PM: micro_avg: validation: 0.000000
09/16 01:03:49 PM: edges-ner-ontonotes_mcc: training: 0.755457 validation: 0.742313
09/16 01:03:49 PM: edges-ner-ontonotes_acc: training: 0.663656 validation: 0.652639
09/16 01:03:49 PM: edges-ner-ontonotes_precision: training: 0.851047 validation: 0.856794
09/16 01:03:49 PM: edges-ner-ontonotes_recall: training: 0.692148 validation: 0.664619
09/16 01:03:49 PM: edges-ner-ontonotes_f1: training: 0.763417 validation: 0.748569
09/16 01:03:49 PM: Global learning rate: 5e-05
09/16 01:03:49 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:03:56 PM: Update 20087: task edges-ner-ontonotes, batch 87 (20087): mcc: 0.7199, acc: 0.6236, precision: 0.8291, recall: 0.6487, f1: 0.7279, edges-ner-ontonotes_loss: 0.0793
09/16 01:04:06 PM: Update 20203: task edges-ner-ontonotes, batch 203 (20203): mcc: 0.7300, acc: 0.6358, precision: 0.8368, recall: 0.6598, f1: 0.7378, edges-ner-ontonotes_loss: 0.0776
09/16 01:04:16 PM: Update 20282: task edges-ner-ontonotes, batch 282 (20282): mcc: 0.7351, acc: 0.6426, precision: 0.8385, recall: 0.6672, f1: 0.7431, edges-ner-ontonotes_loss: 0.0760
09/16 01:04:26 PM: Update 20444: task edges-ner-ontonotes, batch 444 (20444): mcc: 0.7518, acc: 0.6621, precision: 0.8513, recall: 0.6856, f1: 0.7595, edges-ner-ontonotes_loss: 0.0714
09/16 01:04:36 PM: Update 20592: task edges-ner-ontonotes, batch 592 (20592): mcc: 0.7579, acc: 0.6695, precision: 0.8561, recall: 0.6921, f1: 0.7655, edges-ner-ontonotes_loss: 0.0693
09/16 01:04:46 PM: Update 20722: task edges-ner-ontonotes, batch 722 (20722): mcc: 0.7591, acc: 0.6709, precision: 0.8566, recall: 0.6938, f1: 0.7667, edges-ner-ontonotes_loss: 0.0688
09/16 01:04:56 PM: Update 20837: task edges-ner-ontonotes, batch 837 (20837): mcc: 0.7604, acc: 0.6730, precision: 0.8568, recall: 0.6959, f1: 0.7680, edges-ner-ontonotes_loss: 0.0686
09/16 01:05:06 PM: Update 20888: task edges-ner-ontonotes, batch 888 (20888): mcc: 0.7600, acc: 0.6722, precision: 0.8566, recall: 0.6954, f1: 0.7676, edges-ner-ontonotes_loss: 0.0685
09/16 01:05:16 PM: Update 20995: task edges-ner-ontonotes, batch 995 (20995): mcc: 0.7580, acc: 0.6690, precision: 0.8553, recall: 0.6930, f1: 0.7656, edges-ner-ontonotes_loss: 0.0685
09/16 01:05:17 PM: ***** Step 21000 / Validation 21 *****
09/16 01:05:17 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:05:17 PM: Validating...
09/16 01:05:26 PM: Evaluate: task edges-ner-ontonotes, batch 150 (157): mcc: 0.7467, acc: 0.6600, precision: 0.8530, recall: 0.6754, f1: 0.7539, edges-ner-ontonotes_loss: 0.0740
09/16 01:05:27 PM: Best result seen so far for edges-ner-ontonotes.
09/16 01:05:27 PM: Best result seen so far for macro.
09/16 01:05:27 PM: Updating LR scheduler:
09/16 01:05:27 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:05:27 PM: 	# validation passes without improvement: 0
09/16 01:05:27 PM: edges-ner-ontonotes_loss: training: 0.068488 validation: 0.073974
09/16 01:05:27 PM: macro_avg: validation: 0.753126
09/16 01:05:27 PM: micro_avg: validation: 0.000000
09/16 01:05:27 PM: edges-ner-ontonotes_mcc: training: 0.758046 validation: 0.746156
09/16 01:05:27 PM: edges-ner-ontonotes_acc: training: 0.669029 validation: 0.658857
09/16 01:05:27 PM: edges-ner-ontonotes_precision: training: 0.855444 validation: 0.853752
09/16 01:05:27 PM: edges-ner-ontonotes_recall: training: 0.692950 validation: 0.673719
09/16 01:05:27 PM: edges-ner-ontonotes_f1: training: 0.765671 validation: 0.753126
09/16 01:05:27 PM: Global learning rate: 5e-05
09/16 01:05:27 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:05:36 PM: Update 21120: task edges-ner-ontonotes, batch 120 (21120): mcc: 0.7568, acc: 0.6627, precision: 0.8537, recall: 0.6923, f1: 0.7645, edges-ner-ontonotes_loss: 0.0658
09/16 01:05:46 PM: Update 21190: task edges-ner-ontonotes, batch 190 (21190): mcc: 0.7540, acc: 0.6613, precision: 0.8474, recall: 0.6926, f1: 0.7623, edges-ner-ontonotes_loss: 0.0671
09/16 01:05:57 PM: Update 21312: task edges-ner-ontonotes, batch 312 (21312): mcc: 0.7544, acc: 0.6615, precision: 0.8477, recall: 0.6932, f1: 0.7627, edges-ner-ontonotes_loss: 0.0670
09/16 01:06:08 PM: Update 21481: task edges-ner-ontonotes, batch 481 (21481): mcc: 0.7564, acc: 0.6636, precision: 0.8486, recall: 0.6960, f1: 0.7647, edges-ner-ontonotes_loss: 0.0664
09/16 01:06:18 PM: Update 21650: task edges-ner-ontonotes, batch 650 (21650): mcc: 0.7478, acc: 0.6549, precision: 0.8427, recall: 0.6858, f1: 0.7562, edges-ner-ontonotes_loss: 0.0703
09/16 01:06:28 PM: Update 21803: task edges-ner-ontonotes, batch 803 (21803): mcc: 0.7454, acc: 0.6527, precision: 0.8420, recall: 0.6823, f1: 0.7538, edges-ner-ontonotes_loss: 0.0714
09/16 01:06:38 PM: ***** Step 22000 / Validation 22 *****
09/16 01:06:38 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:06:38 PM: Validating...
09/16 01:06:38 PM: Evaluate: task edges-ner-ontonotes, batch 6 (157): mcc: 0.6522, acc: 0.5700, precision: 0.7835, recall: 0.5700, f1: 0.6599, edges-ner-ontonotes_loss: 0.1053
09/16 01:06:46 PM: Updating LR scheduler:
09/16 01:06:46 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:06:46 PM: 	# validation passes without improvement: 1
09/16 01:06:46 PM: edges-ner-ontonotes_loss: training: 0.069900 validation: 0.074575
09/16 01:06:46 PM: macro_avg: validation: 0.742744
09/16 01:06:46 PM: micro_avg: validation: 0.000000
09/16 01:06:46 PM: edges-ner-ontonotes_mcc: training: 0.750699 validation: 0.737756
09/16 01:06:46 PM: edges-ner-ontonotes_acc: training: 0.659193 validation: 0.644449
09/16 01:06:46 PM: edges-ner-ontonotes_precision: training: 0.846821 validation: 0.862906
09/16 01:06:46 PM: edges-ner-ontonotes_recall: training: 0.687407 validation: 0.651956
09/16 01:06:46 PM: edges-ner-ontonotes_f1: training: 0.758832 validation: 0.742744
09/16 01:06:46 PM: Global learning rate: 5e-05
09/16 01:06:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:06:48 PM: Update 22029: task edges-ner-ontonotes, batch 29 (22029): mcc: 0.7901, acc: 0.7135, precision: 0.8731, recall: 0.7342, f1: 0.7976, edges-ner-ontonotes_loss: 0.0619
09/16 01:06:58 PM: Update 22100: task edges-ner-ontonotes, batch 100 (22100): mcc: 0.7877, acc: 0.7077, precision: 0.8719, recall: 0.7309, f1: 0.7952, edges-ner-ontonotes_loss: 0.0619
09/16 01:07:08 PM: Update 22225: task edges-ner-ontonotes, batch 225 (22225): mcc: 0.7744, acc: 0.6899, precision: 0.8659, recall: 0.7127, f1: 0.7819, edges-ner-ontonotes_loss: 0.0640
09/16 01:07:18 PM: Update 22343: task edges-ner-ontonotes, batch 343 (22343): mcc: 0.7727, acc: 0.6883, precision: 0.8642, recall: 0.7112, f1: 0.7803, edges-ner-ontonotes_loss: 0.0648
09/16 01:07:31 PM: Update 22411: task edges-ner-ontonotes, batch 411 (22411): mcc: 0.7707, acc: 0.6860, precision: 0.8621, recall: 0.7095, f1: 0.7784, edges-ner-ontonotes_loss: 0.0652
09/16 01:07:41 PM: Update 22533: task edges-ner-ontonotes, batch 533 (22533): mcc: 0.7642, acc: 0.6761, precision: 0.8593, recall: 0.7004, f1: 0.7718, edges-ner-ontonotes_loss: 0.0662
09/16 01:07:51 PM: Update 22649: task edges-ner-ontonotes, batch 649 (22649): mcc: 0.7627, acc: 0.6735, precision: 0.8577, recall: 0.6992, f1: 0.7704, edges-ner-ontonotes_loss: 0.0663
09/16 01:08:02 PM: Update 22724: task edges-ner-ontonotes, batch 724 (22724): mcc: 0.7626, acc: 0.6730, precision: 0.8573, recall: 0.6994, f1: 0.7703, edges-ner-ontonotes_loss: 0.0661
09/16 01:08:12 PM: Update 22887: task edges-ner-ontonotes, batch 887 (22887): mcc: 0.7618, acc: 0.6716, precision: 0.8557, recall: 0.6993, f1: 0.7696, edges-ner-ontonotes_loss: 0.0660
09/16 01:08:21 PM: ***** Step 23000 / Validation 23 *****
09/16 01:08:21 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:08:21 PM: Validating...
09/16 01:08:22 PM: Evaluate: task edges-ner-ontonotes, batch 4 (157): mcc: 0.5919, acc: 0.5217, precision: 0.7108, recall: 0.5254, f1: 0.6042, edges-ner-ontonotes_loss: 0.1283
09/16 01:08:32 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.7334, acc: 0.6461, precision: 0.8422, recall: 0.6612, f1: 0.7408, edges-ner-ontonotes_loss: 0.0790
09/16 01:08:35 PM: Updating LR scheduler:
09/16 01:08:35 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:08:35 PM: 	# validation passes without improvement: 2
09/16 01:08:35 PM: edges-ner-ontonotes_loss: training: 0.065817 validation: 0.074608
09/16 01:08:35 PM: macro_avg: validation: 0.751834
09/16 01:08:35 PM: micro_avg: validation: 0.000000
09/16 01:08:35 PM: edges-ner-ontonotes_mcc: training: 0.762198 validation: 0.744841
09/16 01:08:35 PM: edges-ner-ontonotes_acc: training: 0.671818 validation: 0.658629
09/16 01:08:35 PM: edges-ner-ontonotes_precision: training: 0.855937 validation: 0.852746
09/16 01:08:35 PM: edges-ner-ontonotes_recall: training: 0.699797 validation: 0.672278
09/16 01:08:35 PM: edges-ner-ontonotes_f1: training: 0.770032 validation: 0.751834
09/16 01:08:35 PM: Global learning rate: 5e-05
09/16 01:08:35 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:08:45 PM: Update 23037: task edges-ner-ontonotes, batch 37 (23037): mcc: 0.7498, acc: 0.6567, precision: 0.8453, recall: 0.6871, f1: 0.7581, edges-ner-ontonotes_loss: 0.0671
09/16 01:08:56 PM: Update 23166: task edges-ner-ontonotes, batch 166 (23166): mcc: 0.7256, acc: 0.6290, precision: 0.8307, recall: 0.6572, f1: 0.7339, edges-ner-ontonotes_loss: 0.0783
09/16 01:09:06 PM: Update 23284: task edges-ner-ontonotes, batch 284 (23284): mcc: 0.7278, acc: 0.6340, precision: 0.8307, recall: 0.6611, f1: 0.7363, edges-ner-ontonotes_loss: 0.0779
09/16 01:09:16 PM: Update 23398: task edges-ner-ontonotes, batch 398 (23398): mcc: 0.7331, acc: 0.6401, precision: 0.8359, recall: 0.6660, f1: 0.7413, edges-ner-ontonotes_loss: 0.0760
09/16 01:09:26 PM: Update 23541: task edges-ner-ontonotes, batch 541 (23541): mcc: 0.7458, acc: 0.6554, precision: 0.8451, recall: 0.6804, f1: 0.7539, edges-ner-ontonotes_loss: 0.0725
09/16 01:09:36 PM: Update 23654: task edges-ner-ontonotes, batch 654 (23654): mcc: 0.7525, acc: 0.6632, precision: 0.8498, recall: 0.6879, f1: 0.7604, edges-ner-ontonotes_loss: 0.0706
09/16 01:09:46 PM: Update 23773: task edges-ner-ontonotes, batch 773 (23773): mcc: 0.7539, acc: 0.6644, precision: 0.8515, recall: 0.6891, f1: 0.7617, edges-ner-ontonotes_loss: 0.0702
09/16 01:09:56 PM: Update 23928: task edges-ner-ontonotes, batch 928 (23928): mcc: 0.7558, acc: 0.6670, precision: 0.8530, recall: 0.6911, f1: 0.7636, edges-ner-ontonotes_loss: 0.0697
09/16 01:10:06 PM: Update 23975: task edges-ner-ontonotes, batch 975 (23975): mcc: 0.7572, acc: 0.6687, precision: 0.8539, recall: 0.6927, f1: 0.7649, edges-ner-ontonotes_loss: 0.0694
09/16 01:10:09 PM: ***** Step 24000 / Validation 24 *****
09/16 01:10:12 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:10:12 PM: Validating...
09/16 01:10:16 PM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.7423, acc: 0.6662, precision: 0.8463, recall: 0.6732, f1: 0.7499, edges-ner-ontonotes_loss: 0.0772
09/16 01:10:26 PM: Updating LR scheduler:
09/16 01:10:26 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:10:26 PM: 	# validation passes without improvement: 3
09/16 01:10:26 PM: edges-ner-ontonotes_loss: training: 0.069357 validation: 0.073966
09/16 01:10:26 PM: macro_avg: validation: 0.749574
09/16 01:10:26 PM: micro_avg: validation: 0.000000
09/16 01:10:26 PM: edges-ner-ontonotes_mcc: training: 0.756336 validation: 0.743160
09/16 01:10:26 PM: edges-ner-ontonotes_acc: training: 0.667489 validation: 0.656809
09/16 01:10:26 PM: edges-ner-ontonotes_precision: training: 0.853313 validation: 0.856155
09/16 01:10:26 PM: edges-ner-ontonotes_recall: training: 0.691766 validation: 0.666591
09/16 01:10:26 PM: edges-ner-ontonotes_f1: training: 0.764094 validation: 0.749574
09/16 01:10:26 PM: Global learning rate: 5e-05
09/16 01:10:26 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:10:26 PM: Update 24001: task edges-ner-ontonotes, batch 1 (24001): mcc: 0.7475, acc: 0.6500, precision: 0.8864, recall: 0.6500, f1: 0.7500, edges-ner-ontonotes_loss: 0.0686
09/16 01:10:36 PM: Update 24119: task edges-ner-ontonotes, batch 119 (24119): mcc: 0.7487, acc: 0.6531, precision: 0.8468, recall: 0.6839, f1: 0.7567, edges-ner-ontonotes_loss: 0.0687
09/16 01:10:46 PM: Update 24237: task edges-ner-ontonotes, batch 237 (24237): mcc: 0.7520, acc: 0.6575, precision: 0.8496, recall: 0.6874, f1: 0.7599, edges-ner-ontonotes_loss: 0.0675
09/16 01:10:57 PM: Update 24330: task edges-ner-ontonotes, batch 330 (24330): mcc: 0.7501, acc: 0.6558, precision: 0.8462, recall: 0.6869, f1: 0.7583, edges-ner-ontonotes_loss: 0.0675
09/16 01:11:07 PM: Update 24461: task edges-ner-ontonotes, batch 461 (24461): mcc: 0.7536, acc: 0.6601, precision: 0.8484, recall: 0.6912, f1: 0.7618, edges-ner-ontonotes_loss: 0.0666
09/16 01:11:17 PM: Update 24564: task edges-ner-ontonotes, batch 564 (24564): mcc: 0.7560, acc: 0.6630, precision: 0.8490, recall: 0.6948, f1: 0.7642, edges-ner-ontonotes_loss: 0.0663
09/16 01:11:27 PM: Update 24642: task edges-ner-ontonotes, batch 642 (24642): mcc: 0.7538, acc: 0.6605, precision: 0.8479, recall: 0.6919, f1: 0.7620, edges-ner-ontonotes_loss: 0.0674
09/16 01:11:37 PM: Update 24819: task edges-ner-ontonotes, batch 819 (24819): mcc: 0.7481, acc: 0.6544, precision: 0.8446, recall: 0.6847, f1: 0.7563, edges-ner-ontonotes_loss: 0.0701
09/16 01:11:47 PM: Update 24938: task edges-ner-ontonotes, batch 938 (24938): mcc: 0.7482, acc: 0.6555, precision: 0.8451, recall: 0.6846, f1: 0.7564, edges-ner-ontonotes_loss: 0.0704
09/16 01:11:50 PM: ***** Step 25000 / Validation 25 *****
09/16 01:11:50 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:11:50 PM: Validating...
09/16 01:11:57 PM: Evaluate: task edges-ner-ontonotes, batch 123 (157): mcc: 0.7363, acc: 0.6463, precision: 0.8576, recall: 0.6537, f1: 0.7419, edges-ner-ontonotes_loss: 0.0762
09/16 01:12:01 PM: Updating LR scheduler:
09/16 01:12:02 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:12:02 PM: 	# validation passes without improvement: 0
09/16 01:12:02 PM: edges-ner-ontonotes_loss: training: 0.070048 validation: 0.074380
09/16 01:12:02 PM: macro_avg: validation: 0.745430
09/16 01:12:02 PM: micro_avg: validation: 0.000000
09/16 01:12:02 PM: edges-ner-ontonotes_mcc: training: 0.749777 validation: 0.739983
09/16 01:12:02 PM: edges-ner-ontonotes_acc: training: 0.657498 validation: 0.650212
09/16 01:12:02 PM: edges-ner-ontonotes_precision: training: 0.846233 validation: 0.861175
09/16 01:12:02 PM: edges-ner-ontonotes_recall: training: 0.686293 validation: 0.657113
09/16 01:12:02 PM: edges-ner-ontonotes_f1: training: 0.757917 validation: 0.745430
09/16 01:12:02 PM: Global learning rate: 2.5e-05
09/16 01:12:02 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:12:07 PM: Update 25062: task edges-ner-ontonotes, batch 62 (25062): mcc: 0.7830, acc: 0.6991, precision: 0.8683, recall: 0.7257, f1: 0.7906, edges-ner-ontonotes_loss: 0.0626
09/16 01:12:17 PM: Update 25197: task edges-ner-ontonotes, batch 197 (25197): mcc: 0.7888, acc: 0.7064, precision: 0.8769, recall: 0.7287, f1: 0.7959, edges-ner-ontonotes_loss: 0.0619
09/16 01:12:27 PM: Update 25236: task edges-ner-ontonotes, batch 236 (25236): mcc: 0.7834, acc: 0.7004, precision: 0.8739, recall: 0.7217, f1: 0.7905, edges-ner-ontonotes_loss: 0.0626
09/16 01:12:38 PM: Update 25366: task edges-ner-ontonotes, batch 366 (25366): mcc: 0.7751, acc: 0.6910, precision: 0.8672, recall: 0.7128, f1: 0.7824, edges-ner-ontonotes_loss: 0.0642
09/16 01:12:53 PM: Update 25523: task edges-ner-ontonotes, batch 523 (25523): mcc: 0.7723, acc: 0.6884, precision: 0.8642, recall: 0.7105, f1: 0.7799, edges-ner-ontonotes_loss: 0.0648
09/16 01:13:03 PM: Update 25632: task edges-ner-ontonotes, batch 632 (25632): mcc: 0.7651, acc: 0.6784, precision: 0.8601, recall: 0.7013, f1: 0.7726, edges-ner-ontonotes_loss: 0.0657
09/16 01:13:13 PM: Update 25744: task edges-ner-ontonotes, batch 744 (25744): mcc: 0.7635, acc: 0.6753, precision: 0.8586, recall: 0.6997, f1: 0.7711, edges-ner-ontonotes_loss: 0.0659
09/16 01:13:24 PM: Update 25836: task edges-ner-ontonotes, batch 836 (25836): mcc: 0.7633, acc: 0.6747, precision: 0.8588, recall: 0.6994, f1: 0.7709, edges-ner-ontonotes_loss: 0.0658
09/16 01:13:34 PM: Update 25951: task edges-ner-ontonotes, batch 951 (25951): mcc: 0.7618, acc: 0.6723, precision: 0.8567, recall: 0.6984, f1: 0.7695, edges-ner-ontonotes_loss: 0.0658
09/16 01:13:39 PM: ***** Step 26000 / Validation 26 *****
09/16 01:13:39 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:13:39 PM: Validating...
09/16 01:13:44 PM: Evaluate: task edges-ner-ontonotes, batch 98 (157): mcc: 0.7374, acc: 0.6524, precision: 0.8493, recall: 0.6622, f1: 0.7442, edges-ner-ontonotes_loss: 0.0780
09/16 01:13:48 PM: Updating LR scheduler:
09/16 01:13:48 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:13:48 PM: 	# validation passes without improvement: 1
09/16 01:13:48 PM: edges-ner-ontonotes_loss: training: 0.065781 validation: 0.073940
09/16 01:13:48 PM: macro_avg: validation: 0.751243
09/16 01:13:48 PM: micro_avg: validation: 0.000000
09/16 01:13:48 PM: edges-ner-ontonotes_mcc: training: 0.761834 validation: 0.744521
09/16 01:13:48 PM: edges-ner-ontonotes_acc: training: 0.672142 validation: 0.657416
09/16 01:13:48 PM: edges-ner-ontonotes_precision: training: 0.856636 validation: 0.854656
09/16 01:13:48 PM: edges-ner-ontonotes_recall: training: 0.698565 validation: 0.670155
09/16 01:13:48 PM: edges-ner-ontonotes_f1: training: 0.769567 validation: 0.751243
09/16 01:13:48 PM: Global learning rate: 2.5e-05
09/16 01:13:48 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:13:55 PM: Update 26067: task edges-ner-ontonotes, batch 67 (26067): mcc: 0.7547, acc: 0.6615, precision: 0.8485, recall: 0.6930, f1: 0.7629, edges-ner-ontonotes_loss: 0.0679
09/16 01:14:06 PM: Update 26149: task edges-ner-ontonotes, batch 149 (26149): mcc: 0.7609, acc: 0.6688, precision: 0.8521, recall: 0.7008, f1: 0.7691, edges-ner-ontonotes_loss: 0.0659
09/16 01:14:16 PM: Update 26330: task edges-ner-ontonotes, batch 330 (26330): mcc: 0.7415, acc: 0.6497, precision: 0.8375, recall: 0.6793, f1: 0.7501, edges-ner-ontonotes_loss: 0.0731
09/16 01:14:26 PM: Update 26451: task edges-ner-ontonotes, batch 451 (26451): mcc: 0.7406, acc: 0.6487, precision: 0.8390, recall: 0.6764, f1: 0.7490, edges-ner-ontonotes_loss: 0.0742
09/16 01:14:36 PM: Update 26519: task edges-ner-ontonotes, batch 519 (26519): mcc: 0.7441, acc: 0.6529, precision: 0.8419, recall: 0.6800, f1: 0.7524, edges-ner-ontonotes_loss: 0.0731
09/16 01:14:46 PM: Update 26671: task edges-ner-ontonotes, batch 671 (26671): mcc: 0.7513, acc: 0.6615, precision: 0.8476, recall: 0.6878, f1: 0.7594, edges-ner-ontonotes_loss: 0.0708
09/16 01:15:01 PM: Update 26766: task edges-ner-ontonotes, batch 766 (26766): mcc: 0.7565, acc: 0.6677, precision: 0.8520, recall: 0.6932, f1: 0.7644, edges-ner-ontonotes_loss: 0.0694
09/16 01:15:11 PM: Update 26872: task edges-ner-ontonotes, batch 872 (26872): mcc: 0.7568, acc: 0.6679, precision: 0.8526, recall: 0.6932, f1: 0.7647, edges-ner-ontonotes_loss: 0.0692
09/16 01:15:21 PM: Update 26985: task edges-ner-ontonotes, batch 985 (26985): mcc: 0.7587, acc: 0.6703, precision: 0.8542, recall: 0.6951, f1: 0.7665, edges-ner-ontonotes_loss: 0.0688
09/16 01:15:23 PM: ***** Step 27000 / Validation 27 *****
09/16 01:15:23 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:15:23 PM: Validating...
09/16 01:15:31 PM: Evaluate: task edges-ner-ontonotes, batch 129 (157): mcc: 0.7414, acc: 0.6517, precision: 0.8582, recall: 0.6620, f1: 0.7474, edges-ner-ontonotes_loss: 0.0755
09/16 01:15:33 PM: Updating LR scheduler:
09/16 01:15:33 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:15:33 PM: 	# validation passes without improvement: 2
09/16 01:15:33 PM: edges-ner-ontonotes_loss: training: 0.068675 validation: 0.073802
09/16 01:15:33 PM: macro_avg: validation: 0.752918
09/16 01:15:33 PM: micro_avg: validation: 0.000000
09/16 01:15:33 PM: edges-ner-ontonotes_mcc: training: 0.758723 validation: 0.747094
09/16 01:15:33 PM: edges-ner-ontonotes_acc: training: 0.670283 validation: 0.657264
09/16 01:15:33 PM: edges-ner-ontonotes_precision: training: 0.854296 validation: 0.863151
09/16 01:15:33 PM: edges-ner-ontonotes_recall: training: 0.695105 validation: 0.667652
09/16 01:15:33 PM: edges-ner-ontonotes_f1: training: 0.766522 validation: 0.752918
09/16 01:15:33 PM: Global learning rate: 2.5e-05
09/16 01:15:33 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:15:45 PM: Update 27079: task edges-ner-ontonotes, batch 79 (27079): mcc: 0.7691, acc: 0.6874, precision: 0.8588, recall: 0.7096, f1: 0.7771, edges-ner-ontonotes_loss: 0.0662
09/16 01:15:55 PM: Update 27184: task edges-ner-ontonotes, batch 184 (27184): mcc: 0.7595, acc: 0.6703, precision: 0.8552, recall: 0.6957, f1: 0.7672, edges-ner-ontonotes_loss: 0.0668
09/16 01:16:05 PM: Update 27299: task edges-ner-ontonotes, batch 299 (27299): mcc: 0.7577, acc: 0.6664, precision: 0.8551, recall: 0.6927, f1: 0.7654, edges-ner-ontonotes_loss: 0.0666
09/16 01:16:15 PM: Update 27398: task edges-ner-ontonotes, batch 398 (27398): mcc: 0.7558, acc: 0.6632, precision: 0.8538, recall: 0.6904, f1: 0.7635, edges-ner-ontonotes_loss: 0.0668
09/16 01:16:25 PM: Update 27558: task edges-ner-ontonotes, batch 558 (27558): mcc: 0.7577, acc: 0.6644, precision: 0.8543, recall: 0.6932, f1: 0.7654, edges-ner-ontonotes_loss: 0.0662
09/16 01:16:37 PM: Update 27705: task edges-ner-ontonotes, batch 705 (27705): mcc: 0.7559, acc: 0.6628, precision: 0.8520, recall: 0.6920, f1: 0.7637, edges-ner-ontonotes_loss: 0.0664
09/16 01:16:47 PM: Update 27892: task edges-ner-ontonotes, batch 892 (27892): mcc: 0.7510, acc: 0.6580, precision: 0.8486, recall: 0.6864, f1: 0.7589, edges-ner-ontonotes_loss: 0.0690
09/16 01:16:53 PM: ***** Step 28000 / Validation 28 *****
09/16 01:16:53 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:16:53 PM: Validating...
09/16 01:16:57 PM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.7398, acc: 0.6524, precision: 0.8568, recall: 0.6603, f1: 0.7459, edges-ner-ontonotes_loss: 0.0781
09/16 01:17:06 PM: Updating LR scheduler:
09/16 01:17:06 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:17:06 PM: 	# validation passes without improvement: 3
09/16 01:17:06 PM: edges-ner-ontonotes_loss: training: 0.069996 validation: 0.073878
09/16 01:17:06 PM: macro_avg: validation: 0.749496
09/16 01:17:06 PM: micro_avg: validation: 0.000000
09/16 01:17:06 PM: edges-ner-ontonotes_mcc: training: 0.749142 validation: 0.743797
09/16 01:17:06 PM: edges-ner-ontonotes_acc: training: 0.656401 validation: 0.650819
09/16 01:17:06 PM: edges-ner-ontonotes_precision: training: 0.847349 validation: 0.861902
09/16 01:17:06 PM: edges-ner-ontonotes_recall: training: 0.684242 validation: 0.663027
09/16 01:17:06 PM: edges-ner-ontonotes_f1: training: 0.757110 validation: 0.749496
09/16 01:17:06 PM: Global learning rate: 2.5e-05
09/16 01:17:06 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:17:08 PM: Update 28001: task edges-ner-ontonotes, batch 1 (28001): mcc: 0.7900, acc: 0.6857, precision: 0.8421, recall: 0.7619, f1: 0.8000, edges-ner-ontonotes_loss: 0.0561
09/16 01:17:18 PM: Update 28050: task edges-ner-ontonotes, batch 50 (28050): mcc: 0.7691, acc: 0.6814, precision: 0.8626, recall: 0.7062, f1: 0.7766, edges-ner-ontonotes_loss: 0.0656
09/16 01:17:28 PM: Update 28243: task edges-ner-ontonotes, batch 243 (28243): mcc: 0.7803, acc: 0.6965, precision: 0.8705, recall: 0.7191, f1: 0.7876, edges-ner-ontonotes_loss: 0.0637
09/16 01:17:38 PM: Update 28386: task edges-ner-ontonotes, batch 386 (28386): mcc: 0.7781, acc: 0.6937, precision: 0.8699, recall: 0.7158, f1: 0.7854, edges-ner-ontonotes_loss: 0.0635
09/16 01:17:48 PM: Update 28499: task edges-ner-ontonotes, batch 499 (28499): mcc: 0.7740, acc: 0.6889, precision: 0.8658, recall: 0.7121, f1: 0.7815, edges-ner-ontonotes_loss: 0.0642
09/16 01:17:58 PM: Update 28620: task edges-ner-ontonotes, batch 620 (28620): mcc: 0.7741, acc: 0.6884, precision: 0.8662, recall: 0.7119, f1: 0.7815, edges-ner-ontonotes_loss: 0.0645
09/16 01:18:08 PM: Update 28665: task edges-ner-ontonotes, batch 665 (28665): mcc: 0.7715, acc: 0.6854, precision: 0.8643, recall: 0.7090, f1: 0.7790, edges-ner-ontonotes_loss: 0.0648
09/16 01:18:18 PM: Update 28781: task edges-ner-ontonotes, batch 781 (28781): mcc: 0.7673, acc: 0.6799, precision: 0.8619, recall: 0.7037, f1: 0.7748, edges-ner-ontonotes_loss: 0.0652
09/16 01:18:28 PM: Update 28901: task edges-ner-ontonotes, batch 901 (28901): mcc: 0.7658, acc: 0.6775, precision: 0.8604, recall: 0.7023, f1: 0.7734, edges-ner-ontonotes_loss: 0.0653
09/16 01:18:38 PM: Update 28972: task edges-ner-ontonotes, batch 972 (28972): mcc: 0.7654, acc: 0.6764, precision: 0.8604, recall: 0.7017, f1: 0.7730, edges-ner-ontonotes_loss: 0.0652
09/16 01:18:40 PM: ***** Step 29000 / Validation 29 *****
09/16 01:18:40 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:18:40 PM: Validating...
09/16 01:18:48 PM: Evaluate: task edges-ner-ontonotes, batch 72 (157): mcc: 0.7378, acc: 0.6524, precision: 0.8506, recall: 0.6620, f1: 0.7445, edges-ner-ontonotes_loss: 0.0779
09/16 01:18:55 PM: Updating LR scheduler:
09/16 01:18:55 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:18:55 PM: 	# validation passes without improvement: 0
09/16 01:18:55 PM: edges-ner-ontonotes_loss: training: 0.065334 validation: 0.073802
09/16 01:18:55 PM: macro_avg: validation: 0.751949
09/16 01:18:55 PM: micro_avg: validation: 0.000000
09/16 01:18:55 PM: edges-ner-ontonotes_mcc: training: 0.764942 validation: 0.744883
09/16 01:18:55 PM: edges-ner-ontonotes_acc: training: 0.675678 validation: 0.659539
09/16 01:18:55 PM: edges-ner-ontonotes_precision: training: 0.859973 validation: 0.852190
09/16 01:18:55 PM: edges-ner-ontonotes_recall: training: 0.701170 validation: 0.672809
09/16 01:18:55 PM: edges-ner-ontonotes_f1: training: 0.772495 validation: 0.751949
09/16 01:18:55 PM: Global learning rate: 1.25e-05
09/16 01:18:55 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:18:58 PM: Update 29017: task edges-ner-ontonotes, batch 17 (29017): mcc: 0.7522, acc: 0.6652, precision: 0.8422, recall: 0.6940, f1: 0.7609, edges-ner-ontonotes_loss: 0.0695
09/16 01:19:08 PM: Update 29151: task edges-ner-ontonotes, batch 151 (29151): mcc: 0.7530, acc: 0.6568, precision: 0.8497, recall: 0.6890, f1: 0.7609, edges-ner-ontonotes_loss: 0.0661
09/16 01:19:24 PM: Update 29261: task edges-ner-ontonotes, batch 261 (29261): mcc: 0.7561, acc: 0.6629, precision: 0.8486, recall: 0.6953, f1: 0.7643, edges-ner-ontonotes_loss: 0.0660
09/16 01:19:34 PM: Update 29385: task edges-ner-ontonotes, batch 385 (29385): mcc: 0.7481, acc: 0.6548, precision: 0.8439, recall: 0.6854, f1: 0.7564, edges-ner-ontonotes_loss: 0.0698
09/16 01:19:44 PM: Update 29543: task edges-ner-ontonotes, batch 543 (29543): mcc: 0.7426, acc: 0.6488, precision: 0.8396, recall: 0.6794, f1: 0.7510, edges-ner-ontonotes_loss: 0.0726
09/16 01:19:54 PM: Update 29595: task edges-ner-ontonotes, batch 595 (29595): mcc: 0.7441, acc: 0.6512, precision: 0.8409, recall: 0.6809, f1: 0.7525, edges-ner-ontonotes_loss: 0.0725
09/16 01:20:04 PM: Update 29708: task edges-ner-ontonotes, batch 708 (29708): mcc: 0.7498, acc: 0.6587, precision: 0.8449, recall: 0.6875, f1: 0.7581, edges-ner-ontonotes_loss: 0.0709
09/16 01:20:14 PM: Update 29861: task edges-ner-ontonotes, batch 861 (29861): mcc: 0.7554, acc: 0.6657, precision: 0.8493, recall: 0.6934, f1: 0.7635, edges-ner-ontonotes_loss: 0.0693
09/16 01:20:24 PM: Update 29944: task edges-ner-ontonotes, batch 944 (29944): mcc: 0.7568, acc: 0.6674, precision: 0.8505, recall: 0.6949, f1: 0.7649, edges-ner-ontonotes_loss: 0.0689
09/16 01:20:30 PM: ***** Step 30000 / Validation 30 *****
09/16 01:20:30 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:20:30 PM: Validating...
09/16 01:20:34 PM: Evaluate: task edges-ner-ontonotes, batch 58 (157): mcc: 0.7585, acc: 0.6782, precision: 0.8637, recall: 0.6868, f1: 0.7651, edges-ner-ontonotes_loss: 0.0732
09/16 01:20:43 PM: Updating LR scheduler:
09/16 01:20:43 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:20:43 PM: 	# validation passes without improvement: 1
09/16 01:20:43 PM: edges-ner-ontonotes_loss: training: 0.068735 validation: 0.073437
09/16 01:20:43 PM: macro_avg: validation: 0.752673
09/16 01:20:43 PM: micro_avg: validation: 0.000000
09/16 01:20:43 PM: edges-ner-ontonotes_mcc: training: 0.757999 validation: 0.746851
09/16 01:20:43 PM: edges-ner-ontonotes_acc: training: 0.668814 validation: 0.656582
09/16 01:20:43 PM: edges-ner-ontonotes_precision: training: 0.851533 validation: 0.863012
09/16 01:20:43 PM: edges-ner-ontonotes_recall: training: 0.696187 validation: 0.667349
09/16 01:20:43 PM: edges-ner-ontonotes_f1: training: 0.766064 validation: 0.752673
09/16 01:20:43 PM: Global learning rate: 1.25e-05
09/16 01:20:43 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:20:44 PM: Update 30012: task edges-ner-ontonotes, batch 12 (30012): mcc: 0.7765, acc: 0.6876, precision: 0.8732, recall: 0.7102, f1: 0.7833, edges-ner-ontonotes_loss: 0.0662
09/16 01:20:54 PM: Update 30162: task edges-ner-ontonotes, batch 162 (30162): mcc: 0.7720, acc: 0.6856, precision: 0.8645, recall: 0.7096, f1: 0.7795, edges-ner-ontonotes_loss: 0.0652
09/16 01:21:04 PM: Update 30236: task edges-ner-ontonotes, batch 236 (30236): mcc: 0.7659, acc: 0.6775, precision: 0.8601, recall: 0.7028, f1: 0.7735, edges-ner-ontonotes_loss: 0.0662
09/16 01:21:14 PM: Update 30341: task edges-ner-ontonotes, batch 341 (30341): mcc: 0.7629, acc: 0.6722, precision: 0.8590, recall: 0.6985, f1: 0.7705, edges-ner-ontonotes_loss: 0.0662
09/16 01:21:24 PM: Update 30478: task edges-ner-ontonotes, batch 478 (30478): mcc: 0.7609, acc: 0.6698, precision: 0.8572, recall: 0.6965, f1: 0.7685, edges-ner-ontonotes_loss: 0.0660
09/16 01:21:34 PM: Update 30562: task edges-ner-ontonotes, batch 562 (30562): mcc: 0.7591, acc: 0.6670, precision: 0.8558, recall: 0.6945, f1: 0.7668, edges-ner-ontonotes_loss: 0.0663
09/16 01:21:44 PM: Update 30747: task edges-ner-ontonotes, batch 747 (30747): mcc: 0.7593, acc: 0.6666, precision: 0.8547, recall: 0.6958, f1: 0.7671, edges-ner-ontonotes_loss: 0.0662
09/16 01:21:54 PM: Update 30836: task edges-ner-ontonotes, batch 836 (30836): mcc: 0.7591, acc: 0.6664, precision: 0.8542, recall: 0.6958, f1: 0.7669, edges-ner-ontonotes_loss: 0.0663
09/16 01:22:04 PM: Update 30978: task edges-ner-ontonotes, batch 978 (30978): mcc: 0.7553, acc: 0.6630, precision: 0.8510, recall: 0.6919, f1: 0.7632, edges-ner-ontonotes_loss: 0.0681
09/16 01:22:06 PM: ***** Step 31000 / Validation 31 *****
09/16 01:22:06 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:22:06 PM: Validating...
09/16 01:22:14 PM: Updating LR scheduler:
09/16 01:22:14 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:22:14 PM: 	# validation passes without improvement: 2
09/16 01:22:14 PM: Ran out of early stopping patience. Stopping training.
09/16 01:22:14 PM: edges-ner-ontonotes_loss: training: 0.068162 validation: 0.073535
09/16 01:22:14 PM: macro_avg: validation: 0.752441
09/16 01:22:14 PM: micro_avg: validation: 0.000000
09/16 01:22:14 PM: edges-ner-ontonotes_mcc: training: 0.755152 validation: 0.746241
09/16 01:22:14 PM: edges-ner-ontonotes_acc: training: 0.662928 validation: 0.656278
09/16 01:22:14 PM: edges-ner-ontonotes_precision: training: 0.850871 validation: 0.859760
09/16 01:22:14 PM: edges-ner-ontonotes_recall: training: 0.691763 validation: 0.668941
09/16 01:22:14 PM: edges-ner-ontonotes_f1: training: 0.763112 validation: 0.752441
09/16 01:22:14 PM: Global learning rate: 1.25e-05
09/16 01:22:14 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:22:14 PM: Stopped training after 31 validation checks
09/16 01:22:14 PM: Trained edges-ner-ontonotes for 31000 batches or 19.949 epochs
09/16 01:22:14 PM: ***** VALIDATION RESULTS *****
09/16 01:22:14 PM: edges-ner-ontonotes_f1 (for best val pass 21): edges-ner-ontonotes_loss: 0.07397, macro_avg: 0.75313, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.74616, edges-ner-ontonotes_acc: 0.65886, edges-ner-ontonotes_precision: 0.85375, edges-ner-ontonotes_recall: 0.67372, edges-ner-ontonotes_f1: 0.75313
09/16 01:22:14 PM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.16654, macro_avg: 0.50932, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.48165, edges-ner-ontonotes_acc: 0.44813, edges-ner-ontonotes_precision: 0.52553, edges-ner-ontonotes_recall: 0.49409, edges-ner-ontonotes_f1: 0.50932
09/16 01:22:14 PM: macro_avg (for best val pass 21): edges-ner-ontonotes_loss: 0.07397, macro_avg: 0.75313, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.74616, edges-ner-ontonotes_acc: 0.65886, edges-ner-ontonotes_precision: 0.85375, edges-ner-ontonotes_recall: 0.67372, edges-ner-ontonotes_f1: 0.75313
09/16 01:22:14 PM: Evaluating...
09/16 01:22:14 PM: Loaded model state from ./experiments/ner-ontonotes-RANDOM-only/run/edges-ner-ontonotes/model_state_target_train_val_21.best.th
09/16 01:22:14 PM: Evaluating on: edges-ner-ontonotes, split: val
09/16 01:22:34 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 01:22:34 PM: Finished evaluating on: edges-ner-ontonotes
09/16 01:22:35 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
09/16 01:22:39 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:22:39 PM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:22:39 PM: Evaluating on: edges-ner-ontonotes, split: test
09/16 01:22:53 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 01:22:53 PM: Finished evaluating on: edges-ner-ontonotes
09/16 01:22:53 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
09/16 01:22:55 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:22:55 PM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-RANDOM-only/run
09/16 01:22:55 PM: Writing results for split 'val' to ./experiments/ner-ontonotes-RANDOM-only/results.tsv
09/16 01:22:55 PM: micro_avg: 0.000, macro_avg: 0.752, edges-ner-ontonotes_mcc: 0.745, edges-ner-ontonotes_acc: 0.658, edges-ner-ontonotes_precision: 0.854, edges-ner-ontonotes_recall: 0.671, edges-ner-ontonotes_f1: 0.752
09/16 01:22:55 PM: Done!
09/16 01:22:55 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
