09/16 09:10:58 AM: Git branch: master
09/16 09:10:58 AM: Git SHA: 1a42459c6cbb693793b9c0d01bca567d99b0baac
09/16 09:10:58 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/srl-ontonotes-rte-top/",
  "exp_name": "experiments/srl-ontonotes-rte-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/srl-ontonotes-rte-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/rte",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/srl-ontonotes-rte-top__run",
  "run_dir": "./experiments/srl-ontonotes-rte-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-srl-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:10:58 AM: Saved config to ./experiments/srl-ontonotes-rte-top/run/params.conf
09/16 09:10:58 AM: Using random seed 1234
09/16 09:10:59 AM: Using GPU 0
09/16 09:10:59 AM: Loading tasks...
09/16 09:10:59 AM: Writing pre-preprocessed tasks to ./experiments/srl-ontonotes-rte-top/
09/16 09:10:59 AM: 	Creating task edges-srl-ontonotes from scratch.
09/16 09:11:05 AM: Read=231480, Skip=21590, Total=253070 from ./probing_data/edges/ontonotes/srl/train.json.retokenized.bert-base-uncased
09/16 09:11:05 AM: Read=32486, Skip=2811, Total=35297 from ./probing_data/edges/ontonotes/srl/development.json.retokenized.bert-base-uncased
09/16 09:11:05 AM: Read=23800, Skip=2915, Total=26715 from ./probing_data/edges/ontonotes/srl/test.json.retokenized.bert-base-uncased
09/16 09:11:08 AM: 	Task 'edges-srl-ontonotes': |train|=231480 |val|=32486 |test|=23800
09/16 09:11:08 AM: 	Finished loading tasks: edges-srl-ontonotes.
09/16 09:11:08 AM: 	Building vocab from scratch.
09/16 09:11:08 AM: 	Counting units for task edges-srl-ontonotes.
09/16 09:11:14 AM: 	Task 'edges-srl-ontonotes': adding vocab namespace 'edges-srl-ontonotes_labels'
09/16 09:11:15 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:11:15 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:11:15 AM: 	Saved vocab to ./experiments/srl-ontonotes-rte-top/vocab
09/16 09:11:15 AM: Loading token dictionary from ./experiments/srl-ontonotes-rte-top/vocab.
09/16 09:11:15 AM: 	Loaded vocab from ./experiments/srl-ontonotes-rte-top/vocab
09/16 09:11:15 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:11:15 AM: 	Vocab namespace tokens: size 23662
09/16 09:11:15 AM: 	Vocab namespace edges-srl-ontonotes_labels: size 66
09/16 09:11:15 AM: 	Vocab namespace chars: size 76
09/16 09:11:15 AM: 	Finished building vocab.
09/16 09:11:15 AM: 	Task edges-srl-ontonotes (train): Indexing from scratch.
09/16 09:11:54 AM: 	Task edges-srl-ontonotes (train): Saved 231480 instances to ./experiments/srl-ontonotes-rte-top/preproc/edges-srl-ontonotes__train_data
09/16 09:11:54 AM: 	Task edges-srl-ontonotes (val): Indexing from scratch.
09/16 09:11:58 AM: 	Task edges-srl-ontonotes (val): Saved 32486 instances to ./experiments/srl-ontonotes-rte-top/preproc/edges-srl-ontonotes__val_data
09/16 09:11:58 AM: 	Task edges-srl-ontonotes (test): Indexing from scratch.
09/16 09:12:02 AM: 	Task edges-srl-ontonotes (test): Saved 23800 instances to ./experiments/srl-ontonotes-rte-top/preproc/edges-srl-ontonotes__test_data
09/16 09:12:02 AM: 	Finished indexing tasks
09/16 09:12:02 AM: 	Creating trimmed target-only version of edges-srl-ontonotes train.
09/16 09:12:02 AM: 	  Training on 
09/16 09:12:02 AM: 	  Evaluating on edges-srl-ontonotes
09/16 09:12:02 AM: 	Finished loading tasks in 62.838s
09/16 09:12:02 AM: 	 Tasks: ['edges-srl-ontonotes']
09/16 09:12:02 AM: Building model...
09/16 09:12:02 AM: Using BERT model (bert-base-uncased).
09/16 09:12:02 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:12:02 AM: models/rte
09/16 09:12:02 AM: loading configuration file models/rte/config.json
09/16 09:12:02 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:12:02 AM: loading weights file models/rte/pytorch_model.bin
09/16 09:12:05 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpbgodnmd8
09/16 09:12:07 AM: copying /tmp/tmpbgodnmd8 to cache at ./experiments/srl-ontonotes-rte-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:07 AM: creating metadata file for ./experiments/srl-ontonotes-rte-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:07 AM: removing temp file /tmp/tmpbgodnmd8
09/16 09:12:07 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/srl-ontonotes-rte-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:07 AM: Initializing parameters
09/16 09:12:07 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:12:07 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:12:07 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:12:07 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:12:07 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:12:07 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:12:07 AM: 	Task 'edges-srl-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-srl-ontonotes"
}
09/16 09:12:11 AM: Model specification:
09/16 09:12:11 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-srl-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=66, bias=True)
      )
    )
  )
)
09/16 09:12:11 AM: Model parameters:
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:11 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 16896 with torch.Size([66, 256])
09/16 09:12:11 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 66 with torch.Size([66])
09/16 09:12:11 AM: Total number of parameters: 110155842 (1.10156e+08)
09/16 09:12:11 AM: Number of trainable parameters: 673602 (673602)
09/16 09:12:11 AM: Finished building model in 9.539s
09/16 09:12:11 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-srl-ontonotes 

09/16 09:12:26 AM: patience = 9
09/16 09:12:26 AM: val_interval = 1000
09/16 09:12:26 AM: max_vals = 250
09/16 09:12:26 AM: cuda_device = 0
09/16 09:12:26 AM: grad_norm = 5.0
09/16 09:12:26 AM: grad_clipping = None
09/16 09:12:26 AM: lr_decay = 0.99
09/16 09:12:26 AM: min_lr = 1e-06
09/16 09:12:26 AM: keep_all_checkpoints = 0
09/16 09:12:26 AM: val_data_limit = 5000
09/16 09:12:26 AM: max_epochs = -1
09/16 09:12:26 AM: dec_val_scale = 250
09/16 09:12:26 AM: training_data_fraction = 1
09/16 09:12:26 AM: type = adam
09/16 09:12:26 AM: parameter_groups = None
09/16 09:12:26 AM: Number of trainable parameters: 673602
09/16 09:12:26 AM: infer_type_and_cast = True
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: lr = 0.0001
09/16 09:12:26 AM: amsgrad = True
09/16 09:12:26 AM: type = reduce_on_plateau
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: mode = max
09/16 09:12:26 AM: factor = 0.5
09/16 09:12:26 AM: patience = 3
09/16 09:12:26 AM: threshold = 0.0001
09/16 09:12:26 AM: threshold_mode = abs
09/16 09:12:26 AM: verbose = True
09/16 09:12:26 AM: type = adam
09/16 09:12:26 AM: parameter_groups = None
09/16 09:12:26 AM: Number of trainable parameters: 673602
09/16 09:12:26 AM: infer_type_and_cast = True
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: lr = 0.0001
09/16 09:12:26 AM: amsgrad = True
09/16 09:12:26 AM: type = reduce_on_plateau
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: mode = max
09/16 09:12:26 AM: factor = 0.5
09/16 09:12:26 AM: patience = 3
09/16 09:12:26 AM: threshold = 0.0001
09/16 09:12:26 AM: threshold_mode = abs
09/16 09:12:26 AM: verbose = True
09/16 09:12:26 AM: Starting training without restoring from a checkpoint.
09/16 09:12:26 AM: Training examples per task, before any subsampling: {'edges-srl-ontonotes': 231480}
09/16 09:12:26 AM: Beginning training with stopping criteria based on metric: edges-srl-ontonotes_f1
09/16 09:12:36 AM: Update 120: task edges-srl-ontonotes, batch 120 (120): mcc: 0.0644, acc: 0.0441, precision: 0.0649, recall: 0.1045, f1: 0.0801, edges-srl-ontonotes_loss: 0.2013
09/16 09:12:46 AM: Update 231: task edges-srl-ontonotes, batch 231 (231): mcc: 0.0540, acc: 0.0307, precision: 0.0734, recall: 0.0614, f1: 0.0669, edges-srl-ontonotes_loss: 0.1393
09/16 09:12:56 AM: Update 342: task edges-srl-ontonotes, batch 342 (342): mcc: 0.0771, acc: 0.0475, precision: 0.1132, recall: 0.0683, f1: 0.0852, edges-srl-ontonotes_loss: 0.1116
09/16 09:13:06 AM: Update 466: task edges-srl-ontonotes, batch 466 (466): mcc: 0.1566, acc: 0.1049, precision: 0.2263, recall: 0.1215, f1: 0.1582, edges-srl-ontonotes_loss: 0.0937
09/16 09:13:16 AM: Update 591: task edges-srl-ontonotes, batch 591 (591): mcc: 0.2390, acc: 0.1633, precision: 0.3396, recall: 0.1800, f1: 0.2353, edges-srl-ontonotes_loss: 0.0818
09/16 09:13:26 AM: Update 683: task edges-srl-ontonotes, batch 683 (683): mcc: 0.2882, acc: 0.1989, precision: 0.4049, recall: 0.2163, f1: 0.2820, edges-srl-ontonotes_loss: 0.0754
09/16 09:13:36 AM: Update 796: task edges-srl-ontonotes, batch 796 (796): mcc: 0.3356, acc: 0.2345, precision: 0.4648, recall: 0.2529, f1: 0.3276, edges-srl-ontonotes_loss: 0.0693
09/16 09:13:47 AM: Update 890: task edges-srl-ontonotes, batch 890 (890): mcc: 0.3705, acc: 0.2620, precision: 0.5066, recall: 0.2812, f1: 0.3616, edges-srl-ontonotes_loss: 0.0652
09/16 09:13:57 AM: Update 976: task edges-srl-ontonotes, batch 976 (976): mcc: 0.3964, acc: 0.2829, precision: 0.5361, recall: 0.3031, f1: 0.3872, edges-srl-ontonotes_loss: 0.0620
09/16 09:13:59 AM: ***** Step 1000 / Validation 1 *****
09/16 09:13:59 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:13:59 AM: Validating...
09/16 09:14:07 AM: Evaluate: task edges-srl-ontonotes, batch 86 (157): mcc: 0.6922, acc: 0.5490, precision: 0.8642, recall: 0.5600, f1: 0.6796, edges-srl-ontonotes_loss: 0.0263
09/16 09:14:13 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:14:13 AM: Best result seen so far for micro.
09/16 09:14:13 AM: Best result seen so far for macro.
09/16 09:14:13 AM: Updating LR scheduler:
09/16 09:14:13 AM: 	Best result seen so far for macro_avg: 0.691
09/16 09:14:13 AM: 	# validation passes without improvement: 0
09/16 09:14:13 AM: edges-srl-ontonotes_loss: training: 0.061237 validation: 0.025762
09/16 09:14:13 AM: macro_avg: validation: 0.691207
09/16 09:14:13 AM: micro_avg: validation: 0.000000
09/16 09:14:13 AM: edges-srl-ontonotes_mcc: training: 0.402700 validation: 0.702427
09/16 09:14:13 AM: edges-srl-ontonotes_acc: training: 0.287998 validation: 0.563159
09/16 09:14:13 AM: edges-srl-ontonotes_precision: training: 0.543102 validation: 0.867302
09/16 09:14:13 AM: edges-srl-ontonotes_recall: training: 0.308513 validation: 0.574552
09/16 09:14:13 AM: edges-srl-ontonotes_f1: training: 0.393497 validation: 0.691207
09/16 09:14:13 AM: Global learning rate: 0.0001
09/16 09:14:13 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:14:17 AM: Update 1037: task edges-srl-ontonotes, batch 37 (1037): mcc: 0.6416, acc: 0.4941, precision: 0.7920, recall: 0.5267, f1: 0.6327, edges-srl-ontonotes_loss: 0.0295
09/16 09:14:27 AM: Update 1144: task edges-srl-ontonotes, batch 144 (1144): mcc: 0.6601, acc: 0.5165, precision: 0.8033, recall: 0.5491, f1: 0.6523, edges-srl-ontonotes_loss: 0.0283
09/16 09:14:37 AM: Update 1253: task edges-srl-ontonotes, batch 253 (1253): mcc: 0.6653, acc: 0.5266, precision: 0.8010, recall: 0.5595, f1: 0.6588, edges-srl-ontonotes_loss: 0.0277
09/16 09:14:47 AM: Update 1355: task edges-srl-ontonotes, batch 355 (1355): mcc: 0.6685, acc: 0.5322, precision: 0.7996, recall: 0.5658, f1: 0.6627, edges-srl-ontonotes_loss: 0.0272
09/16 09:14:57 AM: Update 1472: task edges-srl-ontonotes, batch 472 (1472): mcc: 0.6755, acc: 0.5410, precision: 0.8026, recall: 0.5754, f1: 0.6703, edges-srl-ontonotes_loss: 0.0267
09/16 09:15:07 AM: Update 1574: task edges-srl-ontonotes, batch 574 (1574): mcc: 0.6814, acc: 0.5484, precision: 0.8045, recall: 0.5840, f1: 0.6767, edges-srl-ontonotes_loss: 0.0262
09/16 09:15:17 AM: Update 1688: task edges-srl-ontonotes, batch 688 (1688): mcc: 0.6798, acc: 0.5461, precision: 0.8020, recall: 0.5831, f1: 0.6752, edges-srl-ontonotes_loss: 0.0261
09/16 09:15:27 AM: Update 1786: task edges-srl-ontonotes, batch 786 (1786): mcc: 0.6801, acc: 0.5465, precision: 0.8013, recall: 0.5840, f1: 0.6756, edges-srl-ontonotes_loss: 0.0260
09/16 09:15:40 AM: Update 1879: task edges-srl-ontonotes, batch 879 (1879): mcc: 0.6804, acc: 0.5469, precision: 0.8006, recall: 0.5851, f1: 0.6761, edges-srl-ontonotes_loss: 0.0259
09/16 09:15:50 AM: Update 1980: task edges-srl-ontonotes, batch 980 (1980): mcc: 0.6823, acc: 0.5492, precision: 0.8008, recall: 0.5881, f1: 0.6782, edges-srl-ontonotes_loss: 0.0257
09/16 09:15:52 AM: ***** Step 2000 / Validation 2 *****
09/16 09:15:52 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:15:52 AM: Validating...
09/16 09:16:00 AM: Evaluate: task edges-srl-ontonotes, batch 97 (157): mcc: 0.7451, acc: 0.6266, precision: 0.8648, recall: 0.6476, f1: 0.7406, edges-srl-ontonotes_loss: 0.0206
09/16 09:16:05 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:16:05 AM: Best result seen so far for macro.
09/16 09:16:05 AM: Updating LR scheduler:
09/16 09:16:05 AM: 	Best result seen so far for macro_avg: 0.749
09/16 09:16:05 AM: 	# validation passes without improvement: 0
09/16 09:16:05 AM: edges-srl-ontonotes_loss: training: 0.025699 validation: 0.020188
09/16 09:16:05 AM: macro_avg: validation: 0.748830
09/16 09:16:05 AM: micro_avg: validation: 0.000000
09/16 09:16:05 AM: edges-srl-ontonotes_mcc: training: 0.682506 validation: 0.752715
09/16 09:16:05 AM: edges-srl-ontonotes_acc: training: 0.549415 validation: 0.637595
09/16 09:16:05 AM: edges-srl-ontonotes_precision: training: 0.800550 validation: 0.867024
09/16 09:16:05 AM: edges-srl-ontonotes_recall: training: 0.588740 validation: 0.658995
09/16 09:16:05 AM: edges-srl-ontonotes_f1: training: 0.678499 validation: 0.748830
09/16 09:16:05 AM: Global learning rate: 0.0001
09/16 09:16:05 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:16:10 AM: Update 2054: task edges-srl-ontonotes, batch 54 (2054): mcc: 0.7039, acc: 0.5806, precision: 0.8058, recall: 0.6217, f1: 0.7019, edges-srl-ontonotes_loss: 0.0238
09/16 09:16:20 AM: Update 2171: task edges-srl-ontonotes, batch 171 (2171): mcc: 0.6954, acc: 0.5690, precision: 0.7986, recall: 0.6124, f1: 0.6932, edges-srl-ontonotes_loss: 0.0241
09/16 09:16:30 AM: Update 2266: task edges-srl-ontonotes, batch 266 (2266): mcc: 0.7025, acc: 0.5783, precision: 0.8023, recall: 0.6219, f1: 0.7007, edges-srl-ontonotes_loss: 0.0237
09/16 09:16:40 AM: Update 2363: task edges-srl-ontonotes, batch 363 (2363): mcc: 0.7085, acc: 0.5862, precision: 0.8048, recall: 0.6305, f1: 0.7071, edges-srl-ontonotes_loss: 0.0232
09/16 09:16:50 AM: Update 2468: task edges-srl-ontonotes, batch 468 (2468): mcc: 0.7122, acc: 0.5917, precision: 0.8053, recall: 0.6367, f1: 0.7111, edges-srl-ontonotes_loss: 0.0229
09/16 09:17:00 AM: Update 2568: task edges-srl-ontonotes, batch 568 (2568): mcc: 0.7168, acc: 0.5980, precision: 0.8079, recall: 0.6427, f1: 0.7159, edges-srl-ontonotes_loss: 0.0226
09/16 09:17:10 AM: Update 2677: task edges-srl-ontonotes, batch 677 (2677): mcc: 0.7190, acc: 0.6014, precision: 0.8084, recall: 0.6463, f1: 0.7183, edges-srl-ontonotes_loss: 0.0224
09/16 09:17:20 AM: Update 2788: task edges-srl-ontonotes, batch 788 (2788): mcc: 0.7212, acc: 0.6042, precision: 0.8092, recall: 0.6494, f1: 0.7205, edges-srl-ontonotes_loss: 0.0222
09/16 09:17:30 AM: Update 2861: task edges-srl-ontonotes, batch 861 (2861): mcc: 0.7228, acc: 0.6067, precision: 0.8100, recall: 0.6517, f1: 0.7223, edges-srl-ontonotes_loss: 0.0221
09/16 09:17:40 AM: Update 2960: task edges-srl-ontonotes, batch 960 (2960): mcc: 0.7245, acc: 0.6086, precision: 0.8107, recall: 0.6541, f1: 0.7240, edges-srl-ontonotes_loss: 0.0220
09/16 09:17:45 AM: ***** Step 3000 / Validation 3 *****
09/16 09:17:45 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:17:45 AM: Validating...
09/16 09:17:50 AM: Evaluate: task edges-srl-ontonotes, batch 54 (157): mcc: 0.7478, acc: 0.6430, precision: 0.8409, recall: 0.6710, f1: 0.7464, edges-srl-ontonotes_loss: 0.0202
09/16 09:18:00 AM: Evaluate: task edges-srl-ontonotes, batch 137 (157): mcc: 0.7619, acc: 0.6603, precision: 0.8511, recall: 0.6878, f1: 0.7608, edges-srl-ontonotes_loss: 0.0190
09/16 09:18:03 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:18:03 AM: Best result seen so far for macro.
09/16 09:18:03 AM: Updating LR scheduler:
09/16 09:18:03 AM: 	Best result seen so far for macro_avg: 0.761
09/16 09:18:03 AM: 	# validation passes without improvement: 0
09/16 09:18:03 AM: edges-srl-ontonotes_loss: training: 0.021899 validation: 0.019096
09/16 09:18:03 AM: macro_avg: validation: 0.761354
09/16 09:18:03 AM: micro_avg: validation: 0.000000
09/16 09:18:03 AM: edges-srl-ontonotes_mcc: training: 0.725375 validation: 0.762393
09/16 09:18:03 AM: edges-srl-ontonotes_acc: training: 0.609631 validation: 0.661381
09/16 09:18:03 AM: edges-srl-ontonotes_precision: training: 0.811293 validation: 0.850546
09/16 09:18:03 AM: edges-srl-ontonotes_recall: training: 0.655156 validation: 0.689092
09/16 09:18:03 AM: edges-srl-ontonotes_f1: training: 0.724912 validation: 0.761354
09/16 09:18:03 AM: Global learning rate: 0.0001
09/16 09:18:03 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:18:10 AM: Update 3063: task edges-srl-ontonotes, batch 63 (3063): mcc: 0.7611, acc: 0.6537, precision: 0.8316, recall: 0.7026, f1: 0.7617, edges-srl-ontonotes_loss: 0.0197
09/16 09:18:21 AM: Update 3152: task edges-srl-ontonotes, batch 152 (3152): mcc: 0.7534, acc: 0.6453, precision: 0.8281, recall: 0.6915, f1: 0.7537, edges-srl-ontonotes_loss: 0.0201
09/16 09:18:31 AM: Update 3256: task edges-srl-ontonotes, batch 256 (3256): mcc: 0.7469, acc: 0.6386, precision: 0.8229, recall: 0.6843, f1: 0.7472, edges-srl-ontonotes_loss: 0.0205
09/16 09:18:41 AM: Update 3371: task edges-srl-ontonotes, batch 371 (3371): mcc: 0.7428, acc: 0.6349, precision: 0.8210, recall: 0.6784, f1: 0.7430, edges-srl-ontonotes_loss: 0.0206
09/16 09:18:51 AM: Update 3476: task edges-srl-ontonotes, batch 476 (3476): mcc: 0.7409, acc: 0.6329, precision: 0.8199, recall: 0.6759, f1: 0.7410, edges-srl-ontonotes_loss: 0.0207
09/16 09:19:01 AM: Update 3586: task edges-srl-ontonotes, batch 586 (3586): mcc: 0.7408, acc: 0.6327, precision: 0.8195, recall: 0.6759, f1: 0.7408, edges-srl-ontonotes_loss: 0.0207
09/16 09:19:11 AM: Update 3690: task edges-srl-ontonotes, batch 690 (3690): mcc: 0.7395, acc: 0.6304, precision: 0.8186, recall: 0.6745, f1: 0.7396, edges-srl-ontonotes_loss: 0.0207
09/16 09:19:21 AM: Update 3789: task edges-srl-ontonotes, batch 789 (3789): mcc: 0.7394, acc: 0.6305, precision: 0.8180, recall: 0.6748, f1: 0.7395, edges-srl-ontonotes_loss: 0.0207
09/16 09:19:31 AM: Update 3911: task edges-srl-ontonotes, batch 911 (3911): mcc: 0.7391, acc: 0.6306, precision: 0.8171, recall: 0.6750, f1: 0.7393, edges-srl-ontonotes_loss: 0.0206
09/16 09:19:40 AM: ***** Step 4000 / Validation 4 *****
09/16 09:19:40 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:19:40 AM: Validating...
09/16 09:19:41 AM: Evaluate: task edges-srl-ontonotes, batch 6 (157): mcc: 0.7875, acc: 0.7091, precision: 0.8555, recall: 0.7303, f1: 0.7879, edges-srl-ontonotes_loss: 0.0186
09/16 09:19:52 AM: Evaluate: task edges-srl-ontonotes, batch 104 (157): mcc: 0.7552, acc: 0.6502, precision: 0.8574, recall: 0.6709, f1: 0.7528, edges-srl-ontonotes_loss: 0.0192
09/16 09:19:58 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:19:58 AM: Best result seen so far for macro.
09/16 09:19:58 AM: Updating LR scheduler:
09/16 09:19:58 AM: 	Best result seen so far for macro_avg: 0.764
09/16 09:19:58 AM: 	# validation passes without improvement: 0
09/16 09:19:58 AM: edges-srl-ontonotes_loss: training: 0.020634 validation: 0.018630
09/16 09:19:58 AM: macro_avg: validation: 0.763532
09/16 09:19:58 AM: micro_avg: validation: 0.000000
09/16 09:19:58 AM: edges-srl-ontonotes_mcc: training: 0.738589 validation: 0.765313
09/16 09:19:58 AM: edges-srl-ontonotes_acc: training: 0.629953 validation: 0.664229
09/16 09:19:58 AM: edges-srl-ontonotes_precision: training: 0.816407 validation: 0.860438
09/16 09:19:58 AM: edges-srl-ontonotes_recall: training: 0.674634 validation: 0.686244
09/16 09:19:58 AM: edges-srl-ontonotes_f1: training: 0.738780 validation: 0.763532
09/16 09:19:58 AM: Global learning rate: 0.0001
09/16 09:19:58 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:20:02 AM: Update 4041: task edges-srl-ontonotes, batch 41 (4041): mcc: 0.7352, acc: 0.6278, precision: 0.8123, recall: 0.6719, f1: 0.7354, edges-srl-ontonotes_loss: 0.0208
09/16 09:20:12 AM: Update 4129: task edges-srl-ontonotes, batch 129 (4129): mcc: 0.7372, acc: 0.6323, precision: 0.8130, recall: 0.6750, f1: 0.7376, edges-srl-ontonotes_loss: 0.0206
09/16 09:20:22 AM: Update 4228: task edges-srl-ontonotes, batch 228 (4228): mcc: 0.7455, acc: 0.6411, precision: 0.8195, recall: 0.6845, f1: 0.7459, edges-srl-ontonotes_loss: 0.0201
09/16 09:20:32 AM: Update 4337: task edges-srl-ontonotes, batch 337 (4337): mcc: 0.7496, acc: 0.6467, precision: 0.8224, recall: 0.6896, f1: 0.7501, edges-srl-ontonotes_loss: 0.0199
09/16 09:20:42 AM: Update 4442: task edges-srl-ontonotes, batch 442 (4442): mcc: 0.7500, acc: 0.6474, precision: 0.8217, recall: 0.6908, f1: 0.7506, edges-srl-ontonotes_loss: 0.0199
09/16 09:20:52 AM: Update 4547: task edges-srl-ontonotes, batch 547 (4547): mcc: 0.7519, acc: 0.6495, precision: 0.8229, recall: 0.6932, f1: 0.7525, edges-srl-ontonotes_loss: 0.0197
09/16 09:21:02 AM: Update 4663: task edges-srl-ontonotes, batch 663 (4663): mcc: 0.7530, acc: 0.6510, precision: 0.8239, recall: 0.6944, f1: 0.7536, edges-srl-ontonotes_loss: 0.0196
09/16 09:21:12 AM: Update 4741: task edges-srl-ontonotes, batch 741 (4741): mcc: 0.7515, acc: 0.6496, precision: 0.8232, recall: 0.6923, f1: 0.7521, edges-srl-ontonotes_loss: 0.0198
09/16 09:21:22 AM: Update 4838: task edges-srl-ontonotes, batch 838 (4838): mcc: 0.7494, acc: 0.6471, precision: 0.8219, recall: 0.6895, f1: 0.7499, edges-srl-ontonotes_loss: 0.0199
09/16 09:21:32 AM: Update 4937: task edges-srl-ontonotes, batch 937 (4937): mcc: 0.7484, acc: 0.6459, precision: 0.8218, recall: 0.6879, f1: 0.7489, edges-srl-ontonotes_loss: 0.0200
09/16 09:21:38 AM: ***** Step 5000 / Validation 5 *****
09/16 09:21:38 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:21:38 AM: Validating...
09/16 09:21:42 AM: Evaluate: task edges-srl-ontonotes, batch 50 (157): mcc: 0.7619, acc: 0.6711, precision: 0.8379, recall: 0.6987, f1: 0.7620, edges-srl-ontonotes_loss: 0.0193
09/16 09:21:52 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:21:52 AM: Best result seen so far for macro.
09/16 09:21:52 AM: Updating LR scheduler:
09/16 09:21:52 AM: 	Best result seen so far for macro_avg: 0.768
09/16 09:21:52 AM: 	# validation passes without improvement: 0
09/16 09:21:52 AM: edges-srl-ontonotes_loss: training: 0.019974 validation: 0.018676
09/16 09:21:52 AM: macro_avg: validation: 0.768229
09/16 09:21:52 AM: micro_avg: validation: 0.000000
09/16 09:21:52 AM: edges-srl-ontonotes_mcc: training: 0.748515 validation: 0.768294
09/16 09:21:52 AM: edges-srl-ontonotes_acc: training: 0.645950 validation: 0.676314
09/16 09:21:52 AM: edges-srl-ontonotes_precision: training: 0.822253 validation: 0.845428
09/16 09:21:52 AM: edges-srl-ontonotes_recall: training: 0.687673 validation: 0.703949
09/16 09:21:52 AM: edges-srl-ontonotes_f1: training: 0.748965 validation: 0.768229
09/16 09:21:52 AM: Global learning rate: 0.0001
09/16 09:21:52 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:21:55 AM: Update 5009: task edges-srl-ontonotes, batch 9 (5009): mcc: 0.7357, acc: 0.6350, precision: 0.8096, recall: 0.6751, f1: 0.7362, edges-srl-ontonotes_loss: 0.0216
09/16 09:22:05 AM: Update 5135: task edges-srl-ontonotes, batch 135 (5135): mcc: 0.7548, acc: 0.6505, precision: 0.8282, recall: 0.6941, f1: 0.7553, edges-srl-ontonotes_loss: 0.0194
09/16 09:22:15 AM: Update 5263: task edges-srl-ontonotes, batch 263 (5263): mcc: 0.7642, acc: 0.6629, precision: 0.8341, recall: 0.7060, f1: 0.7647, edges-srl-ontonotes_loss: 0.0189
09/16 09:22:25 AM: Update 5396: task edges-srl-ontonotes, batch 396 (5396): mcc: 0.7724, acc: 0.6731, precision: 0.8394, recall: 0.7167, f1: 0.7732, edges-srl-ontonotes_loss: 0.0182
09/16 09:22:35 AM: Update 5520: task edges-srl-ontonotes, batch 520 (5520): mcc: 0.7808, acc: 0.6836, precision: 0.8457, recall: 0.7265, f1: 0.7816, edges-srl-ontonotes_loss: 0.0177
09/16 09:22:46 AM: Update 5635: task edges-srl-ontonotes, batch 635 (5635): mcc: 0.7874, acc: 0.6922, precision: 0.8501, recall: 0.7348, f1: 0.7883, edges-srl-ontonotes_loss: 0.0172
09/16 09:22:56 AM: Update 5753: task edges-srl-ontonotes, batch 753 (5753): mcc: 0.7897, acc: 0.6949, precision: 0.8512, recall: 0.7380, f1: 0.7906, edges-srl-ontonotes_loss: 0.0171
09/16 09:23:06 AM: Update 5878: task edges-srl-ontonotes, batch 878 (5878): mcc: 0.7933, acc: 0.6994, precision: 0.8536, recall: 0.7427, f1: 0.7943, edges-srl-ontonotes_loss: 0.0168
09/16 09:23:16 AM: Update 5965: task edges-srl-ontonotes, batch 965 (5965): mcc: 0.7946, acc: 0.7010, precision: 0.8540, recall: 0.7446, f1: 0.7956, edges-srl-ontonotes_loss: 0.0168
09/16 09:23:19 AM: ***** Step 6000 / Validation 6 *****
09/16 09:23:19 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:23:19 AM: Validating...
09/16 09:23:26 AM: Evaluate: task edges-srl-ontonotes, batch 90 (157): mcc: 0.7963, acc: 0.7112, precision: 0.8790, recall: 0.7263, f1: 0.7954, edges-srl-ontonotes_loss: 0.0167
09/16 09:23:32 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:23:32 AM: Best result seen so far for macro.
09/16 09:23:32 AM: Updating LR scheduler:
09/16 09:23:32 AM: 	Best result seen so far for macro_avg: 0.802
09/16 09:23:32 AM: 	# validation passes without improvement: 0
09/16 09:23:32 AM: edges-srl-ontonotes_loss: training: 0.016741 validation: 0.016316
09/16 09:23:32 AM: macro_avg: validation: 0.802420
09/16 09:23:32 AM: micro_avg: validation: 0.000000
09/16 09:23:32 AM: edges-srl-ontonotes_mcc: training: 0.795376 validation: 0.802551
09/16 09:23:32 AM: edges-srl-ontonotes_acc: training: 0.702060 validation: 0.721499
09/16 09:23:32 AM: edges-srl-ontonotes_precision: training: 0.854528 validation: 0.875945
09/16 09:23:32 AM: edges-srl-ontonotes_recall: training: 0.745670 validation: 0.740282
09/16 09:23:32 AM: edges-srl-ontonotes_f1: training: 0.796396 validation: 0.802420
09/16 09:23:32 AM: Global learning rate: 0.0001
09/16 09:23:32 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:23:36 AM: Update 6060: task edges-srl-ontonotes, batch 60 (6060): mcc: 0.8118, acc: 0.7204, precision: 0.8659, recall: 0.7660, f1: 0.8129, edges-srl-ontonotes_loss: 0.0157
09/16 09:23:46 AM: Update 6207: task edges-srl-ontonotes, batch 207 (6207): mcc: 0.8154, acc: 0.7287, precision: 0.8659, recall: 0.7728, f1: 0.8168, edges-srl-ontonotes_loss: 0.0153
09/16 09:23:56 AM: Update 6338: task edges-srl-ontonotes, batch 338 (6338): mcc: 0.8165, acc: 0.7308, precision: 0.8675, recall: 0.7734, f1: 0.8177, edges-srl-ontonotes_loss: 0.0152
09/16 09:24:06 AM: Update 6482: task edges-srl-ontonotes, batch 482 (6482): mcc: 0.8196, acc: 0.7356, precision: 0.8699, recall: 0.7769, f1: 0.8208, edges-srl-ontonotes_loss: 0.0150
09/16 09:24:17 AM: Update 6607: task edges-srl-ontonotes, batch 607 (6607): mcc: 0.8185, acc: 0.7345, precision: 0.8691, recall: 0.7757, f1: 0.8197, edges-srl-ontonotes_loss: 0.0151
09/16 09:24:27 AM: Update 6725: task edges-srl-ontonotes, batch 725 (6725): mcc: 0.8124, acc: 0.7273, precision: 0.8642, recall: 0.7688, f1: 0.8137, edges-srl-ontonotes_loss: 0.0156
09/16 09:24:37 AM: Update 6838: task edges-srl-ontonotes, batch 838 (6838): mcc: 0.8086, acc: 0.7227, precision: 0.8610, recall: 0.7644, f1: 0.8098, edges-srl-ontonotes_loss: 0.0158
09/16 09:24:47 AM: Update 6945: task edges-srl-ontonotes, batch 945 (6945): mcc: 0.8043, acc: 0.7176, precision: 0.8577, recall: 0.7594, f1: 0.8055, edges-srl-ontonotes_loss: 0.0161
09/16 09:24:52 AM: ***** Step 7000 / Validation 7 *****
09/16 09:24:52 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:24:52 AM: Validating...
09/16 09:24:57 AM: Evaluate: task edges-srl-ontonotes, batch 60 (157): mcc: 0.8026, acc: 0.7280, precision: 0.8715, recall: 0.7441, f1: 0.8028, edges-srl-ontonotes_loss: 0.0167
09/16 09:25:07 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:25:07 AM: Best result seen so far for macro.
09/16 09:25:07 AM: Updating LR scheduler:
09/16 09:25:07 AM: 	Best result seen so far for macro_avg: 0.817
09/16 09:25:07 AM: 	# validation passes without improvement: 0
09/16 09:25:07 AM: edges-srl-ontonotes_loss: training: 0.016294 validation: 0.015500
09/16 09:25:07 AM: macro_avg: validation: 0.817415
09/16 09:25:07 AM: micro_avg: validation: 0.000000
09/16 09:25:07 AM: edges-srl-ontonotes_mcc: training: 0.801208 validation: 0.816816
09/16 09:25:07 AM: edges-srl-ontonotes_acc: training: 0.713851 validation: 0.747133
09/16 09:25:07 AM: edges-srl-ontonotes_precision: training: 0.855550 validation: 0.878161
09/16 09:25:07 AM: edges-srl-ontonotes_recall: training: 0.755587 validation: 0.764529
09/16 09:25:07 AM: edges-srl-ontonotes_f1: training: 0.802467 validation: 0.817415
09/16 09:25:07 AM: Global learning rate: 0.0001
09/16 09:25:07 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:25:07 AM: Update 7002: task edges-srl-ontonotes, batch 2 (7002): mcc: 0.8096, acc: 0.7188, precision: 0.8652, recall: 0.7625, f1: 0.8106, edges-srl-ontonotes_loss: 0.0175
09/16 09:25:17 AM: Update 7118: task edges-srl-ontonotes, batch 118 (7118): mcc: 0.7729, acc: 0.6790, precision: 0.8312, recall: 0.7246, f1: 0.7742, edges-srl-ontonotes_loss: 0.0182
09/16 09:25:27 AM: Update 7224: task edges-srl-ontonotes, batch 224 (7224): mcc: 0.7700, acc: 0.6777, precision: 0.8311, recall: 0.7193, f1: 0.7712, edges-srl-ontonotes_loss: 0.0186
09/16 09:25:37 AM: Update 7326: task edges-srl-ontonotes, batch 326 (7326): mcc: 0.7770, acc: 0.6864, precision: 0.8377, recall: 0.7265, f1: 0.7782, edges-srl-ontonotes_loss: 0.0182
09/16 09:25:47 AM: Update 7430: task edges-srl-ontonotes, batch 430 (7430): mcc: 0.7791, acc: 0.6882, precision: 0.8399, recall: 0.7285, f1: 0.7802, edges-srl-ontonotes_loss: 0.0179
09/16 09:25:57 AM: Update 7539: task edges-srl-ontonotes, batch 539 (7539): mcc: 0.7835, acc: 0.6935, precision: 0.8434, recall: 0.7336, f1: 0.7846, edges-srl-ontonotes_loss: 0.0176
09/16 09:26:07 AM: Update 7631: task edges-srl-ontonotes, batch 631 (7631): mcc: 0.7859, acc: 0.6963, precision: 0.8451, recall: 0.7364, f1: 0.7870, edges-srl-ontonotes_loss: 0.0174
09/16 09:26:17 AM: Update 7732: task edges-srl-ontonotes, batch 732 (7732): mcc: 0.7873, acc: 0.6984, precision: 0.8462, recall: 0.7382, f1: 0.7885, edges-srl-ontonotes_loss: 0.0173
09/16 09:26:27 AM: Update 7835: task edges-srl-ontonotes, batch 835 (7835): mcc: 0.7900, acc: 0.7019, precision: 0.8483, recall: 0.7412, f1: 0.7911, edges-srl-ontonotes_loss: 0.0171
09/16 09:26:37 AM: Update 7930: task edges-srl-ontonotes, batch 930 (7930): mcc: 0.7903, acc: 0.7023, precision: 0.8483, recall: 0.7417, f1: 0.7914, edges-srl-ontonotes_loss: 0.0171
09/16 09:26:44 AM: ***** Step 8000 / Validation 8 *****
09/16 09:26:44 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:26:44 AM: Validating...
09/16 09:26:48 AM: Evaluate: task edges-srl-ontonotes, batch 38 (157): mcc: 0.8141, acc: 0.7484, precision: 0.8736, recall: 0.7635, f1: 0.8148, edges-srl-ontonotes_loss: 0.0153
09/16 09:26:58 AM: Evaluate: task edges-srl-ontonotes, batch 147 (157): mcc: 0.8277, acc: 0.7626, precision: 0.8831, recall: 0.7803, f1: 0.8285, edges-srl-ontonotes_loss: 0.0142
09/16 09:26:58 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:26:58 AM: Best result seen so far for macro.
09/16 09:26:58 AM: Updating LR scheduler:
09/16 09:26:58 AM: 	Best result seen so far for macro_avg: 0.827
09/16 09:26:58 AM: 	# validation passes without improvement: 0
09/16 09:26:58 AM: edges-srl-ontonotes_loss: training: 0.017116 validation: 0.014411
09/16 09:26:58 AM: macro_avg: validation: 0.827383
09/16 09:26:58 AM: micro_avg: validation: 0.000000
09/16 09:26:58 AM: edges-srl-ontonotes_mcc: training: 0.790256 validation: 0.826560
09/16 09:26:58 AM: edges-srl-ontonotes_acc: training: 0.702205 validation: 0.760988
09/16 09:26:58 AM: edges-srl-ontonotes_precision: training: 0.848066 validation: 0.882568
09/16 09:26:58 AM: edges-srl-ontonotes_recall: training: 0.741888 validation: 0.778693
09/16 09:26:58 AM: edges-srl-ontonotes_f1: training: 0.791432 validation: 0.827383
09/16 09:26:59 AM: Global learning rate: 0.0001
09/16 09:26:59 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:27:08 AM: Update 8096: task edges-srl-ontonotes, batch 96 (8096): mcc: 0.7893, acc: 0.7083, precision: 0.8463, recall: 0.7416, f1: 0.7905, edges-srl-ontonotes_loss: 0.0172
09/16 09:27:19 AM: Update 8186: task edges-srl-ontonotes, batch 186 (8186): mcc: 0.7830, acc: 0.6990, precision: 0.8415, recall: 0.7342, f1: 0.7842, edges-srl-ontonotes_loss: 0.0175
09/16 09:27:29 AM: Update 8289: task edges-srl-ontonotes, batch 289 (8289): mcc: 0.7778, acc: 0.6915, precision: 0.8371, recall: 0.7285, f1: 0.7790, edges-srl-ontonotes_loss: 0.0179
09/16 09:27:39 AM: Update 8408: task edges-srl-ontonotes, batch 408 (8408): mcc: 0.7742, acc: 0.6865, precision: 0.8354, recall: 0.7234, f1: 0.7754, edges-srl-ontonotes_loss: 0.0182
09/16 09:27:49 AM: Update 8500: task edges-srl-ontonotes, batch 500 (8500): mcc: 0.7732, acc: 0.6847, precision: 0.8351, recall: 0.7217, f1: 0.7743, edges-srl-ontonotes_loss: 0.0182
09/16 09:27:59 AM: Update 8604: task edges-srl-ontonotes, batch 604 (8604): mcc: 0.7714, acc: 0.6825, precision: 0.8335, recall: 0.7198, f1: 0.7725, edges-srl-ontonotes_loss: 0.0182
09/16 09:28:09 AM: Update 8740: task edges-srl-ontonotes, batch 740 (8740): mcc: 0.7719, acc: 0.6829, precision: 0.8339, recall: 0.7204, f1: 0.7730, edges-srl-ontonotes_loss: 0.0182
09/16 09:28:19 AM: Update 8832: task edges-srl-ontonotes, batch 832 (8832): mcc: 0.7714, acc: 0.6821, precision: 0.8333, recall: 0.7199, f1: 0.7725, edges-srl-ontonotes_loss: 0.0182
09/16 09:28:29 AM: Update 8915: task edges-srl-ontonotes, batch 915 (8915): mcc: 0.7689, acc: 0.6791, precision: 0.8311, recall: 0.7174, f1: 0.7701, edges-srl-ontonotes_loss: 0.0183
09/16 09:28:38 AM: ***** Step 9000 / Validation 9 *****
09/16 09:28:38 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:28:38 AM: Validating...
09/16 09:28:40 AM: Evaluate: task edges-srl-ontonotes, batch 13 (157): mcc: 0.8272, acc: 0.7655, precision: 0.8719, recall: 0.7894, f1: 0.8286, edges-srl-ontonotes_loss: 0.0145
09/16 09:28:50 AM: Evaluate: task edges-srl-ontonotes, batch 121 (157): mcc: 0.8287, acc: 0.7677, precision: 0.8793, recall: 0.7856, f1: 0.8298, edges-srl-ontonotes_loss: 0.0142
09/16 09:28:53 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:28:53 AM: Best result seen so far for macro.
09/16 09:28:53 AM: Updating LR scheduler:
09/16 09:28:53 AM: 	Best result seen so far for macro_avg: 0.828
09/16 09:28:53 AM: 	# validation passes without improvement: 0
09/16 09:28:53 AM: edges-srl-ontonotes_loss: training: 0.018504 validation: 0.014326
09/16 09:28:53 AM: macro_avg: validation: 0.827998
09/16 09:28:53 AM: micro_avg: validation: 0.000000
09/16 09:28:53 AM: edges-srl-ontonotes_mcc: training: 0.765685 validation: 0.826812
09/16 09:28:53 AM: edges-srl-ontonotes_acc: training: 0.674988 validation: 0.765068
09/16 09:28:53 AM: edges-srl-ontonotes_precision: training: 0.829096 validation: 0.876645
09/16 09:28:53 AM: edges-srl-ontonotes_recall: training: 0.713161 validation: 0.784466
09/16 09:28:53 AM: edges-srl-ontonotes_f1: training: 0.766771 validation: 0.827998
09/16 09:28:53 AM: Global learning rate: 0.0001
09/16 09:28:53 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:29:00 AM: Update 9060: task edges-srl-ontonotes, batch 60 (9060): mcc: 0.7579, acc: 0.6678, precision: 0.8263, recall: 0.7013, f1: 0.7587, edges-srl-ontonotes_loss: 0.0192
09/16 09:29:10 AM: Update 9132: task edges-srl-ontonotes, batch 132 (9132): mcc: 0.7600, acc: 0.6703, precision: 0.8255, recall: 0.7058, f1: 0.7610, edges-srl-ontonotes_loss: 0.0191
09/16 09:29:20 AM: Update 9227: task edges-srl-ontonotes, batch 227 (9227): mcc: 0.7579, acc: 0.6672, precision: 0.8250, recall: 0.7025, f1: 0.7588, edges-srl-ontonotes_loss: 0.0192
09/16 09:29:30 AM: Update 9318: task edges-srl-ontonotes, batch 318 (9318): mcc: 0.7564, acc: 0.6645, precision: 0.8233, recall: 0.7011, f1: 0.7573, edges-srl-ontonotes_loss: 0.0192
09/16 09:29:41 AM: Update 9438: task edges-srl-ontonotes, batch 438 (9438): mcc: 0.7566, acc: 0.6645, precision: 0.8240, recall: 0.7009, f1: 0.7575, edges-srl-ontonotes_loss: 0.0192
09/16 09:29:51 AM: Update 9559: task edges-srl-ontonotes, batch 559 (9559): mcc: 0.7613, acc: 0.6710, precision: 0.8269, recall: 0.7069, f1: 0.7622, edges-srl-ontonotes_loss: 0.0188
09/16 09:30:01 AM: Update 9689: task edges-srl-ontonotes, batch 689 (9689): mcc: 0.7650, acc: 0.6756, precision: 0.8297, recall: 0.7114, f1: 0.7660, edges-srl-ontonotes_loss: 0.0186
09/16 09:30:11 AM: Update 9807: task edges-srl-ontonotes, batch 807 (9807): mcc: 0.7667, acc: 0.6786, precision: 0.8310, recall: 0.7132, f1: 0.7677, edges-srl-ontonotes_loss: 0.0185
09/16 09:30:21 AM: Update 9939: task edges-srl-ontonotes, batch 939 (9939): mcc: 0.7701, acc: 0.6826, precision: 0.8335, recall: 0.7174, f1: 0.7711, edges-srl-ontonotes_loss: 0.0182
09/16 09:30:26 AM: ***** Step 10000 / Validation 10 *****
09/16 09:30:26 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:30:26 AM: Validating...
09/16 09:30:31 AM: Evaluate: task edges-srl-ontonotes, batch 66 (157): mcc: 0.8070, acc: 0.7413, precision: 0.8663, recall: 0.7568, f1: 0.8079, edges-srl-ontonotes_loss: 0.0155
09/16 09:30:37 AM: Updating LR scheduler:
09/16 09:30:37 AM: 	Best result seen so far for macro_avg: 0.828
09/16 09:30:37 AM: 	# validation passes without improvement: 1
09/16 09:30:37 AM: edges-srl-ontonotes_loss: training: 0.018197 validation: 0.014495
09/16 09:30:37 AM: macro_avg: validation: 0.823912
09/16 09:30:37 AM: micro_avg: validation: 0.000000
09/16 09:30:37 AM: edges-srl-ontonotes_mcc: training: 0.770143 validation: 0.822754
09/16 09:30:37 AM: edges-srl-ontonotes_acc: training: 0.682413 validation: 0.761450
09/16 09:30:37 AM: edges-srl-ontonotes_precision: training: 0.833550 validation: 0.874029
09/16 09:30:37 AM: edges-srl-ontonotes_recall: training: 0.717485 validation: 0.779232
09/16 09:30:37 AM: edges-srl-ontonotes_f1: training: 0.771175 validation: 0.823912
09/16 09:30:37 AM: Global learning rate: 0.0001
09/16 09:30:37 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:30:41 AM: Update 10046: task edges-srl-ontonotes, batch 46 (10046): mcc: 0.7905, acc: 0.7149, precision: 0.8466, recall: 0.7437, f1: 0.7918, edges-srl-ontonotes_loss: 0.0169
09/16 09:30:51 AM: Update 10133: task edges-srl-ontonotes, batch 133 (10133): mcc: 0.7904, acc: 0.7140, precision: 0.8443, recall: 0.7456, f1: 0.7919, edges-srl-ontonotes_loss: 0.0169
09/16 09:31:01 AM: Update 10265: task edges-srl-ontonotes, batch 265 (10265): mcc: 0.7915, acc: 0.7117, precision: 0.8467, recall: 0.7455, f1: 0.7929, edges-srl-ontonotes_loss: 0.0169
09/16 09:31:11 AM: Update 10389: task edges-srl-ontonotes, batch 389 (10389): mcc: 0.7920, acc: 0.7112, precision: 0.8478, recall: 0.7454, f1: 0.7933, edges-srl-ontonotes_loss: 0.0169
09/16 09:31:21 AM: Update 10511: task edges-srl-ontonotes, batch 511 (10511): mcc: 0.7880, acc: 0.7061, precision: 0.8446, recall: 0.7407, f1: 0.7893, edges-srl-ontonotes_loss: 0.0171
09/16 09:31:31 AM: Update 10637: task edges-srl-ontonotes, batch 637 (10637): mcc: 0.7866, acc: 0.7045, precision: 0.8449, recall: 0.7378, f1: 0.7877, edges-srl-ontonotes_loss: 0.0172
09/16 09:31:41 AM: Update 10740: task edges-srl-ontonotes, batch 740 (10740): mcc: 0.7856, acc: 0.7035, precision: 0.8445, recall: 0.7364, f1: 0.7868, edges-srl-ontonotes_loss: 0.0172
09/16 09:31:51 AM: Update 10868: task edges-srl-ontonotes, batch 868 (10868): mcc: 0.7841, acc: 0.7014, precision: 0.8439, recall: 0.7342, f1: 0.7852, edges-srl-ontonotes_loss: 0.0173
09/16 09:32:01 AM: Update 11000: task edges-srl-ontonotes, batch 1000 (11000): mcc: 0.7837, acc: 0.7009, precision: 0.8437, recall: 0.7336, f1: 0.7848, edges-srl-ontonotes_loss: 0.0173
09/16 09:32:01 AM: ***** Step 11000 / Validation 11 *****
09/16 09:32:01 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:32:01 AM: Validating...
09/16 09:32:11 AM: Evaluate: task edges-srl-ontonotes, batch 137 (157): mcc: 0.8172, acc: 0.7506, precision: 0.8731, recall: 0.7698, f1: 0.8182, edges-srl-ontonotes_loss: 0.0147
09/16 09:32:13 AM: Updating LR scheduler:
09/16 09:32:13 AM: 	Best result seen so far for macro_avg: 0.828
09/16 09:32:13 AM: 	# validation passes without improvement: 2
09/16 09:32:13 AM: edges-srl-ontonotes_loss: training: 0.017301 validation: 0.014870
09/16 09:32:13 AM: macro_avg: validation: 0.817070
09/16 09:32:13 AM: micro_avg: validation: 0.000000
09/16 09:32:13 AM: edges-srl-ontonotes_mcc: training: 0.783707 validation: 0.816106
09/16 09:32:13 AM: edges-srl-ontonotes_acc: training: 0.700882 validation: 0.749211
09/16 09:32:13 AM: edges-srl-ontonotes_precision: training: 0.843713 validation: 0.872052
09/16 09:32:13 AM: edges-srl-ontonotes_recall: training: 0.733605 validation: 0.768609
09/16 09:32:13 AM: edges-srl-ontonotes_f1: training: 0.784816 validation: 0.817070
09/16 09:32:13 AM: Global learning rate: 0.0001
09/16 09:32:13 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:32:21 AM: Update 11102: task edges-srl-ontonotes, batch 102 (11102): mcc: 0.7760, acc: 0.6917, precision: 0.8358, recall: 0.7263, f1: 0.7772, edges-srl-ontonotes_loss: 0.0176
09/16 09:32:31 AM: Update 11236: task edges-srl-ontonotes, batch 236 (11236): mcc: 0.7739, acc: 0.6893, precision: 0.8356, recall: 0.7226, f1: 0.7750, edges-srl-ontonotes_loss: 0.0178
09/16 09:32:41 AM: Update 11326: task edges-srl-ontonotes, batch 326 (11326): mcc: 0.7757, acc: 0.6908, precision: 0.8374, recall: 0.7243, f1: 0.7768, edges-srl-ontonotes_loss: 0.0176
09/16 09:32:52 AM: Update 11454: task edges-srl-ontonotes, batch 454 (11454): mcc: 0.7775, acc: 0.6942, precision: 0.8381, recall: 0.7270, f1: 0.7786, edges-srl-ontonotes_loss: 0.0175
09/16 09:33:02 AM: Update 11588: task edges-srl-ontonotes, batch 588 (11588): mcc: 0.7808, acc: 0.6989, precision: 0.8406, recall: 0.7310, f1: 0.7820, edges-srl-ontonotes_loss: 0.0174
09/16 09:33:12 AM: Update 11709: task edges-srl-ontonotes, batch 709 (11709): mcc: 0.7824, acc: 0.7005, precision: 0.8416, recall: 0.7330, f1: 0.7835, edges-srl-ontonotes_loss: 0.0172
09/16 09:33:22 AM: Update 11843: task edges-srl-ontonotes, batch 843 (11843): mcc: 0.7845, acc: 0.7028, precision: 0.8434, recall: 0.7353, f1: 0.7857, edges-srl-ontonotes_loss: 0.0171
09/16 09:33:32 AM: Update 11953: task edges-srl-ontonotes, batch 953 (11953): mcc: 0.7844, acc: 0.7030, precision: 0.8430, recall: 0.7355, f1: 0.7856, edges-srl-ontonotes_loss: 0.0171
09/16 09:33:36 AM: ***** Step 12000 / Validation 12 *****
09/16 09:33:36 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:33:36 AM: Validating...
09/16 09:33:42 AM: Evaluate: task edges-srl-ontonotes, batch 77 (157): mcc: 0.8062, acc: 0.7407, precision: 0.8667, recall: 0.7550, f1: 0.8070, edges-srl-ontonotes_loss: 0.0157
09/16 09:33:48 AM: Updating LR scheduler:
09/16 09:33:48 AM: 	Best result seen so far for macro_avg: 0.828
09/16 09:33:48 AM: 	# validation passes without improvement: 3
09/16 09:33:48 AM: edges-srl-ontonotes_loss: training: 0.017227 validation: 0.014896
09/16 09:33:48 AM: macro_avg: validation: 0.818613
09/16 09:33:48 AM: micro_avg: validation: 0.000000
09/16 09:33:48 AM: edges-srl-ontonotes_mcc: training: 0.782963 validation: 0.817586
09/16 09:33:48 AM: edges-srl-ontonotes_acc: training: 0.701182 validation: 0.755215
09/16 09:33:48 AM: edges-srl-ontonotes_precision: training: 0.842228 validation: 0.872203
09/16 09:33:48 AM: edges-srl-ontonotes_recall: training: 0.733536 validation: 0.771226
09/16 09:33:48 AM: edges-srl-ontonotes_f1: training: 0.784133 validation: 0.818613
09/16 09:33:48 AM: Global learning rate: 0.0001
09/16 09:33:48 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:33:52 AM: Update 12044: task edges-srl-ontonotes, batch 44 (12044): mcc: 0.7639, acc: 0.6732, precision: 0.8353, recall: 0.7046, f1: 0.7644, edges-srl-ontonotes_loss: 0.0188
09/16 09:34:02 AM: Update 12153: task edges-srl-ontonotes, batch 153 (12153): mcc: 0.7617, acc: 0.6721, precision: 0.8330, recall: 0.7025, f1: 0.7622, edges-srl-ontonotes_loss: 0.0185
09/16 09:34:13 AM: Update 12255: task edges-srl-ontonotes, batch 255 (12255): mcc: 0.7667, acc: 0.6786, precision: 0.8352, recall: 0.7097, f1: 0.7674, edges-srl-ontonotes_loss: 0.0183
09/16 09:34:23 AM: Update 12398: task edges-srl-ontonotes, batch 398 (12398): mcc: 0.7776, acc: 0.6930, precision: 0.8408, recall: 0.7250, f1: 0.7786, edges-srl-ontonotes_loss: 0.0177
09/16 09:34:33 AM: Update 12533: task edges-srl-ontonotes, batch 533 (12533): mcc: 0.7858, acc: 0.7021, precision: 0.8459, recall: 0.7355, f1: 0.7868, edges-srl-ontonotes_loss: 0.0171
09/16 09:34:44 AM: Update 12671: task edges-srl-ontonotes, batch 671 (12671): mcc: 0.7943, acc: 0.7131, precision: 0.8513, recall: 0.7465, f1: 0.7954, edges-srl-ontonotes_loss: 0.0165
09/16 09:34:54 AM: Update 12831: task edges-srl-ontonotes, batch 831 (12831): mcc: 0.8040, acc: 0.7256, precision: 0.8577, recall: 0.7589, f1: 0.8053, edges-srl-ontonotes_loss: 0.0158
09/16 09:35:04 AM: Update 12966: task edges-srl-ontonotes, batch 966 (12966): mcc: 0.8090, acc: 0.7318, precision: 0.8613, recall: 0.7649, f1: 0.8102, edges-srl-ontonotes_loss: 0.0155
09/16 09:35:06 AM: ***** Step 13000 / Validation 13 *****
09/16 09:35:06 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:35:06 AM: Validating...
09/16 09:35:14 AM: Evaluate: task edges-srl-ontonotes, batch 110 (157): mcc: 0.8260, acc: 0.7621, precision: 0.8815, recall: 0.7786, f1: 0.8269, edges-srl-ontonotes_loss: 0.0144
09/16 09:35:17 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:35:17 AM: Best result seen so far for macro.
09/16 09:35:17 AM: Updating LR scheduler:
09/16 09:35:17 AM: 	Best result seen so far for macro_avg: 0.829
09/16 09:35:17 AM: 	# validation passes without improvement: 0
09/16 09:35:17 AM: edges-srl-ontonotes_loss: training: 0.015422 validation: 0.014328
09/16 09:35:17 AM: macro_avg: validation: 0.828502
09/16 09:35:17 AM: micro_avg: validation: 0.000000
09/16 09:35:17 AM: edges-srl-ontonotes_mcc: training: 0.810353 validation: 0.827447
09/16 09:35:17 AM: edges-srl-ontonotes_acc: training: 0.733592 validation: 0.765299
09/16 09:35:17 AM: edges-srl-ontonotes_precision: training: 0.862404 validation: 0.879419
09/16 09:35:17 AM: edges-srl-ontonotes_recall: training: 0.766504 validation: 0.783158
09/16 09:35:17 AM: edges-srl-ontonotes_f1: training: 0.811631 validation: 0.828502
09/16 09:35:17 AM: Global learning rate: 0.0001
09/16 09:35:17 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-top/run
09/16 09:35:24 AM: Update 13092: task edges-srl-ontonotes, batch 92 (13092): mcc: 0.8335, acc: 0.7646, precision: 0.8779, recall: 0.7958, f1: 0.8348, edges-srl-ontonotes_loss: 0.0138
09/16 09:35:34 AM: Update 13198: task edges-srl-ontonotes, batch 198 (13198): mcc: 0.8402, acc: 0.7728, precision: 0.8834, recall: 0.8034, f1: 0.8415, edges-srl-ontonotes_loss: 0.0134
09/16 09:35:44 AM: Update 13334: task edges-srl-ontonotes, batch 334 (13334): mcc: 0.8423, acc: 0.7747, precision: 0.8844, recall: 0.8064, f1: 0.8436, edges-srl-ontonotes_loss: 0.0133
09/16 09:35:54 AM: Update 13466: task edges-srl-ontonotes, batch 466 (13466): mcc: 0.8435, acc: 0.7758, precision: 0.8856, recall: 0.8077, f1: 0.8449, edges-srl-ontonotes_loss: 0.0132
09/16 09:36:04 AM: Update 13566: task edges-srl-ontonotes, batch 566 (13566): mcc: 0.8433, acc: 0.7761, precision: 0.8848, recall: 0.8082, f1: 0.8447, edges-srl-ontonotes_loss: 0.0132
09/16 09:36:14 AM: Update 13723: task edges-srl-ontonotes, batch 723 (13723): mcc: 0.8448, acc: 0.7781, precision: 0.8859, recall: 0.8099, f1: 0.8462, edges-srl-ontonotes_loss: 0.0131
09/16 09:36:24 AM: Update 13859: task edges-srl-ontonotes, batch 859 (13859): mcc: 0.8433, acc: 0.7767, precision: 0.8843, recall: 0.8085, f1: 0.8447, edges-srl-ontonotes_loss: 0.0132
09/16 09:36:34 AM: Update 13992: task edges-srl-ontonotes, batch 992 (13992): mcc: 0.8379, acc: 0.7700, precision: 0.8804, recall: 0.8020, f1: 0.8393, edges-srl-ontonotes_loss: 0.0136
09/16 09:36:35 AM: ***** Step 14000 / Validation 14 *****
09/16 09:36:35 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:36:35 AM: Validating...
