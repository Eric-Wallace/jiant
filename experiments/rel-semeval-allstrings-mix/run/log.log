10/01 07:31:57 AM: Git branch: master
10/01 07:31:57 AM: Git SHA: 8a5d6bbc81dc2562b6a149e8b00815e7e9113c4c
10/01 07:31:57 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/rel-semeval-allstrings-mix/",
  "exp_name": "experiments/rel-semeval-allstrings-mix",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/rel-semeval-allstrings-mix/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/allstrings",
  "pytorch_transformers_output_mode": "mix",
  "remote_log_name": "experiments/rel-semeval-allstrings-mix__run",
  "run_dir": "./experiments/rel-semeval-allstrings-mix/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-rel-semeval",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 07:31:57 AM: Saved config to ./experiments/rel-semeval-allstrings-mix/run/params.conf
10/01 07:31:57 AM: Using random seed 1234
10/01 07:31:57 AM: Using GPU 0
10/01 07:31:57 AM: Loading tasks...
10/01 07:31:57 AM: Writing pre-preprocessed tasks to ./experiments/rel-semeval-allstrings-mix/
10/01 07:31:57 AM: 	Creating task edges-rel-semeval from scratch.
10/01 07:31:57 AM: Read=6851, Skip=0, Total=6851 from ./probing_data/edges/semeval/train.0.85.json.retokenized.bert-base-uncased
10/01 07:31:57 AM: Read=1149, Skip=0, Total=1149 from ./probing_data/edges/semeval/dev.json.retokenized.bert-base-uncased
10/01 07:31:57 AM: Read=2717, Skip=0, Total=2717 from ./probing_data/edges/semeval/test.json.retokenized.bert-base-uncased
10/01 07:31:57 AM: 	Task 'edges-rel-semeval': |train|=6851 |val|=1149 |test|=2717
10/01 07:31:57 AM: 	Finished loading tasks: edges-rel-semeval.
10/01 07:31:57 AM: 	Building vocab from scratch.
10/01 07:31:57 AM: 	Counting units for task edges-rel-semeval.
10/01 07:31:57 AM: 	Task 'edges-rel-semeval': adding vocab namespace 'edges-rel-semeval_labels'
10/01 07:31:58 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 07:31:59 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 07:31:59 AM: 	Saved vocab to ./experiments/rel-semeval-allstrings-mix/vocab
10/01 07:31:59 AM: Loading token dictionary from ./experiments/rel-semeval-allstrings-mix/vocab.
10/01 07:31:59 AM: 	Loaded vocab from ./experiments/rel-semeval-allstrings-mix/vocab
10/01 07:31:59 AM: 	Vocab namespace bert_uncased: size 30524
10/01 07:31:59 AM: 	Vocab namespace tokens: size 16020
10/01 07:31:59 AM: 	Vocab namespace chars: size 59
10/01 07:31:59 AM: 	Vocab namespace edges-rel-semeval_labels: size 19
10/01 07:31:59 AM: 	Finished building vocab.
10/01 07:31:59 AM: 	Task edges-rel-semeval (train): Indexing from scratch.
10/01 07:31:59 AM: 	Task edges-rel-semeval (train): Saved 6851 instances to ./experiments/rel-semeval-allstrings-mix/preproc/edges-rel-semeval__train_data
10/01 07:31:59 AM: 	Task edges-rel-semeval (val): Indexing from scratch.
10/01 07:32:00 AM: 	Task edges-rel-semeval (val): Saved 1149 instances to ./experiments/rel-semeval-allstrings-mix/preproc/edges-rel-semeval__val_data
10/01 07:32:00 AM: 	Task edges-rel-semeval (test): Indexing from scratch.
10/01 07:32:00 AM: 	Task edges-rel-semeval (test): Saved 2717 instances to ./experiments/rel-semeval-allstrings-mix/preproc/edges-rel-semeval__test_data
10/01 07:32:00 AM: 	Finished indexing tasks
10/01 07:32:00 AM: 	Creating trimmed target-only version of edges-rel-semeval train.
10/01 07:32:00 AM: 	  Training on 
10/01 07:32:00 AM: 	  Evaluating on edges-rel-semeval
10/01 07:32:00 AM: 	Finished loading tasks in 2.838s
10/01 07:32:00 AM: 	 Tasks: ['edges-rel-semeval']
10/01 07:32:00 AM: Building model...
10/01 07:32:00 AM: Using BERT model (bert-base-uncased).
10/01 07:32:00 AM: LOADING A FUNETUNED MODEL from: 
10/01 07:32:00 AM: models/allstrings
10/01 07:32:00 AM: loading configuration file models/allstrings/config.json
10/01 07:32:00 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "random-memorize-all-binary",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 07:32:00 AM: loading weights file models/allstrings/pytorch_model.bin
10/01 07:32:03 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp1ba5vkmn
10/01 07:32:05 AM: copying /tmp/tmp1ba5vkmn to cache at ./experiments/rel-semeval-allstrings-mix/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 07:32:05 AM: creating metadata file for ./experiments/rel-semeval-allstrings-mix/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 07:32:05 AM: removing temp file /tmp/tmp1ba5vkmn
10/01 07:32:05 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/rel-semeval-allstrings-mix/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 07:32:05 AM: NOTE: pytorch_transformers_output_mode='mix', so scalar mixing weights will be fine-tuned even if BERT model is frozen.
10/01 07:32:05 AM: Initializing parameters
10/01 07:32:05 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 07:32:05 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 07:32:05 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 07:32:05 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 07:32:05 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.gamma
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.0
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.1
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.10
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.11
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.12
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.2
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.3
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.4
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.5
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.6
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.7
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.8
10/01 07:32:05 AM:    _text_field_embedder.scalar_mix.scalar_parameters.9
10/01 07:32:05 AM: 	Task 'edges-rel-semeval' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-rel-semeval"
}
10/01 07:32:09 AM: Model specification:
10/01 07:32:09 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
      (scalar_mix): ScalarMix(
        (scalar_parameters): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-rel-semeval_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=19, bias=True)
      )
    )
  )
)
10/01 07:32:09 AM: Model parameters:
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.gamma: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.0: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.1: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.2: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.3: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.4: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.5: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.6: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.7: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.8: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.9: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.10: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.11: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.12: Trainable parameter, count 1 with torch.Size([1])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.classifier.classifier.4.weight: Trainable parameter, count 4864 with torch.Size([19, 256])
10/01 07:32:09 AM: 	edges-rel-semeval_mdl.classifier.classifier.4.bias: Trainable parameter, count 19 with torch.Size([19])
10/01 07:32:09 AM: Total number of parameters: 110143777 (1.10144e+08)
10/01 07:32:09 AM: Number of trainable parameters: 661537 (661537)
10/01 07:32:09 AM: Finished building model in 9.481s
10/01 07:32:09 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-rel-semeval 

10/01 07:32:10 AM: patience = 9
10/01 07:32:10 AM: val_interval = 100
10/01 07:32:10 AM: max_vals = 100
10/01 07:32:10 AM: cuda_device = 0
10/01 07:32:10 AM: grad_norm = 5.0
10/01 07:32:10 AM: grad_clipping = None
10/01 07:32:10 AM: lr_decay = 0.99
10/01 07:32:10 AM: min_lr = 1e-06
10/01 07:32:10 AM: keep_all_checkpoints = 0
10/01 07:32:10 AM: val_data_limit = 5000
10/01 07:32:10 AM: max_epochs = -1
10/01 07:32:10 AM: dec_val_scale = 250
10/01 07:32:10 AM: training_data_fraction = 1
10/01 07:32:10 AM: type = adam
10/01 07:32:10 AM: parameter_groups = None
10/01 07:32:10 AM: Number of trainable parameters: 661537
10/01 07:32:10 AM: infer_type_and_cast = True
10/01 07:32:10 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 07:32:10 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 07:32:10 AM: lr = 0.0001
10/01 07:32:10 AM: amsgrad = True
10/01 07:32:10 AM: type = reduce_on_plateau
10/01 07:32:10 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 07:32:10 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 07:32:10 AM: mode = max
10/01 07:32:10 AM: factor = 0.5
10/01 07:32:10 AM: patience = 3
10/01 07:32:10 AM: threshold = 0.0001
10/01 07:32:10 AM: threshold_mode = abs
10/01 07:32:10 AM: verbose = True
10/01 07:32:10 AM: type = adam
10/01 07:32:10 AM: parameter_groups = None
10/01 07:32:10 AM: Number of trainable parameters: 661537
10/01 07:32:10 AM: infer_type_and_cast = True
10/01 07:32:10 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 07:32:10 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 07:32:10 AM: lr = 0.0001
10/01 07:32:10 AM: amsgrad = True
10/01 07:32:10 AM: type = reduce_on_plateau
10/01 07:32:10 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 07:32:10 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 07:32:10 AM: mode = max
10/01 07:32:10 AM: factor = 0.5
10/01 07:32:10 AM: patience = 3
10/01 07:32:10 AM: threshold = 0.0001
10/01 07:32:10 AM: threshold_mode = abs
10/01 07:32:10 AM: verbose = True
10/01 07:32:10 AM: Starting training without restoring from a checkpoint.
10/01 07:32:10 AM: Training examples per task, before any subsampling: {'edges-rel-semeval': 6851}
10/01 07:32:10 AM: Beginning training with stopping criteria based on metric: edges-rel-semeval_f1
10/01 07:32:16 AM: ***** Step 100 / Validation 1 *****
10/01 07:32:16 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:32:16 AM: Validating...
10/01 07:32:18 AM: Best result seen so far for edges-rel-semeval.
10/01 07:32:18 AM: Best result seen so far for micro.
10/01 07:32:18 AM: Best result seen so far for macro.
10/01 07:32:18 AM: Updating LR scheduler:
10/01 07:32:18 AM: 	Best result seen so far for macro_avg: 0.000
10/01 07:32:18 AM: 	# validation passes without improvement: 0
10/01 07:32:18 AM: edges-rel-semeval_loss: training: 0.244132 validation: 0.181523
10/01 07:32:18 AM: macro_avg: validation: 0.000000
10/01 07:32:18 AM: micro_avg: validation: 0.000000
10/01 07:32:18 AM: edges-rel-semeval_mcc: training: 0.008242 validation: 0.000000
10/01 07:32:18 AM: edges-rel-semeval_acc: training: 0.002523 validation: 0.000000
10/01 07:32:18 AM: edges-rel-semeval_precision: training: 0.066538 validation: 0.000000
10/01 07:32:18 AM: edges-rel-semeval_recall: training: 0.021760 validation: 0.000000
10/01 07:32:18 AM: edges-rel-semeval_f1: training: 0.032795 validation: 0.000000
10/01 07:32:18 AM: Global learning rate: 0.0001
10/01 07:32:18 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:32:20 AM: Update 138: task edges-rel-semeval, batch 38 (138): mcc: 0.0143, acc: 0.0008, precision: 0.3333, recall: 0.0008, f1: 0.0016, edges-rel-semeval_loss: 0.1837
10/01 07:32:23 AM: ***** Step 200 / Validation 2 *****
10/01 07:32:23 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:32:23 AM: Validating...
10/01 07:32:25 AM: Best result seen so far for edges-rel-semeval.
10/01 07:32:25 AM: Best result seen so far for macro.
10/01 07:32:25 AM: Updating LR scheduler:
10/01 07:32:25 AM: 	Best result seen so far for macro_avg: 0.085
10/01 07:32:25 AM: 	# validation passes without improvement: 0
10/01 07:32:25 AM: edges-rel-semeval_loss: training: 0.176189 validation: 0.154171
10/01 07:32:25 AM: macro_avg: validation: 0.084718
10/01 07:32:25 AM: micro_avg: validation: 0.000000
10/01 07:32:25 AM: edges-rel-semeval_mcc: training: 0.069722 validation: 0.196852
10/01 07:32:25 AM: edges-rel-semeval_acc: training: 0.007188 validation: 0.044386
10/01 07:32:25 AM: edges-rel-semeval_precision: training: 0.741935 validation: 0.927273
10/01 07:32:25 AM: edges-rel-semeval_recall: training: 0.007188 validation: 0.044386
10/01 07:32:25 AM: edges-rel-semeval_f1: training: 0.014237 validation: 0.084718
10/01 07:32:25 AM: Global learning rate: 0.0001
10/01 07:32:25 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:32:30 AM: Update 287: task edges-rel-semeval, batch 87 (287): mcc: 0.2736, acc: 0.0951, precision: 0.8292, recall: 0.0969, f1: 0.1735, edges-rel-semeval_loss: 0.1523
10/01 07:32:31 AM: ***** Step 300 / Validation 3 *****
10/01 07:32:31 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:32:31 AM: Validating...
10/01 07:32:33 AM: Best result seen so far for edges-rel-semeval.
10/01 07:32:33 AM: Best result seen so far for macro.
10/01 07:32:33 AM: Updating LR scheduler:
10/01 07:32:33 AM: 	Best result seen so far for macro_avg: 0.243
10/01 07:32:33 AM: 	# validation passes without improvement: 0
10/01 07:32:33 AM: edges-rel-semeval_loss: training: 0.151188 validation: 0.131743
10/01 07:32:33 AM: macro_avg: validation: 0.242697
10/01 07:32:33 AM: micro_avg: validation: 0.000000
10/01 07:32:33 AM: edges-rel-semeval_mcc: training: 0.281585 validation: 0.339724
10/01 07:32:33 AM: edges-rel-semeval_acc: training: 0.102176 validation: 0.140122
10/01 07:32:33 AM: edges-rel-semeval_precision: training: 0.818859 validation: 0.870968
10/01 07:32:33 AM: edges-rel-semeval_recall: training: 0.104068 validation: 0.140992
10/01 07:32:33 AM: edges-rel-semeval_f1: training: 0.184667 validation: 0.242697
10/01 07:32:33 AM: Global learning rate: 0.0001
10/01 07:32:33 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:32:38 AM: ***** Step 400 / Validation 4 *****
10/01 07:32:38 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:32:38 AM: Validating...
10/01 07:32:40 AM: Best result seen so far for edges-rel-semeval.
10/01 07:32:40 AM: Best result seen so far for macro.
10/01 07:32:40 AM: Updating LR scheduler:
10/01 07:32:40 AM: 	Best result seen so far for macro_avg: 0.378
10/01 07:32:40 AM: 	# validation passes without improvement: 0
10/01 07:32:40 AM: edges-rel-semeval_loss: training: 0.132344 validation: 0.118088
10/01 07:32:40 AM: macro_avg: validation: 0.378157
10/01 07:32:40 AM: micro_avg: validation: 0.000000
10/01 07:32:40 AM: edges-rel-semeval_mcc: training: 0.404895 validation: 0.447189
10/01 07:32:40 AM: edges-rel-semeval_acc: training: 0.202813 validation: 0.235857
10/01 07:32:40 AM: edges-rel-semeval_precision: training: 0.831683 validation: 0.876582
10/01 07:32:40 AM: edges-rel-semeval_recall: training: 0.210000 validation: 0.241079
10/01 07:32:40 AM: edges-rel-semeval_f1: training: 0.335329 validation: 0.378157
10/01 07:32:40 AM: Global learning rate: 0.0001
10/01 07:32:40 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:32:40 AM: Update 403: task edges-rel-semeval, batch 3 (403): mcc: 0.4099, acc: 0.2083, precision: 0.7857, recall: 0.2292, f1: 0.3548, edges-rel-semeval_loss: 0.1342
10/01 07:32:46 AM: ***** Step 500 / Validation 5 *****
10/01 07:32:46 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:32:46 AM: Validating...
10/01 07:32:48 AM: Best result seen so far for edges-rel-semeval.
10/01 07:32:48 AM: Best result seen so far for macro.
10/01 07:32:48 AM: Updating LR scheduler:
10/01 07:32:48 AM: 	Best result seen so far for macro_avg: 0.473
10/01 07:32:48 AM: 	# validation passes without improvement: 0
10/01 07:32:48 AM: edges-rel-semeval_loss: training: 0.121581 validation: 0.109331
10/01 07:32:48 AM: macro_avg: validation: 0.473388
10/01 07:32:48 AM: micro_avg: validation: 0.000000
10/01 07:32:48 AM: edges-rel-semeval_mcc: training: 0.471769 validation: 0.512818
10/01 07:32:48 AM: edges-rel-semeval_acc: training: 0.271208 validation: 0.315057
10/01 07:32:48 AM: edges-rel-semeval_precision: training: 0.829807 validation: 0.843750
10/01 07:32:48 AM: edges-rel-semeval_recall: training: 0.284453 validation: 0.328982
10/01 07:32:48 AM: edges-rel-semeval_f1: training: 0.423673 validation: 0.473388
10/01 07:32:48 AM: Global learning rate: 0.0001
10/01 07:32:48 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:32:50 AM: Update 548: task edges-rel-semeval, batch 48 (548): mcc: 0.5327, acc: 0.3418, precision: 0.8359, recall: 0.3581, f1: 0.5014, edges-rel-semeval_loss: 0.1113
10/01 07:32:53 AM: ***** Step 600 / Validation 6 *****
10/01 07:32:53 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:32:53 AM: Validating...
10/01 07:32:55 AM: Best result seen so far for edges-rel-semeval.
10/01 07:32:55 AM: Best result seen so far for macro.
10/01 07:32:55 AM: Updating LR scheduler:
10/01 07:32:55 AM: 	Best result seen so far for macro_avg: 0.531
10/01 07:32:55 AM: 	# validation passes without improvement: 0
10/01 07:32:55 AM: edges-rel-semeval_loss: training: 0.110677 validation: 0.102832
10/01 07:32:55 AM: macro_avg: validation: 0.530879
10/01 07:32:55 AM: micro_avg: validation: 0.000000
10/01 07:32:55 AM: edges-rel-semeval_mcc: training: 0.522429 validation: 0.555702
10/01 07:32:55 AM: edges-rel-semeval_acc: training: 0.337813 validation: 0.375979
10/01 07:32:55 AM: edges-rel-semeval_precision: training: 0.818116 validation: 0.835514
10/01 07:32:55 AM: edges-rel-semeval_recall: training: 0.352812 validation: 0.389034
10/01 07:32:55 AM: edges-rel-semeval_f1: training: 0.493013 validation: 0.530879
10/01 07:32:55 AM: Global learning rate: 0.0001
10/01 07:32:55 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:00 AM: Update 693: task edges-rel-semeval, batch 93 (693): mcc: 0.5671, acc: 0.3817, precision: 0.8379, recall: 0.4035, f1: 0.5447, edges-rel-semeval_loss: 0.1032
10/01 07:33:01 AM: ***** Step 700 / Validation 7 *****
10/01 07:33:01 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:01 AM: Validating...
10/01 07:33:03 AM: Best result seen so far for edges-rel-semeval.
10/01 07:33:03 AM: Best result seen so far for macro.
10/01 07:33:03 AM: Updating LR scheduler:
10/01 07:33:03 AM: 	Best result seen so far for macro_avg: 0.552
10/01 07:33:03 AM: 	# validation passes without improvement: 0
10/01 07:33:03 AM: edges-rel-semeval_loss: training: 0.103200 validation: 0.098143
10/01 07:33:03 AM: macro_avg: validation: 0.551925
10/01 07:33:03 AM: micro_avg: validation: 0.000000
10/01 07:33:03 AM: edges-rel-semeval_mcc: training: 0.566726 validation: 0.572680
10/01 07:33:03 AM: edges-rel-semeval_acc: training: 0.382529 validation: 0.396867
10/01 07:33:03 AM: edges-rel-semeval_precision: training: 0.836601 validation: 0.837168
10/01 07:33:03 AM: edges-rel-semeval_recall: training: 0.403658 validation: 0.411662
10/01 07:33:03 AM: edges-rel-semeval_f1: training: 0.544565 validation: 0.551925
10/01 07:33:03 AM: Global learning rate: 0.0001
10/01 07:33:03 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:08 AM: ***** Step 800 / Validation 8 *****
10/01 07:33:08 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:08 AM: Validating...
10/01 07:33:10 AM: Best result seen so far for edges-rel-semeval.
10/01 07:33:10 AM: Best result seen so far for macro.
10/01 07:33:10 AM: Updating LR scheduler:
10/01 07:33:10 AM: 	Best result seen so far for macro_avg: 0.602
10/01 07:33:10 AM: 	# validation passes without improvement: 0
10/01 07:33:10 AM: edges-rel-semeval_loss: training: 0.098214 validation: 0.093016
10/01 07:33:10 AM: macro_avg: validation: 0.602369
10/01 07:33:10 AM: micro_avg: validation: 0.000000
10/01 07:33:10 AM: edges-rel-semeval_mcc: training: 0.596604 validation: 0.616964
10/01 07:33:10 AM: edges-rel-semeval_acc: training: 0.423125 validation: 0.448216
10/01 07:33:10 AM: edges-rel-semeval_precision: training: 0.835280 validation: 0.855769
10/01 07:33:10 AM: edges-rel-semeval_recall: training: 0.446875 validation: 0.464752
10/01 07:33:10 AM: edges-rel-semeval_f1: training: 0.582248 validation: 0.602369
10/01 07:33:10 AM: Global learning rate: 0.0001
10/01 07:33:10 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:10 AM: Update 805: task edges-rel-semeval, batch 5 (805): mcc: 0.5963, acc: 0.4250, precision: 0.8625, recall: 0.4313, f1: 0.5750, edges-rel-semeval_loss: 0.0985
10/01 07:33:16 AM: ***** Step 900 / Validation 9 *****
10/01 07:33:16 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:16 AM: Validating...
10/01 07:33:18 AM: Updating LR scheduler:
10/01 07:33:18 AM: 	Best result seen so far for macro_avg: 0.602
10/01 07:33:18 AM: 	# validation passes without improvement: 1
10/01 07:33:18 AM: edges-rel-semeval_loss: training: 0.094767 validation: 0.090391
10/01 07:33:18 AM: macro_avg: validation: 0.602151
10/01 07:33:18 AM: micro_avg: validation: 0.000000
10/01 07:33:18 AM: edges-rel-semeval_mcc: training: 0.602035 validation: 0.617781
10/01 07:33:18 AM: edges-rel-semeval_acc: training: 0.434248 validation: 0.449956
10/01 07:33:18 AM: edges-rel-semeval_precision: training: 0.830567 validation: 0.860841
10/01 07:33:18 AM: edges-rel-semeval_recall: training: 0.457584 validation: 0.463011
10/01 07:33:18 AM: edges-rel-semeval_f1: training: 0.590077 validation: 0.602151
10/01 07:33:18 AM: Global learning rate: 0.0001
10/01 07:33:18 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:20 AM: Update 949: task edges-rel-semeval, batch 49 (949): mcc: 0.6459, acc: 0.4834, precision: 0.8552, recall: 0.5083, f1: 0.6376, edges-rel-semeval_loss: 0.0879
10/01 07:33:23 AM: ***** Step 1000 / Validation 10 *****
10/01 07:33:23 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:23 AM: Validating...
10/01 07:33:25 AM: Best result seen so far for edges-rel-semeval.
10/01 07:33:25 AM: Best result seen so far for macro.
10/01 07:33:25 AM: Updating LR scheduler:
10/01 07:33:25 AM: 	Best result seen so far for macro_avg: 0.634
10/01 07:33:25 AM: 	# validation passes without improvement: 0
10/01 07:33:25 AM: edges-rel-semeval_loss: training: 0.087214 validation: 0.087586
10/01 07:33:25 AM: macro_avg: validation: 0.634252
10/01 07:33:25 AM: micro_avg: validation: 0.000000
10/01 07:33:25 AM: edges-rel-semeval_mcc: training: 0.646061 validation: 0.639608
10/01 07:33:25 AM: edges-rel-semeval_acc: training: 0.484375 validation: 0.484769
10/01 07:33:25 AM: edges-rel-semeval_precision: training: 0.850026 validation: 0.836182
10/01 07:33:25 AM: edges-rel-semeval_recall: training: 0.511875 validation: 0.510879
10/01 07:33:25 AM: edges-rel-semeval_f1: training: 0.638970 validation: 0.634252
10/01 07:33:25 AM: Global learning rate: 0.0001
10/01 07:33:25 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:30 AM: Update 1093: task edges-rel-semeval, batch 93 (1093): mcc: 0.6369, acc: 0.4839, precision: 0.8229, recall: 0.5154, f1: 0.6338, edges-rel-semeval_loss: 0.0863
10/01 07:33:31 AM: ***** Step 1100 / Validation 11 *****
10/01 07:33:31 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:31 AM: Validating...
10/01 07:33:33 AM: Best result seen so far for edges-rel-semeval.
10/01 07:33:33 AM: Best result seen so far for macro.
10/01 07:33:33 AM: Updating LR scheduler:
10/01 07:33:33 AM: 	Best result seen so far for macro_avg: 0.642
10/01 07:33:33 AM: 	# validation passes without improvement: 0
10/01 07:33:33 AM: edges-rel-semeval_loss: training: 0.086098 validation: 0.086370
10/01 07:33:33 AM: macro_avg: validation: 0.641921
10/01 07:33:33 AM: micro_avg: validation: 0.000000
10/01 07:33:33 AM: edges-rel-semeval_mcc: training: 0.638557 validation: 0.650508
10/01 07:33:33 AM: edges-rel-semeval_acc: training: 0.485336 validation: 0.495213
10/01 07:33:33 AM: edges-rel-semeval_precision: training: 0.822734 validation: 0.860908
10/01 07:33:33 AM: edges-rel-semeval_recall: training: 0.518133 validation: 0.511749
10/01 07:33:33 AM: edges-rel-semeval_f1: training: 0.635836 validation: 0.641921
10/01 07:33:33 AM: Global learning rate: 0.0001
10/01 07:33:33 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:38 AM: ***** Step 1200 / Validation 12 *****
10/01 07:33:38 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:38 AM: Validating...
10/01 07:33:40 AM: Best result seen so far for edges-rel-semeval.
10/01 07:33:40 AM: Best result seen so far for macro.
10/01 07:33:40 AM: Updating LR scheduler:
10/01 07:33:40 AM: 	Best result seen so far for macro_avg: 0.657
10/01 07:33:40 AM: 	# validation passes without improvement: 0
10/01 07:33:40 AM: edges-rel-semeval_loss: training: 0.081931 validation: 0.083988
10/01 07:33:40 AM: macro_avg: validation: 0.657082
10/01 07:33:40 AM: micro_avg: validation: 0.000000
10/01 07:33:40 AM: edges-rel-semeval_mcc: training: 0.673652 validation: 0.660684
10/01 07:33:40 AM: edges-rel-semeval_acc: training: 0.519375 validation: 0.512620
10/01 07:33:40 AM: edges-rel-semeval_precision: training: 0.855480 validation: 0.846365
10/01 07:33:40 AM: edges-rel-semeval_recall: training: 0.551250 validation: 0.536989
10/01 07:33:40 AM: edges-rel-semeval_f1: training: 0.670467 validation: 0.657082
10/01 07:33:40 AM: Global learning rate: 0.0001
10/01 07:33:40 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:40 AM: Update 1208: task edges-rel-semeval, batch 8 (1208): mcc: 0.6924, acc: 0.5469, precision: 0.8436, recall: 0.5898, f1: 0.6943, edges-rel-semeval_loss: 0.0804
10/01 07:33:46 AM: ***** Step 1300 / Validation 13 *****
10/01 07:33:46 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:46 AM: Validating...
10/01 07:33:48 AM: Best result seen so far for edges-rel-semeval.
10/01 07:33:48 AM: Best result seen so far for macro.
10/01 07:33:48 AM: Updating LR scheduler:
10/01 07:33:48 AM: 	Best result seen so far for macro_avg: 0.670
10/01 07:33:48 AM: 	# validation passes without improvement: 0
10/01 07:33:48 AM: edges-rel-semeval_loss: training: 0.081582 validation: 0.082139
10/01 07:33:48 AM: macro_avg: validation: 0.670478
10/01 07:33:48 AM: micro_avg: validation: 0.000000
10/01 07:33:48 AM: edges-rel-semeval_mcc: training: 0.656586 validation: 0.669833
10/01 07:33:48 AM: edges-rel-semeval_acc: training: 0.505203 validation: 0.530896
10/01 07:33:48 AM: edges-rel-semeval_precision: training: 0.826233 validation: 0.832258
10/01 07:33:48 AM: edges-rel-semeval_recall: training: 0.544308 validation: 0.561358
10/01 07:33:48 AM: edges-rel-semeval_f1: training: 0.656274 validation: 0.670478
10/01 07:33:48 AM: Global learning rate: 0.0001
10/01 07:33:48 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:33:50 AM: Update 1350: task edges-rel-semeval, batch 50 (1350): mcc: 0.6707, acc: 0.5275, precision: 0.8326, recall: 0.5625, f1: 0.6714, edges-rel-semeval_loss: 0.0786
10/01 07:33:53 AM: ***** Step 1400 / Validation 14 *****
10/01 07:33:53 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:33:53 AM: Validating...
10/01 07:33:55 AM: Best result seen so far for edges-rel-semeval.
10/01 07:33:55 AM: Best result seen so far for macro.
10/01 07:33:55 AM: Updating LR scheduler:
10/01 07:33:55 AM: 	Best result seen so far for macro_avg: 0.671
10/01 07:33:55 AM: 	# validation passes without improvement: 0
10/01 07:33:55 AM: edges-rel-semeval_loss: training: 0.076778 validation: 0.081570
10/01 07:33:55 AM: macro_avg: validation: 0.670860
10/01 07:33:55 AM: micro_avg: validation: 0.000000
10/01 07:33:55 AM: edges-rel-semeval_mcc: training: 0.689201 validation: 0.671944
10/01 07:33:55 AM: edges-rel-semeval_acc: training: 0.546250 validation: 0.530896
10/01 07:33:55 AM: edges-rel-semeval_precision: training: 0.847836 validation: 0.843215
10/01 07:33:55 AM: edges-rel-semeval_recall: training: 0.581563 validation: 0.557006
10/01 07:33:55 AM: edges-rel-semeval_f1: training: 0.689898 validation: 0.670860
10/01 07:33:55 AM: Global learning rate: 0.0001
10/01 07:33:55 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:00 AM: ***** Step 1500 / Validation 15 *****
10/01 07:34:00 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:00 AM: Validating...
10/01 07:34:00 AM: Evaluate: task edges-rel-semeval, batch 5 (36): mcc: 0.7186, acc: 0.6000, precision: 0.8547, recall: 0.6250, f1: 0.7220, edges-rel-semeval_loss: 0.0707
10/01 07:34:02 AM: Best result seen so far for edges-rel-semeval.
10/01 07:34:02 AM: Best result seen so far for macro.
10/01 07:34:02 AM: Updating LR scheduler:
10/01 07:34:02 AM: 	Best result seen so far for macro_avg: 0.685
10/01 07:34:02 AM: 	# validation passes without improvement: 0
10/01 07:34:02 AM: edges-rel-semeval_loss: training: 0.075862 validation: 0.079995
10/01 07:34:02 AM: macro_avg: validation: 0.684989
10/01 07:34:02 AM: micro_avg: validation: 0.000000
10/01 07:34:02 AM: edges-rel-semeval_mcc: training: 0.697493 validation: 0.688886
10/01 07:34:02 AM: edges-rel-semeval_acc: training: 0.550937 validation: 0.543951
10/01 07:34:02 AM: edges-rel-semeval_precision: training: 0.857403 validation: 0.872140
10/01 07:34:02 AM: edges-rel-semeval_recall: training: 0.588125 validation: 0.563969
10/01 07:34:02 AM: edges-rel-semeval_f1: training: 0.697683 validation: 0.684989
10/01 07:34:02 AM: Global learning rate: 0.0001
10/01 07:34:02 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:08 AM: ***** Step 1600 / Validation 16 *****
10/01 07:34:08 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:08 AM: Validating...
10/01 07:34:10 AM: Updating LR scheduler:
10/01 07:34:10 AM: 	Best result seen so far for macro_avg: 0.685
10/01 07:34:10 AM: 	# validation passes without improvement: 1
10/01 07:34:10 AM: edges-rel-semeval_loss: training: 0.071486 validation: 0.080678
10/01 07:34:10 AM: macro_avg: validation: 0.680106
10/01 07:34:10 AM: micro_avg: validation: 0.000000
10/01 07:34:10 AM: edges-rel-semeval_mcc: training: 0.719117 validation: 0.684502
10/01 07:34:10 AM: edges-rel-semeval_acc: training: 0.581520 validation: 0.542211
10/01 07:34:10 AM: edges-rel-semeval_precision: training: 0.859008 validation: 0.870924
10/01 07:34:10 AM: edges-rel-semeval_recall: training: 0.622517 validation: 0.557876
10/01 07:34:10 AM: edges-rel-semeval_f1: training: 0.721887 validation: 0.680106
10/01 07:34:10 AM: Global learning rate: 0.0001
10/01 07:34:10 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:10 AM: Update 1601: task edges-rel-semeval, batch 1 (1601): mcc: 0.6586, acc: 0.5312, precision: 0.8500, recall: 0.5312, f1: 0.6538, edges-rel-semeval_loss: 0.0880
10/01 07:34:15 AM: ***** Step 1700 / Validation 17 *****
10/01 07:34:15 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:15 AM: Validating...
10/01 07:34:17 AM: Best result seen so far for edges-rel-semeval.
10/01 07:34:17 AM: Best result seen so far for macro.
10/01 07:34:17 AM: Updating LR scheduler:
10/01 07:34:17 AM: 	Best result seen so far for macro_avg: 0.693
10/01 07:34:17 AM: 	# validation passes without improvement: 0
10/01 07:34:17 AM: edges-rel-semeval_loss: training: 0.072858 validation: 0.078289
10/01 07:34:17 AM: macro_avg: validation: 0.692583
10/01 07:34:17 AM: micro_avg: validation: 0.000000
10/01 07:34:17 AM: edges-rel-semeval_mcc: training: 0.707923 validation: 0.690347
10/01 07:34:17 AM: edges-rel-semeval_acc: training: 0.571562 validation: 0.561358
10/01 07:34:17 AM: edges-rel-semeval_precision: training: 0.843978 validation: 0.839950
10/01 07:34:17 AM: edges-rel-semeval_recall: training: 0.615313 validation: 0.589208
10/01 07:34:17 AM: edges-rel-semeval_f1: training: 0.711730 validation: 0.692583
10/01 07:34:17 AM: Global learning rate: 0.0001
10/01 07:34:17 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:20 AM: Update 1751: task edges-rel-semeval, batch 51 (1751): mcc: 0.7373, acc: 0.6039, precision: 0.8559, recall: 0.6556, f1: 0.7425, edges-rel-semeval_loss: 0.0679
10/01 07:34:23 AM: ***** Step 1800 / Validation 18 *****
10/01 07:34:23 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:23 AM: Validating...
10/01 07:34:25 AM: Updating LR scheduler:
10/01 07:34:25 AM: 	Best result seen so far for macro_avg: 0.693
10/01 07:34:25 AM: 	# validation passes without improvement: 1
10/01 07:34:25 AM: edges-rel-semeval_loss: training: 0.069091 validation: 0.078587
10/01 07:34:25 AM: macro_avg: validation: 0.684128
10/01 07:34:25 AM: micro_avg: validation: 0.000000
10/01 07:34:25 AM: edges-rel-semeval_mcc: training: 0.733302 validation: 0.686007
10/01 07:34:25 AM: edges-rel-semeval_acc: training: 0.601703 validation: 0.545692
10/01 07:34:25 AM: edges-rel-semeval_precision: training: 0.858819 validation: 0.859211
10/01 07:34:25 AM: edges-rel-semeval_recall: training: 0.646484 validation: 0.568320
10/01 07:34:25 AM: edges-rel-semeval_f1: training: 0.737675 validation: 0.684128
10/01 07:34:25 AM: Global learning rate: 0.0001
10/01 07:34:25 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:30 AM: ***** Step 1900 / Validation 19 *****
10/01 07:34:30 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:30 AM: Validating...
10/01 07:34:30 AM: Evaluate: task edges-rel-semeval, batch 13 (36): mcc: 0.7176, acc: 0.5986, precision: 0.8525, recall: 0.6250, f1: 0.7212, edges-rel-semeval_loss: 0.0716
10/01 07:34:32 AM: Updating LR scheduler:
10/01 07:34:32 AM: 	Best result seen so far for macro_avg: 0.693
10/01 07:34:32 AM: 	# validation passes without improvement: 2
10/01 07:34:32 AM: edges-rel-semeval_loss: training: 0.068976 validation: 0.077301
10/01 07:34:32 AM: macro_avg: validation: 0.686722
10/01 07:34:32 AM: micro_avg: validation: 0.000000
10/01 07:34:32 AM: edges-rel-semeval_mcc: training: 0.727260 validation: 0.686742
10/01 07:34:32 AM: edges-rel-semeval_acc: training: 0.595000 validation: 0.556136
10/01 07:34:32 AM: edges-rel-semeval_precision: training: 0.851284 validation: 0.849807
10/01 07:34:32 AM: edges-rel-semeval_recall: training: 0.642187 validation: 0.576153
10/01 07:34:32 AM: edges-rel-semeval_f1: training: 0.732098 validation: 0.686722
10/01 07:34:32 AM: Global learning rate: 0.0001
10/01 07:34:32 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:38 AM: ***** Step 2000 / Validation 20 *****
10/01 07:34:38 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:38 AM: Validating...
10/01 07:34:40 AM: Updating LR scheduler:
10/01 07:34:40 AM: 	Best result seen so far for macro_avg: 0.693
10/01 07:34:40 AM: 	# validation passes without improvement: 3
10/01 07:34:40 AM: edges-rel-semeval_loss: training: 0.065987 validation: 0.078126
10/01 07:34:40 AM: macro_avg: validation: 0.691667
10/01 07:34:40 AM: micro_avg: validation: 0.000000
10/01 07:34:40 AM: edges-rel-semeval_mcc: training: 0.745454 validation: 0.692855
10/01 07:34:40 AM: edges-rel-semeval_acc: training: 0.615579 validation: 0.557006
10/01 07:34:40 AM: edges-rel-semeval_precision: training: 0.867745 validation: 0.861219
10/01 07:34:40 AM: edges-rel-semeval_recall: training: 0.660044 validation: 0.577894
10/01 07:34:40 AM: edges-rel-semeval_f1: training: 0.749776 validation: 0.691667
10/01 07:34:40 AM: Global learning rate: 0.0001
10/01 07:34:40 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:40 AM: Update 2010: task edges-rel-semeval, batch 10 (2010): mcc: 0.7664, acc: 0.6312, precision: 0.9099, recall: 0.6625, f1: 0.7667, edges-rel-semeval_loss: 0.0616
10/01 07:34:45 AM: ***** Step 2100 / Validation 21 *****
10/01 07:34:45 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:45 AM: Validating...
10/01 07:34:47 AM: Best result seen so far for edges-rel-semeval.
10/01 07:34:47 AM: Best result seen so far for macro.
10/01 07:34:47 AM: Updating LR scheduler:
10/01 07:34:47 AM: 	Best result seen so far for macro_avg: 0.703
10/01 07:34:47 AM: 	# validation passes without improvement: 0
10/01 07:34:47 AM: edges-rel-semeval_loss: training: 0.065305 validation: 0.076514
10/01 07:34:47 AM: macro_avg: validation: 0.702813
10/01 07:34:47 AM: micro_avg: validation: 0.000000
10/01 07:34:47 AM: edges-rel-semeval_mcc: training: 0.745251 validation: 0.701226
10/01 07:34:47 AM: edges-rel-semeval_acc: training: 0.618125 validation: 0.570931
10/01 07:34:47 AM: edges-rel-semeval_precision: training: 0.865794 validation: 0.852357
10/01 07:34:47 AM: edges-rel-semeval_recall: training: 0.661250 validation: 0.597911
10/01 07:34:47 AM: edges-rel-semeval_f1: training: 0.749823 validation: 0.702813
10/01 07:34:47 AM: Global learning rate: 0.0001
10/01 07:34:47 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:34:50 AM: Update 2159: task edges-rel-semeval, batch 59 (2159): mcc: 0.7221, acc: 0.5917, precision: 0.8367, recall: 0.6450, f1: 0.7284, edges-rel-semeval_loss: 0.0681
10/01 07:34:52 AM: ***** Step 2200 / Validation 22 *****
10/01 07:34:52 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:34:52 AM: Validating...
10/01 07:34:55 AM: Updating LR scheduler:
10/01 07:34:55 AM: 	Best result seen so far for macro_avg: 0.703
10/01 07:34:55 AM: 	# validation passes without improvement: 1
10/01 07:34:55 AM: edges-rel-semeval_loss: training: 0.064334 validation: 0.075808
10/01 07:34:55 AM: macro_avg: validation: 0.691628
10/01 07:34:55 AM: micro_avg: validation: 0.000000
10/01 07:34:55 AM: edges-rel-semeval_mcc: training: 0.742034 validation: 0.692494
10/01 07:34:55 AM: edges-rel-semeval_acc: training: 0.617155 validation: 0.555265
10/01 07:34:55 AM: edges-rel-semeval_precision: training: 0.856910 validation: 0.859173
10/01 07:34:55 AM: edges-rel-semeval_recall: training: 0.662882 validation: 0.578764
10/01 07:34:55 AM: edges-rel-semeval_f1: training: 0.747511 validation: 0.691628
10/01 07:34:55 AM: Global learning rate: 0.0001
10/01 07:34:55 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:00 AM: ***** Step 2300 / Validation 23 *****
10/01 07:35:00 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:00 AM: Validating...
10/01 07:35:01 AM: Evaluate: task edges-rel-semeval, batch 16 (36): mcc: 0.7348, acc: 0.6211, precision: 0.8378, recall: 0.6660, f1: 0.7421, edges-rel-semeval_loss: 0.0712
10/01 07:35:02 AM: Best result seen so far for edges-rel-semeval.
10/01 07:35:02 AM: Best result seen so far for macro.
10/01 07:35:02 AM: Updating LR scheduler:
10/01 07:35:02 AM: 	Best result seen so far for macro_avg: 0.708
10/01 07:35:02 AM: 	# validation passes without improvement: 0
10/01 07:35:02 AM: edges-rel-semeval_loss: training: 0.065104 validation: 0.075594
10/01 07:35:02 AM: macro_avg: validation: 0.708292
10/01 07:35:02 AM: micro_avg: validation: 0.000000
10/01 07:35:02 AM: edges-rel-semeval_mcc: training: 0.741636 validation: 0.703069
10/01 07:35:02 AM: edges-rel-semeval_acc: training: 0.609375 validation: 0.579634
10/01 07:35:02 AM: edges-rel-semeval_precision: training: 0.860816 validation: 0.831184
10/01 07:35:02 AM: edges-rel-semeval_recall: training: 0.659063 validation: 0.617058
10/01 07:35:02 AM: edges-rel-semeval_f1: training: 0.746549 validation: 0.708292
10/01 07:35:02 AM: Global learning rate: 0.0001
10/01 07:35:02 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:08 AM: ***** Step 2400 / Validation 24 *****
10/01 07:35:08 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:08 AM: Validating...
10/01 07:35:10 AM: Updating LR scheduler:
10/01 07:35:10 AM: 	Best result seen so far for macro_avg: 0.708
10/01 07:35:10 AM: 	# validation passes without improvement: 1
10/01 07:35:10 AM: edges-rel-semeval_loss: training: 0.059986 validation: 0.075471
10/01 07:35:10 AM: macro_avg: validation: 0.687958
10/01 07:35:10 AM: micro_avg: validation: 0.000000
10/01 07:35:10 AM: edges-rel-semeval_mcc: training: 0.775123 validation: 0.689987
10/01 07:35:10 AM: edges-rel-semeval_acc: training: 0.650583 validation: 0.550914
10/01 07:35:10 AM: edges-rel-semeval_precision: training: 0.885371 validation: 0.863338
10/01 07:35:10 AM: edges-rel-semeval_recall: training: 0.696626 validation: 0.571802
10/01 07:35:10 AM: edges-rel-semeval_f1: training: 0.779739 validation: 0.687958
10/01 07:35:10 AM: Global learning rate: 0.0001
10/01 07:35:10 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:11 AM: Update 2409: task edges-rel-semeval, batch 9 (2409): mcc: 0.7800, acc: 0.6632, precision: 0.8899, recall: 0.7014, f1: 0.7845, edges-rel-semeval_loss: 0.0577
10/01 07:35:15 AM: ***** Step 2500 / Validation 25 *****
10/01 07:35:15 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:15 AM: Validating...
10/01 07:35:18 AM: Best result seen so far for edges-rel-semeval.
10/01 07:35:18 AM: Best result seen so far for macro.
10/01 07:35:18 AM: Updating LR scheduler:
10/01 07:35:18 AM: 	Best result seen so far for macro_avg: 0.714
10/01 07:35:18 AM: 	# validation passes without improvement: 0
10/01 07:35:18 AM: edges-rel-semeval_loss: training: 0.060773 validation: 0.073612
10/01 07:35:18 AM: macro_avg: validation: 0.714070
10/01 07:35:18 AM: micro_avg: validation: 0.000000
10/01 07:35:18 AM: edges-rel-semeval_mcc: training: 0.765322 validation: 0.710710
10/01 07:35:18 AM: edges-rel-semeval_acc: training: 0.643750 validation: 0.589208
10/01 07:35:18 AM: edges-rel-semeval_precision: training: 0.873960 validation: 0.848921
10/01 07:35:18 AM: edges-rel-semeval_recall: training: 0.689062 validation: 0.616188
10/01 07:35:18 AM: edges-rel-semeval_f1: training: 0.770575 validation: 0.714070
10/01 07:35:18 AM: Global learning rate: 0.0001
10/01 07:35:18 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:21 AM: Update 2560: task edges-rel-semeval, batch 60 (2560): mcc: 0.7525, acc: 0.6240, precision: 0.8566, recall: 0.6812, f1: 0.7589, edges-rel-semeval_loss: 0.0611
10/01 07:35:23 AM: ***** Step 2600 / Validation 26 *****
10/01 07:35:23 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:23 AM: Validating...
10/01 07:35:25 AM: Best result seen so far for edges-rel-semeval.
10/01 07:35:25 AM: Best result seen so far for macro.
10/01 07:35:25 AM: Updating LR scheduler:
10/01 07:35:25 AM: 	Best result seen so far for macro_avg: 0.715
10/01 07:35:25 AM: 	# validation passes without improvement: 0
10/01 07:35:25 AM: edges-rel-semeval_loss: training: 0.060031 validation: 0.074856
10/01 07:35:25 AM: macro_avg: validation: 0.715455
10/01 07:35:25 AM: micro_avg: validation: 0.000000
10/01 07:35:25 AM: edges-rel-semeval_mcc: training: 0.760715 validation: 0.714764
10/01 07:35:25 AM: edges-rel-semeval_acc: training: 0.635762 validation: 0.584856
10/01 07:35:25 AM: edges-rel-semeval_precision: training: 0.865820 validation: 0.868323
10/01 07:35:25 AM: edges-rel-semeval_recall: training: 0.687796 validation: 0.608355
10/01 07:35:25 AM: edges-rel-semeval_f1: training: 0.766608 validation: 0.715455
10/01 07:35:25 AM: Global learning rate: 0.0001
10/01 07:35:25 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:30 AM: ***** Step 2700 / Validation 27 *****
10/01 07:35:30 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:30 AM: Validating...
10/01 07:35:31 AM: Evaluate: task edges-rel-semeval, batch 5 (36): mcc: 0.7566, acc: 0.6500, precision: 0.8651, recall: 0.6812, f1: 0.7622, edges-rel-semeval_loss: 0.0648
10/01 07:35:33 AM: Updating LR scheduler:
10/01 07:35:33 AM: 	Best result seen so far for macro_avg: 0.715
10/01 07:35:33 AM: 	# validation passes without improvement: 1
10/01 07:35:33 AM: edges-rel-semeval_loss: training: 0.057353 validation: 0.074327
10/01 07:35:33 AM: macro_avg: validation: 0.709082
10/01 07:35:33 AM: micro_avg: validation: 0.000000
10/01 07:35:33 AM: edges-rel-semeval_mcc: training: 0.782894 validation: 0.708462
10/01 07:35:33 AM: edges-rel-semeval_acc: training: 0.666250 validation: 0.582245
10/01 07:35:33 AM: edges-rel-semeval_precision: training: 0.878741 validation: 0.863750
10/01 07:35:33 AM: edges-rel-semeval_recall: training: 0.715625 validation: 0.601393
10/01 07:35:33 AM: edges-rel-semeval_f1: training: 0.788839 validation: 0.709082
10/01 07:35:33 AM: Global learning rate: 0.0001
10/01 07:35:33 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:38 AM: ***** Step 2800 / Validation 28 *****
10/01 07:35:38 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:38 AM: Validating...
10/01 07:35:40 AM: Updating LR scheduler:
10/01 07:35:40 AM: 	Best result seen so far for macro_avg: 0.715
10/01 07:35:40 AM: 	# validation passes without improvement: 2
10/01 07:35:40 AM: edges-rel-semeval_loss: training: 0.059655 validation: 0.074741
10/01 07:35:40 AM: macro_avg: validation: 0.708669
10/01 07:35:40 AM: micro_avg: validation: 0.000000
10/01 07:35:40 AM: edges-rel-semeval_mcc: training: 0.765421 validation: 0.704897
10/01 07:35:40 AM: edges-rel-semeval_acc: training: 0.647745 validation: 0.579634
10/01 07:35:40 AM: edges-rel-semeval_precision: training: 0.863158 validation: 0.841916
10/01 07:35:40 AM: edges-rel-semeval_recall: training: 0.698202 validation: 0.611836
10/01 07:35:40 AM: edges-rel-semeval_f1: training: 0.771966 validation: 0.708669
10/01 07:35:40 AM: Global learning rate: 0.0001
10/01 07:35:40 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:41 AM: Update 2805: task edges-rel-semeval, batch 5 (2805): mcc: 0.8458, acc: 0.7375, precision: 0.8966, recall: 0.8125, f1: 0.8525, edges-rel-semeval_loss: 0.0452
10/01 07:35:45 AM: ***** Step 2900 / Validation 29 *****
10/01 07:35:45 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:45 AM: Validating...
10/01 07:35:47 AM: Updating LR scheduler:
10/01 07:35:47 AM: 	Best result seen so far for macro_avg: 0.715
10/01 07:35:47 AM: 	# validation passes without improvement: 3
10/01 07:35:47 AM: edges-rel-semeval_loss: training: 0.054683 validation: 0.075089
10/01 07:35:47 AM: macro_avg: validation: 0.705643
10/01 07:35:47 AM: micro_avg: validation: 0.000000
10/01 07:35:47 AM: edges-rel-semeval_mcc: training: 0.787107 validation: 0.703141
10/01 07:35:47 AM: edges-rel-semeval_acc: training: 0.666562 validation: 0.576153
10/01 07:35:47 AM: edges-rel-semeval_precision: training: 0.879711 validation: 0.848411
10/01 07:35:47 AM: edges-rel-semeval_recall: training: 0.722188 validation: 0.604003
10/01 07:35:47 AM: edges-rel-semeval_f1: training: 0.793204 validation: 0.705643
10/01 07:35:47 AM: Global learning rate: 0.0001
10/01 07:35:47 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:35:51 AM: Update 2967: task edges-rel-semeval, batch 67 (2967): mcc: 0.7709, acc: 0.6521, precision: 0.8656, recall: 0.7057, f1: 0.7775, edges-rel-semeval_loss: 0.0590
10/01 07:35:52 AM: ***** Step 3000 / Validation 30 *****
10/01 07:35:52 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:35:52 AM: Validating...
10/01 07:35:54 AM: Updating LR scheduler:
10/01 07:35:54 AM: 	Best result seen so far for macro_avg: 0.715
10/01 07:35:54 AM: 	# validation passes without improvement: 0
10/01 07:35:54 AM: edges-rel-semeval_loss: training: 0.059347 validation: 0.074505
10/01 07:35:54 AM: macro_avg: validation: 0.711694
10/01 07:35:54 AM: micro_avg: validation: 0.000000
10/01 07:35:54 AM: edges-rel-semeval_mcc: training: 0.770733 validation: 0.708106
10/01 07:35:54 AM: edges-rel-semeval_acc: training: 0.651875 validation: 0.585727
10/01 07:35:54 AM: edges-rel-semeval_precision: training: 0.865746 validation: 0.845509
10/01 07:35:54 AM: edges-rel-semeval_recall: training: 0.705312 validation: 0.614447
10/01 07:35:54 AM: edges-rel-semeval_f1: training: 0.777338 validation: 0.711694
10/01 07:35:54 AM: Global learning rate: 5e-05
10/01 07:35:54 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:00 AM: ***** Step 3100 / Validation 31 *****
10/01 07:36:00 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:00 AM: Validating...
10/01 07:36:01 AM: Evaluate: task edges-rel-semeval, batch 7 (36): mcc: 0.7692, acc: 0.6518, precision: 0.8895, recall: 0.6830, f1: 0.7727, edges-rel-semeval_loss: 0.0636
10/01 07:36:03 AM: Updating LR scheduler:
10/01 07:36:03 AM: 	Best result seen so far for macro_avg: 0.715
10/01 07:36:03 AM: 	# validation passes without improvement: 1
10/01 07:36:03 AM: edges-rel-semeval_loss: training: 0.054150 validation: 0.075222
10/01 07:36:03 AM: macro_avg: validation: 0.702375
10/01 07:36:03 AM: micro_avg: validation: 0.000000
10/01 07:36:03 AM: edges-rel-semeval_mcc: training: 0.787148 validation: 0.698634
10/01 07:36:03 AM: edges-rel-semeval_acc: training: 0.671397 validation: 0.577023
10/01 07:36:03 AM: edges-rel-semeval_precision: training: 0.877965 validation: 0.837349
10/01 07:36:03 AM: edges-rel-semeval_recall: training: 0.723746 validation: 0.604874
10/01 07:36:03 AM: edges-rel-semeval_f1: training: 0.793431 validation: 0.702375
10/01 07:36:03 AM: Global learning rate: 5e-05
10/01 07:36:03 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:08 AM: ***** Step 3200 / Validation 32 *****
10/01 07:36:08 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:08 AM: Validating...
10/01 07:36:10 AM: Updating LR scheduler:
10/01 07:36:10 AM: 	Best result seen so far for macro_avg: 0.715
10/01 07:36:10 AM: 	# validation passes without improvement: 2
10/01 07:36:10 AM: edges-rel-semeval_loss: training: 0.056199 validation: 0.074675
10/01 07:36:10 AM: macro_avg: validation: 0.715005
10/01 07:36:10 AM: micro_avg: validation: 0.000000
10/01 07:36:10 AM: edges-rel-semeval_mcc: training: 0.790050 validation: 0.711454
10/01 07:36:10 AM: edges-rel-semeval_acc: training: 0.676250 validation: 0.593560
10/01 07:36:10 AM: edges-rel-semeval_precision: training: 0.877728 validation: 0.848268
10/01 07:36:10 AM: edges-rel-semeval_recall: training: 0.729062 validation: 0.617929
10/01 07:36:10 AM: edges-rel-semeval_f1: training: 0.796518 validation: 0.715005
10/01 07:36:10 AM: Global learning rate: 5e-05
10/01 07:36:10 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:11 AM: Update 3218: task edges-rel-semeval, batch 18 (3218): mcc: 0.7895, acc: 0.6788, precision: 0.8704, recall: 0.7344, f1: 0.7966, edges-rel-semeval_loss: 0.0525
10/01 07:36:15 AM: ***** Step 3300 / Validation 33 *****
10/01 07:36:15 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:15 AM: Validating...
10/01 07:36:18 AM: Best result seen so far for edges-rel-semeval.
10/01 07:36:18 AM: Best result seen so far for macro.
10/01 07:36:18 AM: Updating LR scheduler:
10/01 07:36:18 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:36:18 AM: 	# validation passes without improvement: 0
10/01 07:36:18 AM: edges-rel-semeval_loss: training: 0.052173 validation: 0.074041
10/01 07:36:18 AM: macro_avg: validation: 0.724753
10/01 07:36:18 AM: micro_avg: validation: 0.000000
10/01 07:36:18 AM: edges-rel-semeval_mcc: training: 0.793534 validation: 0.719179
10/01 07:36:18 AM: edges-rel-semeval_acc: training: 0.684642 validation: 0.609225
10/01 07:36:18 AM: edges-rel-semeval_precision: training: 0.875328 validation: 0.840413
10/01 07:36:18 AM: edges-rel-semeval_recall: training: 0.737307 validation: 0.637076
10/01 07:36:18 AM: edges-rel-semeval_f1: training: 0.800411 validation: 0.724753
10/01 07:36:18 AM: Global learning rate: 5e-05
10/01 07:36:18 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:21 AM: Update 3365: task edges-rel-semeval, batch 65 (3365): mcc: 0.7954, acc: 0.6837, precision: 0.8800, recall: 0.7365, f1: 0.8019, edges-rel-semeval_loss: 0.0528
10/01 07:36:22 AM: ***** Step 3400 / Validation 34 *****
10/01 07:36:22 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:22 AM: Validating...
10/01 07:36:25 AM: Updating LR scheduler:
10/01 07:36:25 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:36:25 AM: 	# validation passes without improvement: 1
10/01 07:36:25 AM: edges-rel-semeval_loss: training: 0.053104 validation: 0.075057
10/01 07:36:25 AM: macro_avg: validation: 0.710794
10/01 07:36:25 AM: micro_avg: validation: 0.000000
10/01 07:36:25 AM: edges-rel-semeval_mcc: training: 0.799576 validation: 0.708883
10/01 07:36:25 AM: edges-rel-semeval_acc: training: 0.688750 validation: 0.583116
10/01 07:36:25 AM: edges-rel-semeval_precision: training: 0.884572 validation: 0.856442
10/01 07:36:25 AM: edges-rel-semeval_recall: training: 0.740000 validation: 0.607485
10/01 07:36:25 AM: edges-rel-semeval_f1: training: 0.805853 validation: 0.710794
10/01 07:36:25 AM: Global learning rate: 5e-05
10/01 07:36:25 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:30 AM: ***** Step 3500 / Validation 35 *****
10/01 07:36:30 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:30 AM: Validating...
10/01 07:36:31 AM: Evaluate: task edges-rel-semeval, batch 11 (36): mcc: 0.7534, acc: 0.6449, precision: 0.8681, recall: 0.6733, f1: 0.7584, edges-rel-semeval_loss: 0.0691
10/01 07:36:32 AM: Updating LR scheduler:
10/01 07:36:32 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:36:32 AM: 	# validation passes without improvement: 2
10/01 07:36:32 AM: edges-rel-semeval_loss: training: 0.050728 validation: 0.074291
10/01 07:36:32 AM: macro_avg: validation: 0.714643
10/01 07:36:32 AM: micro_avg: validation: 0.000000
10/01 07:36:32 AM: edges-rel-semeval_mcc: training: 0.805738 validation: 0.709876
10/01 07:36:32 AM: edges-rel-semeval_acc: training: 0.697256 validation: 0.594430
10/01 07:36:32 AM: edges-rel-semeval_precision: training: 0.885374 validation: 0.839202
10/01 07:36:32 AM: edges-rel-semeval_recall: training: 0.750237 validation: 0.622280
10/01 07:36:32 AM: edges-rel-semeval_f1: training: 0.812223 validation: 0.714643
10/01 07:36:32 AM: Global learning rate: 5e-05
10/01 07:36:32 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:37 AM: ***** Step 3600 / Validation 36 *****
10/01 07:36:37 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:37 AM: Validating...
10/01 07:36:39 AM: Updating LR scheduler:
10/01 07:36:39 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:36:39 AM: 	# validation passes without improvement: 3
10/01 07:36:39 AM: edges-rel-semeval_loss: training: 0.052598 validation: 0.074733
10/01 07:36:39 AM: macro_avg: validation: 0.714071
10/01 07:36:39 AM: micro_avg: validation: 0.000000
10/01 07:36:39 AM: edges-rel-semeval_mcc: training: 0.796246 validation: 0.709580
10/01 07:36:39 AM: edges-rel-semeval_acc: training: 0.684688 validation: 0.596171
10/01 07:36:39 AM: edges-rel-semeval_precision: training: 0.877407 validation: 0.840802
10/01 07:36:39 AM: edges-rel-semeval_recall: training: 0.740313 validation: 0.620540
10/01 07:36:39 AM: edges-rel-semeval_f1: training: 0.803051 validation: 0.714071
10/01 07:36:39 AM: Global learning rate: 5e-05
10/01 07:36:39 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:41 AM: Update 3628: task edges-rel-semeval, batch 28 (3628): mcc: 0.8037, acc: 0.6964, precision: 0.8892, recall: 0.7433, f1: 0.8097, edges-rel-semeval_loss: 0.0498
10/01 07:36:45 AM: ***** Step 3700 / Validation 37 *****
10/01 07:36:45 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:45 AM: Validating...
10/01 07:36:47 AM: Updating LR scheduler:
10/01 07:36:47 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:36:47 AM: 	# validation passes without improvement: 0
10/01 07:36:47 AM: edges-rel-semeval_loss: training: 0.051944 validation: 0.074989
10/01 07:36:47 AM: macro_avg: validation: 0.711246
10/01 07:36:47 AM: micro_avg: validation: 0.000000
10/01 07:36:47 AM: edges-rel-semeval_mcc: training: 0.795630 validation: 0.708478
10/01 07:36:47 AM: edges-rel-semeval_acc: training: 0.685588 validation: 0.588338
10/01 07:36:47 AM: edges-rel-semeval_precision: training: 0.884367 validation: 0.850909
10/01 07:36:47 AM: edges-rel-semeval_recall: training: 0.733207 validation: 0.610966
10/01 07:36:47 AM: edges-rel-semeval_f1: training: 0.801724 validation: 0.711246
10/01 07:36:47 AM: Global learning rate: 2.5e-05
10/01 07:36:47 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:36:51 AM: Update 3775: task edges-rel-semeval, batch 75 (3775): mcc: 0.8044, acc: 0.6950, precision: 0.8847, recall: 0.7483, f1: 0.8108, edges-rel-semeval_loss: 0.0506
10/01 07:36:52 AM: ***** Step 3800 / Validation 38 *****
10/01 07:36:52 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:36:52 AM: Validating...
10/01 07:36:54 AM: Updating LR scheduler:
10/01 07:36:54 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:36:54 AM: 	# validation passes without improvement: 1
10/01 07:36:54 AM: edges-rel-semeval_loss: training: 0.049942 validation: 0.074340
10/01 07:36:54 AM: macro_avg: validation: 0.711607
10/01 07:36:54 AM: micro_avg: validation: 0.000000
10/01 07:36:54 AM: edges-rel-semeval_mcc: training: 0.810373 validation: 0.708948
10/01 07:36:54 AM: edges-rel-semeval_acc: training: 0.703438 validation: 0.590078
10/01 07:36:54 AM: edges-rel-semeval_precision: training: 0.888644 validation: 0.851942
10/01 07:36:54 AM: edges-rel-semeval_recall: training: 0.755625 validation: 0.610966
10/01 07:36:54 AM: edges-rel-semeval_f1: training: 0.816754 validation: 0.711607
10/01 07:36:54 AM: Global learning rate: 2.5e-05
10/01 07:36:54 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:00 AM: ***** Step 3900 / Validation 39 *****
10/01 07:37:00 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:37:00 AM: Validating...
10/01 07:37:01 AM: Evaluate: task edges-rel-semeval, batch 16 (36): mcc: 0.7416, acc: 0.6309, precision: 0.8619, recall: 0.6582, f1: 0.7464, edges-rel-semeval_loss: 0.0691
10/01 07:37:02 AM: Updating LR scheduler:
10/01 07:37:02 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:37:02 AM: 	# validation passes without improvement: 2
10/01 07:37:02 AM: edges-rel-semeval_loss: training: 0.049114 validation: 0.074512
10/01 07:37:02 AM: macro_avg: validation: 0.714431
10/01 07:37:02 AM: micro_avg: validation: 0.000000
10/01 07:37:02 AM: edges-rel-semeval_mcc: training: 0.811826 validation: 0.712388
10/01 07:37:02 AM: edges-rel-semeval_acc: training: 0.700095 validation: 0.588338
10/01 07:37:02 AM: edges-rel-semeval_precision: training: 0.891993 validation: 0.858364
10/01 07:37:02 AM: edges-rel-semeval_recall: training: 0.755282 validation: 0.611836
10/01 07:37:02 AM: edges-rel-semeval_f1: training: 0.817964 validation: 0.714431
10/01 07:37:02 AM: Global learning rate: 2.5e-05
10/01 07:37:02 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:07 AM: ***** Step 4000 / Validation 40 *****
10/01 07:37:07 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:37:07 AM: Validating...
10/01 07:37:09 AM: Updating LR scheduler:
10/01 07:37:09 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:37:09 AM: 	# validation passes without improvement: 3
10/01 07:37:09 AM: edges-rel-semeval_loss: training: 0.050996 validation: 0.074630
10/01 07:37:09 AM: macro_avg: validation: 0.710405
10/01 07:37:09 AM: micro_avg: validation: 0.000000
10/01 07:37:09 AM: edges-rel-semeval_mcc: training: 0.805232 validation: 0.709678
10/01 07:37:09 AM: edges-rel-semeval_acc: training: 0.693125 validation: 0.587467
10/01 07:37:09 AM: edges-rel-semeval_precision: training: 0.877174 validation: 0.864090
10/01 07:37:09 AM: edges-rel-semeval_recall: training: 0.756562 validation: 0.603133
10/01 07:37:09 AM: edges-rel-semeval_f1: training: 0.812416 validation: 0.710405
10/01 07:37:09 AM: Global learning rate: 2.5e-05
10/01 07:37:09 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:11 AM: Update 4034: task edges-rel-semeval, batch 34 (4034): mcc: 0.8160, acc: 0.7004, precision: 0.8898, recall: 0.7647, f1: 0.8225, edges-rel-semeval_loss: 0.0488
10/01 07:37:15 AM: ***** Step 4100 / Validation 41 *****
10/01 07:37:15 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:37:15 AM: Validating...
10/01 07:37:17 AM: Updating LR scheduler:
10/01 07:37:17 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:37:17 AM: 	# validation passes without improvement: 0
10/01 07:37:17 AM: edges-rel-semeval_loss: training: 0.049603 validation: 0.074203
10/01 07:37:17 AM: macro_avg: validation: 0.716734
10/01 07:37:17 AM: micro_avg: validation: 0.000000
10/01 07:37:17 AM: edges-rel-semeval_mcc: training: 0.808941 validation: 0.713454
10/01 07:37:17 AM: edges-rel-semeval_acc: training: 0.697572 validation: 0.591819
10/01 07:37:17 AM: edges-rel-semeval_precision: training: 0.885767 validation: 0.851497
10/01 07:37:17 AM: edges-rel-semeval_recall: training: 0.755598 validation: 0.618799
10/01 07:37:17 AM: edges-rel-semeval_f1: training: 0.815521 validation: 0.716734
10/01 07:37:17 AM: Global learning rate: 1.25e-05
10/01 07:37:17 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:21 AM: Update 4179: task edges-rel-semeval, batch 79 (4179): mcc: 0.8141, acc: 0.7073, precision: 0.8918, recall: 0.7595, f1: 0.8203, edges-rel-semeval_loss: 0.0490
10/01 07:37:22 AM: ***** Step 4200 / Validation 42 *****
10/01 07:37:22 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:37:22 AM: Validating...
10/01 07:37:24 AM: Updating LR scheduler:
10/01 07:37:24 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:37:24 AM: 	# validation passes without improvement: 1
10/01 07:37:24 AM: edges-rel-semeval_loss: training: 0.048752 validation: 0.074000
10/01 07:37:24 AM: macro_avg: validation: 0.714864
10/01 07:37:24 AM: micro_avg: validation: 0.000000
10/01 07:37:24 AM: edges-rel-semeval_mcc: training: 0.815823 validation: 0.711975
10/01 07:37:24 AM: edges-rel-semeval_acc: training: 0.711562 validation: 0.592689
10/01 07:37:24 AM: edges-rel-semeval_precision: training: 0.891527 validation: 0.852835
10/01 07:37:24 AM: edges-rel-semeval_recall: training: 0.762812 validation: 0.615318
10/01 07:37:24 AM: edges-rel-semeval_f1: training: 0.822162 validation: 0.714864
10/01 07:37:24 AM: Global learning rate: 1.25e-05
10/01 07:37:24 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:29 AM: ***** Step 4300 / Validation 43 *****
10/01 07:37:29 AM: edges-rel-semeval: trained on 100 batches, 0.465 epochs
10/01 07:37:29 AM: Validating...
10/01 07:37:31 AM: Evaluate: task edges-rel-semeval, batch 31 (36): mcc: 0.7439, acc: 0.6270, precision: 0.8743, recall: 0.6522, f1: 0.7471, edges-rel-semeval_loss: 0.0688
10/01 07:37:31 AM: Updating LR scheduler:
10/01 07:37:31 AM: 	Best result seen so far for macro_avg: 0.725
10/01 07:37:31 AM: 	# validation passes without improvement: 2
10/01 07:37:31 AM: Ran out of early stopping patience. Stopping training.
10/01 07:37:31 AM: edges-rel-semeval_loss: training: 0.050551 validation: 0.073998
10/01 07:37:31 AM: macro_avg: validation: 0.717380
10/01 07:37:31 AM: micro_avg: validation: 0.000000
10/01 07:37:31 AM: edges-rel-semeval_mcc: training: 0.800010 validation: 0.714057
10/01 07:37:31 AM: edges-rel-semeval_acc: training: 0.691875 validation: 0.594430
10/01 07:37:31 AM: edges-rel-semeval_precision: training: 0.879705 validation: 0.851675
10/01 07:37:31 AM: edges-rel-semeval_recall: training: 0.745000 validation: 0.619669
10/01 07:37:31 AM: edges-rel-semeval_f1: training: 0.806768 validation: 0.717380
10/01 07:37:31 AM: Global learning rate: 1.25e-05
10/01 07:37:31 AM: Saving checkpoints to: ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:31 AM: Stopped training after 43 validation checks
10/01 07:37:31 AM: Trained edges-rel-semeval for 4300 batches or 20.000 epochs
10/01 07:37:31 AM: ***** VALIDATION RESULTS *****
10/01 07:37:31 AM: edges-rel-semeval_f1 (for best val pass 33): edges-rel-semeval_loss: 0.07404, macro_avg: 0.72475, micro_avg: 0.00000, edges-rel-semeval_mcc: 0.71918, edges-rel-semeval_acc: 0.60923, edges-rel-semeval_precision: 0.84041, edges-rel-semeval_recall: 0.63708, edges-rel-semeval_f1: 0.72475
10/01 07:37:31 AM: micro_avg (for best val pass 1): edges-rel-semeval_loss: 0.18152, macro_avg: 0.00000, micro_avg: 0.00000, edges-rel-semeval_mcc: 0.00000, edges-rel-semeval_acc: 0.00000, edges-rel-semeval_precision: 0.00000, edges-rel-semeval_recall: 0.00000, edges-rel-semeval_f1: 0.00000
10/01 07:37:31 AM: macro_avg (for best val pass 33): edges-rel-semeval_loss: 0.07404, macro_avg: 0.72475, micro_avg: 0.00000, edges-rel-semeval_mcc: 0.71918, edges-rel-semeval_acc: 0.60923, edges-rel-semeval_precision: 0.84041, edges-rel-semeval_recall: 0.63708, edges-rel-semeval_f1: 0.72475
10/01 07:37:31 AM: Evaluating...
10/01 07:37:31 AM: Loaded model state from ./experiments/rel-semeval-allstrings-mix/run/edges-rel-semeval/model_state_target_train_val_33.best.th
10/01 07:37:31 AM: Evaluating on: edges-rel-semeval, split: val
10/01 07:37:34 AM: Task 'edges-rel-semeval': sorting predictions by 'idx'
10/01 07:37:34 AM: Finished evaluating on: edges-rel-semeval
10/01 07:37:34 AM: Task 'edges-rel-semeval': joining predictions with input split 'val'
10/01 07:37:34 AM: Task 'edges-rel-semeval': Wrote predictions to ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:34 AM: Wrote all preds for split 'val' to ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:34 AM: Evaluating on: edges-rel-semeval, split: test
10/01 07:37:39 AM: Task 'edges-rel-semeval': sorting predictions by 'idx'
10/01 07:37:39 AM: Finished evaluating on: edges-rel-semeval
10/01 07:37:39 AM: Task 'edges-rel-semeval': joining predictions with input split 'test'
10/01 07:37:39 AM: Task 'edges-rel-semeval': Wrote predictions to ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:39 AM: Wrote all preds for split 'test' to ./experiments/rel-semeval-allstrings-mix/run
10/01 07:37:39 AM: Writing results for split 'val' to ./experiments/rel-semeval-allstrings-mix/results.tsv
10/01 07:37:39 AM: micro_avg: 0.000, macro_avg: 0.725, edges-rel-semeval_mcc: 0.719, edges-rel-semeval_acc: 0.609, edges-rel-semeval_precision: 0.840, edges-rel-semeval_recall: 0.637, edges-rel-semeval_f1: 0.725
10/01 07:37:39 AM: Done!
10/01 07:37:39 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
