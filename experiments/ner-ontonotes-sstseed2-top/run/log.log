10/01 03:08:54 AM: Git branch: master
10/01 03:08:54 AM: Git SHA: 62183b2d03f2fae12b41eef8779808b6d354875e
10/01 03:08:55 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-sstseed2-top/",
  "exp_name": "experiments/ner-ontonotes-sstseed2-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-sstseed2-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/sstseed2",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-sstseed2-top__run",
  "run_dir": "./experiments/ner-ontonotes-sstseed2-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 03:08:55 AM: Saved config to ./experiments/ner-ontonotes-sstseed2-top/run/params.conf
10/01 03:08:55 AM: Using random seed 1234
10/01 03:09:18 AM: Using GPU 0
10/01 03:09:18 AM: Loading tasks...
10/01 03:09:18 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-sstseed2-top/
10/01 03:09:18 AM: 	Creating task edges-ner-ontonotes from scratch.
10/01 03:09:20 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
10/01 03:09:20 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
10/01 03:09:20 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
10/01 03:09:20 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
10/01 03:09:20 AM: 	Finished loading tasks: edges-ner-ontonotes.
10/01 03:09:20 AM: 	Building vocab from scratch.
10/01 03:09:20 AM: 	Counting units for task edges-ner-ontonotes.
10/01 03:09:22 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
10/01 03:09:23 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:23 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 03:09:23 AM: 	Saved vocab to ./experiments/ner-ontonotes-sstseed2-top/vocab
10/01 03:09:23 AM: Loading token dictionary from ./experiments/ner-ontonotes-sstseed2-top/vocab.
10/01 03:09:23 AM: 	Loaded vocab from ./experiments/ner-ontonotes-sstseed2-top/vocab
10/01 03:09:23 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
10/01 03:09:23 AM: 	Vocab namespace tokens: size 22840
10/01 03:09:23 AM: 	Vocab namespace bert_uncased: size 30524
10/01 03:09:23 AM: 	Vocab namespace chars: size 77
10/01 03:09:23 AM: 	Finished building vocab.
10/01 03:09:23 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
10/01 03:09:32 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-sstseed2-top/preproc/edges-ner-ontonotes__train_data
10/01 03:09:32 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
10/01 03:09:33 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-sstseed2-top/preproc/edges-ner-ontonotes__val_data
10/01 03:09:33 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
10/01 03:09:34 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-sstseed2-top/preproc/edges-ner-ontonotes__test_data
10/01 03:09:34 AM: 	Finished indexing tasks
10/01 03:09:34 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
10/01 03:09:34 AM: 	  Training on 
10/01 03:09:34 AM: 	  Evaluating on edges-ner-ontonotes
10/01 03:09:34 AM: 	Finished loading tasks in 15.514s
10/01 03:09:34 AM: 	 Tasks: ['edges-ner-ontonotes']
10/01 03:09:34 AM: Building model...
10/01 03:09:34 AM: Using BERT model (bert-base-uncased).
10/01 03:09:34 AM: LOADING A FUNETUNED MODEL from: 
10/01 03:09:34 AM: models/sstseed2
10/01 03:09:34 AM: loading configuration file models/sstseed2/config.json
10/01 03:09:34 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst-2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 03:09:34 AM: loading weights file models/sstseed2/pytorch_model.bin
10/01 03:09:37 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp26kt78ql
10/01 03:09:40 AM: copying /tmp/tmp26kt78ql to cache at ./experiments/ner-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:40 AM: creating metadata file for ./experiments/ner-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:40 AM: removing temp file /tmp/tmp26kt78ql
10/01 03:09:40 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:40 AM: Initializing parameters
10/01 03:09:40 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 03:09:40 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 03:09:40 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 03:09:40 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 03:09:40 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 03:09:40 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 03:09:40 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
10/01 03:09:44 AM: Model specification:
10/01 03:09:44 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
10/01 03:09:44 AM: Model parameters:
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:44 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:44 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 03:09:44 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:09:44 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
10/01 03:09:44 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
10/01 03:09:44 AM: Total number of parameters: 109688338 (1.09688e+08)
10/01 03:09:44 AM: Number of trainable parameters: 206098 (206098)
10/01 03:09:44 AM: Finished building model in 10.690s
10/01 03:09:44 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

10/01 03:09:48 AM: patience = 9
10/01 03:09:48 AM: val_interval = 1000
10/01 03:09:48 AM: max_vals = 250
10/01 03:09:48 AM: cuda_device = 0
10/01 03:09:48 AM: grad_norm = 5.0
10/01 03:09:48 AM: grad_clipping = None
10/01 03:09:48 AM: lr_decay = 0.99
10/01 03:09:48 AM: min_lr = 1e-06
10/01 03:09:48 AM: keep_all_checkpoints = 0
10/01 03:09:48 AM: val_data_limit = 5000
10/01 03:09:48 AM: max_epochs = -1
10/01 03:09:48 AM: dec_val_scale = 250
10/01 03:09:48 AM: training_data_fraction = 1
10/01 03:09:48 AM: type = adam
10/01 03:09:48 AM: parameter_groups = None
10/01 03:09:48 AM: Number of trainable parameters: 206098
10/01 03:09:48 AM: infer_type_and_cast = True
10/01 03:09:48 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:48 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:48 AM: lr = 0.0001
10/01 03:09:48 AM: amsgrad = True
10/01 03:09:48 AM: type = reduce_on_plateau
10/01 03:09:48 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:48 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:48 AM: mode = max
10/01 03:09:48 AM: factor = 0.5
10/01 03:09:48 AM: patience = 3
10/01 03:09:48 AM: threshold = 0.0001
10/01 03:09:48 AM: threshold_mode = abs
10/01 03:09:48 AM: verbose = True
10/01 03:09:48 AM: type = adam
10/01 03:09:48 AM: parameter_groups = None
10/01 03:09:48 AM: Number of trainable parameters: 206098
10/01 03:09:48 AM: infer_type_and_cast = True
10/01 03:09:48 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:48 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:48 AM: lr = 0.0001
10/01 03:09:48 AM: amsgrad = True
10/01 03:09:48 AM: type = reduce_on_plateau
10/01 03:09:48 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:48 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:48 AM: mode = max
10/01 03:09:48 AM: factor = 0.5
10/01 03:09:48 AM: patience = 3
10/01 03:09:48 AM: threshold = 0.0001
10/01 03:09:48 AM: threshold_mode = abs
10/01 03:09:48 AM: verbose = True
10/01 03:09:48 AM: Starting training without restoring from a checkpoint.
10/01 03:09:48 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
10/01 03:09:48 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
10/01 03:09:58 AM: Update 141: task edges-ner-ontonotes, batch 141 (141): mcc: 0.0253, acc: 0.0207, precision: 0.0880, recall: 0.0488, f1: 0.0628, edges-ner-ontonotes_loss: 0.2625
10/01 03:10:08 AM: Update 313: task edges-ner-ontonotes, batch 313 (313): mcc: 0.2390, acc: 0.1550, precision: 0.4163, recall: 0.1687, f1: 0.2401, edges-ner-ontonotes_loss: 0.1843
10/01 03:10:18 AM: Update 427: task edges-ner-ontonotes, batch 427 (427): mcc: 0.3931, acc: 0.2690, precision: 0.6136, recall: 0.2802, f1: 0.3847, edges-ner-ontonotes_loss: 0.1598
10/01 03:10:28 AM: Update 569: task edges-ner-ontonotes, batch 569 (569): mcc: 0.5096, acc: 0.3716, precision: 0.7288, recall: 0.3829, f1: 0.5020, edges-ner-ontonotes_loss: 0.1385
10/01 03:10:38 AM: Update 692: task edges-ner-ontonotes, batch 692 (692): mcc: 0.5786, acc: 0.4404, precision: 0.7847, recall: 0.4517, f1: 0.5733, edges-ner-ontonotes_loss: 0.1250
10/01 03:10:48 AM: Update 830: task edges-ner-ontonotes, batch 830 (830): mcc: 0.6335, acc: 0.5002, precision: 0.8220, recall: 0.5120, f1: 0.6310, edges-ner-ontonotes_loss: 0.1132
10/01 03:10:58 AM: Update 946: task edges-ner-ontonotes, batch 946 (946): mcc: 0.6678, acc: 0.5402, precision: 0.8419, recall: 0.5526, f1: 0.6672, edges-ner-ontonotes_loss: 0.1050
10/01 03:11:02 AM: ***** Step 1000 / Validation 1 *****
10/01 03:11:02 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:11:02 AM: Validating...
10/01 03:11:08 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.8253, acc: 0.7553, precision: 0.9068, recall: 0.7672, f1: 0.8312, edges-ner-ontonotes_loss: 0.0600
10/01 03:11:18 AM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.8658, acc: 0.8058, precision: 0.9310, recall: 0.8181, f1: 0.8709, edges-ner-ontonotes_loss: 0.0498
10/01 03:11:20 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:11:20 AM: Best result seen so far for micro.
10/01 03:11:20 AM: Best result seen so far for macro.
10/01 03:11:20 AM: Updating LR scheduler:
10/01 03:11:20 AM: 	Best result seen so far for macro_avg: 0.869
10/01 03:11:20 AM: 	# validation passes without improvement: 0
10/01 03:11:20 AM: edges-ner-ontonotes_loss: training: 0.101864 validation: 0.049528
10/01 03:11:20 AM: macro_avg: validation: 0.869344
10/01 03:11:20 AM: micro_avg: validation: 0.000000
10/01 03:11:20 AM: edges-ner-ontonotes_mcc: training: 0.680438 validation: 0.864192
10/01 03:11:20 AM: edges-ner-ontonotes_acc: training: 0.555401 validation: 0.804292
10/01 03:11:20 AM: edges-ner-ontonotes_precision: training: 0.848593 validation: 0.929404
10/01 03:11:20 AM: edges-ner-ontonotes_recall: training: 0.568075 validation: 0.816576
10/01 03:11:20 AM: edges-ner-ontonotes_f1: training: 0.680561 validation: 0.869344
10/01 03:11:20 AM: Global learning rate: 0.0001
10/01 03:11:20 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:11:28 AM: Update 1114: task edges-ner-ontonotes, batch 114 (1114): mcc: 0.8680, acc: 0.8050, precision: 0.9334, recall: 0.8199, f1: 0.8730, edges-ner-ontonotes_loss: 0.0451
10/01 03:11:38 AM: Update 1248: task edges-ner-ontonotes, batch 248 (1248): mcc: 0.8726, acc: 0.8114, precision: 0.9343, recall: 0.8273, f1: 0.8775, edges-ner-ontonotes_loss: 0.0431
10/01 03:11:48 AM: Update 1362: task edges-ner-ontonotes, batch 362 (1362): mcc: 0.8596, acc: 0.7928, precision: 0.9293, recall: 0.8083, f1: 0.8646, edges-ner-ontonotes_loss: 0.0478
10/01 03:11:58 AM: Update 1508: task edges-ner-ontonotes, batch 508 (1508): mcc: 0.8541, acc: 0.7852, precision: 0.9266, recall: 0.8010, f1: 0.8593, edges-ner-ontonotes_loss: 0.0492
10/01 03:12:08 AM: Update 1650: task edges-ner-ontonotes, batch 650 (1650): mcc: 0.8519, acc: 0.7823, precision: 0.9254, recall: 0.7982, f1: 0.8571, edges-ner-ontonotes_loss: 0.0497
10/01 03:12:18 AM: Update 1819: task edges-ner-ontonotes, batch 819 (1819): mcc: 0.8519, acc: 0.7825, precision: 0.9252, recall: 0.7983, f1: 0.8571, edges-ner-ontonotes_loss: 0.0489
10/01 03:12:28 AM: Update 1947: task edges-ner-ontonotes, batch 947 (1947): mcc: 0.8547, acc: 0.7863, precision: 0.9260, recall: 0.8026, f1: 0.8599, edges-ner-ontonotes_loss: 0.0477
10/01 03:12:32 AM: ***** Step 2000 / Validation 2 *****
10/01 03:12:32 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:12:32 AM: Validating...
10/01 03:12:38 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.8763, acc: 0.8239, precision: 0.9350, recall: 0.8333, f1: 0.8812, edges-ner-ontonotes_loss: 0.0392
10/01 03:12:48 AM: Evaluate: task edges-ner-ontonotes, batch 145 (157): mcc: 0.8980, acc: 0.8504, precision: 0.9473, recall: 0.8613, f1: 0.9023, edges-ner-ontonotes_loss: 0.0344
10/01 03:12:50 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:12:50 AM: Best result seen so far for macro.
10/01 03:12:50 AM: Updating LR scheduler:
10/01 03:12:50 AM: 	Best result seen so far for macro_avg: 0.903
10/01 03:12:50 AM: 	# validation passes without improvement: 0
10/01 03:12:50 AM: edges-ner-ontonotes_loss: training: 0.047210 validation: 0.033993
10/01 03:12:50 AM: macro_avg: validation: 0.902512
10/01 03:12:50 AM: micro_avg: validation: 0.000000
10/01 03:12:50 AM: edges-ner-ontonotes_mcc: training: 0.856348 validation: 0.898144
10/01 03:12:50 AM: edges-ner-ontonotes_acc: training: 0.788754 validation: 0.850622
10/01 03:12:50 AM: edges-ner-ontonotes_precision: training: 0.926539 validation: 0.946567
10/01 03:12:50 AM: edges-ner-ontonotes_recall: training: 0.805070 validation: 0.862375
10/01 03:12:50 AM: edges-ner-ontonotes_f1: training: 0.861544 validation: 0.902512
10/01 03:12:50 AM: Global learning rate: 0.0001
10/01 03:12:50 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:12:59 AM: Update 2126: task edges-ner-ontonotes, batch 126 (2126): mcc: 0.8880, acc: 0.8337, precision: 0.9398, recall: 0.8501, f1: 0.8927, edges-ner-ontonotes_loss: 0.0354
10/01 03:13:09 AM: Update 2241: task edges-ner-ontonotes, batch 241 (2241): mcc: 0.8931, acc: 0.8407, precision: 0.9423, recall: 0.8572, f1: 0.8977, edges-ner-ontonotes_loss: 0.0343
10/01 03:13:19 AM: Update 2379: task edges-ner-ontonotes, batch 379 (2379): mcc: 0.8993, acc: 0.8497, precision: 0.9447, recall: 0.8661, f1: 0.9037, edges-ner-ontonotes_loss: 0.0328
10/01 03:13:29 AM: Update 2496: task edges-ner-ontonotes, batch 496 (2496): mcc: 0.9028, acc: 0.8542, precision: 0.9465, recall: 0.8710, f1: 0.9072, edges-ner-ontonotes_loss: 0.0317
10/01 03:13:39 AM: Update 2631: task edges-ner-ontonotes, batch 631 (2631): mcc: 0.9037, acc: 0.8555, precision: 0.9464, recall: 0.8728, f1: 0.9081, edges-ner-ontonotes_loss: 0.0313
10/01 03:13:49 AM: Update 2770: task edges-ner-ontonotes, batch 770 (2770): mcc: 0.9050, acc: 0.8574, precision: 0.9464, recall: 0.8751, f1: 0.9094, edges-ner-ontonotes_loss: 0.0308
10/01 03:13:59 AM: Update 2880: task edges-ner-ontonotes, batch 880 (2880): mcc: 0.9026, acc: 0.8542, precision: 0.9450, recall: 0.8720, f1: 0.9070, edges-ner-ontonotes_loss: 0.0318
10/01 03:14:07 AM: ***** Step 3000 / Validation 3 *****
10/01 03:14:07 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:14:07 AM: Validating...
10/01 03:14:09 AM: Evaluate: task edges-ner-ontonotes, batch 17 (157): mcc: 0.8308, acc: 0.7726, precision: 0.8935, recall: 0.7889, f1: 0.8379, edges-ner-ontonotes_loss: 0.0446
10/01 03:14:19 AM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.8971, acc: 0.8524, precision: 0.9430, recall: 0.8639, f1: 0.9017, edges-ner-ontonotes_loss: 0.0334
10/01 03:14:25 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:14:25 AM: Best result seen so far for macro.
10/01 03:14:25 AM: Updating LR scheduler:
10/01 03:14:25 AM: 	Best result seen so far for macro_avg: 0.914
10/01 03:14:25 AM: 	# validation passes without improvement: 0
10/01 03:14:25 AM: edges-ner-ontonotes_loss: training: 0.033119 validation: 0.030069
10/01 03:14:25 AM: macro_avg: validation: 0.914052
10/01 03:14:25 AM: micro_avg: validation: 0.000000
10/01 03:14:25 AM: edges-ner-ontonotes_mcc: training: 0.899326 validation: 0.909953
10/01 03:14:25 AM: edges-ner-ontonotes_acc: training: 0.850009 validation: 0.868896
10/01 03:14:25 AM: edges-ner-ontonotes_precision: training: 0.943046 validation: 0.950618
10/01 03:14:25 AM: edges-ner-ontonotes_recall: training: 0.867812 validation: 0.880194
10/01 03:14:25 AM: edges-ner-ontonotes_f1: training: 0.903866 validation: 0.914052
10/01 03:14:25 AM: Global learning rate: 0.0001
10/01 03:14:25 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:14:29 AM: Update 3062: task edges-ner-ontonotes, batch 62 (3062): mcc: 0.8654, acc: 0.8046, precision: 0.9220, recall: 0.8256, f1: 0.8711, edges-ner-ontonotes_loss: 0.0443
10/01 03:14:39 AM: Update 3192: task edges-ner-ontonotes, batch 192 (3192): mcc: 0.8667, acc: 0.8072, precision: 0.9217, recall: 0.8280, f1: 0.8724, edges-ner-ontonotes_loss: 0.0428
10/01 03:14:49 AM: Update 3362: task edges-ner-ontonotes, batch 362 (3362): mcc: 0.8727, acc: 0.8138, precision: 0.9254, recall: 0.8355, f1: 0.8782, edges-ner-ontonotes_loss: 0.0403
10/01 03:14:59 AM: Update 3492: task edges-ner-ontonotes, batch 492 (3492): mcc: 0.8778, acc: 0.8220, precision: 0.9273, recall: 0.8432, f1: 0.8832, edges-ner-ontonotes_loss: 0.0388
10/01 03:15:09 AM: Update 3633: task edges-ner-ontonotes, batch 633 (3633): mcc: 0.8846, acc: 0.8317, precision: 0.9304, recall: 0.8526, f1: 0.8898, edges-ner-ontonotes_loss: 0.0370
10/01 03:15:19 AM: Update 3750: task edges-ner-ontonotes, batch 750 (3750): mcc: 0.8890, acc: 0.8374, precision: 0.9331, recall: 0.8583, f1: 0.8941, edges-ner-ontonotes_loss: 0.0357
10/01 03:15:29 AM: Update 3886: task edges-ner-ontonotes, batch 886 (3886): mcc: 0.8948, acc: 0.8455, precision: 0.9360, recall: 0.8661, f1: 0.8997, edges-ner-ontonotes_loss: 0.0342
10/01 03:15:37 AM: ***** Step 4000 / Validation 4 *****
10/01 03:15:37 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:15:37 AM: Validating...
10/01 03:15:39 AM: Evaluate: task edges-ner-ontonotes, batch 24 (157): mcc: 0.8466, acc: 0.7967, precision: 0.8970, recall: 0.8144, f1: 0.8537, edges-ner-ontonotes_loss: 0.0447
10/01 03:15:50 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9108, acc: 0.8722, precision: 0.9465, recall: 0.8857, f1: 0.9151, edges-ner-ontonotes_loss: 0.0292
10/01 03:15:55 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:15:55 AM: Best result seen so far for macro.
10/01 03:15:55 AM: Updating LR scheduler:
10/01 03:15:55 AM: 	Best result seen so far for macro_avg: 0.925
10/01 03:15:55 AM: 	# validation passes without improvement: 0
10/01 03:15:55 AM: edges-ner-ontonotes_loss: training: 0.033100 validation: 0.026386
10/01 03:15:55 AM: macro_avg: validation: 0.924589
10/01 03:15:55 AM: micro_avg: validation: 0.000000
10/01 03:15:55 AM: edges-ner-ontonotes_mcc: training: 0.898311 validation: 0.920722
10/01 03:15:55 AM: edges-ner-ontonotes_acc: training: 0.850114 validation: 0.885274
10/01 03:15:55 AM: edges-ner-ontonotes_precision: training: 0.938004 validation: 0.952190
10/01 03:15:55 AM: edges-ner-ontonotes_recall: training: 0.870687 validation: 0.898544
10/01 03:15:55 AM: edges-ner-ontonotes_f1: training: 0.903093 validation: 0.924589
10/01 03:15:55 AM: Global learning rate: 0.0001
10/01 03:15:55 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:16:00 AM: Update 4052: task edges-ner-ontonotes, batch 52 (4052): mcc: 0.9303, acc: 0.8952, precision: 0.9567, recall: 0.9121, f1: 0.9338, edges-ner-ontonotes_loss: 0.0228
10/01 03:16:10 AM: Update 4188: task edges-ner-ontonotes, batch 188 (4188): mcc: 0.9246, acc: 0.8863, precision: 0.9515, recall: 0.9064, f1: 0.9284, edges-ner-ontonotes_loss: 0.0243
10/01 03:16:20 AM: Update 4328: task edges-ner-ontonotes, batch 328 (4328): mcc: 0.9230, acc: 0.8840, precision: 0.9505, recall: 0.9045, f1: 0.9269, edges-ner-ontonotes_loss: 0.0247
10/01 03:16:30 AM: Update 4455: task edges-ner-ontonotes, batch 455 (4455): mcc: 0.9167, acc: 0.8754, precision: 0.9470, recall: 0.8961, f1: 0.9208, edges-ner-ontonotes_loss: 0.0272
10/01 03:16:40 AM: Update 4600: task edges-ner-ontonotes, batch 600 (4600): mcc: 0.9094, acc: 0.8658, precision: 0.9430, recall: 0.8864, f1: 0.9138, edges-ner-ontonotes_loss: 0.0308
10/01 03:16:50 AM: Update 4737: task edges-ner-ontonotes, batch 737 (4737): mcc: 0.9053, acc: 0.8605, precision: 0.9402, recall: 0.8815, f1: 0.9099, edges-ner-ontonotes_loss: 0.0322
10/01 03:17:01 AM: Update 4903: task edges-ner-ontonotes, batch 903 (4903): mcc: 0.9036, acc: 0.8581, precision: 0.9389, recall: 0.8796, f1: 0.9083, edges-ner-ontonotes_loss: 0.0326
10/01 03:17:08 AM: ***** Step 5000 / Validation 5 *****
10/01 03:17:08 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:17:08 AM: Validating...
10/01 03:17:11 AM: Evaluate: task edges-ner-ontonotes, batch 25 (157): mcc: 0.8670, acc: 0.8184, precision: 0.9152, recall: 0.8348, f1: 0.8731, edges-ner-ontonotes_loss: 0.0394
10/01 03:17:21 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9168, acc: 0.8783, precision: 0.9507, recall: 0.8928, f1: 0.9208, edges-ner-ontonotes_loss: 0.0273
10/01 03:17:26 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:17:26 AM: Best result seen so far for macro.
10/01 03:17:26 AM: Updating LR scheduler:
10/01 03:17:26 AM: 	Best result seen so far for macro_avg: 0.928
10/01 03:17:26 AM: 	# validation passes without improvement: 0
10/01 03:17:26 AM: edges-ner-ontonotes_loss: training: 0.032773 validation: 0.025152
10/01 03:17:26 AM: macro_avg: validation: 0.928346
10/01 03:17:26 AM: micro_avg: validation: 0.000000
10/01 03:17:26 AM: edges-ner-ontonotes_mcc: training: 0.902864 validation: 0.924689
10/01 03:17:26 AM: edges-ner-ontonotes_acc: training: 0.857193 validation: 0.888914
10/01 03:17:26 AM: edges-ner-ontonotes_precision: training: 0.938392 validation: 0.955900
10/01 03:17:26 AM: edges-ner-ontonotes_recall: training: 0.878705 validation: 0.902335
10/01 03:17:26 AM: edges-ner-ontonotes_f1: training: 0.907568 validation: 0.928346
10/01 03:17:26 AM: Global learning rate: 0.0001
10/01 03:17:26 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:17:31 AM: Update 5068: task edges-ner-ontonotes, batch 68 (5068): mcc: 0.9037, acc: 0.8608, precision: 0.9373, recall: 0.8814, f1: 0.9085, edges-ner-ontonotes_loss: 0.0302
10/01 03:17:41 AM: Update 5206: task edges-ner-ontonotes, batch 206 (5206): mcc: 0.9081, acc: 0.8664, precision: 0.9401, recall: 0.8868, f1: 0.9127, edges-ner-ontonotes_loss: 0.0290
10/01 03:17:51 AM: Update 5340: task edges-ner-ontonotes, batch 340 (5340): mcc: 0.9137, acc: 0.8728, precision: 0.9431, recall: 0.8942, f1: 0.9180, edges-ner-ontonotes_loss: 0.0274
10/01 03:18:01 AM: Update 5479: task edges-ner-ontonotes, batch 479 (5479): mcc: 0.9173, acc: 0.8777, precision: 0.9453, recall: 0.8988, f1: 0.9215, edges-ner-ontonotes_loss: 0.0262
10/01 03:18:13 AM: Update 5608: task edges-ner-ontonotes, batch 608 (5608): mcc: 0.9197, acc: 0.8806, precision: 0.9466, recall: 0.9020, f1: 0.9237, edges-ner-ontonotes_loss: 0.0256
10/01 03:18:23 AM: Update 5741: task edges-ner-ontonotes, batch 741 (5741): mcc: 0.9210, acc: 0.8820, precision: 0.9471, recall: 0.9039, f1: 0.9250, edges-ner-ontonotes_loss: 0.0252
10/01 03:18:33 AM: Update 5884: task edges-ner-ontonotes, batch 884 (5884): mcc: 0.9219, acc: 0.8832, precision: 0.9479, recall: 0.9050, f1: 0.9259, edges-ner-ontonotes_loss: 0.0250
10/01 03:18:42 AM: ***** Step 6000 / Validation 6 *****
10/01 03:18:42 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:18:42 AM: Validating...
10/01 03:18:43 AM: Evaluate: task edges-ner-ontonotes, batch 5 (157): mcc: 0.8139, acc: 0.7463, precision: 0.8805, recall: 0.7701, f1: 0.8217, edges-ner-ontonotes_loss: 0.0482
10/01 03:18:53 AM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.9111, acc: 0.8716, precision: 0.9442, recall: 0.8884, f1: 0.9155, edges-ner-ontonotes_loss: 0.0291
10/01 03:19:00 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:19:00 AM: Best result seen so far for macro.
10/01 03:19:00 AM: Updating LR scheduler:
10/01 03:19:00 AM: 	Best result seen so far for macro_avg: 0.930
10/01 03:19:00 AM: 	# validation passes without improvement: 0
10/01 03:19:00 AM: edges-ner-ontonotes_loss: training: 0.026129 validation: 0.024778
10/01 03:19:00 AM: macro_avg: validation: 0.929970
10/01 03:19:00 AM: micro_avg: validation: 0.000000
10/01 03:19:00 AM: edges-ner-ontonotes_mcc: training: 0.919464 validation: 0.926284
10/01 03:19:00 AM: edges-ner-ontonotes_acc: training: 0.879759 validation: 0.892023
10/01 03:19:00 AM: edges-ner-ontonotes_precision: training: 0.946497 validation: 0.953842
10/01 03:19:00 AM: edges-ner-ontonotes_recall: training: 0.901677 validation: 0.907264
10/01 03:19:00 AM: edges-ner-ontonotes_f1: training: 0.923544 validation: 0.929970
10/01 03:19:00 AM: Global learning rate: 0.0001
10/01 03:19:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:19:03 AM: Update 6036: task edges-ner-ontonotes, batch 36 (6036): mcc: 0.8989, acc: 0.8523, precision: 0.9341, recall: 0.8755, f1: 0.9039, edges-ner-ontonotes_loss: 0.0365
10/01 03:19:13 AM: Update 6182: task edges-ner-ontonotes, batch 182 (6182): mcc: 0.8879, acc: 0.8391, precision: 0.9261, recall: 0.8627, f1: 0.8933, edges-ner-ontonotes_loss: 0.0391
10/01 03:19:23 AM: Update 6324: task edges-ner-ontonotes, batch 324 (6324): mcc: 0.8889, acc: 0.8398, precision: 0.9270, recall: 0.8638, f1: 0.8943, edges-ner-ontonotes_loss: 0.0378
10/01 03:19:33 AM: Update 6495: task edges-ner-ontonotes, batch 495 (6495): mcc: 0.8904, acc: 0.8415, precision: 0.9278, recall: 0.8657, f1: 0.8957, edges-ner-ontonotes_loss: 0.0363
10/01 03:19:43 AM: Update 6621: task edges-ner-ontonotes, batch 621 (6621): mcc: 0.8955, acc: 0.8479, precision: 0.9312, recall: 0.8720, f1: 0.9006, edges-ner-ontonotes_loss: 0.0347
10/01 03:19:53 AM: Update 6770: task edges-ner-ontonotes, batch 770 (6770): mcc: 0.8995, acc: 0.8540, precision: 0.9339, recall: 0.8768, f1: 0.9045, edges-ner-ontonotes_loss: 0.0333
10/01 03:20:03 AM: Update 6887: task edges-ner-ontonotes, batch 887 (6887): mcc: 0.9029, acc: 0.8586, precision: 0.9359, recall: 0.8811, f1: 0.9077, edges-ner-ontonotes_loss: 0.0322
10/01 03:20:11 AM: ***** Step 7000 / Validation 7 *****
10/01 03:20:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:20:11 AM: Validating...
10/01 03:20:13 AM: Evaluate: task edges-ner-ontonotes, batch 21 (157): mcc: 0.8728, acc: 0.8161, precision: 0.9183, recall: 0.8423, f1: 0.8787, edges-ner-ontonotes_loss: 0.0341
10/01 03:20:23 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.9193, acc: 0.8836, precision: 0.9469, recall: 0.9009, f1: 0.9234, edges-ner-ontonotes_loss: 0.0263
10/01 03:20:29 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:20:29 AM: Best result seen so far for macro.
10/01 03:20:29 AM: Updating LR scheduler:
10/01 03:20:29 AM: 	Best result seen so far for macro_avg: 0.934
10/01 03:20:29 AM: 	# validation passes without improvement: 0
10/01 03:20:29 AM: edges-ner-ontonotes_loss: training: 0.031037 validation: 0.023350
10/01 03:20:29 AM: macro_avg: validation: 0.933653
10/01 03:20:29 AM: micro_avg: validation: 0.000000
10/01 03:20:29 AM: edges-ner-ontonotes_mcc: training: 0.906309 validation: 0.930066
10/01 03:20:29 AM: edges-ner-ontonotes_acc: training: 0.863007 validation: 0.898620
10/01 03:20:29 AM: edges-ner-ontonotes_precision: training: 0.937937 validation: 0.953661
10/01 03:20:29 AM: edges-ner-ontonotes_recall: training: 0.885507 validation: 0.914468
10/01 03:20:29 AM: edges-ner-ontonotes_f1: training: 0.910968 validation: 0.933653
10/01 03:20:29 AM: Global learning rate: 0.0001
10/01 03:20:29 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:20:33 AM: Update 7062: task edges-ner-ontonotes, batch 62 (7062): mcc: 0.9307, acc: 0.8959, precision: 0.9477, recall: 0.9215, f1: 0.9344, edges-ner-ontonotes_loss: 0.0218
10/01 03:20:43 AM: Update 7181: task edges-ner-ontonotes, batch 181 (7181): mcc: 0.9312, acc: 0.8966, precision: 0.9518, recall: 0.9183, f1: 0.9348, edges-ner-ontonotes_loss: 0.0218
10/01 03:20:53 AM: Update 7323: task edges-ner-ontonotes, batch 323 (7323): mcc: 0.9313, acc: 0.8963, precision: 0.9531, recall: 0.9173, f1: 0.9348, edges-ner-ontonotes_loss: 0.0220
10/01 03:21:03 AM: Update 7459: task edges-ner-ontonotes, batch 459 (7459): mcc: 0.9295, acc: 0.8938, precision: 0.9514, recall: 0.9157, f1: 0.9332, edges-ner-ontonotes_loss: 0.0223
10/01 03:21:14 AM: Update 7575: task edges-ner-ontonotes, batch 575 (7575): mcc: 0.9228, acc: 0.8855, precision: 0.9469, recall: 0.9075, f1: 0.9268, edges-ner-ontonotes_loss: 0.0252
10/01 03:21:24 AM: Update 7717: task edges-ner-ontonotes, batch 717 (7717): mcc: 0.9171, acc: 0.8774, precision: 0.9434, recall: 0.9003, f1: 0.9213, edges-ner-ontonotes_loss: 0.0278
10/01 03:21:34 AM: Update 7857: task edges-ner-ontonotes, batch 857 (7857): mcc: 0.9137, acc: 0.8731, precision: 0.9413, recall: 0.8960, f1: 0.9181, edges-ner-ontonotes_loss: 0.0292
10/01 03:21:42 AM: ***** Step 8000 / Validation 8 *****
10/01 03:21:42 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:21:42 AM: Validating...
10/01 03:21:44 AM: Evaluate: task edges-ner-ontonotes, batch 17 (157): mcc: 0.8730, acc: 0.8157, precision: 0.9184, recall: 0.8426, f1: 0.8789, edges-ner-ontonotes_loss: 0.0336
10/01 03:21:54 AM: Evaluate: task edges-ner-ontonotes, batch 108 (157): mcc: 0.9188, acc: 0.8817, precision: 0.9495, recall: 0.8975, f1: 0.9228, edges-ner-ontonotes_loss: 0.0260
10/01 03:22:00 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:22:00 AM: Best result seen so far for macro.
10/01 03:22:00 AM: Updating LR scheduler:
10/01 03:22:00 AM: 	Best result seen so far for macro_avg: 0.934
10/01 03:22:00 AM: 	# validation passes without improvement: 1
10/01 03:22:00 AM: edges-ner-ontonotes_loss: training: 0.029659 validation: 0.023287
10/01 03:22:00 AM: macro_avg: validation: 0.933701
10/01 03:22:00 AM: micro_avg: validation: 0.000000
10/01 03:22:00 AM: edges-ner-ontonotes_mcc: training: 0.912281 validation: 0.930195
10/01 03:22:00 AM: edges-ner-ontonotes_acc: training: 0.871206 validation: 0.897483
10/01 03:22:00 AM: edges-ner-ontonotes_precision: training: 0.940307 validation: 0.956498
10/01 03:22:00 AM: edges-ner-ontonotes_recall: training: 0.894305 validation: 0.911965
10/01 03:22:00 AM: edges-ner-ontonotes_f1: training: 0.916729 validation: 0.933701
10/01 03:22:00 AM: Global learning rate: 0.0001
10/01 03:22:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:22:04 AM: Update 8064: task edges-ner-ontonotes, batch 64 (8064): mcc: 0.9002, acc: 0.8529, precision: 0.9328, recall: 0.8791, f1: 0.9052, edges-ner-ontonotes_loss: 0.0314
10/01 03:22:14 AM: Update 8188: task edges-ner-ontonotes, batch 188 (8188): mcc: 0.9096, acc: 0.8676, precision: 0.9365, recall: 0.8931, f1: 0.9143, edges-ner-ontonotes_loss: 0.0288
10/01 03:22:24 AM: Update 8327: task edges-ner-ontonotes, batch 327 (8327): mcc: 0.9136, acc: 0.8733, precision: 0.9400, recall: 0.8971, f1: 0.9180, edges-ner-ontonotes_loss: 0.0277
10/01 03:22:34 AM: Update 8449: task edges-ner-ontonotes, batch 449 (8449): mcc: 0.9160, acc: 0.8765, precision: 0.9414, recall: 0.9002, f1: 0.9204, edges-ner-ontonotes_loss: 0.0271
10/01 03:22:44 AM: Update 8586: task edges-ner-ontonotes, batch 586 (8586): mcc: 0.9209, acc: 0.8830, precision: 0.9444, recall: 0.9064, f1: 0.9250, edges-ner-ontonotes_loss: 0.0257
10/01 03:22:55 AM: Update 8720: task edges-ner-ontonotes, batch 720 (8720): mcc: 0.9238, acc: 0.8867, precision: 0.9464, recall: 0.9099, f1: 0.9278, edges-ner-ontonotes_loss: 0.0248
10/01 03:23:05 AM: Update 8857: task edges-ner-ontonotes, batch 857 (8857): mcc: 0.9256, acc: 0.8890, precision: 0.9478, recall: 0.9119, f1: 0.9295, edges-ner-ontonotes_loss: 0.0243
10/01 03:23:15 AM: Update 8995: task edges-ner-ontonotes, batch 995 (8995): mcc: 0.9265, acc: 0.8900, precision: 0.9483, recall: 0.9130, f1: 0.9304, edges-ner-ontonotes_loss: 0.0239
10/01 03:23:15 AM: ***** Step 9000 / Validation 9 *****
10/01 03:23:15 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:23:15 AM: Validating...
10/01 03:23:25 AM: Evaluate: task edges-ner-ontonotes, batch 87 (157): mcc: 0.9181, acc: 0.8821, precision: 0.9458, recall: 0.8997, f1: 0.9222, edges-ner-ontonotes_loss: 0.0275
10/01 03:23:33 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:23:33 AM: Best result seen so far for macro.
10/01 03:23:33 AM: Updating LR scheduler:
10/01 03:23:33 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:23:33 AM: 	# validation passes without improvement: 0
10/01 03:23:33 AM: edges-ner-ontonotes_loss: training: 0.023893 validation: 0.022792
10/01 03:23:33 AM: macro_avg: validation: 0.935839
10/01 03:23:33 AM: micro_avg: validation: 0.000000
10/01 03:23:33 AM: edges-ner-ontonotes_mcc: training: 0.926516 validation: 0.932355
10/01 03:23:33 AM: edges-ner-ontonotes_acc: training: 0.889953 validation: 0.902563
10/01 03:23:33 AM: edges-ner-ontonotes_precision: training: 0.948345 validation: 0.955008
10/01 03:23:33 AM: edges-ner-ontonotes_recall: training: 0.913015 validation: 0.917425
10/01 03:23:33 AM: edges-ner-ontonotes_f1: training: 0.930345 validation: 0.935839
10/01 03:23:33 AM: Global learning rate: 0.0001
10/01 03:23:33 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:23:35 AM: Update 9020: task edges-ner-ontonotes, batch 20 (9020): mcc: 0.9303, acc: 0.8942, precision: 0.9499, recall: 0.9187, f1: 0.9340, edges-ner-ontonotes_loss: 0.0235
10/01 03:23:45 AM: Update 9141: task edges-ner-ontonotes, batch 141 (9141): mcc: 0.9018, acc: 0.8576, precision: 0.9332, recall: 0.8817, f1: 0.9067, edges-ner-ontonotes_loss: 0.0343
10/01 03:23:55 AM: Update 9287: task edges-ner-ontonotes, batch 287 (9287): mcc: 0.8966, acc: 0.8518, precision: 0.9295, recall: 0.8757, f1: 0.9018, edges-ner-ontonotes_loss: 0.0360
10/01 03:24:05 AM: Update 9428: task edges-ner-ontonotes, batch 428 (9428): mcc: 0.8958, acc: 0.8503, precision: 0.9281, recall: 0.8755, f1: 0.9010, edges-ner-ontonotes_loss: 0.0357
10/01 03:24:15 AM: Update 9600: task edges-ner-ontonotes, batch 600 (9600): mcc: 0.8973, acc: 0.8521, precision: 0.9293, recall: 0.8771, f1: 0.9024, edges-ner-ontonotes_loss: 0.0346
10/01 03:24:25 AM: Update 9727: task edges-ner-ontonotes, batch 727 (9727): mcc: 0.9004, acc: 0.8559, precision: 0.9316, recall: 0.8806, f1: 0.9054, edges-ner-ontonotes_loss: 0.0334
10/01 03:24:35 AM: Update 9874: task edges-ner-ontonotes, batch 874 (9874): mcc: 0.9041, acc: 0.8607, precision: 0.9339, recall: 0.8852, f1: 0.9089, edges-ner-ontonotes_loss: 0.0320
10/01 03:24:45 AM: Update 9998: task edges-ner-ontonotes, batch 998 (9998): mcc: 0.9070, acc: 0.8646, precision: 0.9358, recall: 0.8888, f1: 0.9117, edges-ner-ontonotes_loss: 0.0310
10/01 03:24:45 AM: ***** Step 10000 / Validation 10 *****
10/01 03:24:45 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:24:45 AM: Validating...
10/01 03:24:55 AM: Evaluate: task edges-ner-ontonotes, batch 95 (157): mcc: 0.9233, acc: 0.8874, precision: 0.9535, recall: 0.9021, f1: 0.9271, edges-ner-ontonotes_loss: 0.0259
10/01 03:25:03 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:25:03 AM: Best result seen so far for macro.
10/01 03:25:03 AM: Updating LR scheduler:
10/01 03:25:03 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:25:03 AM: 	# validation passes without improvement: 0
10/01 03:25:03 AM: edges-ner-ontonotes_loss: training: 0.030969 validation: 0.022504
10/01 03:25:03 AM: macro_avg: validation: 0.936246
10/01 03:25:03 AM: micro_avg: validation: 0.000000
10/01 03:25:03 AM: edges-ner-ontonotes_mcc: training: 0.907034 validation: 0.932825
10/01 03:25:03 AM: edges-ner-ontonotes_acc: training: 0.864690 validation: 0.902184
10/01 03:25:03 AM: edges-ner-ontonotes_precision: training: 0.935848 validation: 0.956928
10/01 03:25:03 AM: edges-ner-ontonotes_recall: training: 0.888858 validation: 0.916439
10/01 03:25:03 AM: edges-ner-ontonotes_f1: training: 0.911748 validation: 0.936246
10/01 03:25:03 AM: Global learning rate: 0.0001
10/01 03:25:03 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:25:05 AM: Update 10032: task edges-ner-ontonotes, batch 32 (10032): mcc: 0.9310, acc: 0.8944, precision: 0.9517, recall: 0.9181, f1: 0.9346, edges-ner-ontonotes_loss: 0.0204
10/01 03:25:15 AM: Update 10175: task edges-ner-ontonotes, batch 175 (10175): mcc: 0.9331, acc: 0.8989, precision: 0.9533, recall: 0.9204, f1: 0.9366, edges-ner-ontonotes_loss: 0.0209
10/01 03:25:25 AM: Update 10289: task edges-ner-ontonotes, batch 289 (10289): mcc: 0.9346, acc: 0.9008, precision: 0.9540, recall: 0.9225, f1: 0.9380, edges-ner-ontonotes_loss: 0.0207
10/01 03:25:35 AM: Update 10432: task edges-ner-ontonotes, batch 432 (10432): mcc: 0.9338, acc: 0.8999, precision: 0.9531, recall: 0.9220, f1: 0.9373, edges-ner-ontonotes_loss: 0.0208
10/01 03:25:45 AM: Update 10574: task edges-ner-ontonotes, batch 574 (10574): mcc: 0.9340, acc: 0.9000, precision: 0.9536, recall: 0.9218, f1: 0.9375, edges-ner-ontonotes_loss: 0.0211
10/01 03:25:55 AM: Update 10697: task edges-ner-ontonotes, batch 697 (10697): mcc: 0.9284, acc: 0.8927, precision: 0.9502, recall: 0.9147, f1: 0.9321, edges-ner-ontonotes_loss: 0.0235
10/01 03:26:05 AM: Update 10844: task edges-ner-ontonotes, batch 844 (10844): mcc: 0.9235, acc: 0.8864, precision: 0.9473, recall: 0.9085, f1: 0.9275, edges-ner-ontonotes_loss: 0.0259
10/01 03:26:15 AM: Update 10980: task edges-ner-ontonotes, batch 980 (10980): mcc: 0.9204, acc: 0.8822, precision: 0.9450, recall: 0.9048, f1: 0.9245, edges-ner-ontonotes_loss: 0.0271
10/01 03:26:16 AM: ***** Step 11000 / Validation 11 *****
10/01 03:26:16 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:26:16 AM: Validating...
10/01 03:26:25 AM: Evaluate: task edges-ner-ontonotes, batch 85 (157): mcc: 0.9204, acc: 0.8847, precision: 0.9484, recall: 0.9016, f1: 0.9244, edges-ner-ontonotes_loss: 0.0264
10/01 03:26:34 AM: Updating LR scheduler:
10/01 03:26:34 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:26:34 AM: 	# validation passes without improvement: 1
10/01 03:26:34 AM: edges-ner-ontonotes_loss: training: 0.027159 validation: 0.022569
10/01 03:26:34 AM: macro_avg: validation: 0.935672
10/01 03:26:34 AM: micro_avg: validation: 0.000000
10/01 03:26:34 AM: edges-ner-ontonotes_mcc: training: 0.920115 validation: 0.932215
10/01 03:26:34 AM: edges-ner-ontonotes_acc: training: 0.881913 validation: 0.900971
10/01 03:26:34 AM: edges-ner-ontonotes_precision: training: 0.944852 validation: 0.956226
10/01 03:26:34 AM: edges-ner-ontonotes_recall: training: 0.904478 validation: 0.915984
10/01 03:26:34 AM: edges-ner-ontonotes_f1: training: 0.924224 validation: 0.935672
10/01 03:26:34 AM: Global learning rate: 0.0001
10/01 03:26:34 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:26:35 AM: Update 11021: task edges-ner-ontonotes, batch 21 (11021): mcc: 0.9080, acc: 0.8637, precision: 0.9410, recall: 0.8857, f1: 0.9125, edges-ner-ontonotes_loss: 0.0317
10/01 03:26:45 AM: Update 11192: task edges-ner-ontonotes, batch 192 (11192): mcc: 0.9045, acc: 0.8606, precision: 0.9335, recall: 0.8864, f1: 0.9094, edges-ner-ontonotes_loss: 0.0305
10/01 03:26:55 AM: Update 11317: task edges-ner-ontonotes, batch 317 (11317): mcc: 0.9103, acc: 0.8690, precision: 0.9375, recall: 0.8933, f1: 0.9149, edges-ner-ontonotes_loss: 0.0290
10/01 03:27:05 AM: Update 11456: task edges-ner-ontonotes, batch 456 (11456): mcc: 0.9144, acc: 0.8748, precision: 0.9406, recall: 0.8979, f1: 0.9188, edges-ner-ontonotes_loss: 0.0277
10/01 03:27:16 AM: Update 11583: task edges-ner-ontonotes, batch 583 (11583): mcc: 0.9185, acc: 0.8804, precision: 0.9429, recall: 0.9034, f1: 0.9227, edges-ner-ontonotes_loss: 0.0265
10/01 03:27:26 AM: Update 11723: task edges-ner-ontonotes, batch 723 (11723): mcc: 0.9223, acc: 0.8857, precision: 0.9452, recall: 0.9082, f1: 0.9264, edges-ner-ontonotes_loss: 0.0253
10/01 03:27:36 AM: Update 11849: task edges-ner-ontonotes, batch 849 (11849): mcc: 0.9249, acc: 0.8890, precision: 0.9467, recall: 0.9116, f1: 0.9288, edges-ner-ontonotes_loss: 0.0245
10/01 03:27:46 AM: Update 11989: task edges-ner-ontonotes, batch 989 (11989): mcc: 0.9265, acc: 0.8908, precision: 0.9478, recall: 0.9135, f1: 0.9303, edges-ner-ontonotes_loss: 0.0241
10/01 03:27:46 AM: ***** Step 12000 / Validation 12 *****
10/01 03:27:46 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:27:46 AM: Validating...
10/01 03:27:56 AM: Evaluate: task edges-ner-ontonotes, batch 90 (157): mcc: 0.9214, acc: 0.8873, precision: 0.9461, recall: 0.9057, f1: 0.9255, edges-ner-ontonotes_loss: 0.0266
10/01 03:28:04 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:28:04 AM: Best result seen so far for macro.
10/01 03:28:04 AM: Updating LR scheduler:
10/01 03:28:04 AM: 	Best result seen so far for macro_avg: 0.938
10/01 03:28:04 AM: 	# validation passes without improvement: 0
10/01 03:28:04 AM: edges-ner-ontonotes_loss: training: 0.024049 validation: 0.022082
10/01 03:28:04 AM: macro_avg: validation: 0.938264
10/01 03:28:04 AM: micro_avg: validation: 0.000000
10/01 03:28:04 AM: edges-ner-ontonotes_mcc: training: 0.926557 validation: 0.934851
10/01 03:28:04 AM: edges-ner-ontonotes_acc: training: 0.890891 validation: 0.907188
10/01 03:28:04 AM: edges-ner-ontonotes_precision: training: 0.947831 validation: 0.954570
10/01 03:28:04 AM: edges-ner-ontonotes_recall: training: 0.913592 validation: 0.922505
10/01 03:28:04 AM: edges-ner-ontonotes_f1: training: 0.930396 validation: 0.938264
10/01 03:28:04 AM: Global learning rate: 0.0001
10/01 03:28:04 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:28:06 AM: Update 12025: task edges-ner-ontonotes, batch 25 (12025): mcc: 0.9334, acc: 0.8988, precision: 0.9563, recall: 0.9181, f1: 0.9368, edges-ner-ontonotes_loss: 0.0215
10/01 03:28:16 AM: Update 12145: task edges-ner-ontonotes, batch 145 (12145): mcc: 0.9392, acc: 0.9072, precision: 0.9571, recall: 0.9282, f1: 0.9424, edges-ner-ontonotes_loss: 0.0198
10/01 03:28:26 AM: Update 12289: task edges-ner-ontonotes, batch 289 (12289): mcc: 0.9175, acc: 0.8788, precision: 0.9427, recall: 0.9016, f1: 0.9217, edges-ner-ontonotes_loss: 0.0285
10/01 03:28:36 AM: Update 12436: task edges-ner-ontonotes, batch 436 (12436): mcc: 0.9111, acc: 0.8706, precision: 0.9390, recall: 0.8935, f1: 0.9157, edges-ner-ontonotes_loss: 0.0312
10/01 03:28:46 AM: Update 12580: task edges-ner-ontonotes, batch 580 (12580): mcc: 0.9087, acc: 0.8673, precision: 0.9366, recall: 0.8912, f1: 0.9134, edges-ner-ontonotes_loss: 0.0314
10/01 03:28:56 AM: Update 12756: task edges-ner-ontonotes, batch 756 (12756): mcc: 0.9081, acc: 0.8664, precision: 0.9364, recall: 0.8903, f1: 0.9128, edges-ner-ontonotes_loss: 0.0312
10/01 03:29:06 AM: Update 12880: task edges-ner-ontonotes, batch 880 (12880): mcc: 0.9099, acc: 0.8688, precision: 0.9373, recall: 0.8927, f1: 0.9145, edges-ner-ontonotes_loss: 0.0303
10/01 03:29:15 AM: ***** Step 13000 / Validation 13 *****
10/01 03:29:15 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:29:15 AM: Validating...
10/01 03:29:16 AM: Evaluate: task edges-ner-ontonotes, batch 15 (157): mcc: 0.8727, acc: 0.8143, precision: 0.9265, recall: 0.8346, f1: 0.8782, edges-ner-ontonotes_loss: 0.0357
10/01 03:29:26 AM: Evaluate: task edges-ner-ontonotes, batch 108 (157): mcc: 0.9256, acc: 0.8928, precision: 0.9533, recall: 0.9065, f1: 0.9293, edges-ner-ontonotes_loss: 0.0246
10/01 03:29:32 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:29:32 AM: Best result seen so far for macro.
10/01 03:29:32 AM: Updating LR scheduler:
10/01 03:29:32 AM: 	Best result seen so far for macro_avg: 0.939
10/01 03:29:32 AM: 	# validation passes without improvement: 0
10/01 03:29:32 AM: edges-ner-ontonotes_loss: training: 0.029613 validation: 0.021641
10/01 03:29:32 AM: macro_avg: validation: 0.939104
10/01 03:29:32 AM: micro_avg: validation: 0.000000
10/01 03:29:32 AM: edges-ner-ontonotes_mcc: training: 0.911676 validation: 0.935835
10/01 03:29:32 AM: edges-ner-ontonotes_acc: training: 0.871406 validation: 0.907037
10/01 03:29:32 AM: edges-ner-ontonotes_precision: training: 0.938376 validation: 0.959345
10/01 03:29:32 AM: edges-ner-ontonotes_recall: training: 0.895043 validation: 0.919700
10/01 03:29:32 AM: edges-ner-ontonotes_f1: training: 0.916198 validation: 0.939104
10/01 03:29:32 AM: Global learning rate: 0.0001
10/01 03:29:32 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:29:36 AM: Update 13062: task edges-ner-ontonotes, batch 62 (13062): mcc: 0.9225, acc: 0.8845, precision: 0.9423, recall: 0.9115, f1: 0.9266, edges-ner-ontonotes_loss: 0.0249
10/01 03:29:46 AM: Update 13182: task edges-ner-ontonotes, batch 182 (13182): mcc: 0.9322, acc: 0.8970, precision: 0.9499, recall: 0.9221, f1: 0.9358, edges-ner-ontonotes_loss: 0.0219
10/01 03:29:56 AM: Update 13322: task edges-ner-ontonotes, batch 322 (13322): mcc: 0.9342, acc: 0.9008, precision: 0.9512, recall: 0.9246, f1: 0.9377, edges-ner-ontonotes_loss: 0.0211
10/01 03:30:06 AM: Update 13443: task edges-ner-ontonotes, batch 443 (13443): mcc: 0.9343, acc: 0.9012, precision: 0.9516, recall: 0.9245, f1: 0.9378, edges-ner-ontonotes_loss: 0.0209
10/01 03:30:16 AM: Update 13585: task edges-ner-ontonotes, batch 585 (13585): mcc: 0.9347, acc: 0.9011, precision: 0.9522, recall: 0.9245, f1: 0.9381, edges-ner-ontonotes_loss: 0.0207
10/01 03:30:26 AM: Update 13708: task edges-ner-ontonotes, batch 708 (13708): mcc: 0.9343, acc: 0.9004, precision: 0.9520, recall: 0.9239, f1: 0.9378, edges-ner-ontonotes_loss: 0.0209
10/01 03:30:37 AM: Update 13858: task edges-ner-ontonotes, batch 858 (13858): mcc: 0.9287, acc: 0.8933, precision: 0.9485, recall: 0.9170, f1: 0.9325, edges-ner-ontonotes_loss: 0.0235
10/01 03:30:46 AM: ***** Step 14000 / Validation 14 *****
10/01 03:30:46 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:30:46 AM: Validating...
10/01 03:30:47 AM: Evaluate: task edges-ner-ontonotes, batch 1 (157): mcc: 0.8070, acc: 0.7213, precision: 0.8846, recall: 0.7541, f1: 0.8142, edges-ner-ontonotes_loss: 0.0483
10/01 03:30:57 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.9209, acc: 0.8876, precision: 0.9478, recall: 0.9031, f1: 0.9249, edges-ner-ontonotes_loss: 0.0266
10/01 03:31:04 AM: Updating LR scheduler:
10/01 03:31:04 AM: 	Best result seen so far for macro_avg: 0.939
10/01 03:31:04 AM: 	# validation passes without improvement: 1
10/01 03:31:04 AM: edges-ner-ontonotes_loss: training: 0.025333 validation: 0.022893
10/01 03:31:04 AM: macro_avg: validation: 0.935248
10/01 03:31:04 AM: micro_avg: validation: 0.000000
10/01 03:31:04 AM: edges-ner-ontonotes_mcc: training: 0.924433 validation: 0.931750
10/01 03:31:04 AM: edges-ner-ontonotes_acc: training: 0.888009 validation: 0.901729
10/01 03:31:04 AM: edges-ner-ontonotes_precision: training: 0.945755 validation: 0.955174
10/01 03:31:04 AM: edges-ner-ontonotes_recall: training: 0.911649 validation: 0.916136
10/01 03:31:04 AM: edges-ner-ontonotes_f1: training: 0.928389 validation: 0.935248
10/01 03:31:04 AM: Global learning rate: 0.0001
10/01 03:31:04 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:31:07 AM: Update 14017: task edges-ner-ontonotes, batch 17 (14017): mcc: 0.9024, acc: 0.8613, precision: 0.9345, recall: 0.8816, f1: 0.9073, edges-ner-ontonotes_loss: 0.0296
10/01 03:31:17 AM: Update 14190: task edges-ner-ontonotes, batch 190 (14190): mcc: 0.9078, acc: 0.8660, precision: 0.9357, recall: 0.8903, f1: 0.9125, edges-ner-ontonotes_loss: 0.0301
10/01 03:31:27 AM: Update 14331: task edges-ner-ontonotes, batch 331 (14331): mcc: 0.9062, acc: 0.8647, precision: 0.9336, recall: 0.8895, f1: 0.9110, edges-ner-ontonotes_loss: 0.0304
10/01 03:31:37 AM: Update 14479: task edges-ner-ontonotes, batch 479 (14479): mcc: 0.9109, acc: 0.8706, precision: 0.9367, recall: 0.8952, f1: 0.9155, edges-ner-ontonotes_loss: 0.0289
10/01 03:31:47 AM: Update 14626: task edges-ner-ontonotes, batch 626 (14626): mcc: 0.9155, acc: 0.8769, precision: 0.9400, recall: 0.9005, f1: 0.9198, edges-ner-ontonotes_loss: 0.0276
10/01 03:31:57 AM: Update 14744: task edges-ner-ontonotes, batch 744 (14744): mcc: 0.9191, acc: 0.8817, precision: 0.9422, recall: 0.9051, f1: 0.9233, edges-ner-ontonotes_loss: 0.0264
10/01 03:32:07 AM: Update 14884: task edges-ner-ontonotes, batch 884 (14884): mcc: 0.9228, acc: 0.8867, precision: 0.9448, recall: 0.9095, f1: 0.9268, edges-ner-ontonotes_loss: 0.0253
10/01 03:32:16 AM: ***** Step 15000 / Validation 15 *****
10/01 03:32:16 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:32:16 AM: Validating...
10/01 03:32:17 AM: Evaluate: task edges-ner-ontonotes, batch 6 (157): mcc: 0.8260, acc: 0.7675, precision: 0.8827, recall: 0.7900, f1: 0.8338, edges-ner-ontonotes_loss: 0.0461
10/01 03:32:27 AM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.9206, acc: 0.8866, precision: 0.9472, recall: 0.9031, f1: 0.9246, edges-ner-ontonotes_loss: 0.0264
10/01 03:32:34 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:32:34 AM: Best result seen so far for macro.
10/01 03:32:34 AM: Updating LR scheduler:
10/01 03:32:34 AM: 	Best result seen so far for macro_avg: 0.940
10/01 03:32:34 AM: 	# validation passes without improvement: 0
10/01 03:32:34 AM: edges-ner-ontonotes_loss: training: 0.024745 validation: 0.021597
10/01 03:32:34 AM: macro_avg: validation: 0.940127
10/01 03:32:34 AM: micro_avg: validation: 0.000000
10/01 03:32:34 AM: edges-ner-ontonotes_mcc: training: 0.924733 validation: 0.936846
10/01 03:32:34 AM: edges-ner-ontonotes_acc: training: 0.888935 validation: 0.908553
10/01 03:32:34 AM: edges-ner-ontonotes_precision: training: 0.946034 validation: 0.957537
10/01 03:32:34 AM: edges-ner-ontonotes_recall: training: 0.911937 validation: 0.923339
10/01 03:32:34 AM: edges-ner-ontonotes_f1: training: 0.928673 validation: 0.940127
10/01 03:32:34 AM: Global learning rate: 0.0001
10/01 03:32:34 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:32:37 AM: Update 15041: task edges-ner-ontonotes, batch 41 (15041): mcc: 0.9364, acc: 0.9027, precision: 0.9530, recall: 0.9269, f1: 0.9398, edges-ner-ontonotes_loss: 0.0203
10/01 03:32:47 AM: Update 15182: task edges-ner-ontonotes, batch 182 (15182): mcc: 0.9358, acc: 0.9020, precision: 0.9539, recall: 0.9250, f1: 0.9392, edges-ner-ontonotes_loss: 0.0205
10/01 03:32:57 AM: Update 15293: task edges-ner-ontonotes, batch 293 (15293): mcc: 0.9320, acc: 0.8975, precision: 0.9512, recall: 0.9204, f1: 0.9355, edges-ner-ontonotes_loss: 0.0222
10/01 03:33:07 AM: Update 15443: task edges-ner-ontonotes, batch 443 (15443): mcc: 0.9218, acc: 0.8848, precision: 0.9445, recall: 0.9080, f1: 0.9259, edges-ner-ontonotes_loss: 0.0266
10/01 03:33:17 AM: Update 15581: task edges-ner-ontonotes, batch 581 (15581): mcc: 0.9163, acc: 0.8781, precision: 0.9409, recall: 0.9013, f1: 0.9207, edges-ner-ontonotes_loss: 0.0290
10/01 03:33:27 AM: Update 15754: task edges-ner-ontonotes, batch 754 (15754): mcc: 0.9142, acc: 0.8749, precision: 0.9393, recall: 0.8988, f1: 0.9186, edges-ner-ontonotes_loss: 0.0294
10/01 03:33:37 AM: Update 15897: task edges-ner-ontonotes, batch 897 (15897): mcc: 0.9135, acc: 0.8741, precision: 0.9387, recall: 0.8982, f1: 0.9180, edges-ner-ontonotes_loss: 0.0293
10/01 03:33:44 AM: ***** Step 16000 / Validation 16 *****
10/01 03:33:44 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:33:44 AM: Validating...
10/01 03:33:47 AM: Evaluate: task edges-ner-ontonotes, batch 34 (157): mcc: 0.8984, acc: 0.8595, precision: 0.9314, recall: 0.8771, f1: 0.9034, edges-ner-ontonotes_loss: 0.0319
10/01 03:33:57 AM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.9282, acc: 0.8975, precision: 0.9527, recall: 0.9120, f1: 0.9319, edges-ner-ontonotes_loss: 0.0238
10/01 03:34:02 AM: Updating LR scheduler:
10/01 03:34:02 AM: 	Best result seen so far for macro_avg: 0.940
10/01 03:34:02 AM: 	# validation passes without improvement: 1
10/01 03:34:02 AM: edges-ner-ontonotes_loss: training: 0.028738 validation: 0.021399
10/01 03:34:02 AM: macro_avg: validation: 0.939231
10/01 03:34:02 AM: micro_avg: validation: 0.000000
10/01 03:34:02 AM: edges-ner-ontonotes_mcc: training: 0.914711 validation: 0.935917
10/01 03:34:02 AM: edges-ner-ontonotes_acc: training: 0.875545 validation: 0.907719
10/01 03:34:02 AM: edges-ner-ontonotes_precision: training: 0.939593 validation: 0.957392
10/01 03:34:02 AM: edges-ner-ontonotes_recall: training: 0.899514 validation: 0.921747
10/01 03:34:02 AM: edges-ner-ontonotes_f1: training: 0.919116 validation: 0.939231
10/01 03:34:02 AM: Global learning rate: 0.0001
10/01 03:34:02 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:34:07 AM: Update 16073: task edges-ner-ontonotes, batch 73 (16073): mcc: 0.9206, acc: 0.8865, precision: 0.9414, recall: 0.9088, f1: 0.9248, edges-ner-ontonotes_loss: 0.0247
10/01 03:34:18 AM: Update 16192: task edges-ner-ontonotes, batch 192 (16192): mcc: 0.9215, acc: 0.8861, precision: 0.9429, recall: 0.9090, f1: 0.9256, edges-ner-ontonotes_loss: 0.0249
10/01 03:34:28 AM: Update 16340: task edges-ner-ontonotes, batch 340 (16340): mcc: 0.9277, acc: 0.8932, precision: 0.9472, recall: 0.9163, f1: 0.9315, edges-ner-ontonotes_loss: 0.0227
10/01 03:34:38 AM: Update 16470: task edges-ner-ontonotes, batch 470 (16470): mcc: 0.9317, acc: 0.8982, precision: 0.9502, recall: 0.9209, f1: 0.9353, edges-ner-ontonotes_loss: 0.0216
10/01 03:34:48 AM: Update 16581: task edges-ner-ontonotes, batch 581 (16581): mcc: 0.9330, acc: 0.9000, precision: 0.9510, recall: 0.9226, f1: 0.9366, edges-ner-ontonotes_loss: 0.0213
10/01 03:34:58 AM: Update 16719: task edges-ner-ontonotes, batch 719 (16719): mcc: 0.9335, acc: 0.9004, precision: 0.9513, recall: 0.9232, f1: 0.9370, edges-ner-ontonotes_loss: 0.0211
10/01 03:35:08 AM: Update 16843: task edges-ner-ontonotes, batch 843 (16843): mcc: 0.9329, acc: 0.8995, precision: 0.9510, recall: 0.9223, f1: 0.9364, edges-ner-ontonotes_loss: 0.0214
10/01 03:35:18 AM: Update 16982: task edges-ner-ontonotes, batch 982 (16982): mcc: 0.9283, acc: 0.8937, precision: 0.9481, recall: 0.9166, f1: 0.9321, edges-ner-ontonotes_loss: 0.0235
10/01 03:35:19 AM: ***** Step 17000 / Validation 17 *****
10/01 03:35:19 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:35:19 AM: Validating...
10/01 03:35:28 AM: Evaluate: task edges-ner-ontonotes, batch 85 (157): mcc: 0.9175, acc: 0.8831, precision: 0.9440, recall: 0.9004, f1: 0.9217, edges-ner-ontonotes_loss: 0.0277
10/01 03:35:37 AM: Updating LR scheduler:
10/01 03:35:37 AM: 	Best result seen so far for macro_avg: 0.940
10/01 03:35:37 AM: 	# validation passes without improvement: 2
10/01 03:35:37 AM: edges-ner-ontonotes_loss: training: 0.023800 validation: 0.023021
10/01 03:35:37 AM: macro_avg: validation: 0.935050
10/01 03:35:37 AM: micro_avg: validation: 0.000000
10/01 03:35:37 AM: edges-ner-ontonotes_mcc: training: 0.927745 validation: 0.931511
10/01 03:35:37 AM: edges-ner-ontonotes_acc: training: 0.892966 validation: 0.901805
10/01 03:35:37 AM: edges-ner-ontonotes_precision: training: 0.947672 validation: 0.953857
10/01 03:35:37 AM: edges-ner-ontonotes_recall: training: 0.915965 validation: 0.916970
10/01 03:35:37 AM: edges-ner-ontonotes_f1: training: 0.931549 validation: 0.935050
10/01 03:35:37 AM: Global learning rate: 0.0001
10/01 03:35:37 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:35:38 AM: Update 17015: task edges-ner-ontonotes, batch 15 (17015): mcc: 0.9071, acc: 0.8608, precision: 0.9375, recall: 0.8875, f1: 0.9118, edges-ner-ontonotes_loss: 0.0347
10/01 03:35:48 AM: Update 17151: task edges-ner-ontonotes, batch 151 (17151): mcc: 0.9053, acc: 0.8625, precision: 0.9345, recall: 0.8869, f1: 0.9101, edges-ner-ontonotes_loss: 0.0339
10/01 03:35:58 AM: Update 17322: task edges-ner-ontonotes, batch 322 (17322): mcc: 0.9054, acc: 0.8627, precision: 0.9338, recall: 0.8878, f1: 0.9102, edges-ner-ontonotes_loss: 0.0319
10/01 03:36:08 AM: Update 17460: task edges-ner-ontonotes, batch 460 (17460): mcc: 0.9072, acc: 0.8653, precision: 0.9343, recall: 0.8906, f1: 0.9120, edges-ner-ontonotes_loss: 0.0309
10/01 03:36:18 AM: Update 17601: task edges-ner-ontonotes, batch 601 (17601): mcc: 0.9116, acc: 0.8716, precision: 0.9373, recall: 0.8960, f1: 0.9162, edges-ner-ontonotes_loss: 0.0295
10/01 03:36:29 AM: Update 17743: task edges-ner-ontonotes, batch 743 (17743): mcc: 0.9145, acc: 0.8753, precision: 0.9392, recall: 0.8995, f1: 0.9189, edges-ner-ontonotes_loss: 0.0283
10/01 03:36:39 AM: Update 17877: task edges-ner-ontonotes, batch 877 (17877): mcc: 0.9191, acc: 0.8813, precision: 0.9422, recall: 0.9052, f1: 0.9233, edges-ner-ontonotes_loss: 0.0270
10/01 03:36:48 AM: ***** Step 18000 / Validation 18 *****
10/01 03:36:48 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:36:48 AM: Validating...
10/01 03:36:49 AM: Evaluate: task edges-ner-ontonotes, batch 15 (157): mcc: 0.8674, acc: 0.8164, precision: 0.9138, recall: 0.8367, f1: 0.8735, edges-ner-ontonotes_loss: 0.0371
10/01 03:36:59 AM: Evaluate: task edges-ner-ontonotes, batch 101 (157): mcc: 0.9216, acc: 0.8879, precision: 0.9462, recall: 0.9060, f1: 0.9257, edges-ner-ontonotes_loss: 0.0257
10/01 03:37:06 AM: Updating LR scheduler:
10/01 03:37:06 AM: 	Best result seen so far for macro_avg: 0.940
10/01 03:37:06 AM: 	# validation passes without improvement: 3
10/01 03:37:06 AM: edges-ner-ontonotes_loss: training: 0.025929 validation: 0.021296
10/01 03:37:06 AM: macro_avg: validation: 0.939647
10/01 03:37:06 AM: micro_avg: validation: 0.000000
10/01 03:37:06 AM: edges-ner-ontonotes_mcc: training: 0.922635 validation: 0.936316
10/01 03:37:06 AM: edges-ner-ontonotes_acc: training: 0.886093 validation: 0.907643
10/01 03:37:06 AM: edges-ner-ontonotes_precision: training: 0.944583 validation: 0.956054
10/01 03:37:06 AM: edges-ner-ontonotes_recall: training: 0.909435 validation: 0.923794
10/01 03:37:06 AM: edges-ner-ontonotes_f1: training: 0.926676 validation: 0.939647
10/01 03:37:06 AM: Global learning rate: 0.0001
10/01 03:37:06 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:37:09 AM: Update 18039: task edges-ner-ontonotes, batch 39 (18039): mcc: 0.9380, acc: 0.9041, precision: 0.9532, recall: 0.9297, f1: 0.9413, edges-ner-ontonotes_loss: 0.0201
10/01 03:37:19 AM: Update 18158: task edges-ner-ontonotes, batch 158 (18158): mcc: 0.9384, acc: 0.9054, precision: 0.9538, recall: 0.9300, f1: 0.9417, edges-ner-ontonotes_loss: 0.0201
10/01 03:37:29 AM: Update 18298: task edges-ner-ontonotes, batch 298 (18298): mcc: 0.9369, acc: 0.9033, precision: 0.9529, recall: 0.9280, f1: 0.9403, edges-ner-ontonotes_loss: 0.0201
10/01 03:37:39 AM: Update 18417: task edges-ner-ontonotes, batch 417 (18417): mcc: 0.9308, acc: 0.8962, precision: 0.9488, recall: 0.9206, f1: 0.9345, edges-ner-ontonotes_loss: 0.0226
10/01 03:37:49 AM: Update 18559: task edges-ner-ontonotes, batch 559 (18559): mcc: 0.9246, acc: 0.8880, precision: 0.9453, recall: 0.9124, f1: 0.9285, edges-ner-ontonotes_loss: 0.0257
10/01 03:37:59 AM: Update 18686: task edges-ner-ontonotes, batch 686 (18686): mcc: 0.9210, acc: 0.8831, precision: 0.9432, recall: 0.9077, f1: 0.9251, edges-ner-ontonotes_loss: 0.0273
10/01 03:38:09 AM: Update 18855: task edges-ner-ontonotes, batch 855 (18855): mcc: 0.9192, acc: 0.8805, precision: 0.9419, recall: 0.9055, f1: 0.9234, edges-ner-ontonotes_loss: 0.0277
10/01 03:38:19 AM: Update 18998: task edges-ner-ontonotes, batch 998 (18998): mcc: 0.9182, acc: 0.8793, precision: 0.9411, recall: 0.9045, f1: 0.9224, edges-ner-ontonotes_loss: 0.0279
10/01 03:38:19 AM: ***** Step 19000 / Validation 19 *****
10/01 03:38:19 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:38:19 AM: Validating...
10/01 03:38:29 AM: Evaluate: task edges-ner-ontonotes, batch 95 (157): mcc: 0.9290, acc: 0.8968, precision: 0.9531, recall: 0.9130, f1: 0.9326, edges-ner-ontonotes_loss: 0.0243
10/01 03:38:37 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:38:37 AM: Best result seen so far for macro.
10/01 03:38:37 AM: Updating LR scheduler:
10/01 03:38:37 AM: 	Best result seen so far for macro_avg: 0.941
10/01 03:38:37 AM: 	# validation passes without improvement: 0
10/01 03:38:37 AM: edges-ner-ontonotes_loss: training: 0.027878 validation: 0.021182
10/01 03:38:37 AM: macro_avg: validation: 0.940532
10/01 03:38:37 AM: micro_avg: validation: 0.000000
10/01 03:38:37 AM: edges-ner-ontonotes_mcc: training: 0.918161 validation: 0.937267
10/01 03:38:37 AM: edges-ner-ontonotes_acc: training: 0.879293 validation: 0.908781
10/01 03:38:37 AM: edges-ner-ontonotes_precision: training: 0.941066 validation: 0.957642
10/01 03:38:37 AM: edges-ner-ontonotes_recall: training: 0.904511 validation: 0.924022
10/01 03:38:37 AM: edges-ner-ontonotes_f1: training: 0.922427 validation: 0.940532
10/01 03:38:37 AM: Global learning rate: 0.0001
10/01 03:38:37 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:38:39 AM: Update 19031: task edges-ner-ontonotes, batch 31 (19031): mcc: 0.9218, acc: 0.8864, precision: 0.9437, recall: 0.9087, f1: 0.9259, edges-ner-ontonotes_loss: 0.0241
10/01 03:38:49 AM: Update 19167: task edges-ner-ontonotes, batch 167 (19167): mcc: 0.9228, acc: 0.8864, precision: 0.9450, recall: 0.9094, f1: 0.9269, edges-ner-ontonotes_loss: 0.0247
10/01 03:39:00 AM: Update 19299: task edges-ner-ontonotes, batch 299 (19299): mcc: 0.9248, acc: 0.8892, precision: 0.9458, recall: 0.9123, f1: 0.9287, edges-ner-ontonotes_loss: 0.0241
10/01 03:39:10 AM: Update 19434: task edges-ner-ontonotes, batch 434 (19434): mcc: 0.9297, acc: 0.8959, precision: 0.9487, recall: 0.9187, f1: 0.9335, edges-ner-ontonotes_loss: 0.0226
10/01 03:39:20 AM: Update 19574: task edges-ner-ontonotes, batch 574 (19574): mcc: 0.9330, acc: 0.9000, precision: 0.9510, recall: 0.9225, f1: 0.9366, edges-ner-ontonotes_loss: 0.0215
10/01 03:39:30 AM: Update 19700: task edges-ner-ontonotes, batch 700 (19700): mcc: 0.9341, acc: 0.9010, precision: 0.9516, recall: 0.9240, f1: 0.9376, edges-ner-ontonotes_loss: 0.0212
10/01 03:39:40 AM: Update 19837: task edges-ner-ontonotes, batch 837 (19837): mcc: 0.9344, acc: 0.9012, precision: 0.9519, recall: 0.9244, f1: 0.9379, edges-ner-ontonotes_loss: 0.0210
10/01 03:39:50 AM: Update 19957: task edges-ner-ontonotes, batch 957 (19957): mcc: 0.9334, acc: 0.8998, precision: 0.9509, recall: 0.9233, f1: 0.9369, edges-ner-ontonotes_loss: 0.0214
10/01 03:39:53 AM: ***** Step 20000 / Validation 20 *****
10/01 03:39:53 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:39:53 AM: Validating...
10/01 03:40:00 AM: Evaluate: task edges-ner-ontonotes, batch 70 (157): mcc: 0.9163, acc: 0.8819, precision: 0.9425, recall: 0.8997, f1: 0.9206, edges-ner-ontonotes_loss: 0.0278
10/01 03:40:10 AM: Evaluate: task edges-ner-ontonotes, batch 153 (157): mcc: 0.9356, acc: 0.9075, precision: 0.9577, recall: 0.9210, f1: 0.9390, edges-ner-ontonotes_loss: 0.0219
10/01 03:40:10 AM: Updating LR scheduler:
10/01 03:40:10 AM: 	Best result seen so far for macro_avg: 0.941
10/01 03:40:10 AM: 	# validation passes without improvement: 1
10/01 03:40:10 AM: edges-ner-ontonotes_loss: training: 0.022202 validation: 0.021661
10/01 03:40:10 AM: macro_avg: validation: 0.939495
10/01 03:40:10 AM: micro_avg: validation: 0.000000
10/01 03:40:10 AM: edges-ner-ontonotes_mcc: training: 0.931641 validation: 0.936198
10/01 03:40:10 AM: edges-ner-ontonotes_acc: training: 0.897546 validation: 0.908326
10/01 03:40:10 AM: edges-ner-ontonotes_precision: training: 0.949764 validation: 0.957775
10/01 03:40:10 AM: edges-ner-ontonotes_recall: training: 0.921203 validation: 0.921899
10/01 03:40:10 AM: edges-ner-ontonotes_f1: training: 0.935266 validation: 0.939495
10/01 03:40:10 AM: Global learning rate: 0.0001
10/01 03:40:10 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:40:20 AM: Update 20133: task edges-ner-ontonotes, batch 133 (20133): mcc: 0.9025, acc: 0.8604, precision: 0.9308, recall: 0.8854, f1: 0.9075, edges-ner-ontonotes_loss: 0.0339
10/01 03:40:30 AM: Update 20248: task edges-ner-ontonotes, batch 248 (20248): mcc: 0.9006, acc: 0.8578, precision: 0.9294, recall: 0.8832, f1: 0.9057, edges-ner-ontonotes_loss: 0.0347
10/01 03:40:40 AM: Update 20413: task edges-ner-ontonotes, batch 413 (20413): mcc: 0.9038, acc: 0.8611, precision: 0.9316, recall: 0.8870, f1: 0.9088, edges-ner-ontonotes_loss: 0.0328
10/01 03:40:50 AM: Update 20545: task edges-ner-ontonotes, batch 545 (20545): mcc: 0.9057, acc: 0.8638, precision: 0.9324, recall: 0.8897, f1: 0.9106, edges-ner-ontonotes_loss: 0.0317
10/01 03:41:00 AM: Update 20683: task edges-ner-ontonotes, batch 683 (20683): mcc: 0.9104, acc: 0.8703, precision: 0.9354, recall: 0.8956, f1: 0.9151, edges-ner-ontonotes_loss: 0.0302
10/01 03:41:10 AM: Update 20819: task edges-ner-ontonotes, batch 819 (20819): mcc: 0.9129, acc: 0.8735, precision: 0.9372, recall: 0.8985, f1: 0.9174, edges-ner-ontonotes_loss: 0.0292
10/01 03:41:20 AM: Update 20932: task edges-ner-ontonotes, batch 932 (20932): mcc: 0.9160, acc: 0.8775, precision: 0.9395, recall: 0.9021, f1: 0.9204, edges-ner-ontonotes_loss: 0.0282
10/01 03:41:26 AM: ***** Step 21000 / Validation 21 *****
10/01 03:41:26 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:41:26 AM: Validating...
10/01 03:41:31 AM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.9070, acc: 0.8722, precision: 0.9337, recall: 0.8909, f1: 0.9118, edges-ner-ontonotes_loss: 0.0302
10/01 03:41:41 AM: Evaluate: task edges-ner-ontonotes, batch 128 (157): mcc: 0.9326, acc: 0.9045, precision: 0.9536, recall: 0.9194, f1: 0.9361, edges-ner-ontonotes_loss: 0.0230
10/01 03:41:44 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:41:44 AM: Best result seen so far for macro.
10/01 03:41:44 AM: Updating LR scheduler:
10/01 03:41:44 AM: 	Best result seen so far for macro_avg: 0.941
10/01 03:41:44 AM: 	# validation passes without improvement: 2
10/01 03:41:44 AM: edges-ner-ontonotes_loss: training: 0.027545 validation: 0.021264
10/01 03:41:44 AM: macro_avg: validation: 0.940601
10/01 03:41:44 AM: micro_avg: validation: 0.000000
10/01 03:41:44 AM: edges-ner-ontonotes_mcc: training: 0.917910 validation: 0.937301
10/01 03:41:44 AM: edges-ner-ontonotes_acc: training: 0.879794 validation: 0.910525
10/01 03:41:44 AM: edges-ner-ontonotes_precision: training: 0.940577 validation: 0.955919
10/01 03:41:44 AM: edges-ner-ontonotes_recall: training: 0.904520 validation: 0.925766
10/01 03:41:44 AM: edges-ner-ontonotes_f1: training: 0.922196 validation: 0.940601
10/01 03:41:44 AM: Global learning rate: 0.0001
10/01 03:41:44 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:41:51 AM: Update 21089: task edges-ner-ontonotes, batch 89 (21089): mcc: 0.9388, acc: 0.9081, precision: 0.9538, recall: 0.9307, f1: 0.9421, edges-ner-ontonotes_loss: 0.0184
10/01 03:42:01 AM: Update 21201: task edges-ner-ontonotes, batch 201 (21201): mcc: 0.9389, acc: 0.9076, precision: 0.9544, recall: 0.9302, f1: 0.9421, edges-ner-ontonotes_loss: 0.0195
10/01 03:42:11 AM: Update 21335: task edges-ner-ontonotes, batch 335 (21335): mcc: 0.9389, acc: 0.9073, precision: 0.9548, recall: 0.9298, f1: 0.9421, edges-ner-ontonotes_loss: 0.0194
10/01 03:42:21 AM: Update 21467: task edges-ner-ontonotes, batch 467 (21467): mcc: 0.9384, acc: 0.9069, precision: 0.9542, recall: 0.9296, f1: 0.9417, edges-ner-ontonotes_loss: 0.0198
10/01 03:42:31 AM: Update 21593: task edges-ner-ontonotes, batch 593 (21593): mcc: 0.9316, acc: 0.8981, precision: 0.9499, recall: 0.9210, f1: 0.9353, edges-ner-ontonotes_loss: 0.0230
10/01 03:42:41 AM: Update 21730: task edges-ner-ontonotes, batch 730 (21730): mcc: 0.9260, acc: 0.8906, precision: 0.9462, recall: 0.9142, f1: 0.9299, edges-ner-ontonotes_loss: 0.0252
10/01 03:42:51 AM: Update 21869: task edges-ner-ontonotes, batch 869 (21869): mcc: 0.9231, acc: 0.8866, precision: 0.9443, recall: 0.9106, f1: 0.9271, edges-ner-ontonotes_loss: 0.0263
10/01 03:42:59 AM: ***** Step 22000 / Validation 22 *****
10/01 03:42:59 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:42:59 AM: Validating...
10/01 03:43:01 AM: Evaluate: task edges-ner-ontonotes, batch 25 (157): mcc: 0.8770, acc: 0.8380, precision: 0.9137, recall: 0.8544, f1: 0.8831, edges-ner-ontonotes_loss: 0.0354
10/01 03:43:11 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9286, acc: 0.8982, precision: 0.9537, recall: 0.9118, f1: 0.9323, edges-ner-ontonotes_loss: 0.0235
10/01 03:43:16 AM: Updating LR scheduler:
10/01 03:43:16 AM: 	Best result seen so far for macro_avg: 0.941
10/01 03:43:16 AM: 	# validation passes without improvement: 3
10/01 03:43:16 AM: edges-ner-ontonotes_loss: training: 0.026696 validation: 0.021066
10/01 03:43:16 AM: macro_avg: validation: 0.939862
10/01 03:43:16 AM: micro_avg: validation: 0.000000
10/01 03:43:16 AM: edges-ner-ontonotes_mcc: training: 0.921904 validation: 0.936610
10/01 03:43:16 AM: edges-ner-ontonotes_acc: training: 0.884973 validation: 0.908705
10/01 03:43:16 AM: edges-ner-ontonotes_precision: training: 0.943371 validation: 0.959113
10/01 03:43:16 AM: edges-ner-ontonotes_recall: training: 0.909251 validation: 0.921368
10/01 03:43:16 AM: edges-ner-ontonotes_f1: training: 0.925997 validation: 0.939862
10/01 03:43:16 AM: Global learning rate: 0.0001
10/01 03:43:16 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:43:22 AM: Update 22089: task edges-ner-ontonotes, batch 89 (22089): mcc: 0.9139, acc: 0.8713, precision: 0.9401, recall: 0.8975, f1: 0.9183, edges-ner-ontonotes_loss: 0.0275
10/01 03:43:32 AM: Update 22208: task edges-ner-ontonotes, batch 208 (22208): mcc: 0.9186, acc: 0.8795, precision: 0.9409, recall: 0.9054, f1: 0.9228, edges-ner-ontonotes_loss: 0.0261
10/01 03:43:42 AM: Update 22349: task edges-ner-ontonotes, batch 349 (22349): mcc: 0.9213, acc: 0.8836, precision: 0.9435, recall: 0.9080, f1: 0.9254, edges-ner-ontonotes_loss: 0.0253
10/01 03:43:52 AM: Update 22470: task edges-ner-ontonotes, batch 470 (22470): mcc: 0.9250, acc: 0.8884, precision: 0.9456, recall: 0.9129, f1: 0.9290, edges-ner-ontonotes_loss: 0.0241
10/01 03:44:02 AM: Update 22614: task edges-ner-ontonotes, batch 614 (22614): mcc: 0.9289, acc: 0.8937, precision: 0.9480, recall: 0.9179, f1: 0.9327, edges-ner-ontonotes_loss: 0.0229
10/01 03:44:12 AM: Update 22734: task edges-ner-ontonotes, batch 734 (22734): mcc: 0.9317, acc: 0.8971, precision: 0.9503, recall: 0.9207, f1: 0.9353, edges-ner-ontonotes_loss: 0.0220
10/01 03:44:22 AM: Update 22871: task edges-ner-ontonotes, batch 871 (22871): mcc: 0.9327, acc: 0.8987, precision: 0.9509, recall: 0.9220, f1: 0.9362, edges-ner-ontonotes_loss: 0.0217
10/01 03:44:31 AM: ***** Step 23000 / Validation 23 *****
10/01 03:44:31 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:44:31 AM: Validating...
10/01 03:44:32 AM: Evaluate: task edges-ner-ontonotes, batch 6 (157): mcc: 0.8217, acc: 0.7725, precision: 0.8689, recall: 0.7950, f1: 0.8303, edges-ner-ontonotes_loss: 0.0457
10/01 03:44:42 AM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.9221, acc: 0.8912, precision: 0.9462, recall: 0.9068, f1: 0.9261, edges-ner-ontonotes_loss: 0.0261
10/01 03:44:49 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:44:49 AM: Best result seen so far for macro.
10/01 03:44:49 AM: Updating LR scheduler:
10/01 03:44:49 AM: 	Best result seen so far for macro_avg: 0.941
10/01 03:44:49 AM: 	# validation passes without improvement: 0
10/01 03:44:49 AM: edges-ner-ontonotes_loss: training: 0.021440 validation: 0.021184
10/01 03:44:49 AM: macro_avg: validation: 0.941176
10/01 03:44:49 AM: micro_avg: validation: 0.000000
10/01 03:44:49 AM: edges-ner-ontonotes_mcc: training: 0.933402 validation: 0.937897
10/01 03:44:49 AM: edges-ner-ontonotes_acc: training: 0.899848 validation: 0.912420
10/01 03:44:49 AM: edges-ner-ontonotes_precision: training: 0.951649 validation: 0.955896
10/01 03:44:49 AM: edges-ner-ontonotes_recall: training: 0.922653 validation: 0.926903
10/01 03:44:49 AM: edges-ner-ontonotes_f1: training: 0.936927 validation: 0.941176
10/01 03:44:49 AM: Global learning rate: 0.0001
10/01 03:44:49 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:44:53 AM: Update 23037: task edges-ner-ontonotes, batch 37 (23037): mcc: 0.9384, acc: 0.9091, precision: 0.9527, recall: 0.9310, f1: 0.9417, edges-ner-ontonotes_loss: 0.0194
10/01 03:45:03 AM: Update 23181: task edges-ner-ontonotes, batch 181 (23181): mcc: 0.9079, acc: 0.8685, precision: 0.9351, recall: 0.8911, f1: 0.9126, edges-ner-ontonotes_loss: 0.0324
10/01 03:45:13 AM: Update 23326: task edges-ner-ontonotes, batch 326 (23326): mcc: 0.9065, acc: 0.8656, precision: 0.9327, recall: 0.8909, f1: 0.9113, edges-ner-ontonotes_loss: 0.0331
10/01 03:45:23 AM: Update 23458: task edges-ner-ontonotes, batch 458 (23458): mcc: 0.9070, acc: 0.8659, precision: 0.9331, recall: 0.8915, f1: 0.9118, edges-ner-ontonotes_loss: 0.0321
10/01 03:45:33 AM: Update 23631: task edges-ner-ontonotes, batch 631 (23631): mcc: 0.9074, acc: 0.8658, precision: 0.9335, recall: 0.8918, f1: 0.9122, edges-ner-ontonotes_loss: 0.0314
10/01 03:45:43 AM: Update 23754: task edges-ner-ontonotes, batch 754 (23754): mcc: 0.9101, acc: 0.8694, precision: 0.9351, recall: 0.8953, f1: 0.9148, edges-ner-ontonotes_loss: 0.0303
10/01 03:45:53 AM: Update 23900: task edges-ner-ontonotes, batch 900 (23900): mcc: 0.9128, acc: 0.8731, precision: 0.9372, recall: 0.8983, f1: 0.9173, edges-ner-ontonotes_loss: 0.0292
10/01 03:46:02 AM: ***** Step 24000 / Validation 24 *****
10/01 03:46:02 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:46:02 AM: Validating...
10/01 03:46:03 AM: Evaluate: task edges-ner-ontonotes, batch 14 (157): mcc: 0.8655, acc: 0.8138, precision: 0.9151, recall: 0.8320, f1: 0.8716, edges-ner-ontonotes_loss: 0.0368
10/01 03:46:13 AM: Evaluate: task edges-ner-ontonotes, batch 105 (157): mcc: 0.9263, acc: 0.8962, precision: 0.9486, recall: 0.9123, f1: 0.9301, edges-ner-ontonotes_loss: 0.0246
10/01 03:46:20 AM: Updating LR scheduler:
10/01 03:46:20 AM: 	Best result seen so far for macro_avg: 0.941
10/01 03:46:20 AM: 	# validation passes without improvement: 1
10/01 03:46:20 AM: edges-ner-ontonotes_loss: training: 0.028502 validation: 0.021062
10/01 03:46:20 AM: macro_avg: validation: 0.941172
10/01 03:46:20 AM: micro_avg: validation: 0.000000
10/01 03:46:20 AM: edges-ner-ontonotes_mcc: training: 0.914947 validation: 0.937881
10/01 03:46:20 AM: edges-ner-ontonotes_acc: training: 0.875831 validation: 0.911738
10/01 03:46:20 AM: edges-ner-ontonotes_precision: training: 0.938686 validation: 0.955323
10/01 03:46:20 AM: edges-ner-ontonotes_recall: training: 0.900833 validation: 0.927434
10/01 03:46:20 AM: edges-ner-ontonotes_f1: training: 0.919370 validation: 0.941172
10/01 03:46:20 AM: Global learning rate: 0.0001
10/01 03:46:20 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:46:23 AM: Update 24049: task edges-ner-ontonotes, batch 49 (24049): mcc: 0.9450, acc: 0.9183, precision: 0.9582, recall: 0.9381, f1: 0.9480, edges-ner-ontonotes_loss: 0.0182
10/01 03:46:33 AM: Update 24185: task edges-ner-ontonotes, batch 185 (24185): mcc: 0.9404, acc: 0.9095, precision: 0.9546, recall: 0.9328, f1: 0.9436, edges-ner-ontonotes_loss: 0.0190
10/01 03:46:43 AM: Update 24296: task edges-ner-ontonotes, batch 296 (24296): mcc: 0.9405, acc: 0.9093, precision: 0.9553, recall: 0.9324, f1: 0.9437, edges-ner-ontonotes_loss: 0.0189
10/01 03:46:53 AM: Update 24433: task edges-ner-ontonotes, batch 433 (24433): mcc: 0.9401, acc: 0.9085, precision: 0.9550, recall: 0.9320, f1: 0.9433, edges-ner-ontonotes_loss: 0.0191
10/01 03:47:03 AM: Update 24564: task edges-ner-ontonotes, batch 564 (24564): mcc: 0.9399, acc: 0.9081, precision: 0.9549, recall: 0.9317, f1: 0.9432, edges-ner-ontonotes_loss: 0.0192
10/01 03:47:14 AM: Update 24687: task edges-ner-ontonotes, batch 687 (24687): mcc: 0.9344, acc: 0.9009, precision: 0.9515, recall: 0.9246, f1: 0.9379, edges-ner-ontonotes_loss: 0.0218
10/01 03:47:24 AM: Update 24825: task edges-ner-ontonotes, batch 825 (24825): mcc: 0.9294, acc: 0.8945, precision: 0.9484, recall: 0.9184, f1: 0.9331, edges-ner-ontonotes_loss: 0.0240
10/01 03:47:34 AM: Update 24955: task edges-ner-ontonotes, batch 955 (24955): mcc: 0.9269, acc: 0.8910, precision: 0.9466, recall: 0.9153, f1: 0.9307, edges-ner-ontonotes_loss: 0.0249
10/01 03:47:36 AM: ***** Step 25000 / Validation 25 *****
10/01 03:47:36 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:47:36 AM: Validating...
10/01 03:47:44 AM: Evaluate: task edges-ner-ontonotes, batch 64 (157): mcc: 0.9184, acc: 0.8853, precision: 0.9448, recall: 0.9014, f1: 0.9226, edges-ner-ontonotes_loss: 0.0265
10/01 03:47:54 AM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.9367, acc: 0.9089, precision: 0.9585, recall: 0.9221, f1: 0.9400, edges-ner-ontonotes_loss: 0.0215
10/01 03:47:55 AM: Updating LR scheduler:
10/01 03:47:55 AM: 	Best result seen so far for macro_avg: 0.941
10/01 03:47:55 AM: 	# validation passes without improvement: 2
10/01 03:47:55 AM: edges-ner-ontonotes_loss: training: 0.025165 validation: 0.021099
10/01 03:47:55 AM: macro_avg: validation: 0.940599
10/01 03:47:55 AM: micro_avg: validation: 0.000000
10/01 03:47:55 AM: edges-ner-ontonotes_mcc: training: 0.926294 validation: 0.937373
10/01 03:47:55 AM: edges-ner-ontonotes_acc: training: 0.890391 validation: 0.909615
10/01 03:47:55 AM: edges-ner-ontonotes_precision: training: 0.946092 validation: 0.959171
10/01 03:47:55 AM: edges-ner-ontonotes_recall: training: 0.914797 validation: 0.922733
10/01 03:47:55 AM: edges-ner-ontonotes_f1: training: 0.930182 validation: 0.940599
10/01 03:47:55 AM: Global learning rate: 0.0001
10/01 03:47:55 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:48:04 AM: Update 25133: task edges-ner-ontonotes, batch 133 (25133): mcc: 0.9114, acc: 0.8685, precision: 0.9380, recall: 0.8949, f1: 0.9160, edges-ner-ontonotes_loss: 0.0285
10/01 03:48:14 AM: Update 25259: task edges-ner-ontonotes, batch 259 (25259): mcc: 0.9131, acc: 0.8716, precision: 0.9393, recall: 0.8968, f1: 0.9176, edges-ner-ontonotes_loss: 0.0280
10/01 03:48:24 AM: Update 25398: task edges-ner-ontonotes, batch 398 (25398): mcc: 0.9179, acc: 0.8789, precision: 0.9414, recall: 0.9038, f1: 0.9222, edges-ner-ontonotes_loss: 0.0265
10/01 03:48:34 AM: Update 25523: task edges-ner-ontonotes, batch 523 (25523): mcc: 0.9203, acc: 0.8821, precision: 0.9430, recall: 0.9065, f1: 0.9244, edges-ner-ontonotes_loss: 0.0257
10/01 03:48:44 AM: Update 25653: task edges-ner-ontonotes, batch 653 (25653): mcc: 0.9245, acc: 0.8878, precision: 0.9458, recall: 0.9118, f1: 0.9285, edges-ner-ontonotes_loss: 0.0245
10/01 03:48:54 AM: Update 25787: task edges-ner-ontonotes, batch 787 (25787): mcc: 0.9277, acc: 0.8922, precision: 0.9476, recall: 0.9160, f1: 0.9315, edges-ner-ontonotes_loss: 0.0235
10/01 03:49:04 AM: Update 25903: task edges-ner-ontonotes, batch 903 (25903): mcc: 0.9301, acc: 0.8953, precision: 0.9493, recall: 0.9188, f1: 0.9338, edges-ner-ontonotes_loss: 0.0228
10/01 03:49:12 AM: ***** Step 26000 / Validation 26 *****
10/01 03:49:12 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:49:12 AM: Validating...
10/01 03:49:15 AM: Evaluate: task edges-ner-ontonotes, batch 31 (157): mcc: 0.8916, acc: 0.8527, precision: 0.9234, recall: 0.8723, f1: 0.8971, edges-ner-ontonotes_loss: 0.0359
10/01 03:49:25 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9280, acc: 0.8988, precision: 0.9486, recall: 0.9156, f1: 0.9318, edges-ner-ontonotes_loss: 0.0247
10/01 03:49:30 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:49:30 AM: Best result seen so far for macro.
10/01 03:49:30 AM: Updating LR scheduler:
10/01 03:49:30 AM: 	Best result seen so far for macro_avg: 0.942
10/01 03:49:30 AM: 	# validation passes without improvement: 0
10/01 03:49:30 AM: edges-ner-ontonotes_loss: training: 0.022543 validation: 0.021354
10/01 03:49:30 AM: macro_avg: validation: 0.941697
10/01 03:49:30 AM: micro_avg: validation: 0.000000
10/01 03:49:30 AM: edges-ner-ontonotes_mcc: training: 0.930819 validation: 0.938428
10/01 03:49:30 AM: edges-ner-ontonotes_acc: training: 0.896468 validation: 0.912875
10/01 03:49:30 AM: edges-ner-ontonotes_precision: training: 0.949646 validation: 0.955439
10/01 03:49:30 AM: edges-ner-ontonotes_recall: training: 0.919781 validation: 0.928344
10/01 03:49:30 AM: edges-ner-ontonotes_f1: training: 0.934475 validation: 0.941697
10/01 03:49:30 AM: Global learning rate: 0.0001
10/01 03:49:30 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:49:35 AM: Update 26061: task edges-ner-ontonotes, batch 61 (26061): mcc: 0.9401, acc: 0.9093, precision: 0.9563, recall: 0.9307, f1: 0.9433, edges-ner-ontonotes_loss: 0.0199
10/01 03:49:45 AM: Update 26176: task edges-ner-ontonotes, batch 176 (26176): mcc: 0.9337, acc: 0.8993, precision: 0.9527, recall: 0.9221, f1: 0.9372, edges-ner-ontonotes_loss: 0.0218
10/01 03:49:55 AM: Update 26315: task edges-ner-ontonotes, batch 315 (26315): mcc: 0.9203, acc: 0.8825, precision: 0.9428, recall: 0.9069, f1: 0.9245, edges-ner-ontonotes_loss: 0.0277
10/01 03:50:06 AM: Update 26453: task edges-ner-ontonotes, batch 453 (26453): mcc: 0.9160, acc: 0.8772, precision: 0.9402, recall: 0.9013, f1: 0.9203, edges-ner-ontonotes_loss: 0.0295
10/01 03:50:17 AM: Update 26614: task edges-ner-ontonotes, batch 614 (26614): mcc: 0.9140, acc: 0.8742, precision: 0.9388, recall: 0.8990, f1: 0.9185, edges-ner-ontonotes_loss: 0.0295
10/01 03:50:28 AM: Update 26766: task edges-ner-ontonotes, batch 766 (26766): mcc: 0.9143, acc: 0.8744, precision: 0.9386, recall: 0.8996, f1: 0.9187, edges-ner-ontonotes_loss: 0.0291
10/01 03:50:38 AM: Update 26903: task edges-ner-ontonotes, batch 903 (26903): mcc: 0.9159, acc: 0.8766, precision: 0.9397, recall: 0.9017, f1: 0.9203, edges-ner-ontonotes_loss: 0.0284
10/01 03:50:45 AM: ***** Step 27000 / Validation 27 *****
10/01 03:50:45 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:50:45 AM: Validating...
10/01 03:50:48 AM: Evaluate: task edges-ner-ontonotes, batch 31 (157): mcc: 0.8988, acc: 0.8632, precision: 0.9294, recall: 0.8798, f1: 0.9039, edges-ner-ontonotes_loss: 0.0324
10/01 03:50:58 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9312, acc: 0.9029, precision: 0.9509, recall: 0.9193, f1: 0.9348, edges-ner-ontonotes_loss: 0.0234
10/01 03:51:03 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:51:03 AM: Best result seen so far for macro.
10/01 03:51:03 AM: Updating LR scheduler:
10/01 03:51:03 AM: 	Best result seen so far for macro_avg: 0.942
10/01 03:51:03 AM: 	# validation passes without improvement: 0
10/01 03:51:03 AM: edges-ner-ontonotes_loss: training: 0.027816 validation: 0.020837
10/01 03:51:03 AM: macro_avg: validation: 0.942402
10/01 03:51:03 AM: micro_avg: validation: 0.000000
10/01 03:51:03 AM: edges-ner-ontonotes_mcc: training: 0.917165 validation: 0.939170
10/01 03:51:03 AM: edges-ner-ontonotes_acc: training: 0.878047 validation: 0.913558
10/01 03:51:03 AM: edges-ner-ontonotes_precision: training: 0.940768 validation: 0.955928
10/01 03:51:03 AM: edges-ner-ontonotes_recall: training: 0.902944 validation: 0.929254
10/01 03:51:03 AM: edges-ner-ontonotes_f1: training: 0.921468 validation: 0.942402
10/01 03:51:03 AM: Global learning rate: 0.0001
10/01 03:51:03 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:51:08 AM: Update 27065: task edges-ner-ontonotes, batch 65 (27065): mcc: 0.9248, acc: 0.8884, precision: 0.9432, recall: 0.9148, f1: 0.9288, edges-ner-ontonotes_loss: 0.0246
10/01 03:51:18 AM: Update 27175: task edges-ner-ontonotes, batch 175 (27175): mcc: 0.9346, acc: 0.9024, precision: 0.9501, recall: 0.9265, f1: 0.9381, edges-ner-ontonotes_loss: 0.0214
10/01 03:51:28 AM: Update 27305: task edges-ner-ontonotes, batch 305 (27305): mcc: 0.9368, acc: 0.9049, precision: 0.9527, recall: 0.9280, f1: 0.9402, edges-ner-ontonotes_loss: 0.0200
10/01 03:51:38 AM: Update 27425: task edges-ner-ontonotes, batch 425 (27425): mcc: 0.9384, acc: 0.9069, precision: 0.9538, recall: 0.9299, f1: 0.9417, edges-ner-ontonotes_loss: 0.0196
10/01 03:51:48 AM: Update 27560: task edges-ner-ontonotes, batch 560 (27560): mcc: 0.9391, acc: 0.9074, precision: 0.9543, recall: 0.9307, f1: 0.9423, edges-ner-ontonotes_loss: 0.0193
10/01 03:51:58 AM: Update 27694: task edges-ner-ontonotes, batch 694 (27694): mcc: 0.9391, acc: 0.9076, precision: 0.9544, recall: 0.9307, f1: 0.9424, edges-ner-ontonotes_loss: 0.0194
10/01 03:52:08 AM: Update 27801: task edges-ner-ontonotes, batch 801 (27801): mcc: 0.9349, acc: 0.9022, precision: 0.9516, recall: 0.9255, f1: 0.9383, edges-ner-ontonotes_loss: 0.0210
10/01 03:52:18 AM: Update 27949: task edges-ner-ontonotes, batch 949 (27949): mcc: 0.9299, acc: 0.8958, precision: 0.9485, recall: 0.9192, f1: 0.9336, edges-ner-ontonotes_loss: 0.0234
10/01 03:52:22 AM: ***** Step 28000 / Validation 28 *****
10/01 03:52:22 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:52:22 AM: Validating...
10/01 03:52:28 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9206, acc: 0.8904, precision: 0.9441, recall: 0.9061, f1: 0.9247, edges-ner-ontonotes_loss: 0.0270
10/01 03:52:38 AM: Evaluate: task edges-ner-ontonotes, batch 142 (157): mcc: 0.9360, acc: 0.9089, precision: 0.9562, recall: 0.9230, f1: 0.9393, edges-ner-ontonotes_loss: 0.0223
10/01 03:52:40 AM: Updating LR scheduler:
10/01 03:52:40 AM: 	Best result seen so far for macro_avg: 0.942
10/01 03:52:40 AM: 	# validation passes without improvement: 1
10/01 03:52:40 AM: edges-ner-ontonotes_loss: training: 0.023991 validation: 0.021784
10/01 03:52:40 AM: macro_avg: validation: 0.940106
10/01 03:52:40 AM: micro_avg: validation: 0.000000
10/01 03:52:40 AM: edges-ner-ontonotes_mcc: training: 0.928625 validation: 0.936802
10/01 03:52:40 AM: edges-ner-ontonotes_acc: training: 0.894125 validation: 0.909994
10/01 03:52:40 AM: edges-ner-ontonotes_precision: training: 0.947613 validation: 0.956597
10/01 03:52:40 AM: edges-ner-ontonotes_recall: training: 0.917669 validation: 0.924173
10/01 03:52:40 AM: edges-ner-ontonotes_f1: training: 0.932401 validation: 0.940106
10/01 03:52:40 AM: Global learning rate: 0.0001
10/01 03:52:40 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:52:48 AM: Update 28105: task edges-ner-ontonotes, batch 105 (28105): mcc: 0.9077, acc: 0.8644, precision: 0.9328, recall: 0.8930, f1: 0.9125, edges-ner-ontonotes_loss: 0.0310
10/01 03:52:58 AM: Update 28270: task edges-ner-ontonotes, batch 270 (28270): mcc: 0.9092, acc: 0.8674, precision: 0.9334, recall: 0.8952, f1: 0.9139, edges-ner-ontonotes_loss: 0.0299
10/01 03:53:08 AM: Update 28397: task edges-ner-ontonotes, batch 397 (28397): mcc: 0.9128, acc: 0.8722, precision: 0.9369, recall: 0.8986, f1: 0.9173, edges-ner-ontonotes_loss: 0.0286
10/01 03:53:18 AM: Update 28532: task edges-ner-ontonotes, batch 532 (28532): mcc: 0.9162, acc: 0.8771, precision: 0.9387, recall: 0.9031, f1: 0.9206, edges-ner-ontonotes_loss: 0.0276
10/01 03:53:28 AM: Update 28654: task edges-ner-ontonotes, batch 654 (28654): mcc: 0.9202, acc: 0.8831, precision: 0.9415, recall: 0.9079, f1: 0.9244, edges-ner-ontonotes_loss: 0.0264
10/01 03:53:38 AM: Update 28784: task edges-ner-ontonotes, batch 784 (28784): mcc: 0.9241, acc: 0.8882, precision: 0.9441, recall: 0.9128, f1: 0.9282, edges-ner-ontonotes_loss: 0.0251
10/01 03:53:48 AM: Update 28921: task edges-ner-ontonotes, batch 921 (28921): mcc: 0.9274, acc: 0.8925, precision: 0.9464, recall: 0.9166, f1: 0.9313, edges-ner-ontonotes_loss: 0.0240
10/01 03:53:56 AM: ***** Step 29000 / Validation 29 *****
10/01 03:53:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:53:56 AM: Validating...
10/01 03:53:58 AM: Evaluate: task edges-ner-ontonotes, batch 24 (157): mcc: 0.8715, acc: 0.8272, precision: 0.9111, recall: 0.8469, f1: 0.8778, edges-ner-ontonotes_loss: 0.0405
10/01 03:54:09 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.9275, acc: 0.8965, precision: 0.9505, recall: 0.9127, f1: 0.9312, edges-ner-ontonotes_loss: 0.0248
10/01 03:54:14 AM: Updating LR scheduler:
10/01 03:54:14 AM: 	Best result seen so far for macro_avg: 0.942
10/01 03:54:14 AM: 	# validation passes without improvement: 2
10/01 03:54:14 AM: edges-ner-ontonotes_loss: training: 0.023655 validation: 0.021376
10/01 03:54:14 AM: macro_avg: validation: 0.941589
10/01 03:54:14 AM: micro_avg: validation: 0.000000
10/01 03:54:14 AM: edges-ner-ontonotes_mcc: training: 0.928198 validation: 0.938351
10/01 03:54:14 AM: edges-ner-ontonotes_acc: training: 0.893504 validation: 0.911662
10/01 03:54:14 AM: edges-ner-ontonotes_precision: training: 0.946804 validation: 0.957152
10/01 03:54:14 AM: edges-ner-ontonotes_recall: training: 0.917662 validation: 0.926524
10/01 03:54:14 AM: edges-ner-ontonotes_f1: training: 0.932005 validation: 0.941589
10/01 03:54:14 AM: Global learning rate: 0.0001
10/01 03:54:14 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:54:19 AM: Update 29060: task edges-ner-ontonotes, batch 60 (29060): mcc: 0.9382, acc: 0.9050, precision: 0.9556, recall: 0.9278, f1: 0.9415, edges-ner-ontonotes_loss: 0.0208
10/01 03:54:29 AM: Update 29192: task edges-ner-ontonotes, batch 192 (29192): mcc: 0.9402, acc: 0.9082, precision: 0.9568, recall: 0.9304, f1: 0.9434, edges-ner-ontonotes_loss: 0.0199
10/01 03:54:39 AM: Update 29310: task edges-ner-ontonotes, batch 310 (29310): mcc: 0.9359, acc: 0.9031, precision: 0.9542, recall: 0.9249, f1: 0.9393, edges-ner-ontonotes_loss: 0.0215
10/01 03:54:49 AM: Update 29451: task edges-ner-ontonotes, batch 451 (29451): mcc: 0.9257, acc: 0.8898, precision: 0.9472, recall: 0.9126, f1: 0.9296, edges-ner-ontonotes_loss: 0.0259
10/01 03:54:59 AM: Update 29567: task edges-ner-ontonotes, batch 567 (29567): mcc: 0.9220, acc: 0.8851, precision: 0.9449, recall: 0.9079, f1: 0.9260, edges-ner-ontonotes_loss: 0.0273
10/01 03:55:09 AM: Update 29732: task edges-ner-ontonotes, batch 732 (29732): mcc: 0.9197, acc: 0.8818, precision: 0.9426, recall: 0.9059, f1: 0.9239, edges-ner-ontonotes_loss: 0.0278
10/01 03:55:19 AM: Update 29878: task edges-ner-ontonotes, batch 878 (29878): mcc: 0.9187, acc: 0.8805, precision: 0.9418, recall: 0.9048, f1: 0.9229, edges-ner-ontonotes_loss: 0.0278
10/01 03:55:28 AM: ***** Step 30000 / Validation 30 *****
10/01 03:55:28 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:55:28 AM: Validating...
10/01 03:55:29 AM: Evaluate: task edges-ner-ontonotes, batch 10 (157): mcc: 0.8636, acc: 0.8121, precision: 0.9144, recall: 0.8292, f1: 0.8697, edges-ner-ontonotes_loss: 0.0348
10/01 03:55:39 AM: Evaluate: task edges-ner-ontonotes, batch 101 (157): mcc: 0.9259, acc: 0.8941, precision: 0.9522, recall: 0.9082, f1: 0.9297, edges-ner-ontonotes_loss: 0.0247
10/01 03:55:46 AM: Updating LR scheduler:
10/01 03:55:46 AM: 	Best result seen so far for macro_avg: 0.942
10/01 03:55:46 AM: 	# validation passes without improvement: 3
10/01 03:55:46 AM: edges-ner-ontonotes_loss: training: 0.027394 validation: 0.020743
10/01 03:55:46 AM: macro_avg: validation: 0.942321
10/01 03:55:46 AM: micro_avg: validation: 0.000000
10/01 03:55:46 AM: edges-ner-ontonotes_mcc: training: 0.919096 validation: 0.939170
10/01 03:55:46 AM: edges-ner-ontonotes_acc: training: 0.881071 validation: 0.912496
10/01 03:55:46 AM: edges-ner-ontonotes_precision: training: 0.941816 validation: 0.959887
10/01 03:55:46 AM: edges-ner-ontonotes_recall: training: 0.905526 validation: 0.925387
10/01 03:55:46 AM: edges-ner-ontonotes_f1: training: 0.923315 validation: 0.942321
10/01 03:55:46 AM: Global learning rate: 0.0001
10/01 03:55:46 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:55:49 AM: Update 30044: task edges-ner-ontonotes, batch 44 (30044): mcc: 0.9264, acc: 0.8909, precision: 0.9453, recall: 0.9158, f1: 0.9303, edges-ner-ontonotes_loss: 0.0232
10/01 03:55:59 AM: Update 30187: task edges-ner-ontonotes, batch 187 (30187): mcc: 0.9276, acc: 0.8941, precision: 0.9461, recall: 0.9173, f1: 0.9315, edges-ner-ontonotes_loss: 0.0232
10/01 03:56:09 AM: Update 30303: task edges-ner-ontonotes, batch 303 (30303): mcc: 0.9320, acc: 0.8986, precision: 0.9488, recall: 0.9227, f1: 0.9356, edges-ner-ontonotes_loss: 0.0215
10/01 03:56:19 AM: Update 30441: task edges-ner-ontonotes, batch 441 (30441): mcc: 0.9356, acc: 0.9033, precision: 0.9514, recall: 0.9271, f1: 0.9391, edges-ner-ontonotes_loss: 0.0206
10/01 03:56:30 AM: Update 30565: task edges-ner-ontonotes, batch 565 (30565): mcc: 0.9370, acc: 0.9052, precision: 0.9529, recall: 0.9282, f1: 0.9404, edges-ner-ontonotes_loss: 0.0202
10/01 03:56:40 AM: Update 30704: task edges-ner-ontonotes, batch 704 (30704): mcc: 0.9376, acc: 0.9057, precision: 0.9533, recall: 0.9289, f1: 0.9409, edges-ner-ontonotes_loss: 0.0200
10/01 03:56:50 AM: Update 30821: task edges-ner-ontonotes, batch 821 (30821): mcc: 0.9377, acc: 0.9059, precision: 0.9535, recall: 0.9289, f1: 0.9410, edges-ner-ontonotes_loss: 0.0200
10/01 03:57:00 AM: Update 30962: task edges-ner-ontonotes, batch 962 (30962): mcc: 0.9326, acc: 0.8993, precision: 0.9504, recall: 0.9224, f1: 0.9362, edges-ner-ontonotes_loss: 0.0222
10/01 03:57:02 AM: ***** Step 31000 / Validation 31 *****
10/01 03:57:02 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:57:02 AM: Validating...
10/01 03:57:10 AM: Evaluate: task edges-ner-ontonotes, batch 72 (157): mcc: 0.9180, acc: 0.8862, precision: 0.9424, recall: 0.9029, f1: 0.9222, edges-ner-ontonotes_loss: 0.0278
10/01 03:57:20 AM: Evaluate: task edges-ner-ontonotes, batch 156 (157): mcc: 0.9390, acc: 0.9136, precision: 0.9576, recall: 0.9272, f1: 0.9422, edges-ner-ontonotes_loss: 0.0214
10/01 03:57:20 AM: Updating LR scheduler:
10/01 03:57:20 AM: 	Best result seen so far for macro_avg: 0.942
10/01 03:57:20 AM: 	# validation passes without improvement: 0
10/01 03:57:20 AM: edges-ner-ontonotes_loss: training: 0.022556 validation: 0.021335
10/01 03:57:20 AM: macro_avg: validation: 0.942174
10/01 03:57:20 AM: micro_avg: validation: 0.000000
10/01 03:57:20 AM: edges-ner-ontonotes_mcc: training: 0.931848 validation: 0.938967
10/01 03:57:20 AM: edges-ner-ontonotes_acc: training: 0.898361 validation: 0.913634
10/01 03:57:20 AM: edges-ner-ontonotes_precision: training: 0.949900 validation: 0.957632
10/01 03:57:20 AM: edges-ner-ontonotes_recall: training: 0.921457 validation: 0.927207
10/01 03:57:20 AM: edges-ner-ontonotes_f1: training: 0.935463 validation: 0.942174
10/01 03:57:20 AM: Global learning rate: 5e-05
10/01 03:57:20 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:57:30 AM: Update 31124: task edges-ner-ontonotes, batch 124 (31124): mcc: 0.9005, acc: 0.8580, precision: 0.9288, recall: 0.8836, f1: 0.9056, edges-ner-ontonotes_loss: 0.0347
10/01 03:57:40 AM: Update 31296: task edges-ner-ontonotes, batch 296 (31296): mcc: 0.9038, acc: 0.8613, precision: 0.9299, recall: 0.8886, f1: 0.9088, edges-ner-ontonotes_loss: 0.0320
10/01 03:57:50 AM: Update 31435: task edges-ner-ontonotes, batch 435 (31435): mcc: 0.9080, acc: 0.8659, precision: 0.9331, recall: 0.8932, f1: 0.9127, edges-ner-ontonotes_loss: 0.0305
10/01 03:58:00 AM: Update 31582: task edges-ner-ontonotes, batch 582 (31582): mcc: 0.9138, acc: 0.8738, precision: 0.9374, recall: 0.8999, f1: 0.9183, edges-ner-ontonotes_loss: 0.0285
10/01 03:58:10 AM: Update 31721: task edges-ner-ontonotes, batch 721 (31721): mcc: 0.9169, acc: 0.8783, precision: 0.9394, recall: 0.9038, f1: 0.9213, edges-ner-ontonotes_loss: 0.0274
10/01 03:58:20 AM: Update 31837: task edges-ner-ontonotes, batch 837 (31837): mcc: 0.9209, acc: 0.8834, precision: 0.9424, recall: 0.9083, f1: 0.9251, edges-ner-ontonotes_loss: 0.0263
10/01 03:58:30 AM: Update 31978: task edges-ner-ontonotes, batch 978 (31978): mcc: 0.9246, acc: 0.8883, precision: 0.9448, recall: 0.9128, f1: 0.9286, edges-ner-ontonotes_loss: 0.0251
10/01 03:58:32 AM: ***** Step 32000 / Validation 32 *****
10/01 03:58:32 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:58:32 AM: Validating...
10/01 03:58:40 AM: Evaluate: task edges-ner-ontonotes, batch 82 (157): mcc: 0.9233, acc: 0.8913, precision: 0.9494, recall: 0.9060, f1: 0.9272, edges-ner-ontonotes_loss: 0.0262
10/01 03:58:49 AM: Updating LR scheduler:
10/01 03:58:49 AM: 	Best result seen so far for macro_avg: 0.942
10/01 03:58:49 AM: 	# validation passes without improvement: 1
10/01 03:58:49 AM: edges-ner-ontonotes_loss: training: 0.024932 validation: 0.020964
10/01 03:58:49 AM: macro_avg: validation: 0.941961
10/01 03:58:49 AM: micro_avg: validation: 0.000000
10/01 03:58:49 AM: edges-ner-ontonotes_mcc: training: 0.924982 validation: 0.938764
10/01 03:58:49 AM: edges-ner-ontonotes_acc: training: 0.888828 validation: 0.912800
10/01 03:58:49 AM: edges-ner-ontonotes_precision: training: 0.945122 validation: 0.958409
10/01 03:58:49 AM: edges-ner-ontonotes_recall: training: 0.913293 validation: 0.926069
10/01 03:58:49 AM: edges-ner-ontonotes_f1: training: 0.928935 validation: 0.941961
10/01 03:58:49 AM: Global learning rate: 5e-05
10/01 03:58:49 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 03:58:50 AM: Update 32013: task edges-ner-ontonotes, batch 13 (32013): mcc: 0.9454, acc: 0.9203, precision: 0.9559, recall: 0.9409, f1: 0.9483, edges-ner-ontonotes_loss: 0.0175
10/01 03:59:00 AM: Update 32131: task edges-ner-ontonotes, batch 131 (32131): mcc: 0.9388, acc: 0.9080, precision: 0.9535, recall: 0.9309, f1: 0.9421, edges-ner-ontonotes_loss: 0.0192
10/01 03:59:10 AM: Update 32268: task edges-ner-ontonotes, batch 268 (32268): mcc: 0.9407, acc: 0.9103, precision: 0.9560, recall: 0.9321, f1: 0.9439, edges-ner-ontonotes_loss: 0.0187
10/01 03:59:20 AM: Update 32390: task edges-ner-ontonotes, batch 390 (32390): mcc: 0.9388, acc: 0.9076, precision: 0.9549, recall: 0.9295, f1: 0.9420, edges-ner-ontonotes_loss: 0.0197
10/01 03:59:30 AM: Update 32536: task edges-ner-ontonotes, batch 536 (32536): mcc: 0.9298, acc: 0.8959, precision: 0.9488, recall: 0.9186, f1: 0.9335, edges-ner-ontonotes_loss: 0.0236
10/01 03:59:41 AM: Update 32677: task edges-ner-ontonotes, batch 677 (32677): mcc: 0.9254, acc: 0.8901, precision: 0.9462, recall: 0.9130, f1: 0.9293, edges-ner-ontonotes_loss: 0.0256
10/01 03:59:51 AM: Update 32846: task edges-ner-ontonotes, batch 846 (32846): mcc: 0.9234, acc: 0.8873, precision: 0.9446, recall: 0.9107, f1: 0.9274, edges-ner-ontonotes_loss: 0.0260
10/01 04:00:02 AM: Update 32990: task edges-ner-ontonotes, batch 990 (32990): mcc: 0.9220, acc: 0.8853, precision: 0.9437, recall: 0.9091, f1: 0.9261, edges-ner-ontonotes_loss: 0.0264
10/01 04:00:02 AM: ***** Step 33000 / Validation 33 *****
10/01 04:00:02 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:00:02 AM: Validating...
10/01 04:00:12 AM: Evaluate: task edges-ner-ontonotes, batch 88 (157): mcc: 0.9286, acc: 0.8992, precision: 0.9521, recall: 0.9133, f1: 0.9323, edges-ner-ontonotes_loss: 0.0245
10/01 04:00:20 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:00:20 AM: Best result seen so far for macro.
10/01 04:00:20 AM: Updating LR scheduler:
10/01 04:00:20 AM: 	Best result seen so far for macro_avg: 0.942
10/01 04:00:20 AM: 	# validation passes without improvement: 2
10/01 04:00:20 AM: edges-ner-ontonotes_loss: training: 0.026372 validation: 0.020855
10/01 04:00:20 AM: macro_avg: validation: 0.942483
10/01 04:00:20 AM: micro_avg: validation: 0.000000
10/01 04:00:20 AM: edges-ner-ontonotes_mcc: training: 0.921970 validation: 0.939323
10/01 04:00:20 AM: edges-ner-ontonotes_acc: training: 0.885296 validation: 0.913558
10/01 04:00:20 AM: edges-ner-ontonotes_precision: training: 0.943560 validation: 0.959246
10/01 04:00:20 AM: edges-ner-ontonotes_recall: training: 0.909190 validation: 0.926297
10/01 04:00:20 AM: edges-ner-ontonotes_f1: training: 0.926056 validation: 0.942483
10/01 04:00:20 AM: Global learning rate: 5e-05
10/01 04:00:20 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:00:22 AM: Update 33023: task edges-ner-ontonotes, batch 23 (33023): mcc: 0.9187, acc: 0.8827, precision: 0.9386, recall: 0.9079, f1: 0.9230, edges-ner-ontonotes_loss: 0.0267
10/01 04:00:32 AM: Update 33166: task edges-ner-ontonotes, batch 166 (33166): mcc: 0.9286, acc: 0.8953, precision: 0.9478, recall: 0.9175, f1: 0.9324, edges-ner-ontonotes_loss: 0.0230
10/01 04:00:43 AM: Update 33303: task edges-ner-ontonotes, batch 303 (33303): mcc: 0.9287, acc: 0.8946, precision: 0.9476, recall: 0.9178, f1: 0.9324, edges-ner-ontonotes_loss: 0.0229
10/01 04:00:53 AM: Update 33442: task edges-ner-ontonotes, batch 442 (33442): mcc: 0.9326, acc: 0.8995, precision: 0.9499, recall: 0.9229, f1: 0.9362, edges-ner-ontonotes_loss: 0.0215
10/01 04:01:03 AM: Update 33580: task edges-ner-ontonotes, batch 580 (33580): mcc: 0.9359, acc: 0.9043, precision: 0.9522, recall: 0.9269, f1: 0.9394, edges-ner-ontonotes_loss: 0.0206
10/01 04:01:13 AM: Update 33708: task edges-ner-ontonotes, batch 708 (33708): mcc: 0.9373, acc: 0.9058, precision: 0.9533, recall: 0.9284, f1: 0.9407, edges-ner-ontonotes_loss: 0.0202
10/01 04:01:23 AM: Update 33844: task edges-ner-ontonotes, batch 844 (33844): mcc: 0.9378, acc: 0.9063, precision: 0.9535, recall: 0.9291, f1: 0.9412, edges-ner-ontonotes_loss: 0.0200
10/01 04:01:33 AM: Update 33959: task edges-ner-ontonotes, batch 959 (33959): mcc: 0.9364, acc: 0.9043, precision: 0.9526, recall: 0.9274, f1: 0.9398, edges-ner-ontonotes_loss: 0.0204
10/01 04:01:36 AM: ***** Step 34000 / Validation 34 *****
10/01 04:01:36 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:01:36 AM: Validating...
10/01 04:01:43 AM: Evaluate: task edges-ner-ontonotes, batch 70 (157): mcc: 0.9167, acc: 0.8819, precision: 0.9473, recall: 0.8958, f1: 0.9208, edges-ner-ontonotes_loss: 0.0277
10/01 04:01:54 AM: Evaluate: task edges-ner-ontonotes, batch 154 (157): mcc: 0.9375, acc: 0.9106, precision: 0.9603, recall: 0.9219, f1: 0.9407, edges-ner-ontonotes_loss: 0.0215
10/01 04:01:54 AM: Updating LR scheduler:
10/01 04:01:54 AM: 	Best result seen so far for macro_avg: 0.942
10/01 04:01:54 AM: 	# validation passes without improvement: 3
10/01 04:01:54 AM: edges-ner-ontonotes_loss: training: 0.021040 validation: 0.021340
10/01 04:01:54 AM: macro_avg: validation: 0.941172
10/01 04:01:54 AM: micro_avg: validation: 0.000000
10/01 04:01:54 AM: edges-ner-ontonotes_mcc: training: 0.935096 validation: 0.937998
10/01 04:01:54 AM: edges-ner-ontonotes_acc: training: 0.902518 validation: 0.911283
10/01 04:01:54 AM: edges-ner-ontonotes_precision: training: 0.951844 validation: 0.960527
10/01 04:01:54 AM: edges-ner-ontonotes_recall: training: 0.925632 validation: 0.922581
10/01 04:01:54 AM: edges-ner-ontonotes_f1: training: 0.938555 validation: 0.941172
10/01 04:01:54 AM: Global learning rate: 5e-05
10/01 04:01:54 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:02:04 AM: Update 34141: task edges-ner-ontonotes, batch 141 (34141): mcc: 0.8980, acc: 0.8539, precision: 0.9283, recall: 0.8793, f1: 0.9031, edges-ner-ontonotes_loss: 0.0361
10/01 04:02:14 AM: Update 34268: task edges-ner-ontonotes, batch 268 (34268): mcc: 0.9040, acc: 0.8619, precision: 0.9317, recall: 0.8871, f1: 0.9089, edges-ner-ontonotes_loss: 0.0337
10/01 04:02:24 AM: Update 34438: task edges-ner-ontonotes, batch 438 (34438): mcc: 0.9060, acc: 0.8639, precision: 0.9328, recall: 0.8899, f1: 0.9109, edges-ner-ontonotes_loss: 0.0320
10/01 04:02:34 AM: Update 34576: task edges-ner-ontonotes, batch 576 (34576): mcc: 0.9087, acc: 0.8677, precision: 0.9346, recall: 0.8933, f1: 0.9135, edges-ner-ontonotes_loss: 0.0309
10/01 04:02:44 AM: Update 34721: task edges-ner-ontonotes, batch 721 (34721): mcc: 0.9127, acc: 0.8730, precision: 0.9369, recall: 0.8985, f1: 0.9173, edges-ner-ontonotes_loss: 0.0295
10/01 04:02:55 AM: Update 34859: task edges-ner-ontonotes, batch 859 (34859): mcc: 0.9153, acc: 0.8766, precision: 0.9387, recall: 0.9015, f1: 0.9198, edges-ner-ontonotes_loss: 0.0285
10/01 04:03:05 AM: Update 35000: task edges-ner-ontonotes, batch 1000 (35000): mcc: 0.9198, acc: 0.8823, precision: 0.9418, recall: 0.9068, f1: 0.9240, edges-ner-ontonotes_loss: 0.0271
10/01 04:03:05 AM: ***** Step 35000 / Validation 35 *****
10/01 04:03:05 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:03:05 AM: Validating...
10/01 04:03:15 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.9287, acc: 0.8982, precision: 0.9544, recall: 0.9112, f1: 0.9323, edges-ner-ontonotes_loss: 0.0248
10/01 04:03:22 AM: Updating LR scheduler:
10/01 04:03:22 AM: 	Best result seen so far for macro_avg: 0.942
10/01 04:03:22 AM: 	# validation passes without improvement: 0
10/01 04:03:22 AM: edges-ner-ontonotes_loss: training: 0.027062 validation: 0.020802
10/01 04:03:22 AM: macro_avg: validation: 0.942225
10/01 04:03:22 AM: micro_avg: validation: 0.000000
10/01 04:03:22 AM: edges-ner-ontonotes_mcc: training: 0.919764 validation: 0.939060
10/01 04:03:22 AM: edges-ner-ontonotes_acc: training: 0.882310 validation: 0.912875
10/01 04:03:22 AM: edges-ner-ontonotes_precision: training: 0.941827 validation: 0.959444
10/01 04:03:22 AM: edges-ner-ontonotes_recall: training: 0.906761 validation: 0.925614
10/01 04:03:22 AM: edges-ner-ontonotes_f1: training: 0.923962 validation: 0.942225
10/01 04:03:22 AM: Global learning rate: 2.5e-05
10/01 04:03:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:03:25 AM: Update 35036: task edges-ner-ontonotes, batch 36 (35036): mcc: 0.9443, acc: 0.9128, precision: 0.9614, recall: 0.9336, f1: 0.9473, edges-ner-ontonotes_loss: 0.0181
10/01 04:03:35 AM: Update 35170: task edges-ner-ontonotes, batch 170 (35170): mcc: 0.9450, acc: 0.9153, precision: 0.9593, recall: 0.9369, f1: 0.9480, edges-ner-ontonotes_loss: 0.0177
10/01 04:03:45 AM: Update 35284: task edges-ner-ontonotes, batch 284 (35284): mcc: 0.9427, acc: 0.9123, precision: 0.9577, recall: 0.9341, f1: 0.9457, edges-ner-ontonotes_loss: 0.0183
10/01 04:03:55 AM: Update 35426: task edges-ner-ontonotes, batch 426 (35426): mcc: 0.9426, acc: 0.9122, precision: 0.9579, recall: 0.9338, f1: 0.9457, edges-ner-ontonotes_loss: 0.0184
10/01 04:04:05 AM: Update 35554: task edges-ner-ontonotes, batch 554 (35554): mcc: 0.9377, acc: 0.9060, precision: 0.9549, recall: 0.9275, f1: 0.9410, edges-ner-ontonotes_loss: 0.0206
10/01 04:04:15 AM: Update 35693: task edges-ner-ontonotes, batch 693 (35693): mcc: 0.9316, acc: 0.8982, precision: 0.9505, recall: 0.9204, f1: 0.9352, edges-ner-ontonotes_loss: 0.0233
10/01 04:04:25 AM: Update 35828: task edges-ner-ontonotes, batch 828 (35828): mcc: 0.9276, acc: 0.8932, precision: 0.9477, recall: 0.9157, f1: 0.9314, edges-ner-ontonotes_loss: 0.0250
10/01 04:04:35 AM: ***** Step 36000 / Validation 36 *****
10/01 04:04:35 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:04:35 AM: Validating...
10/01 04:04:35 AM: Evaluate: task edges-ner-ontonotes, batch 3 (157): mcc: 0.7535, acc: 0.6814, precision: 0.8202, recall: 0.7157, f1: 0.7644, edges-ner-ontonotes_loss: 0.0556
10/01 04:04:45 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.9305, acc: 0.9002, precision: 0.9557, recall: 0.9132, f1: 0.9340, edges-ner-ontonotes_loss: 0.0241
10/01 04:04:53 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:04:53 AM: Best result seen so far for macro.
10/01 04:04:53 AM: Updating LR scheduler:
10/01 04:04:53 AM: 	Best result seen so far for macro_avg: 0.943
10/01 04:04:53 AM: 	# validation passes without improvement: 0
10/01 04:04:53 AM: edges-ner-ontonotes_loss: training: 0.025466 validation: 0.020562
10/01 04:04:53 AM: macro_avg: validation: 0.943197
10/01 04:04:53 AM: micro_avg: validation: 0.000000
10/01 04:04:53 AM: edges-ner-ontonotes_mcc: training: 0.925616 validation: 0.940086
10/01 04:04:53 AM: edges-ner-ontonotes_acc: training: 0.890393 validation: 0.914013
10/01 04:04:53 AM: edges-ner-ontonotes_precision: training: 0.946070 validation: 0.960317
10/01 04:04:53 AM: edges-ner-ontonotes_recall: training: 0.913552 validation: 0.926676
10/01 04:04:53 AM: edges-ner-ontonotes_f1: training: 0.929527 validation: 0.943197
10/01 04:04:53 AM: Global learning rate: 2.5e-05
10/01 04:04:53 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:04:55 AM: Update 36043: task edges-ner-ontonotes, batch 43 (36043): mcc: 0.9160, acc: 0.8763, precision: 0.9412, recall: 0.9004, f1: 0.9204, edges-ner-ontonotes_loss: 0.0278
10/01 04:05:05 AM: Update 36166: task edges-ner-ontonotes, batch 166 (36166): mcc: 0.9213, acc: 0.8861, precision: 0.9427, recall: 0.9088, f1: 0.9254, edges-ner-ontonotes_loss: 0.0260
10/01 04:05:15 AM: Update 36306: task edges-ner-ontonotes, batch 306 (36306): mcc: 0.9271, acc: 0.8929, precision: 0.9467, recall: 0.9157, f1: 0.9309, edges-ner-ontonotes_loss: 0.0242
10/01 04:05:25 AM: Update 36434: task edges-ner-ontonotes, batch 434 (36434): mcc: 0.9269, acc: 0.8930, precision: 0.9465, recall: 0.9155, f1: 0.9308, edges-ner-ontonotes_loss: 0.0237
10/01 04:05:35 AM: Update 36569: task edges-ner-ontonotes, batch 569 (36569): mcc: 0.9312, acc: 0.8984, precision: 0.9499, recall: 0.9203, f1: 0.9349, edges-ner-ontonotes_loss: 0.0224
10/01 04:05:45 AM: Update 36709: task edges-ner-ontonotes, batch 709 (36709): mcc: 0.9337, acc: 0.9013, precision: 0.9512, recall: 0.9237, f1: 0.9373, edges-ner-ontonotes_loss: 0.0215
10/01 04:05:55 AM: Update 36828: task edges-ner-ontonotes, batch 828 (36828): mcc: 0.9345, acc: 0.9021, precision: 0.9517, recall: 0.9247, f1: 0.9380, edges-ner-ontonotes_loss: 0.0213
10/01 04:06:05 AM: Update 36964: task edges-ner-ontonotes, batch 964 (36964): mcc: 0.9352, acc: 0.9029, precision: 0.9521, recall: 0.9257, f1: 0.9387, edges-ner-ontonotes_loss: 0.0210
10/01 04:06:08 AM: ***** Step 37000 / Validation 37 *****
10/01 04:06:08 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:06:08 AM: Validating...
10/01 04:06:15 AM: Evaluate: task edges-ner-ontonotes, batch 70 (157): mcc: 0.9191, acc: 0.8861, precision: 0.9464, recall: 0.9011, f1: 0.9232, edges-ner-ontonotes_loss: 0.0273
10/01 04:06:25 AM: Evaluate: task edges-ner-ontonotes, batch 152 (157): mcc: 0.9386, acc: 0.9128, precision: 0.9588, recall: 0.9253, f1: 0.9418, edges-ner-ontonotes_loss: 0.0210
10/01 04:06:26 AM: Updating LR scheduler:
10/01 04:06:26 AM: 	Best result seen so far for macro_avg: 0.943
10/01 04:06:26 AM: 	# validation passes without improvement: 1
10/01 04:06:26 AM: edges-ner-ontonotes_loss: training: 0.020966 validation: 0.020740
10/01 04:06:26 AM: macro_avg: validation: 0.942488
10/01 04:06:26 AM: micro_avg: validation: 0.000000
10/01 04:06:26 AM: edges-ner-ontonotes_mcc: training: 0.935393 validation: 0.939326
10/01 04:06:26 AM: edges-ner-ontonotes_acc: training: 0.903083 validation: 0.913861
10/01 04:06:26 AM: edges-ner-ontonotes_precision: training: 0.952217 validation: 0.959174
10/01 04:06:26 AM: edges-ner-ontonotes_recall: training: 0.925823 validation: 0.926372
10/01 04:06:26 AM: edges-ner-ontonotes_f1: training: 0.938834 validation: 0.942488
10/01 04:06:26 AM: Global learning rate: 2.5e-05
10/01 04:06:26 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:06:35 AM: Update 37113: task edges-ner-ontonotes, batch 113 (37113): mcc: 0.9139, acc: 0.8762, precision: 0.9388, recall: 0.8987, f1: 0.9183, edges-ner-ontonotes_loss: 0.0293
10/01 04:06:45 AM: Update 37257: task edges-ner-ontonotes, batch 257 (37257): mcc: 0.9086, acc: 0.8682, precision: 0.9350, recall: 0.8927, f1: 0.9133, edges-ner-ontonotes_loss: 0.0311
10/01 04:06:56 AM: Update 37383: task edges-ner-ontonotes, batch 383 (37383): mcc: 0.9095, acc: 0.8692, precision: 0.9354, recall: 0.8939, f1: 0.9142, edges-ner-ontonotes_loss: 0.0312
10/01 04:07:06 AM: Update 37555: task edges-ner-ontonotes, batch 555 (37555): mcc: 0.9100, acc: 0.8697, precision: 0.9354, recall: 0.8948, f1: 0.9147, edges-ner-ontonotes_loss: 0.0303
10/01 04:07:16 AM: Update 37691: task edges-ner-ontonotes, batch 691 (37691): mcc: 0.9112, acc: 0.8711, precision: 0.9363, recall: 0.8963, f1: 0.9158, edges-ner-ontonotes_loss: 0.0298
10/01 04:07:26 AM: Update 37837: task edges-ner-ontonotes, batch 837 (37837): mcc: 0.9144, acc: 0.8756, precision: 0.9385, recall: 0.9000, f1: 0.9189, edges-ner-ontonotes_loss: 0.0286
10/01 04:07:37 AM: Update 37971: task edges-ner-ontonotes, batch 971 (37971): mcc: 0.9165, acc: 0.8783, precision: 0.9400, recall: 0.9024, f1: 0.9208, edges-ner-ontonotes_loss: 0.0277
10/01 04:07:39 AM: ***** Step 38000 / Validation 38 *****
10/01 04:07:39 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:07:39 AM: Validating...
10/01 04:07:47 AM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.9221, acc: 0.8904, precision: 0.9473, recall: 0.9057, f1: 0.9261, edges-ner-ontonotes_loss: 0.0261
10/01 04:07:56 AM: Updating LR scheduler:
10/01 04:07:56 AM: 	Best result seen so far for macro_avg: 0.943
10/01 04:07:56 AM: 	# validation passes without improvement: 2
10/01 04:07:56 AM: edges-ner-ontonotes_loss: training: 0.027508 validation: 0.020387
10/01 04:07:56 AM: macro_avg: validation: 0.943181
10/01 04:07:56 AM: micro_avg: validation: 0.000000
10/01 04:07:56 AM: edges-ner-ontonotes_mcc: training: 0.917207 validation: 0.940045
10/01 04:07:56 AM: edges-ner-ontonotes_acc: training: 0.879358 validation: 0.914392
10/01 04:07:56 AM: edges-ner-ontonotes_precision: training: 0.940347 validation: 0.959228
10/01 04:07:56 AM: edges-ner-ontonotes_recall: training: 0.903433 validation: 0.927662
10/01 04:07:56 AM: edges-ner-ontonotes_f1: training: 0.921520 validation: 0.943181
10/01 04:07:56 AM: Global learning rate: 2.5e-05
10/01 04:07:56 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:07:57 AM: Update 38004: task edges-ner-ontonotes, batch 4 (38004): mcc: 0.9444, acc: 0.9041, precision: 0.9503, recall: 0.9448, f1: 0.9475, edges-ner-ontonotes_loss: 0.0184
10/01 04:08:07 AM: Update 38143: task edges-ner-ontonotes, batch 143 (38143): mcc: 0.9414, acc: 0.9102, precision: 0.9554, recall: 0.9340, f1: 0.9446, edges-ner-ontonotes_loss: 0.0185
10/01 04:08:17 AM: Update 38281: task edges-ner-ontonotes, batch 281 (38281): mcc: 0.9449, acc: 0.9155, precision: 0.9579, recall: 0.9380, f1: 0.9479, edges-ner-ontonotes_loss: 0.0177
10/01 04:08:27 AM: Update 38399: task edges-ner-ontonotes, batch 399 (38399): mcc: 0.9435, acc: 0.9135, precision: 0.9565, recall: 0.9367, f1: 0.9465, edges-ner-ontonotes_loss: 0.0181
10/01 04:08:37 AM: Update 38534: task edges-ner-ontonotes, batch 534 (38534): mcc: 0.9425, acc: 0.9121, precision: 0.9558, recall: 0.9356, f1: 0.9456, edges-ner-ontonotes_loss: 0.0183
10/01 04:08:47 AM: Update 38661: task edges-ner-ontonotes, batch 661 (38661): mcc: 0.9377, acc: 0.9058, precision: 0.9530, recall: 0.9293, f1: 0.9410, edges-ner-ontonotes_loss: 0.0205
10/01 04:08:57 AM: Update 38807: task edges-ner-ontonotes, batch 807 (38807): mcc: 0.9322, acc: 0.8984, precision: 0.9499, recall: 0.9221, f1: 0.9358, edges-ner-ontonotes_loss: 0.0230
10/01 04:09:07 AM: Update 38942: task edges-ner-ontonotes, batch 942 (38942): mcc: 0.9291, acc: 0.8943, precision: 0.9479, recall: 0.9182, f1: 0.9328, edges-ner-ontonotes_loss: 0.0242
10/01 04:09:10 AM: ***** Step 39000 / Validation 39 *****
10/01 04:09:10 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:09:10 AM: Validating...
10/01 04:09:17 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9246, acc: 0.8967, precision: 0.9472, recall: 0.9106, f1: 0.9285, edges-ner-ontonotes_loss: 0.0254
10/01 04:09:27 AM: Evaluate: task edges-ner-ontonotes, batch 145 (157): mcc: 0.9386, acc: 0.9136, precision: 0.9574, recall: 0.9268, f1: 0.9419, edges-ner-ontonotes_loss: 0.0211
10/01 04:09:28 AM: Updating LR scheduler:
10/01 04:09:28 AM: 	Best result seen so far for macro_avg: 0.943
10/01 04:09:28 AM: 	# validation passes without improvement: 3
10/01 04:09:28 AM: edges-ner-ontonotes_loss: training: 0.024616 validation: 0.020578
10/01 04:09:28 AM: macro_avg: validation: 0.942808
10/01 04:09:28 AM: micro_avg: validation: 0.000000
10/01 04:09:28 AM: edges-ner-ontonotes_mcc: training: 0.927945 validation: 0.939631
10/01 04:09:28 AM: edges-ner-ontonotes_acc: training: 0.892934 validation: 0.914619
10/01 04:09:28 AM: edges-ner-ontonotes_precision: training: 0.947046 validation: 0.957971
10/01 04:09:28 AM: edges-ner-ontonotes_recall: training: 0.916950 validation: 0.928116
10/01 04:09:28 AM: edges-ner-ontonotes_f1: training: 0.931755 validation: 0.942808
10/01 04:09:28 AM: Global learning rate: 2.5e-05
10/01 04:09:28 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:09:37 AM: Update 39151: task edges-ner-ontonotes, batch 151 (39151): mcc: 0.9149, acc: 0.8776, precision: 0.9381, recall: 0.9012, f1: 0.9193, edges-ner-ontonotes_loss: 0.0280
10/01 04:09:47 AM: Update 39281: task edges-ner-ontonotes, batch 281 (39281): mcc: 0.9191, acc: 0.8818, precision: 0.9421, recall: 0.9053, f1: 0.9234, edges-ner-ontonotes_loss: 0.0263
10/01 04:09:57 AM: Update 39420: task edges-ner-ontonotes, batch 420 (39420): mcc: 0.9214, acc: 0.8854, precision: 0.9425, recall: 0.9091, f1: 0.9255, edges-ner-ontonotes_loss: 0.0254
10/01 04:10:07 AM: Update 39542: task edges-ner-ontonotes, batch 542 (39542): mcc: 0.9249, acc: 0.8899, precision: 0.9450, recall: 0.9132, f1: 0.9288, edges-ner-ontonotes_loss: 0.0244
10/01 04:10:17 AM: Update 39683: task edges-ner-ontonotes, batch 683 (39683): mcc: 0.9293, acc: 0.8955, precision: 0.9483, recall: 0.9184, f1: 0.9331, edges-ner-ontonotes_loss: 0.0230
10/01 04:10:27 AM: Update 39818: task edges-ner-ontonotes, batch 818 (39818): mcc: 0.9320, acc: 0.8990, precision: 0.9497, recall: 0.9220, f1: 0.9356, edges-ner-ontonotes_loss: 0.0221
10/01 04:10:37 AM: Update 39947: task edges-ner-ontonotes, batch 947 (39947): mcc: 0.9331, acc: 0.9005, precision: 0.9505, recall: 0.9232, f1: 0.9366, edges-ner-ontonotes_loss: 0.0218
10/01 04:10:41 AM: ***** Step 40000 / Validation 40 *****
10/01 04:10:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:10:41 AM: Validating...
10/01 04:10:48 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9228, acc: 0.8919, precision: 0.9476, recall: 0.9068, f1: 0.9268, edges-ner-ontonotes_loss: 0.0264
10/01 04:10:58 AM: Evaluate: task edges-ner-ontonotes, batch 143 (157): mcc: 0.9386, acc: 0.9123, precision: 0.9589, recall: 0.9252, f1: 0.9417, edges-ner-ontonotes_loss: 0.0212
10/01 04:10:59 AM: Updating LR scheduler:
10/01 04:10:59 AM: 	Best result seen so far for macro_avg: 0.943
10/01 04:10:59 AM: 	# validation passes without improvement: 0
10/01 04:10:59 AM: edges-ner-ontonotes_loss: training: 0.021640 validation: 0.020721
10/01 04:10:59 AM: macro_avg: validation: 0.942437
10/01 04:10:59 AM: micro_avg: validation: 0.000000
10/01 04:10:59 AM: edges-ner-ontonotes_mcc: training: 0.933622 validation: 0.939263
10/01 04:10:59 AM: edges-ner-ontonotes_acc: training: 0.901049 validation: 0.913254
10/01 04:10:59 AM: edges-ner-ontonotes_precision: training: 0.951028 validation: 0.958663
10/01 04:10:59 AM: edges-ner-ontonotes_recall: training: 0.923672 validation: 0.926752
10/01 04:10:59 AM: edges-ner-ontonotes_f1: training: 0.937150 validation: 0.942437
10/01 04:10:59 AM: Global learning rate: 1.25e-05
10/01 04:10:59 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:11:08 AM: Update 40116: task edges-ner-ontonotes, batch 116 (40116): mcc: 0.9414, acc: 0.9084, precision: 0.9578, recall: 0.9316, f1: 0.9445, edges-ner-ontonotes_loss: 0.0184
10/01 04:11:18 AM: Update 40232: task edges-ner-ontonotes, batch 232 (40232): mcc: 0.9278, acc: 0.8914, precision: 0.9481, recall: 0.9156, f1: 0.9316, edges-ner-ontonotes_loss: 0.0243
10/01 04:11:28 AM: Update 40378: task edges-ner-ontonotes, batch 378 (40378): mcc: 0.9200, acc: 0.8822, precision: 0.9436, recall: 0.9055, f1: 0.9242, edges-ner-ontonotes_loss: 0.0280
10/01 04:11:38 AM: Update 40508: task edges-ner-ontonotes, batch 508 (40508): mcc: 0.9165, acc: 0.8778, precision: 0.9408, recall: 0.9017, f1: 0.9209, edges-ner-ontonotes_loss: 0.0292
10/01 04:11:48 AM: Update 40672: task edges-ner-ontonotes, batch 672 (40672): mcc: 0.9156, acc: 0.8768, precision: 0.9396, recall: 0.9012, f1: 0.9200, edges-ner-ontonotes_loss: 0.0290
10/01 04:11:58 AM: Update 40812: task edges-ner-ontonotes, batch 812 (40812): mcc: 0.9164, acc: 0.8774, precision: 0.9399, recall: 0.9023, f1: 0.9207, edges-ner-ontonotes_loss: 0.0284
10/01 04:12:08 AM: Update 40955: task edges-ner-ontonotes, batch 955 (40955): mcc: 0.9181, acc: 0.8802, precision: 0.9408, recall: 0.9047, f1: 0.9224, edges-ner-ontonotes_loss: 0.0276
10/01 04:12:11 AM: ***** Step 41000 / Validation 41 *****
10/01 04:12:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:12:11 AM: Validating...
10/01 04:12:18 AM: Evaluate: task edges-ner-ontonotes, batch 67 (157): mcc: 0.9197, acc: 0.8875, precision: 0.9456, recall: 0.9031, f1: 0.9238, edges-ner-ontonotes_loss: 0.0263
10/01 04:12:28 AM: Evaluate: task edges-ner-ontonotes, batch 147 (157): mcc: 0.9386, acc: 0.9125, precision: 0.9584, recall: 0.9258, f1: 0.9418, edges-ner-ontonotes_loss: 0.0208
10/01 04:12:29 AM: Updating LR scheduler:
10/01 04:12:29 AM: 	Best result seen so far for macro_avg: 0.943
10/01 04:12:29 AM: 	# validation passes without improvement: 1
10/01 04:12:29 AM: edges-ner-ontonotes_loss: training: 0.027439 validation: 0.020270
10/01 04:12:29 AM: macro_avg: validation: 0.942995
10/01 04:12:29 AM: micro_avg: validation: 0.000000
10/01 04:12:29 AM: edges-ner-ontonotes_mcc: training: 0.918351 validation: 0.939846
10/01 04:12:29 AM: edges-ner-ontonotes_acc: training: 0.880442 validation: 0.914164
10/01 04:12:29 AM: edges-ner-ontonotes_precision: training: 0.940861 validation: 0.958925
10/01 04:12:29 AM: edges-ner-ontonotes_recall: training: 0.905067 validation: 0.927586
10/01 04:12:29 AM: edges-ner-ontonotes_f1: training: 0.922617 validation: 0.942995
10/01 04:12:29 AM: Global learning rate: 1.25e-05
10/01 04:12:29 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstseed2-top/run
10/01 04:12:38 AM: Update 41100: task edges-ner-ontonotes, batch 100 (41100): mcc: 0.9300, acc: 0.8964, precision: 0.9461, recall: 0.9216, f1: 0.9337, edges-ner-ontonotes_loss: 0.0228
10/01 04:12:48 AM: Update 41232: task edges-ner-ontonotes, batch 232 (41232): mcc: 0.9376, acc: 0.9065, precision: 0.9521, recall: 0.9301, f1: 0.9409, edges-ner-ontonotes_loss: 0.0202
10/01 04:12:58 AM: Update 41366: task edges-ner-ontonotes, batch 366 (41366): mcc: 0.9392, acc: 0.9087, precision: 0.9537, recall: 0.9314, f1: 0.9424, edges-ner-ontonotes_loss: 0.0194
10/01 04:13:08 AM: Update 41482: task edges-ner-ontonotes, batch 482 (41482): mcc: 0.9387, acc: 0.9075, precision: 0.9537, recall: 0.9306, f1: 0.9420, edges-ner-ontonotes_loss: 0.0195
10/01 04:13:18 AM: Update 41617: task edges-ner-ontonotes, batch 617 (41617): mcc: 0.9387, acc: 0.9073, precision: 0.9536, recall: 0.9307, f1: 0.9420, edges-ner-ontonotes_loss: 0.0195
10/01 04:13:28 AM: Update 41725: task edges-ner-ontonotes, batch 725 (41725): mcc: 0.9377, acc: 0.9060, precision: 0.9534, recall: 0.9291, f1: 0.9410, edges-ner-ontonotes_loss: 0.0198
10/01 04:13:38 AM: Update 41859: task edges-ner-ontonotes, batch 859 (41859): mcc: 0.9328, acc: 0.8997, precision: 0.9501, recall: 0.9231, f1: 0.9364, edges-ner-ontonotes_loss: 0.0220
