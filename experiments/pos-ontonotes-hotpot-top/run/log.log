09/16 06:46:18 AM: Git branch: master
09/16 06:46:18 AM: Git SHA: fb3796f035a61c062bc75b422b0939a7eeec20ff
09/16 06:46:18 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/pos-ontonotes-hotpot-top/",
  "exp_name": "experiments/pos-ontonotes-hotpot-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/pos-ontonotes-hotpot-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/hotpot",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/pos-ontonotes-hotpot-top__run",
  "run_dir": "./experiments/pos-ontonotes-hotpot-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-pos-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 06:46:18 AM: Saved config to ./experiments/pos-ontonotes-hotpot-top/run/params.conf
09/16 06:46:18 AM: Using random seed 1234
09/16 06:46:19 AM: Using GPU 0
09/16 06:46:19 AM: Loading tasks...
09/16 06:46:19 AM: Writing pre-preprocessed tasks to ./experiments/pos-ontonotes-hotpot-top/
09/16 06:46:19 AM: 	Creating task edges-pos-ontonotes from scratch.
09/16 06:46:34 AM: Read=110514, Skip=5298, Total=115812 from ./probing_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased
09/16 06:46:35 AM: Read=15060, Skip=620, Total=15680 from ./probing_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased
09/16 06:46:38 AM: Read=11462, Skip=755, Total=12217 from ./probing_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased
09/16 06:46:48 AM: 	Task 'edges-pos-ontonotes': |train|=110514 |val|=15060 |test|=11462
09/16 06:46:48 AM: 	Finished loading tasks: edges-pos-ontonotes.
09/16 06:46:48 AM: 	Building vocab from scratch.
09/16 06:46:48 AM: 	Counting units for task edges-pos-ontonotes.
09/16 06:46:50 AM: 	Task 'edges-pos-ontonotes': adding vocab namespace 'edges-pos-ontonotes_labels'
09/16 06:46:51 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:46:51 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 06:46:51 AM: 	Saved vocab to ./experiments/pos-ontonotes-hotpot-top/vocab
09/16 06:46:51 AM: Loading token dictionary from ./experiments/pos-ontonotes-hotpot-top/vocab.
09/16 06:46:51 AM: 	Loaded vocab from ./experiments/pos-ontonotes-hotpot-top/vocab
09/16 06:46:51 AM: 	Vocab namespace bert_uncased: size 30524
09/16 06:46:51 AM: 	Vocab namespace tokens: size 24015
09/16 06:46:51 AM: 	Vocab namespace edges-pos-ontonotes_labels: size 48
09/16 06:46:51 AM: 	Vocab namespace chars: size 81
09/16 06:46:51 AM: 	Finished building vocab.
09/16 06:46:51 AM: 	Task edges-pos-ontonotes (train): Indexing from scratch.
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (train): Saved 110514 instances to ./experiments/pos-ontonotes-hotpot-top/preproc/edges-pos-ontonotes__train_data
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (val): Indexing from scratch.
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (val): Saved 15060 instances to ./experiments/pos-ontonotes-hotpot-top/preproc/edges-pos-ontonotes__val_data
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (test): Indexing from scratch.
09/16 06:47:27 AM: 	Task edges-pos-ontonotes (test): Saved 11462 instances to ./experiments/pos-ontonotes-hotpot-top/preproc/edges-pos-ontonotes__test_data
09/16 06:47:27 AM: 	Finished indexing tasks
09/16 06:47:27 AM: 	Creating trimmed target-only version of edges-pos-ontonotes train.
09/16 06:47:27 AM: 	  Training on 
09/16 06:47:27 AM: 	  Evaluating on edges-pos-ontonotes
09/16 06:47:27 AM: 	Finished loading tasks in 68.510s
09/16 06:47:27 AM: 	 Tasks: ['edges-pos-ontonotes']
09/16 06:47:27 AM: Building model...
09/16 06:47:27 AM: Using BERT model (bert-base-uncased).
09/16 06:47:27 AM: LOADING A FUNETUNED MODEL from: 
09/16 06:47:27 AM: models/hotpot
09/16 06:47:27 AM: loading configuration file models/hotpot/config.json
09/16 06:47:27 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 06:47:27 AM: loading weights file models/hotpot/pytorch_model.bin
09/16 06:47:30 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpq7tmtxhf
09/16 06:47:32 AM: copying /tmp/tmpq7tmtxhf to cache at ./experiments/pos-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: creating metadata file for ./experiments/pos-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: removing temp file /tmp/tmpq7tmtxhf
09/16 06:47:32 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/pos-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: Initializing parameters
09/16 06:47:32 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 06:47:32 AM: 	Task 'edges-pos-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-pos-ontonotes"
}
09/16 06:47:36 AM: Model specification:
09/16 06:47:36 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-pos-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=48, bias=True)
    )
  )
)
09/16 06:47:36 AM: Model parameters:
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 24576 with torch.Size([48, 512])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 48 with torch.Size([48])
09/16 06:47:36 AM: Total number of parameters: 109703728 (1.09704e+08)
09/16 06:47:36 AM: Number of trainable parameters: 221488 (221488)
09/16 06:47:36 AM: Finished building model in 8.971s
09/16 06:47:36 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-pos-ontonotes 

09/16 06:48:40 AM: patience = 9
09/16 06:48:40 AM: val_interval = 1000
09/16 06:48:40 AM: max_vals = 250
09/16 06:48:40 AM: cuda_device = 0
09/16 06:48:40 AM: grad_norm = 5.0
09/16 06:48:40 AM: grad_clipping = None
09/16 06:48:40 AM: lr_decay = 0.99
09/16 06:48:40 AM: min_lr = 1e-06
09/16 06:48:40 AM: keep_all_checkpoints = 0
09/16 06:48:40 AM: val_data_limit = 5000
09/16 06:48:40 AM: max_epochs = -1
09/16 06:48:40 AM: dec_val_scale = 250
09/16 06:48:40 AM: training_data_fraction = 1
09/16 06:48:40 AM: type = adam
09/16 06:48:40 AM: parameter_groups = None
09/16 06:48:40 AM: Number of trainable parameters: 221488
09/16 06:48:40 AM: infer_type_and_cast = True
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: lr = 0.0001
09/16 06:48:40 AM: amsgrad = True
09/16 06:48:40 AM: type = reduce_on_plateau
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: mode = max
09/16 06:48:40 AM: factor = 0.5
09/16 06:48:40 AM: patience = 3
09/16 06:48:40 AM: threshold = 0.0001
09/16 06:48:40 AM: threshold_mode = abs
09/16 06:48:40 AM: verbose = True
09/16 06:48:40 AM: type = adam
09/16 06:48:40 AM: parameter_groups = None
09/16 06:48:40 AM: Number of trainable parameters: 221488
09/16 06:48:40 AM: infer_type_and_cast = True
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: lr = 0.0001
09/16 06:48:40 AM: amsgrad = True
09/16 06:48:40 AM: type = reduce_on_plateau
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: mode = max
09/16 06:48:40 AM: factor = 0.5
09/16 06:48:40 AM: patience = 3
09/16 06:48:40 AM: threshold = 0.0001
09/16 06:48:40 AM: threshold_mode = abs
09/16 06:48:40 AM: verbose = True
09/16 06:48:40 AM: Starting training without restoring from a checkpoint.
09/16 06:48:40 AM: Training examples per task, before any subsampling: {'edges-pos-ontonotes': 110514}
09/16 06:48:40 AM: Beginning training with stopping criteria based on metric: edges-pos-ontonotes_f1
09/16 06:48:50 AM: Update 81: task edges-pos-ontonotes, batch 81 (81): mcc: -0.0044, acc: 0.0008, precision: 0.0186, recall: 0.0636, f1: 0.0287, edges-pos-ontonotes_loss: 0.2387
09/16 06:49:00 AM: Update 173: task edges-pos-ontonotes, batch 173 (173): mcc: -0.0017, acc: 0.0018, precision: 0.0195, recall: 0.0311, f1: 0.0240, edges-pos-ontonotes_loss: 0.1562
09/16 06:49:10 AM: Update 268: task edges-pos-ontonotes, batch 268 (268): mcc: 0.0013, acc: 0.0037, precision: 0.0221, recall: 0.0230, f1: 0.0225, edges-pos-ontonotes_loss: 0.1287
09/16 06:49:20 AM: Update 337: task edges-pos-ontonotes, batch 337 (337): mcc: 0.0057, acc: 0.0064, precision: 0.0271, recall: 0.0218, f1: 0.0241, edges-pos-ontonotes_loss: 0.1181
09/16 06:49:30 AM: Update 414: task edges-pos-ontonotes, batch 414 (414): mcc: 0.0107, acc: 0.0091, precision: 0.0340, recall: 0.0217, f1: 0.0265, edges-pos-ontonotes_loss: 0.1101
09/16 06:49:41 AM: Update 488: task edges-pos-ontonotes, batch 488 (488): mcc: 0.0205, acc: 0.0151, precision: 0.0483, recall: 0.0261, f1: 0.0339, edges-pos-ontonotes_loss: 0.1043
09/16 06:49:51 AM: Update 558: task edges-pos-ontonotes, batch 558 (558): mcc: 0.0299, acc: 0.0198, precision: 0.0637, recall: 0.0301, f1: 0.0409, edges-pos-ontonotes_loss: 0.1002
09/16 06:50:02 AM: Update 627: task edges-pos-ontonotes, batch 627 (627): mcc: 0.0405, acc: 0.0251, precision: 0.0818, recall: 0.0351, f1: 0.0491, edges-pos-ontonotes_loss: 0.0967
09/16 06:50:12 AM: Update 678: task edges-pos-ontonotes, batch 678 (678): mcc: 0.0488, acc: 0.0286, precision: 0.0978, recall: 0.0382, f1: 0.0549, edges-pos-ontonotes_loss: 0.0948
09/16 06:50:22 AM: Update 734: task edges-pos-ontonotes, batch 734 (734): mcc: 0.0564, acc: 0.0314, precision: 0.1136, recall: 0.0408, f1: 0.0601, edges-pos-ontonotes_loss: 0.0929
09/16 06:50:32 AM: Update 798: task edges-pos-ontonotes, batch 798 (798): mcc: 0.0650, acc: 0.0349, precision: 0.1314, recall: 0.0442, f1: 0.0662, edges-pos-ontonotes_loss: 0.0910
09/16 06:50:42 AM: Update 863: task edges-pos-ontonotes, batch 863 (863): mcc: 0.0737, acc: 0.0384, precision: 0.1497, recall: 0.0477, f1: 0.0723, edges-pos-ontonotes_loss: 0.0892
09/16 06:50:52 AM: Update 918: task edges-pos-ontonotes, batch 918 (918): mcc: 0.0819, acc: 0.0414, precision: 0.1675, recall: 0.0508, f1: 0.0780, edges-pos-ontonotes_loss: 0.0879
09/16 06:51:03 AM: Update 965: task edges-pos-ontonotes, batch 965 (965): mcc: 0.0882, acc: 0.0438, precision: 0.1816, recall: 0.0532, f1: 0.0823, edges-pos-ontonotes_loss: 0.0868
09/16 06:51:09 AM: ***** Step 1000 / Validation 1 *****
09/16 06:51:09 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:51:09 AM: Validating...
09/16 06:51:13 AM: Evaluate: task edges-pos-ontonotes, batch 25 (157): mcc: 0.2787, acc: 0.0991, precision: 0.7607, recall: 0.1054, f1: 0.1851, edges-pos-ontonotes_loss: 0.0654
09/16 06:51:23 AM: Evaluate: task edges-pos-ontonotes, batch 92 (157): mcc: 0.2977, acc: 0.1084, precision: 0.7966, recall: 0.1145, f1: 0.2003, edges-pos-ontonotes_loss: 0.0654
09/16 06:51:33 AM: Evaluate: task edges-pos-ontonotes, batch 139 (157): mcc: 0.2952, acc: 0.1096, precision: 0.7610, recall: 0.1181, f1: 0.2045, edges-pos-ontonotes_loss: 0.0646
09/16 06:51:36 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:51:36 AM: Best result seen so far for micro.
09/16 06:51:36 AM: Best result seen so far for macro.
09/16 06:51:36 AM: Updating LR scheduler:
09/16 06:51:36 AM: 	Best result seen so far for macro_avg: 0.204
09/16 06:51:36 AM: 	# validation passes without improvement: 0
09/16 06:51:36 AM: edges-pos-ontonotes_loss: training: 0.086128 validation: 0.064438
09/16 06:51:36 AM: macro_avg: validation: 0.204444
09/16 06:51:36 AM: micro_avg: validation: 0.000000
09/16 06:51:36 AM: edges-pos-ontonotes_mcc: training: 0.093637 validation: 0.294256
09/16 06:51:36 AM: edges-pos-ontonotes_acc: training: 0.045865 validation: 0.109485
09/16 06:51:36 AM: edges-pos-ontonotes_precision: training: 0.193587 validation: 0.756007
09/16 06:51:36 AM: edges-pos-ontonotes_recall: training: 0.055358 validation: 0.118205
09/16 06:51:36 AM: edges-pos-ontonotes_f1: training: 0.086096 validation: 0.204444
09/16 06:51:36 AM: Global learning rate: 0.0001
09/16 06:51:36 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 06:51:43 AM: Update 1037: task edges-pos-ontonotes, batch 37 (1037): mcc: 0.2650, acc: 0.0924, precision: 0.7083, recall: 0.1027, f1: 0.1794, edges-pos-ontonotes_loss: 0.0654
09/16 06:51:53 AM: Update 1091: task edges-pos-ontonotes, batch 91 (1091): mcc: 0.2743, acc: 0.1004, precision: 0.6941, recall: 0.1124, f1: 0.1935, edges-pos-ontonotes_loss: 0.0651
09/16 06:52:03 AM: Update 1147: task edges-pos-ontonotes, batch 147 (1147): mcc: 0.2816, acc: 0.1056, precision: 0.6969, recall: 0.1180, f1: 0.2018, edges-pos-ontonotes_loss: 0.0646
09/16 06:52:13 AM: Update 1204: task edges-pos-ontonotes, batch 204 (1204): mcc: 0.2855, acc: 0.1082, precision: 0.7007, recall: 0.1206, f1: 0.2058, edges-pos-ontonotes_loss: 0.0643
09/16 06:52:33 AM: Update 1253: task edges-pos-ontonotes, batch 253 (1253): mcc: 0.2905, acc: 0.1119, precision: 0.7021, recall: 0.1245, f1: 0.2115, edges-pos-ontonotes_loss: 0.0641
09/16 06:52:43 AM: Update 1311: task edges-pos-ontonotes, batch 311 (1311): mcc: 0.2948, acc: 0.1148, precision: 0.7066, recall: 0.1274, f1: 0.2159, edges-pos-ontonotes_loss: 0.0638
09/16 06:52:53 AM: Update 1369: task edges-pos-ontonotes, batch 369 (1369): mcc: 0.3005, acc: 0.1188, precision: 0.7116, recall: 0.1313, f1: 0.2217, edges-pos-ontonotes_loss: 0.0635
09/16 06:53:03 AM: Update 1427: task edges-pos-ontonotes, batch 427 (1427): mcc: 0.3068, acc: 0.1233, precision: 0.7157, recall: 0.1360, f1: 0.2286, edges-pos-ontonotes_loss: 0.0632
09/16 06:53:13 AM: Update 1481: task edges-pos-ontonotes, batch 481 (1481): mcc: 0.3120, acc: 0.1272, precision: 0.7181, recall: 0.1402, f1: 0.2346, edges-pos-ontonotes_loss: 0.0629
09/16 06:53:23 AM: Update 1536: task edges-pos-ontonotes, batch 536 (1536): mcc: 0.3170, acc: 0.1308, precision: 0.7215, recall: 0.1440, f1: 0.2401, edges-pos-ontonotes_loss: 0.0627
09/16 06:53:33 AM: Update 1586: task edges-pos-ontonotes, batch 586 (1586): mcc: 0.3212, acc: 0.1340, precision: 0.7245, recall: 0.1472, f1: 0.2447, edges-pos-ontonotes_loss: 0.0624
09/16 06:53:43 AM: Update 1641: task edges-pos-ontonotes, batch 641 (1641): mcc: 0.3262, acc: 0.1378, precision: 0.7272, recall: 0.1512, f1: 0.2503, edges-pos-ontonotes_loss: 0.0621
09/16 06:53:53 AM: Update 1699: task edges-pos-ontonotes, batch 699 (1699): mcc: 0.3316, acc: 0.1419, precision: 0.7308, recall: 0.1554, f1: 0.2563, edges-pos-ontonotes_loss: 0.0618
09/16 06:54:03 AM: Update 1753: task edges-pos-ontonotes, batch 753 (1753): mcc: 0.3358, acc: 0.1452, precision: 0.7325, recall: 0.1589, f1: 0.2612, edges-pos-ontonotes_loss: 0.0616
09/16 06:54:13 AM: Update 1812: task edges-pos-ontonotes, batch 812 (1812): mcc: 0.3401, acc: 0.1487, precision: 0.7347, recall: 0.1626, f1: 0.2662, edges-pos-ontonotes_loss: 0.0613
09/16 06:54:23 AM: Update 1866: task edges-pos-ontonotes, batch 866 (1866): mcc: 0.3443, acc: 0.1521, precision: 0.7367, recall: 0.1661, f1: 0.2711, edges-pos-ontonotes_loss: 0.0610
09/16 06:54:34 AM: Update 1908: task edges-pos-ontonotes, batch 908 (1908): mcc: 0.3452, acc: 0.1530, precision: 0.7357, recall: 0.1672, f1: 0.2724, edges-pos-ontonotes_loss: 0.0609
09/16 06:54:44 AM: Update 1977: task edges-pos-ontonotes, batch 977 (1977): mcc: 0.3482, acc: 0.1554, precision: 0.7372, recall: 0.1697, f1: 0.2759, edges-pos-ontonotes_loss: 0.0606
09/16 06:54:46 AM: ***** Step 2000 / Validation 2 *****
09/16 06:54:46 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:54:46 AM: Validating...
09/16 06:54:54 AM: Evaluate: task edges-pos-ontonotes, batch 50 (157): mcc: 0.4249, acc: 0.2097, precision: 0.8393, recall: 0.2203, f1: 0.3490, edges-pos-ontonotes_loss: 0.0578
09/16 06:55:04 AM: Evaluate: task edges-pos-ontonotes, batch 110 (157): mcc: 0.4578, acc: 0.2418, precision: 0.8439, recall: 0.2540, f1: 0.3905, edges-pos-ontonotes_loss: 0.0548
09/16 06:55:14 AM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.4574, acc: 0.2444, precision: 0.8263, recall: 0.2592, f1: 0.3946, edges-pos-ontonotes_loss: 0.0542
09/16 06:55:14 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:55:14 AM: Best result seen so far for macro.
09/16 06:55:14 AM: Updating LR scheduler:
09/16 06:55:14 AM: 	Best result seen so far for macro_avg: 0.395
09/16 06:55:14 AM: 	# validation passes without improvement: 0
09/16 06:55:14 AM: edges-pos-ontonotes_loss: training: 0.060493 validation: 0.054229
09/16 06:55:14 AM: macro_avg: validation: 0.394506
09/16 06:55:14 AM: micro_avg: validation: 0.000000
09/16 06:55:14 AM: edges-pos-ontonotes_mcc: training: 0.349446 validation: 0.457307
09/16 06:55:14 AM: edges-pos-ontonotes_acc: training: 0.156373 validation: 0.244283
09/16 06:55:14 AM: edges-pos-ontonotes_precision: training: 0.738303 validation: 0.826275
09/16 06:55:14 AM: edges-pos-ontonotes_recall: training: 0.170666 validation: 0.259109
09/16 06:55:14 AM: edges-pos-ontonotes_f1: training: 0.277245 validation: 0.394506
09/16 06:55:14 AM: Global learning rate: 0.0001
09/16 06:55:14 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 06:55:24 AM: Update 2072: task edges-pos-ontonotes, batch 72 (2072): mcc: 0.4131, acc: 0.2119, precision: 0.7617, recall: 0.2305, f1: 0.3539, edges-pos-ontonotes_loss: 0.0552
09/16 06:55:34 AM: Update 2138: task edges-pos-ontonotes, batch 138 (2138): mcc: 0.4084, acc: 0.2091, precision: 0.7526, recall: 0.2280, f1: 0.3500, edges-pos-ontonotes_loss: 0.0550
09/16 06:55:44 AM: Update 2199: task edges-pos-ontonotes, batch 199 (2199): mcc: 0.4098, acc: 0.2110, precision: 0.7519, recall: 0.2299, f1: 0.3521, edges-pos-ontonotes_loss: 0.0545
09/16 06:55:54 AM: Update 2295: task edges-pos-ontonotes, batch 295 (2295): mcc: 0.4286, acc: 0.2255, precision: 0.7764, recall: 0.2430, f1: 0.3701, edges-pos-ontonotes_loss: 0.0530
09/16 06:56:04 AM: Update 2390: task edges-pos-ontonotes, batch 390 (2390): mcc: 0.4451, acc: 0.2392, precision: 0.7930, recall: 0.2563, f1: 0.3874, edges-pos-ontonotes_loss: 0.0519
09/16 06:56:14 AM: Update 2485: task edges-pos-ontonotes, batch 485 (2485): mcc: 0.4582, acc: 0.2506, precision: 0.8041, recall: 0.2676, f1: 0.4015, edges-pos-ontonotes_loss: 0.0509
09/16 06:56:24 AM: Update 2580: task edges-pos-ontonotes, batch 580 (2580): mcc: 0.4669, acc: 0.2591, precision: 0.8092, recall: 0.2760, f1: 0.4116, edges-pos-ontonotes_loss: 0.0505
09/16 06:56:34 AM: Update 2687: task edges-pos-ontonotes, batch 687 (2687): mcc: 0.4776, acc: 0.2697, precision: 0.8144, recall: 0.2867, f1: 0.4241, edges-pos-ontonotes_loss: 0.0498
09/16 06:56:44 AM: Update 2798: task edges-pos-ontonotes, batch 798 (2798): mcc: 0.4887, acc: 0.2808, precision: 0.8200, recall: 0.2980, f1: 0.4372, edges-pos-ontonotes_loss: 0.0489
09/16 06:56:54 AM: Update 2914: task edges-pos-ontonotes, batch 914 (2914): mcc: 0.4875, acc: 0.2805, precision: 0.8160, recall: 0.2981, f1: 0.4366, edges-pos-ontonotes_loss: 0.0490
09/16 06:57:00 AM: ***** Step 3000 / Validation 3 *****
09/16 06:57:00 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:57:00 AM: Validating...
09/16 06:57:04 AM: Evaluate: task edges-pos-ontonotes, batch 31 (157): mcc: 0.5171, acc: 0.3130, precision: 0.8158, recall: 0.3351, f1: 0.4751, edges-pos-ontonotes_loss: 0.0478
09/16 06:57:14 AM: Evaluate: task edges-pos-ontonotes, batch 97 (157): mcc: 0.5466, acc: 0.3454, precision: 0.8270, recall: 0.3688, f1: 0.5101, edges-pos-ontonotes_loss: 0.0459
09/16 06:57:24 AM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.5276, acc: 0.3271, precision: 0.7973, recall: 0.3571, f1: 0.4933, edges-pos-ontonotes_loss: 0.0476
09/16 06:57:27 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:57:27 AM: Best result seen so far for macro.
09/16 06:57:27 AM: Updating LR scheduler:
09/16 06:57:27 AM: 	Best result seen so far for macro_avg: 0.496
09/16 06:57:27 AM: 	# validation passes without improvement: 0
09/16 06:57:27 AM: edges-pos-ontonotes_loss: training: 0.048818 validation: 0.047773
09/16 06:57:27 AM: macro_avg: validation: 0.495546
09/16 06:57:27 AM: micro_avg: validation: 0.000000
09/16 06:57:27 AM: edges-pos-ontonotes_mcc: training: 0.487507 validation: 0.529510
09/16 06:57:27 AM: edges-pos-ontonotes_acc: training: 0.280778 validation: 0.328688
09/16 06:57:27 AM: edges-pos-ontonotes_precision: training: 0.815681 validation: 0.797909
09/16 06:57:27 AM: edges-pos-ontonotes_recall: training: 0.298161 validation: 0.359366
09/16 06:57:27 AM: edges-pos-ontonotes_f1: training: 0.436694 validation: 0.495546
09/16 06:57:27 AM: Global learning rate: 0.0001
09/16 06:57:27 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 06:57:34 AM: Update 3095: task edges-pos-ontonotes, batch 95 (3095): mcc: 0.4773, acc: 0.2811, precision: 0.7831, recall: 0.2982, f1: 0.4319, edges-pos-ontonotes_loss: 0.0478
09/16 06:57:44 AM: Update 3159: task edges-pos-ontonotes, batch 159 (3159): mcc: 0.4466, acc: 0.2558, precision: 0.7378, recall: 0.2781, f1: 0.4039, edges-pos-ontonotes_loss: 0.0490
09/16 06:57:54 AM: Update 3215: task edges-pos-ontonotes, batch 215 (3215): mcc: 0.4361, acc: 0.2457, precision: 0.7248, recall: 0.2703, f1: 0.3937, edges-pos-ontonotes_loss: 0.0504
09/16 06:58:05 AM: Update 3274: task edges-pos-ontonotes, batch 274 (3274): mcc: 0.4342, acc: 0.2425, precision: 0.7245, recall: 0.2680, f1: 0.3913, edges-pos-ontonotes_loss: 0.0514
09/16 06:58:15 AM: Update 3337: task edges-pos-ontonotes, batch 337 (3337): mcc: 0.4348, acc: 0.2425, precision: 0.7267, recall: 0.2679, f1: 0.3915, edges-pos-ontonotes_loss: 0.0518
09/16 06:58:25 AM: Update 3407: task edges-pos-ontonotes, batch 407 (3407): mcc: 0.4386, acc: 0.2456, precision: 0.7315, recall: 0.2708, f1: 0.3952, edges-pos-ontonotes_loss: 0.0518
09/16 06:58:42 AM: Update 3461: task edges-pos-ontonotes, batch 461 (3461): mcc: 0.4399, acc: 0.2468, precision: 0.7328, recall: 0.2719, f1: 0.3966, edges-pos-ontonotes_loss: 0.0519
09/16 06:58:52 AM: Update 3561: task edges-pos-ontonotes, batch 561 (3561): mcc: 0.4482, acc: 0.2538, precision: 0.7443, recall: 0.2776, f1: 0.4044, edges-pos-ontonotes_loss: 0.0508
09/16 06:59:02 AM: Update 3657: task edges-pos-ontonotes, batch 657 (3657): mcc: 0.4559, acc: 0.2604, precision: 0.7533, recall: 0.2835, f1: 0.4120, edges-pos-ontonotes_loss: 0.0498
09/16 06:59:12 AM: Update 3743: task edges-pos-ontonotes, batch 743 (3743): mcc: 0.4618, acc: 0.2657, precision: 0.7590, recall: 0.2885, f1: 0.4181, edges-pos-ontonotes_loss: 0.0491
09/16 06:59:23 AM: Update 3812: task edges-pos-ontonotes, batch 812 (3812): mcc: 0.4659, acc: 0.2699, precision: 0.7595, recall: 0.2935, f1: 0.4234, edges-pos-ontonotes_loss: 0.0487
09/16 06:59:33 AM: Update 3881: task edges-pos-ontonotes, batch 881 (3881): mcc: 0.4714, acc: 0.2753, precision: 0.7608, recall: 0.2998, f1: 0.4301, edges-pos-ontonotes_loss: 0.0484
09/16 06:59:43 AM: Update 3959: task edges-pos-ontonotes, batch 959 (3959): mcc: 0.4768, acc: 0.2807, precision: 0.7628, recall: 0.3058, f1: 0.4366, edges-pos-ontonotes_loss: 0.0480
09/16 06:59:48 AM: ***** Step 4000 / Validation 4 *****
09/16 06:59:48 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:59:48 AM: Validating...
09/16 06:59:53 AM: Evaluate: task edges-pos-ontonotes, batch 37 (157): mcc: 0.6025, acc: 0.4019, precision: 0.8808, recall: 0.4191, f1: 0.5680, edges-pos-ontonotes_loss: 0.0406
09/16 07:00:03 AM: Evaluate: task edges-pos-ontonotes, batch 101 (157): mcc: 0.6116, acc: 0.4124, precision: 0.8810, recall: 0.4317, f1: 0.5794, edges-pos-ontonotes_loss: 0.0397
09/16 07:00:13 AM: Evaluate: task edges-pos-ontonotes, batch 148 (157): mcc: 0.5859, acc: 0.3842, precision: 0.8618, recall: 0.4057, f1: 0.5516, edges-pos-ontonotes_loss: 0.0416
09/16 07:00:15 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:00:15 AM: Best result seen so far for macro.
09/16 07:00:15 AM: Updating LR scheduler:
09/16 07:00:15 AM: 	Best result seen so far for macro_avg: 0.552
09/16 07:00:15 AM: 	# validation passes without improvement: 0
09/16 07:00:15 AM: edges-pos-ontonotes_loss: training: 0.047775 validation: 0.041666
09/16 07:00:15 AM: macro_avg: validation: 0.551511
09/16 07:00:15 AM: micro_avg: validation: 0.000000
09/16 07:00:15 AM: edges-pos-ontonotes_mcc: training: 0.479659 validation: 0.585780
09/16 07:00:15 AM: edges-pos-ontonotes_acc: training: 0.283480 validation: 0.384171
09/16 07:00:15 AM: edges-pos-ontonotes_precision: training: 0.764420 validation: 0.861436
09/16 07:00:15 AM: edges-pos-ontonotes_recall: training: 0.308830 validation: 0.405590
09/16 07:00:15 AM: edges-pos-ontonotes_f1: training: 0.439928 validation: 0.551511
09/16 07:00:15 AM: Global learning rate: 0.0001
09/16 07:00:15 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:00:23 AM: Update 4062: task edges-pos-ontonotes, batch 62 (4062): mcc: 0.5370, acc: 0.3438, precision: 0.7801, recall: 0.3782, f1: 0.5095, edges-pos-ontonotes_loss: 0.0446
09/16 07:00:33 AM: Update 4113: task edges-pos-ontonotes, batch 113 (4113): mcc: 0.5107, acc: 0.3208, precision: 0.7566, recall: 0.3535, f1: 0.4819, edges-pos-ontonotes_loss: 0.0462
09/16 07:00:43 AM: Update 4171: task edges-pos-ontonotes, batch 171 (4171): mcc: 0.4988, acc: 0.3085, precision: 0.7504, recall: 0.3403, f1: 0.4682, edges-pos-ontonotes_loss: 0.0477
09/16 07:00:53 AM: Update 4231: task edges-pos-ontonotes, batch 231 (4231): mcc: 0.4959, acc: 0.3061, precision: 0.7492, recall: 0.3368, f1: 0.4647, edges-pos-ontonotes_loss: 0.0482
09/16 07:01:03 AM: Update 4289: task edges-pos-ontonotes, batch 289 (4289): mcc: 0.4919, acc: 0.3030, precision: 0.7442, recall: 0.3339, f1: 0.4609, edges-pos-ontonotes_loss: 0.0486
09/16 07:01:14 AM: Update 4346: task edges-pos-ontonotes, batch 346 (4346): mcc: 0.4893, acc: 0.3003, precision: 0.7432, recall: 0.3308, f1: 0.4579, edges-pos-ontonotes_loss: 0.0489
09/16 07:01:25 AM: Update 4400: task edges-pos-ontonotes, batch 400 (4400): mcc: 0.4889, acc: 0.2996, precision: 0.7437, recall: 0.3300, f1: 0.4571, edges-pos-ontonotes_loss: 0.0490
09/16 07:01:35 AM: Update 4456: task edges-pos-ontonotes, batch 456 (4456): mcc: 0.4879, acc: 0.2987, precision: 0.7434, recall: 0.3288, f1: 0.4559, edges-pos-ontonotes_loss: 0.0491
09/16 07:01:45 AM: Update 4513: task edges-pos-ontonotes, batch 513 (4513): mcc: 0.4890, acc: 0.2996, precision: 0.7448, recall: 0.3297, f1: 0.4570, edges-pos-ontonotes_loss: 0.0490
09/16 07:01:55 AM: Update 4564: task edges-pos-ontonotes, batch 564 (4564): mcc: 0.4871, acc: 0.2978, precision: 0.7432, recall: 0.3278, f1: 0.4549, edges-pos-ontonotes_loss: 0.0491
09/16 07:02:05 AM: Update 4621: task edges-pos-ontonotes, batch 621 (4621): mcc: 0.4885, acc: 0.2990, precision: 0.7450, recall: 0.3289, f1: 0.4563, edges-pos-ontonotes_loss: 0.0490
09/16 07:02:15 AM: Update 4674: task edges-pos-ontonotes, batch 674 (4674): mcc: 0.4899, acc: 0.3001, precision: 0.7466, recall: 0.3300, f1: 0.4577, edges-pos-ontonotes_loss: 0.0490
09/16 07:02:25 AM: Update 4720: task edges-pos-ontonotes, batch 720 (4720): mcc: 0.4903, acc: 0.3005, precision: 0.7470, recall: 0.3304, f1: 0.4581, edges-pos-ontonotes_loss: 0.0489
09/16 07:02:35 AM: Update 4778: task edges-pos-ontonotes, batch 778 (4778): mcc: 0.4919, acc: 0.3019, precision: 0.7486, recall: 0.3317, f1: 0.4597, edges-pos-ontonotes_loss: 0.0488
09/16 07:02:45 AM: Update 4834: task edges-pos-ontonotes, batch 834 (4834): mcc: 0.4931, acc: 0.3031, precision: 0.7496, recall: 0.3328, f1: 0.4610, edges-pos-ontonotes_loss: 0.0488
09/16 07:02:56 AM: Update 4892: task edges-pos-ontonotes, batch 892 (4892): mcc: 0.4940, acc: 0.3041, precision: 0.7505, recall: 0.3338, f1: 0.4621, edges-pos-ontonotes_loss: 0.0487
09/16 07:03:06 AM: Update 4950: task edges-pos-ontonotes, batch 950 (4950): mcc: 0.4950, acc: 0.3050, precision: 0.7510, recall: 0.3348, f1: 0.4631, edges-pos-ontonotes_loss: 0.0486
09/16 07:03:15 AM: ***** Step 5000 / Validation 5 *****
09/16 07:03:15 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:03:15 AM: Validating...
09/16 07:03:16 AM: Evaluate: task edges-pos-ontonotes, batch 2 (157): mcc: 0.6472, acc: 0.4542, precision: 0.8949, recall: 0.4752, f1: 0.6207, edges-pos-ontonotes_loss: 0.0359
09/16 07:03:26 AM: Evaluate: task edges-pos-ontonotes, batch 73 (157): mcc: 0.6275, acc: 0.4284, precision: 0.8967, recall: 0.4460, f1: 0.5957, edges-pos-ontonotes_loss: 0.0393
09/16 07:03:36 AM: Evaluate: task edges-pos-ontonotes, batch 125 (157): mcc: 0.6141, acc: 0.4139, precision: 0.8861, recall: 0.4327, f1: 0.5814, edges-pos-ontonotes_loss: 0.0394
09/16 07:03:43 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:03:43 AM: Best result seen so far for macro.
09/16 07:03:43 AM: Updating LR scheduler:
09/16 07:03:43 AM: 	Best result seen so far for macro_avg: 0.574
09/16 07:03:43 AM: 	# validation passes without improvement: 0
09/16 07:03:43 AM: edges-pos-ontonotes_loss: training: 0.048634 validation: 0.039694
09/16 07:03:43 AM: macro_avg: validation: 0.573746
09/16 07:03:43 AM: micro_avg: validation: 0.000000
09/16 07:03:43 AM: edges-pos-ontonotes_mcc: training: 0.495744 validation: 0.607533
09/16 07:03:43 AM: edges-pos-ontonotes_acc: training: 0.305914 validation: 0.405939
09/16 07:03:43 AM: edges-pos-ontonotes_precision: training: 0.751442 validation: 0.883322
09/16 07:03:43 AM: edges-pos-ontonotes_recall: training: 0.335611 validation: 0.424849
09/16 07:03:43 AM: edges-pos-ontonotes_f1: training: 0.463993 validation: 0.573746
09/16 07:03:43 AM: Global learning rate: 0.0001
09/16 07:03:43 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:03:46 AM: Update 5021: task edges-pos-ontonotes, batch 21 (5021): mcc: 0.5324, acc: 0.3422, precision: 0.7802, recall: 0.3718, f1: 0.5036, edges-pos-ontonotes_loss: 0.0455
09/16 07:03:57 AM: Update 5026: task edges-pos-ontonotes, batch 26 (5026): mcc: 0.5225, acc: 0.3313, precision: 0.7740, recall: 0.3612, f1: 0.4926, edges-pos-ontonotes_loss: 0.0464
09/16 07:04:07 AM: Update 5088: task edges-pos-ontonotes, batch 88 (5088): mcc: 0.5216, acc: 0.3307, precision: 0.7714, recall: 0.3612, f1: 0.4920, edges-pos-ontonotes_loss: 0.0467
09/16 07:04:17 AM: Update 5142: task edges-pos-ontonotes, batch 142 (5142): mcc: 0.5177, acc: 0.3275, precision: 0.7673, recall: 0.3578, f1: 0.4880, edges-pos-ontonotes_loss: 0.0471
09/16 07:04:27 AM: Update 5202: task edges-pos-ontonotes, batch 202 (5202): mcc: 0.5199, acc: 0.3300, precision: 0.7687, recall: 0.3602, f1: 0.4905, edges-pos-ontonotes_loss: 0.0468
09/16 07:04:37 AM: Update 5259: task edges-pos-ontonotes, batch 259 (5259): mcc: 0.5210, acc: 0.3314, precision: 0.7694, recall: 0.3614, f1: 0.4918, edges-pos-ontonotes_loss: 0.0469
09/16 07:04:47 AM: Update 5313: task edges-pos-ontonotes, batch 313 (5313): mcc: 0.5201, acc: 0.3305, precision: 0.7685, recall: 0.3606, f1: 0.4909, edges-pos-ontonotes_loss: 0.0469
09/16 07:04:57 AM: Update 5352: task edges-pos-ontonotes, batch 352 (5352): mcc: 0.5148, acc: 0.3264, precision: 0.7619, recall: 0.3565, f1: 0.4858, edges-pos-ontonotes_loss: 0.0471
09/16 07:05:07 AM: Update 5421: task edges-pos-ontonotes, batch 421 (5421): mcc: 0.5171, acc: 0.3287, precision: 0.7629, recall: 0.3592, f1: 0.4885, edges-pos-ontonotes_loss: 0.0464
09/16 07:05:17 AM: Update 5494: task edges-pos-ontonotes, batch 494 (5494): mcc: 0.5213, acc: 0.3328, precision: 0.7653, recall: 0.3638, f1: 0.4931, edges-pos-ontonotes_loss: 0.0457
09/16 07:05:28 AM: Update 5571: task edges-pos-ontonotes, batch 571 (5571): mcc: 0.5239, acc: 0.3356, precision: 0.7663, recall: 0.3669, f1: 0.4963, edges-pos-ontonotes_loss: 0.0452
09/16 07:05:38 AM: Update 5630: task edges-pos-ontonotes, batch 630 (5630): mcc: 0.5225, acc: 0.3345, precision: 0.7640, recall: 0.3661, f1: 0.4950, edges-pos-ontonotes_loss: 0.0451
09/16 07:05:48 AM: Update 5700: task edges-pos-ontonotes, batch 700 (5700): mcc: 0.5280, acc: 0.3398, precision: 0.7680, recall: 0.3718, f1: 0.5010, edges-pos-ontonotes_loss: 0.0445
09/16 07:05:58 AM: Update 5797: task edges-pos-ontonotes, batch 797 (5797): mcc: 0.5377, acc: 0.3493, precision: 0.7758, recall: 0.3814, f1: 0.5114, edges-pos-ontonotes_loss: 0.0434
09/16 07:06:08 AM: Update 5886: task edges-pos-ontonotes, batch 886 (5886): mcc: 0.5460, acc: 0.3574, precision: 0.7823, recall: 0.3898, f1: 0.5203, edges-pos-ontonotes_loss: 0.0426
09/16 07:06:18 AM: Update 5966: task edges-pos-ontonotes, batch 966 (5966): mcc: 0.5524, acc: 0.3640, precision: 0.7870, recall: 0.3964, f1: 0.5272, edges-pos-ontonotes_loss: 0.0420
09/16 07:06:21 AM: ***** Step 6000 / Validation 6 *****
09/16 07:06:21 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:06:21 AM: Validating...
09/16 07:06:28 AM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.6151, acc: 0.4151, precision: 0.9014, recall: 0.4264, f1: 0.5789, edges-pos-ontonotes_loss: 0.0387
09/16 07:06:38 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.6312, acc: 0.4351, precision: 0.9043, recall: 0.4473, f1: 0.5986, edges-pos-ontonotes_loss: 0.0370
09/16 07:06:48 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.6125, acc: 0.4123, precision: 0.8974, recall: 0.4249, f1: 0.5767, edges-pos-ontonotes_loss: 0.0384
09/16 07:06:48 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:06:48 AM: Best result seen so far for macro.
09/16 07:06:48 AM: Updating LR scheduler:
09/16 07:06:48 AM: 	Best result seen so far for macro_avg: 0.577
09/16 07:06:48 AM: 	# validation passes without improvement: 0
09/16 07:06:48 AM: edges-pos-ontonotes_loss: training: 0.041863 validation: 0.038425
09/16 07:06:48 AM: macro_avg: validation: 0.577172
09/16 07:06:48 AM: micro_avg: validation: 0.000000
09/16 07:06:48 AM: edges-pos-ontonotes_mcc: training: 0.554505 validation: 0.612935
09/16 07:06:48 AM: edges-pos-ontonotes_acc: training: 0.366166 validation: 0.412690
09/16 07:06:48 AM: edges-pos-ontonotes_precision: training: 0.788563 validation: 0.897452
09/16 07:06:48 AM: edges-pos-ontonotes_recall: training: 0.398556 validation: 0.425368
09/16 07:06:48 AM: edges-pos-ontonotes_f1: training: 0.529495 validation: 0.577172
09/16 07:06:48 AM: Global learning rate: 0.0001
09/16 07:06:48 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:06:58 AM: Update 6113: task edges-pos-ontonotes, batch 113 (6113): mcc: 0.6553, acc: 0.4812, precision: 0.8502, recall: 0.5131, f1: 0.6400, edges-pos-ontonotes_loss: 0.0356
09/16 07:07:08 AM: Update 6214: task edges-pos-ontonotes, batch 214 (6214): mcc: 0.6580, acc: 0.4857, precision: 0.8483, recall: 0.5185, f1: 0.6436, edges-pos-ontonotes_loss: 0.0353
09/16 07:07:18 AM: Update 6320: task edges-pos-ontonotes, batch 320 (6320): mcc: 0.6452, acc: 0.4714, precision: 0.8384, recall: 0.5049, f1: 0.6302, edges-pos-ontonotes_loss: 0.0365
09/16 07:07:28 AM: Update 6448: task edges-pos-ontonotes, batch 448 (6448): mcc: 0.6224, acc: 0.4445, precision: 0.8265, recall: 0.4771, f1: 0.6050, edges-pos-ontonotes_loss: 0.0380
09/16 07:07:40 AM: Update 6591: task edges-pos-ontonotes, batch 591 (6591): mcc: 0.6095, acc: 0.4308, precision: 0.8178, recall: 0.4628, f1: 0.5911, edges-pos-ontonotes_loss: 0.0388
09/16 07:07:50 AM: Update 6662: task edges-pos-ontonotes, batch 662 (6662): mcc: 0.5915, acc: 0.4109, precision: 0.8032, recall: 0.4443, f1: 0.5722, edges-pos-ontonotes_loss: 0.0398
09/16 07:08:00 AM: Update 6719: task edges-pos-ontonotes, batch 719 (6719): mcc: 0.5746, acc: 0.3924, precision: 0.7910, recall: 0.4263, f1: 0.5540, edges-pos-ontonotes_loss: 0.0406
09/16 07:08:10 AM: Update 6775: task edges-pos-ontonotes, batch 775 (6775): mcc: 0.5633, acc: 0.3801, precision: 0.7829, recall: 0.4142, f1: 0.5417, edges-pos-ontonotes_loss: 0.0411
09/16 07:08:20 AM: Update 6835: task edges-pos-ontonotes, batch 835 (6835): mcc: 0.5559, acc: 0.3721, precision: 0.7774, recall: 0.4065, f1: 0.5338, edges-pos-ontonotes_loss: 0.0417
09/16 07:08:30 AM: Update 6900: task edges-pos-ontonotes, batch 900 (6900): mcc: 0.5521, acc: 0.3677, precision: 0.7751, recall: 0.4022, f1: 0.5296, edges-pos-ontonotes_loss: 0.0420
09/16 07:08:41 AM: Update 6969: task edges-pos-ontonotes, batch 969 (6969): mcc: 0.5505, acc: 0.3659, precision: 0.7750, recall: 0.4000, f1: 0.5277, edges-pos-ontonotes_loss: 0.0421
09/16 07:08:44 AM: ***** Step 7000 / Validation 7 *****
09/16 07:08:44 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:08:44 AM: Validating...
09/16 07:08:51 AM: Evaluate: task edges-pos-ontonotes, batch 47 (157): mcc: 0.6481, acc: 0.4527, precision: 0.9130, recall: 0.4668, f1: 0.6177, edges-pos-ontonotes_loss: 0.0353
09/16 07:09:01 AM: Evaluate: task edges-pos-ontonotes, batch 107 (157): mcc: 0.6486, acc: 0.4546, precision: 0.9074, recall: 0.4704, f1: 0.6196, edges-pos-ontonotes_loss: 0.0345
09/16 07:09:11 AM: Evaluate: task edges-pos-ontonotes, batch 154 (157): mcc: 0.6209, acc: 0.4208, precision: 0.8941, recall: 0.4380, f1: 0.5880, edges-pos-ontonotes_loss: 0.0364
09/16 07:09:11 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:09:11 AM: Best result seen so far for macro.
09/16 07:09:11 AM: Updating LR scheduler:
09/16 07:09:11 AM: 	Best result seen so far for macro_avg: 0.588
09/16 07:09:11 AM: 	# validation passes without improvement: 0
09/16 07:09:11 AM: edges-pos-ontonotes_loss: training: 0.042059 validation: 0.036401
09/16 07:09:11 AM: macro_avg: validation: 0.588050
09/16 07:09:11 AM: micro_avg: validation: 0.000000
09/16 07:09:11 AM: edges-pos-ontonotes_mcc: training: 0.550283 validation: 0.620915
09/16 07:09:11 AM: edges-pos-ontonotes_acc: training: 0.365683 validation: 0.420722
09/16 07:09:11 AM: edges-pos-ontonotes_precision: training: 0.775136 validation: 0.894070
09/16 07:09:11 AM: edges-pos-ontonotes_recall: training: 0.399601 validation: 0.438099
09/16 07:09:11 AM: edges-pos-ontonotes_f1: training: 0.527344 validation: 0.588050
09/16 07:09:11 AM: Global learning rate: 0.0001
09/16 07:09:11 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:09:21 AM: Update 7091: task edges-pos-ontonotes, batch 91 (7091): mcc: 0.5689, acc: 0.3810, precision: 0.8083, recall: 0.4087, f1: 0.5429, edges-pos-ontonotes_loss: 0.0383
09/16 07:09:31 AM: Update 7183: task edges-pos-ontonotes, batch 183 (7183): mcc: 0.5735, acc: 0.3863, precision: 0.8101, recall: 0.4143, f1: 0.5483, edges-pos-ontonotes_loss: 0.0386
09/16 07:09:41 AM: Update 7254: task edges-pos-ontonotes, batch 254 (7254): mcc: 0.5711, acc: 0.3849, precision: 0.8027, recall: 0.4148, f1: 0.5470, edges-pos-ontonotes_loss: 0.0389
09/16 07:09:51 AM: Update 7326: task edges-pos-ontonotes, batch 326 (7326): mcc: 0.5693, acc: 0.3841, precision: 0.7933, recall: 0.4173, f1: 0.5469, edges-pos-ontonotes_loss: 0.0393
09/16 07:10:01 AM: Update 7403: task edges-pos-ontonotes, batch 403 (7403): mcc: 0.5723, acc: 0.3877, precision: 0.7912, recall: 0.4229, f1: 0.5511, edges-pos-ontonotes_loss: 0.0393
09/16 07:10:11 AM: Update 7474: task edges-pos-ontonotes, batch 474 (7474): mcc: 0.5721, acc: 0.3877, precision: 0.7881, recall: 0.4241, f1: 0.5515, edges-pos-ontonotes_loss: 0.0395
09/16 07:10:30 AM: Update 7547: task edges-pos-ontonotes, batch 547 (7547): mcc: 0.5735, acc: 0.3897, precision: 0.7870, recall: 0.4269, f1: 0.5535, edges-pos-ontonotes_loss: 0.0394
09/16 07:10:40 AM: Update 7600: task edges-pos-ontonotes, batch 600 (7600): mcc: 0.5638, acc: 0.3803, precision: 0.7781, recall: 0.4176, f1: 0.5435, edges-pos-ontonotes_loss: 0.0402
09/16 07:10:51 AM: Update 7658: task edges-pos-ontonotes, batch 658 (7658): mcc: 0.5577, acc: 0.3746, precision: 0.7723, recall: 0.4119, f1: 0.5373, edges-pos-ontonotes_loss: 0.0408
09/16 07:11:01 AM: Update 7720: task edges-pos-ontonotes, batch 720 (7720): mcc: 0.5541, acc: 0.3708, precision: 0.7689, recall: 0.4085, f1: 0.5335, edges-pos-ontonotes_loss: 0.0413
09/16 07:11:11 AM: Update 7787: task edges-pos-ontonotes, batch 787 (7787): mcc: 0.5529, acc: 0.3696, precision: 0.7676, recall: 0.4074, f1: 0.5323, edges-pos-ontonotes_loss: 0.0417
09/16 07:11:21 AM: Update 7841: task edges-pos-ontonotes, batch 841 (7841): mcc: 0.5494, acc: 0.3660, precision: 0.7646, recall: 0.4040, f1: 0.5287, edges-pos-ontonotes_loss: 0.0420
09/16 07:11:31 AM: Update 7884: task edges-pos-ontonotes, batch 884 (7884): mcc: 0.5473, acc: 0.3639, precision: 0.7625, recall: 0.4020, f1: 0.5265, edges-pos-ontonotes_loss: 0.0422
09/16 07:11:41 AM: Update 7940: task edges-pos-ontonotes, batch 940 (7940): mcc: 0.5462, acc: 0.3628, precision: 0.7615, recall: 0.4010, f1: 0.5254, edges-pos-ontonotes_loss: 0.0425
09/16 07:11:51 AM: Update 7993: task edges-pos-ontonotes, batch 993 (7993): mcc: 0.5451, acc: 0.3618, precision: 0.7605, recall: 0.4000, f1: 0.5243, edges-pos-ontonotes_loss: 0.0427
09/16 07:11:52 AM: ***** Step 8000 / Validation 8 *****
09/16 07:11:52 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:11:52 AM: Validating...
09/16 07:12:01 AM: Evaluate: task edges-pos-ontonotes, batch 64 (157): mcc: 0.6638, acc: 0.4745, precision: 0.9112, recall: 0.4903, f1: 0.6376, edges-pos-ontonotes_loss: 0.0347
09/16 07:12:11 AM: Evaluate: task edges-pos-ontonotes, batch 118 (157): mcc: 0.6577, acc: 0.4683, precision: 0.9027, recall: 0.4861, f1: 0.6319, edges-pos-ontonotes_loss: 0.0345
09/16 07:12:19 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:12:19 AM: Best result seen so far for macro.
09/16 07:12:19 AM: Updating LR scheduler:
09/16 07:12:19 AM: 	Best result seen so far for macro_avg: 0.617
09/16 07:12:19 AM: 	# validation passes without improvement: 0
09/16 07:12:19 AM: edges-pos-ontonotes_loss: training: 0.042661 validation: 0.035330
09/16 07:12:19 AM: macro_avg: validation: 0.616583
09/16 07:12:19 AM: micro_avg: validation: 0.000000
09/16 07:12:19 AM: edges-pos-ontonotes_mcc: training: 0.545129 validation: 0.644356
09/16 07:12:19 AM: edges-pos-ontonotes_acc: training: 0.361864 validation: 0.451147
09/16 07:12:19 AM: edges-pos-ontonotes_precision: training: 0.760448 validation: 0.897448
09/16 07:12:19 AM: edges-pos-ontonotes_recall: training: 0.400062 validation: 0.469613
09/16 07:12:19 AM: edges-pos-ontonotes_f1: training: 0.524297 validation: 0.616583
09/16 07:12:19 AM: Global learning rate: 0.0001
09/16 07:12:19 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:12:21 AM: Update 8010: task edges-pos-ontonotes, batch 10 (8010): mcc: 0.5316, acc: 0.3474, precision: 0.7491, recall: 0.3866, f1: 0.5100, edges-pos-ontonotes_loss: 0.0458
09/16 07:12:31 AM: Update 8069: task edges-pos-ontonotes, batch 69 (8069): mcc: 0.5420, acc: 0.3575, precision: 0.7597, recall: 0.3960, f1: 0.5206, edges-pos-ontonotes_loss: 0.0449
09/16 07:12:42 AM: Update 8121: task edges-pos-ontonotes, batch 121 (8121): mcc: 0.5356, acc: 0.3517, precision: 0.7538, recall: 0.3898, f1: 0.5139, edges-pos-ontonotes_loss: 0.0454
09/16 07:12:53 AM: Update 8173: task edges-pos-ontonotes, batch 173 (8173): mcc: 0.5355, acc: 0.3520, precision: 0.7526, recall: 0.3903, f1: 0.5141, edges-pos-ontonotes_loss: 0.0454
09/16 07:13:03 AM: Update 8227: task edges-pos-ontonotes, batch 227 (8227): mcc: 0.5332, acc: 0.3498, precision: 0.7519, recall: 0.3874, f1: 0.5113, edges-pos-ontonotes_loss: 0.0457
09/16 07:13:13 AM: Update 8283: task edges-pos-ontonotes, batch 283 (8283): mcc: 0.5350, acc: 0.3517, precision: 0.7533, recall: 0.3893, f1: 0.5133, edges-pos-ontonotes_loss: 0.0455
09/16 07:13:23 AM: Update 8343: task edges-pos-ontonotes, batch 343 (8343): mcc: 0.5366, acc: 0.3530, precision: 0.7551, recall: 0.3906, f1: 0.5149, edges-pos-ontonotes_loss: 0.0454
09/16 07:13:33 AM: Update 8401: task edges-pos-ontonotes, batch 401 (8401): mcc: 0.5386, acc: 0.3550, precision: 0.7566, recall: 0.3928, f1: 0.5171, edges-pos-ontonotes_loss: 0.0452
09/16 07:13:43 AM: Update 8459: task edges-pos-ontonotes, batch 459 (8459): mcc: 0.5403, acc: 0.3567, precision: 0.7579, recall: 0.3945, f1: 0.5189, edges-pos-ontonotes_loss: 0.0452
09/16 07:13:53 AM: Update 8503: task edges-pos-ontonotes, batch 503 (8503): mcc: 0.5400, acc: 0.3564, precision: 0.7572, recall: 0.3944, f1: 0.5186, edges-pos-ontonotes_loss: 0.0452
09/16 07:14:03 AM: Update 8557: task edges-pos-ontonotes, batch 557 (8557): mcc: 0.5400, acc: 0.3563, precision: 0.7572, recall: 0.3943, f1: 0.5186, edges-pos-ontonotes_loss: 0.0452
09/16 07:14:13 AM: Update 8614: task edges-pos-ontonotes, batch 614 (8614): mcc: 0.5415, acc: 0.3580, precision: 0.7583, recall: 0.3960, f1: 0.5203, edges-pos-ontonotes_loss: 0.0452
09/16 07:14:23 AM: Update 8671: task edges-pos-ontonotes, batch 671 (8671): mcc: 0.5429, acc: 0.3593, precision: 0.7594, recall: 0.3974, f1: 0.5218, edges-pos-ontonotes_loss: 0.0450
09/16 07:14:33 AM: Update 8732: task edges-pos-ontonotes, batch 732 (8732): mcc: 0.5443, acc: 0.3608, precision: 0.7606, recall: 0.3988, f1: 0.5233, edges-pos-ontonotes_loss: 0.0449
09/16 07:14:43 AM: Update 8787: task edges-pos-ontonotes, batch 787 (8787): mcc: 0.5435, acc: 0.3600, precision: 0.7598, recall: 0.3980, f1: 0.5224, edges-pos-ontonotes_loss: 0.0449
