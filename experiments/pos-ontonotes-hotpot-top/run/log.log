09/16 06:46:18 AM: Git branch: master
09/16 06:46:18 AM: Git SHA: fb3796f035a61c062bc75b422b0939a7eeec20ff
09/16 06:46:18 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/pos-ontonotes-hotpot-top/",
  "exp_name": "experiments/pos-ontonotes-hotpot-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/pos-ontonotes-hotpot-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/hotpot",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/pos-ontonotes-hotpot-top__run",
  "run_dir": "./experiments/pos-ontonotes-hotpot-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-pos-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 06:46:18 AM: Saved config to ./experiments/pos-ontonotes-hotpot-top/run/params.conf
09/16 06:46:18 AM: Using random seed 1234
09/16 06:46:19 AM: Using GPU 0
09/16 06:46:19 AM: Loading tasks...
09/16 06:46:19 AM: Writing pre-preprocessed tasks to ./experiments/pos-ontonotes-hotpot-top/
09/16 06:46:19 AM: 	Creating task edges-pos-ontonotes from scratch.
09/16 06:46:34 AM: Read=110514, Skip=5298, Total=115812 from ./probing_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased
09/16 06:46:35 AM: Read=15060, Skip=620, Total=15680 from ./probing_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased
09/16 06:46:38 AM: Read=11462, Skip=755, Total=12217 from ./probing_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased
09/16 06:46:48 AM: 	Task 'edges-pos-ontonotes': |train|=110514 |val|=15060 |test|=11462
09/16 06:46:48 AM: 	Finished loading tasks: edges-pos-ontonotes.
09/16 06:46:48 AM: 	Building vocab from scratch.
09/16 06:46:48 AM: 	Counting units for task edges-pos-ontonotes.
09/16 06:46:50 AM: 	Task 'edges-pos-ontonotes': adding vocab namespace 'edges-pos-ontonotes_labels'
09/16 06:46:51 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:46:51 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 06:46:51 AM: 	Saved vocab to ./experiments/pos-ontonotes-hotpot-top/vocab
09/16 06:46:51 AM: Loading token dictionary from ./experiments/pos-ontonotes-hotpot-top/vocab.
09/16 06:46:51 AM: 	Loaded vocab from ./experiments/pos-ontonotes-hotpot-top/vocab
09/16 06:46:51 AM: 	Vocab namespace bert_uncased: size 30524
09/16 06:46:51 AM: 	Vocab namespace tokens: size 24015
09/16 06:46:51 AM: 	Vocab namespace edges-pos-ontonotes_labels: size 48
09/16 06:46:51 AM: 	Vocab namespace chars: size 81
09/16 06:46:51 AM: 	Finished building vocab.
09/16 06:46:51 AM: 	Task edges-pos-ontonotes (train): Indexing from scratch.
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (train): Saved 110514 instances to ./experiments/pos-ontonotes-hotpot-top/preproc/edges-pos-ontonotes__train_data
09/16 06:47:20 AM: 	Task edges-pos-ontonotes (val): Indexing from scratch.
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (val): Saved 15060 instances to ./experiments/pos-ontonotes-hotpot-top/preproc/edges-pos-ontonotes__val_data
09/16 06:47:24 AM: 	Task edges-pos-ontonotes (test): Indexing from scratch.
09/16 06:47:27 AM: 	Task edges-pos-ontonotes (test): Saved 11462 instances to ./experiments/pos-ontonotes-hotpot-top/preproc/edges-pos-ontonotes__test_data
09/16 06:47:27 AM: 	Finished indexing tasks
09/16 06:47:27 AM: 	Creating trimmed target-only version of edges-pos-ontonotes train.
09/16 06:47:27 AM: 	  Training on 
09/16 06:47:27 AM: 	  Evaluating on edges-pos-ontonotes
09/16 06:47:27 AM: 	Finished loading tasks in 68.510s
09/16 06:47:27 AM: 	 Tasks: ['edges-pos-ontonotes']
09/16 06:47:27 AM: Building model...
09/16 06:47:27 AM: Using BERT model (bert-base-uncased).
09/16 06:47:27 AM: LOADING A FUNETUNED MODEL from: 
09/16 06:47:27 AM: models/hotpot
09/16 06:47:27 AM: loading configuration file models/hotpot/config.json
09/16 06:47:27 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 06:47:27 AM: loading weights file models/hotpot/pytorch_model.bin
09/16 06:47:30 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpq7tmtxhf
09/16 06:47:32 AM: copying /tmp/tmpq7tmtxhf to cache at ./experiments/pos-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: creating metadata file for ./experiments/pos-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: removing temp file /tmp/tmpq7tmtxhf
09/16 06:47:32 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/pos-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 06:47:32 AM: Initializing parameters
09/16 06:47:32 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 06:47:32 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 06:47:32 AM: 	Task 'edges-pos-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-pos-ontonotes"
}
09/16 06:47:36 AM: Model specification:
09/16 06:47:36 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-pos-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=48, bias=True)
    )
  )
)
09/16 06:47:36 AM: Model parameters:
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 06:47:36 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 24576 with torch.Size([48, 512])
09/16 06:47:36 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 48 with torch.Size([48])
09/16 06:47:36 AM: Total number of parameters: 109703728 (1.09704e+08)
09/16 06:47:36 AM: Number of trainable parameters: 221488 (221488)
09/16 06:47:36 AM: Finished building model in 8.971s
09/16 06:47:36 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-pos-ontonotes 

09/16 06:48:40 AM: patience = 9
09/16 06:48:40 AM: val_interval = 1000
09/16 06:48:40 AM: max_vals = 250
09/16 06:48:40 AM: cuda_device = 0
09/16 06:48:40 AM: grad_norm = 5.0
09/16 06:48:40 AM: grad_clipping = None
09/16 06:48:40 AM: lr_decay = 0.99
09/16 06:48:40 AM: min_lr = 1e-06
09/16 06:48:40 AM: keep_all_checkpoints = 0
09/16 06:48:40 AM: val_data_limit = 5000
09/16 06:48:40 AM: max_epochs = -1
09/16 06:48:40 AM: dec_val_scale = 250
09/16 06:48:40 AM: training_data_fraction = 1
09/16 06:48:40 AM: type = adam
09/16 06:48:40 AM: parameter_groups = None
09/16 06:48:40 AM: Number of trainable parameters: 221488
09/16 06:48:40 AM: infer_type_and_cast = True
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: lr = 0.0001
09/16 06:48:40 AM: amsgrad = True
09/16 06:48:40 AM: type = reduce_on_plateau
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: mode = max
09/16 06:48:40 AM: factor = 0.5
09/16 06:48:40 AM: patience = 3
09/16 06:48:40 AM: threshold = 0.0001
09/16 06:48:40 AM: threshold_mode = abs
09/16 06:48:40 AM: verbose = True
09/16 06:48:40 AM: type = adam
09/16 06:48:40 AM: parameter_groups = None
09/16 06:48:40 AM: Number of trainable parameters: 221488
09/16 06:48:40 AM: infer_type_and_cast = True
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: lr = 0.0001
09/16 06:48:40 AM: amsgrad = True
09/16 06:48:40 AM: type = reduce_on_plateau
09/16 06:48:40 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 06:48:40 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 06:48:40 AM: mode = max
09/16 06:48:40 AM: factor = 0.5
09/16 06:48:40 AM: patience = 3
09/16 06:48:40 AM: threshold = 0.0001
09/16 06:48:40 AM: threshold_mode = abs
09/16 06:48:40 AM: verbose = True
09/16 06:48:40 AM: Starting training without restoring from a checkpoint.
09/16 06:48:40 AM: Training examples per task, before any subsampling: {'edges-pos-ontonotes': 110514}
09/16 06:48:40 AM: Beginning training with stopping criteria based on metric: edges-pos-ontonotes_f1
09/16 06:48:50 AM: Update 81: task edges-pos-ontonotes, batch 81 (81): mcc: -0.0044, acc: 0.0008, precision: 0.0186, recall: 0.0636, f1: 0.0287, edges-pos-ontonotes_loss: 0.2387
09/16 06:49:00 AM: Update 173: task edges-pos-ontonotes, batch 173 (173): mcc: -0.0017, acc: 0.0018, precision: 0.0195, recall: 0.0311, f1: 0.0240, edges-pos-ontonotes_loss: 0.1562
09/16 06:49:10 AM: Update 268: task edges-pos-ontonotes, batch 268 (268): mcc: 0.0013, acc: 0.0037, precision: 0.0221, recall: 0.0230, f1: 0.0225, edges-pos-ontonotes_loss: 0.1287
09/16 06:49:20 AM: Update 337: task edges-pos-ontonotes, batch 337 (337): mcc: 0.0057, acc: 0.0064, precision: 0.0271, recall: 0.0218, f1: 0.0241, edges-pos-ontonotes_loss: 0.1181
09/16 06:49:30 AM: Update 414: task edges-pos-ontonotes, batch 414 (414): mcc: 0.0107, acc: 0.0091, precision: 0.0340, recall: 0.0217, f1: 0.0265, edges-pos-ontonotes_loss: 0.1101
09/16 06:49:41 AM: Update 488: task edges-pos-ontonotes, batch 488 (488): mcc: 0.0205, acc: 0.0151, precision: 0.0483, recall: 0.0261, f1: 0.0339, edges-pos-ontonotes_loss: 0.1043
09/16 06:49:51 AM: Update 558: task edges-pos-ontonotes, batch 558 (558): mcc: 0.0299, acc: 0.0198, precision: 0.0637, recall: 0.0301, f1: 0.0409, edges-pos-ontonotes_loss: 0.1002
09/16 06:50:02 AM: Update 627: task edges-pos-ontonotes, batch 627 (627): mcc: 0.0405, acc: 0.0251, precision: 0.0818, recall: 0.0351, f1: 0.0491, edges-pos-ontonotes_loss: 0.0967
09/16 06:50:12 AM: Update 678: task edges-pos-ontonotes, batch 678 (678): mcc: 0.0488, acc: 0.0286, precision: 0.0978, recall: 0.0382, f1: 0.0549, edges-pos-ontonotes_loss: 0.0948
09/16 06:50:22 AM: Update 734: task edges-pos-ontonotes, batch 734 (734): mcc: 0.0564, acc: 0.0314, precision: 0.1136, recall: 0.0408, f1: 0.0601, edges-pos-ontonotes_loss: 0.0929
09/16 06:50:32 AM: Update 798: task edges-pos-ontonotes, batch 798 (798): mcc: 0.0650, acc: 0.0349, precision: 0.1314, recall: 0.0442, f1: 0.0662, edges-pos-ontonotes_loss: 0.0910
09/16 06:50:42 AM: Update 863: task edges-pos-ontonotes, batch 863 (863): mcc: 0.0737, acc: 0.0384, precision: 0.1497, recall: 0.0477, f1: 0.0723, edges-pos-ontonotes_loss: 0.0892
09/16 06:50:52 AM: Update 918: task edges-pos-ontonotes, batch 918 (918): mcc: 0.0819, acc: 0.0414, precision: 0.1675, recall: 0.0508, f1: 0.0780, edges-pos-ontonotes_loss: 0.0879
09/16 06:51:03 AM: Update 965: task edges-pos-ontonotes, batch 965 (965): mcc: 0.0882, acc: 0.0438, precision: 0.1816, recall: 0.0532, f1: 0.0823, edges-pos-ontonotes_loss: 0.0868
09/16 06:51:09 AM: ***** Step 1000 / Validation 1 *****
09/16 06:51:09 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:51:09 AM: Validating...
09/16 06:51:13 AM: Evaluate: task edges-pos-ontonotes, batch 25 (157): mcc: 0.2787, acc: 0.0991, precision: 0.7607, recall: 0.1054, f1: 0.1851, edges-pos-ontonotes_loss: 0.0654
09/16 06:51:23 AM: Evaluate: task edges-pos-ontonotes, batch 92 (157): mcc: 0.2977, acc: 0.1084, precision: 0.7966, recall: 0.1145, f1: 0.2003, edges-pos-ontonotes_loss: 0.0654
09/16 06:51:33 AM: Evaluate: task edges-pos-ontonotes, batch 139 (157): mcc: 0.2952, acc: 0.1096, precision: 0.7610, recall: 0.1181, f1: 0.2045, edges-pos-ontonotes_loss: 0.0646
09/16 06:51:36 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:51:36 AM: Best result seen so far for micro.
09/16 06:51:36 AM: Best result seen so far for macro.
09/16 06:51:36 AM: Updating LR scheduler:
09/16 06:51:36 AM: 	Best result seen so far for macro_avg: 0.204
09/16 06:51:36 AM: 	# validation passes without improvement: 0
09/16 06:51:36 AM: edges-pos-ontonotes_loss: training: 0.086128 validation: 0.064438
09/16 06:51:36 AM: macro_avg: validation: 0.204444
09/16 06:51:36 AM: micro_avg: validation: 0.000000
09/16 06:51:36 AM: edges-pos-ontonotes_mcc: training: 0.093637 validation: 0.294256
09/16 06:51:36 AM: edges-pos-ontonotes_acc: training: 0.045865 validation: 0.109485
09/16 06:51:36 AM: edges-pos-ontonotes_precision: training: 0.193587 validation: 0.756007
09/16 06:51:36 AM: edges-pos-ontonotes_recall: training: 0.055358 validation: 0.118205
09/16 06:51:36 AM: edges-pos-ontonotes_f1: training: 0.086096 validation: 0.204444
09/16 06:51:36 AM: Global learning rate: 0.0001
09/16 06:51:36 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 06:51:43 AM: Update 1037: task edges-pos-ontonotes, batch 37 (1037): mcc: 0.2650, acc: 0.0924, precision: 0.7083, recall: 0.1027, f1: 0.1794, edges-pos-ontonotes_loss: 0.0654
09/16 06:51:53 AM: Update 1091: task edges-pos-ontonotes, batch 91 (1091): mcc: 0.2743, acc: 0.1004, precision: 0.6941, recall: 0.1124, f1: 0.1935, edges-pos-ontonotes_loss: 0.0651
09/16 06:52:03 AM: Update 1147: task edges-pos-ontonotes, batch 147 (1147): mcc: 0.2816, acc: 0.1056, precision: 0.6969, recall: 0.1180, f1: 0.2018, edges-pos-ontonotes_loss: 0.0646
09/16 06:52:13 AM: Update 1204: task edges-pos-ontonotes, batch 204 (1204): mcc: 0.2855, acc: 0.1082, precision: 0.7007, recall: 0.1206, f1: 0.2058, edges-pos-ontonotes_loss: 0.0643
09/16 06:52:33 AM: Update 1253: task edges-pos-ontonotes, batch 253 (1253): mcc: 0.2905, acc: 0.1119, precision: 0.7021, recall: 0.1245, f1: 0.2115, edges-pos-ontonotes_loss: 0.0641
09/16 06:52:43 AM: Update 1311: task edges-pos-ontonotes, batch 311 (1311): mcc: 0.2948, acc: 0.1148, precision: 0.7066, recall: 0.1274, f1: 0.2159, edges-pos-ontonotes_loss: 0.0638
09/16 06:52:53 AM: Update 1369: task edges-pos-ontonotes, batch 369 (1369): mcc: 0.3005, acc: 0.1188, precision: 0.7116, recall: 0.1313, f1: 0.2217, edges-pos-ontonotes_loss: 0.0635
09/16 06:53:03 AM: Update 1427: task edges-pos-ontonotes, batch 427 (1427): mcc: 0.3068, acc: 0.1233, precision: 0.7157, recall: 0.1360, f1: 0.2286, edges-pos-ontonotes_loss: 0.0632
09/16 06:53:13 AM: Update 1481: task edges-pos-ontonotes, batch 481 (1481): mcc: 0.3120, acc: 0.1272, precision: 0.7181, recall: 0.1402, f1: 0.2346, edges-pos-ontonotes_loss: 0.0629
09/16 06:53:23 AM: Update 1536: task edges-pos-ontonotes, batch 536 (1536): mcc: 0.3170, acc: 0.1308, precision: 0.7215, recall: 0.1440, f1: 0.2401, edges-pos-ontonotes_loss: 0.0627
09/16 06:53:33 AM: Update 1586: task edges-pos-ontonotes, batch 586 (1586): mcc: 0.3212, acc: 0.1340, precision: 0.7245, recall: 0.1472, f1: 0.2447, edges-pos-ontonotes_loss: 0.0624
09/16 06:53:43 AM: Update 1641: task edges-pos-ontonotes, batch 641 (1641): mcc: 0.3262, acc: 0.1378, precision: 0.7272, recall: 0.1512, f1: 0.2503, edges-pos-ontonotes_loss: 0.0621
09/16 06:53:53 AM: Update 1699: task edges-pos-ontonotes, batch 699 (1699): mcc: 0.3316, acc: 0.1419, precision: 0.7308, recall: 0.1554, f1: 0.2563, edges-pos-ontonotes_loss: 0.0618
09/16 06:54:03 AM: Update 1753: task edges-pos-ontonotes, batch 753 (1753): mcc: 0.3358, acc: 0.1452, precision: 0.7325, recall: 0.1589, f1: 0.2612, edges-pos-ontonotes_loss: 0.0616
09/16 06:54:13 AM: Update 1812: task edges-pos-ontonotes, batch 812 (1812): mcc: 0.3401, acc: 0.1487, precision: 0.7347, recall: 0.1626, f1: 0.2662, edges-pos-ontonotes_loss: 0.0613
09/16 06:54:23 AM: Update 1866: task edges-pos-ontonotes, batch 866 (1866): mcc: 0.3443, acc: 0.1521, precision: 0.7367, recall: 0.1661, f1: 0.2711, edges-pos-ontonotes_loss: 0.0610
09/16 06:54:34 AM: Update 1908: task edges-pos-ontonotes, batch 908 (1908): mcc: 0.3452, acc: 0.1530, precision: 0.7357, recall: 0.1672, f1: 0.2724, edges-pos-ontonotes_loss: 0.0609
09/16 06:54:44 AM: Update 1977: task edges-pos-ontonotes, batch 977 (1977): mcc: 0.3482, acc: 0.1554, precision: 0.7372, recall: 0.1697, f1: 0.2759, edges-pos-ontonotes_loss: 0.0606
09/16 06:54:46 AM: ***** Step 2000 / Validation 2 *****
09/16 06:54:46 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:54:46 AM: Validating...
09/16 06:54:54 AM: Evaluate: task edges-pos-ontonotes, batch 50 (157): mcc: 0.4249, acc: 0.2097, precision: 0.8393, recall: 0.2203, f1: 0.3490, edges-pos-ontonotes_loss: 0.0578
09/16 06:55:04 AM: Evaluate: task edges-pos-ontonotes, batch 110 (157): mcc: 0.4578, acc: 0.2418, precision: 0.8439, recall: 0.2540, f1: 0.3905, edges-pos-ontonotes_loss: 0.0548
09/16 06:55:14 AM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.4574, acc: 0.2444, precision: 0.8263, recall: 0.2592, f1: 0.3946, edges-pos-ontonotes_loss: 0.0542
09/16 06:55:14 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:55:14 AM: Best result seen so far for macro.
09/16 06:55:14 AM: Updating LR scheduler:
09/16 06:55:14 AM: 	Best result seen so far for macro_avg: 0.395
09/16 06:55:14 AM: 	# validation passes without improvement: 0
09/16 06:55:14 AM: edges-pos-ontonotes_loss: training: 0.060493 validation: 0.054229
09/16 06:55:14 AM: macro_avg: validation: 0.394506
09/16 06:55:14 AM: micro_avg: validation: 0.000000
09/16 06:55:14 AM: edges-pos-ontonotes_mcc: training: 0.349446 validation: 0.457307
09/16 06:55:14 AM: edges-pos-ontonotes_acc: training: 0.156373 validation: 0.244283
09/16 06:55:14 AM: edges-pos-ontonotes_precision: training: 0.738303 validation: 0.826275
09/16 06:55:14 AM: edges-pos-ontonotes_recall: training: 0.170666 validation: 0.259109
09/16 06:55:14 AM: edges-pos-ontonotes_f1: training: 0.277245 validation: 0.394506
09/16 06:55:14 AM: Global learning rate: 0.0001
09/16 06:55:14 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 06:55:24 AM: Update 2072: task edges-pos-ontonotes, batch 72 (2072): mcc: 0.4131, acc: 0.2119, precision: 0.7617, recall: 0.2305, f1: 0.3539, edges-pos-ontonotes_loss: 0.0552
09/16 06:55:34 AM: Update 2138: task edges-pos-ontonotes, batch 138 (2138): mcc: 0.4084, acc: 0.2091, precision: 0.7526, recall: 0.2280, f1: 0.3500, edges-pos-ontonotes_loss: 0.0550
09/16 06:55:44 AM: Update 2199: task edges-pos-ontonotes, batch 199 (2199): mcc: 0.4098, acc: 0.2110, precision: 0.7519, recall: 0.2299, f1: 0.3521, edges-pos-ontonotes_loss: 0.0545
09/16 06:55:54 AM: Update 2295: task edges-pos-ontonotes, batch 295 (2295): mcc: 0.4286, acc: 0.2255, precision: 0.7764, recall: 0.2430, f1: 0.3701, edges-pos-ontonotes_loss: 0.0530
09/16 06:56:04 AM: Update 2390: task edges-pos-ontonotes, batch 390 (2390): mcc: 0.4451, acc: 0.2392, precision: 0.7930, recall: 0.2563, f1: 0.3874, edges-pos-ontonotes_loss: 0.0519
09/16 06:56:14 AM: Update 2485: task edges-pos-ontonotes, batch 485 (2485): mcc: 0.4582, acc: 0.2506, precision: 0.8041, recall: 0.2676, f1: 0.4015, edges-pos-ontonotes_loss: 0.0509
09/16 06:56:24 AM: Update 2580: task edges-pos-ontonotes, batch 580 (2580): mcc: 0.4669, acc: 0.2591, precision: 0.8092, recall: 0.2760, f1: 0.4116, edges-pos-ontonotes_loss: 0.0505
09/16 06:56:34 AM: Update 2687: task edges-pos-ontonotes, batch 687 (2687): mcc: 0.4776, acc: 0.2697, precision: 0.8144, recall: 0.2867, f1: 0.4241, edges-pos-ontonotes_loss: 0.0498
09/16 06:56:44 AM: Update 2798: task edges-pos-ontonotes, batch 798 (2798): mcc: 0.4887, acc: 0.2808, precision: 0.8200, recall: 0.2980, f1: 0.4372, edges-pos-ontonotes_loss: 0.0489
09/16 06:56:54 AM: Update 2914: task edges-pos-ontonotes, batch 914 (2914): mcc: 0.4875, acc: 0.2805, precision: 0.8160, recall: 0.2981, f1: 0.4366, edges-pos-ontonotes_loss: 0.0490
09/16 06:57:00 AM: ***** Step 3000 / Validation 3 *****
09/16 06:57:00 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:57:00 AM: Validating...
09/16 06:57:04 AM: Evaluate: task edges-pos-ontonotes, batch 31 (157): mcc: 0.5171, acc: 0.3130, precision: 0.8158, recall: 0.3351, f1: 0.4751, edges-pos-ontonotes_loss: 0.0478
09/16 06:57:14 AM: Evaluate: task edges-pos-ontonotes, batch 97 (157): mcc: 0.5466, acc: 0.3454, precision: 0.8270, recall: 0.3688, f1: 0.5101, edges-pos-ontonotes_loss: 0.0459
09/16 06:57:24 AM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.5276, acc: 0.3271, precision: 0.7973, recall: 0.3571, f1: 0.4933, edges-pos-ontonotes_loss: 0.0476
09/16 06:57:27 AM: Best result seen so far for edges-pos-ontonotes.
09/16 06:57:27 AM: Best result seen so far for macro.
09/16 06:57:27 AM: Updating LR scheduler:
09/16 06:57:27 AM: 	Best result seen so far for macro_avg: 0.496
09/16 06:57:27 AM: 	# validation passes without improvement: 0
09/16 06:57:27 AM: edges-pos-ontonotes_loss: training: 0.048818 validation: 0.047773
09/16 06:57:27 AM: macro_avg: validation: 0.495546
09/16 06:57:27 AM: micro_avg: validation: 0.000000
09/16 06:57:27 AM: edges-pos-ontonotes_mcc: training: 0.487507 validation: 0.529510
09/16 06:57:27 AM: edges-pos-ontonotes_acc: training: 0.280778 validation: 0.328688
09/16 06:57:27 AM: edges-pos-ontonotes_precision: training: 0.815681 validation: 0.797909
09/16 06:57:27 AM: edges-pos-ontonotes_recall: training: 0.298161 validation: 0.359366
09/16 06:57:27 AM: edges-pos-ontonotes_f1: training: 0.436694 validation: 0.495546
09/16 06:57:27 AM: Global learning rate: 0.0001
09/16 06:57:27 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 06:57:34 AM: Update 3095: task edges-pos-ontonotes, batch 95 (3095): mcc: 0.4773, acc: 0.2811, precision: 0.7831, recall: 0.2982, f1: 0.4319, edges-pos-ontonotes_loss: 0.0478
09/16 06:57:44 AM: Update 3159: task edges-pos-ontonotes, batch 159 (3159): mcc: 0.4466, acc: 0.2558, precision: 0.7378, recall: 0.2781, f1: 0.4039, edges-pos-ontonotes_loss: 0.0490
09/16 06:57:54 AM: Update 3215: task edges-pos-ontonotes, batch 215 (3215): mcc: 0.4361, acc: 0.2457, precision: 0.7248, recall: 0.2703, f1: 0.3937, edges-pos-ontonotes_loss: 0.0504
09/16 06:58:05 AM: Update 3274: task edges-pos-ontonotes, batch 274 (3274): mcc: 0.4342, acc: 0.2425, precision: 0.7245, recall: 0.2680, f1: 0.3913, edges-pos-ontonotes_loss: 0.0514
09/16 06:58:15 AM: Update 3337: task edges-pos-ontonotes, batch 337 (3337): mcc: 0.4348, acc: 0.2425, precision: 0.7267, recall: 0.2679, f1: 0.3915, edges-pos-ontonotes_loss: 0.0518
09/16 06:58:25 AM: Update 3407: task edges-pos-ontonotes, batch 407 (3407): mcc: 0.4386, acc: 0.2456, precision: 0.7315, recall: 0.2708, f1: 0.3952, edges-pos-ontonotes_loss: 0.0518
09/16 06:58:42 AM: Update 3461: task edges-pos-ontonotes, batch 461 (3461): mcc: 0.4399, acc: 0.2468, precision: 0.7328, recall: 0.2719, f1: 0.3966, edges-pos-ontonotes_loss: 0.0519
09/16 06:58:52 AM: Update 3561: task edges-pos-ontonotes, batch 561 (3561): mcc: 0.4482, acc: 0.2538, precision: 0.7443, recall: 0.2776, f1: 0.4044, edges-pos-ontonotes_loss: 0.0508
09/16 06:59:02 AM: Update 3657: task edges-pos-ontonotes, batch 657 (3657): mcc: 0.4559, acc: 0.2604, precision: 0.7533, recall: 0.2835, f1: 0.4120, edges-pos-ontonotes_loss: 0.0498
09/16 06:59:12 AM: Update 3743: task edges-pos-ontonotes, batch 743 (3743): mcc: 0.4618, acc: 0.2657, precision: 0.7590, recall: 0.2885, f1: 0.4181, edges-pos-ontonotes_loss: 0.0491
09/16 06:59:23 AM: Update 3812: task edges-pos-ontonotes, batch 812 (3812): mcc: 0.4659, acc: 0.2699, precision: 0.7595, recall: 0.2935, f1: 0.4234, edges-pos-ontonotes_loss: 0.0487
09/16 06:59:33 AM: Update 3881: task edges-pos-ontonotes, batch 881 (3881): mcc: 0.4714, acc: 0.2753, precision: 0.7608, recall: 0.2998, f1: 0.4301, edges-pos-ontonotes_loss: 0.0484
09/16 06:59:43 AM: Update 3959: task edges-pos-ontonotes, batch 959 (3959): mcc: 0.4768, acc: 0.2807, precision: 0.7628, recall: 0.3058, f1: 0.4366, edges-pos-ontonotes_loss: 0.0480
09/16 06:59:48 AM: ***** Step 4000 / Validation 4 *****
09/16 06:59:48 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 06:59:48 AM: Validating...
09/16 06:59:53 AM: Evaluate: task edges-pos-ontonotes, batch 37 (157): mcc: 0.6025, acc: 0.4019, precision: 0.8808, recall: 0.4191, f1: 0.5680, edges-pos-ontonotes_loss: 0.0406
09/16 07:00:03 AM: Evaluate: task edges-pos-ontonotes, batch 101 (157): mcc: 0.6116, acc: 0.4124, precision: 0.8810, recall: 0.4317, f1: 0.5794, edges-pos-ontonotes_loss: 0.0397
09/16 07:00:13 AM: Evaluate: task edges-pos-ontonotes, batch 148 (157): mcc: 0.5859, acc: 0.3842, precision: 0.8618, recall: 0.4057, f1: 0.5516, edges-pos-ontonotes_loss: 0.0416
09/16 07:00:15 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:00:15 AM: Best result seen so far for macro.
09/16 07:00:15 AM: Updating LR scheduler:
09/16 07:00:15 AM: 	Best result seen so far for macro_avg: 0.552
09/16 07:00:15 AM: 	# validation passes without improvement: 0
09/16 07:00:15 AM: edges-pos-ontonotes_loss: training: 0.047775 validation: 0.041666
09/16 07:00:15 AM: macro_avg: validation: 0.551511
09/16 07:00:15 AM: micro_avg: validation: 0.000000
09/16 07:00:15 AM: edges-pos-ontonotes_mcc: training: 0.479659 validation: 0.585780
09/16 07:00:15 AM: edges-pos-ontonotes_acc: training: 0.283480 validation: 0.384171
09/16 07:00:15 AM: edges-pos-ontonotes_precision: training: 0.764420 validation: 0.861436
09/16 07:00:15 AM: edges-pos-ontonotes_recall: training: 0.308830 validation: 0.405590
09/16 07:00:15 AM: edges-pos-ontonotes_f1: training: 0.439928 validation: 0.551511
09/16 07:00:15 AM: Global learning rate: 0.0001
09/16 07:00:15 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:00:23 AM: Update 4062: task edges-pos-ontonotes, batch 62 (4062): mcc: 0.5370, acc: 0.3438, precision: 0.7801, recall: 0.3782, f1: 0.5095, edges-pos-ontonotes_loss: 0.0446
09/16 07:00:33 AM: Update 4113: task edges-pos-ontonotes, batch 113 (4113): mcc: 0.5107, acc: 0.3208, precision: 0.7566, recall: 0.3535, f1: 0.4819, edges-pos-ontonotes_loss: 0.0462
09/16 07:00:43 AM: Update 4171: task edges-pos-ontonotes, batch 171 (4171): mcc: 0.4988, acc: 0.3085, precision: 0.7504, recall: 0.3403, f1: 0.4682, edges-pos-ontonotes_loss: 0.0477
09/16 07:00:53 AM: Update 4231: task edges-pos-ontonotes, batch 231 (4231): mcc: 0.4959, acc: 0.3061, precision: 0.7492, recall: 0.3368, f1: 0.4647, edges-pos-ontonotes_loss: 0.0482
09/16 07:01:03 AM: Update 4289: task edges-pos-ontonotes, batch 289 (4289): mcc: 0.4919, acc: 0.3030, precision: 0.7442, recall: 0.3339, f1: 0.4609, edges-pos-ontonotes_loss: 0.0486
09/16 07:01:14 AM: Update 4346: task edges-pos-ontonotes, batch 346 (4346): mcc: 0.4893, acc: 0.3003, precision: 0.7432, recall: 0.3308, f1: 0.4579, edges-pos-ontonotes_loss: 0.0489
09/16 07:01:25 AM: Update 4400: task edges-pos-ontonotes, batch 400 (4400): mcc: 0.4889, acc: 0.2996, precision: 0.7437, recall: 0.3300, f1: 0.4571, edges-pos-ontonotes_loss: 0.0490
09/16 07:01:35 AM: Update 4456: task edges-pos-ontonotes, batch 456 (4456): mcc: 0.4879, acc: 0.2987, precision: 0.7434, recall: 0.3288, f1: 0.4559, edges-pos-ontonotes_loss: 0.0491
09/16 07:01:45 AM: Update 4513: task edges-pos-ontonotes, batch 513 (4513): mcc: 0.4890, acc: 0.2996, precision: 0.7448, recall: 0.3297, f1: 0.4570, edges-pos-ontonotes_loss: 0.0490
09/16 07:01:55 AM: Update 4564: task edges-pos-ontonotes, batch 564 (4564): mcc: 0.4871, acc: 0.2978, precision: 0.7432, recall: 0.3278, f1: 0.4549, edges-pos-ontonotes_loss: 0.0491
09/16 07:02:05 AM: Update 4621: task edges-pos-ontonotes, batch 621 (4621): mcc: 0.4885, acc: 0.2990, precision: 0.7450, recall: 0.3289, f1: 0.4563, edges-pos-ontonotes_loss: 0.0490
09/16 07:02:15 AM: Update 4674: task edges-pos-ontonotes, batch 674 (4674): mcc: 0.4899, acc: 0.3001, precision: 0.7466, recall: 0.3300, f1: 0.4577, edges-pos-ontonotes_loss: 0.0490
09/16 07:02:25 AM: Update 4720: task edges-pos-ontonotes, batch 720 (4720): mcc: 0.4903, acc: 0.3005, precision: 0.7470, recall: 0.3304, f1: 0.4581, edges-pos-ontonotes_loss: 0.0489
09/16 07:02:35 AM: Update 4778: task edges-pos-ontonotes, batch 778 (4778): mcc: 0.4919, acc: 0.3019, precision: 0.7486, recall: 0.3317, f1: 0.4597, edges-pos-ontonotes_loss: 0.0488
09/16 07:02:45 AM: Update 4834: task edges-pos-ontonotes, batch 834 (4834): mcc: 0.4931, acc: 0.3031, precision: 0.7496, recall: 0.3328, f1: 0.4610, edges-pos-ontonotes_loss: 0.0488
09/16 07:02:56 AM: Update 4892: task edges-pos-ontonotes, batch 892 (4892): mcc: 0.4940, acc: 0.3041, precision: 0.7505, recall: 0.3338, f1: 0.4621, edges-pos-ontonotes_loss: 0.0487
09/16 07:03:06 AM: Update 4950: task edges-pos-ontonotes, batch 950 (4950): mcc: 0.4950, acc: 0.3050, precision: 0.7510, recall: 0.3348, f1: 0.4631, edges-pos-ontonotes_loss: 0.0486
09/16 07:03:15 AM: ***** Step 5000 / Validation 5 *****
09/16 07:03:15 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:03:15 AM: Validating...
09/16 07:03:16 AM: Evaluate: task edges-pos-ontonotes, batch 2 (157): mcc: 0.6472, acc: 0.4542, precision: 0.8949, recall: 0.4752, f1: 0.6207, edges-pos-ontonotes_loss: 0.0359
09/16 07:03:26 AM: Evaluate: task edges-pos-ontonotes, batch 73 (157): mcc: 0.6275, acc: 0.4284, precision: 0.8967, recall: 0.4460, f1: 0.5957, edges-pos-ontonotes_loss: 0.0393
09/16 07:03:36 AM: Evaluate: task edges-pos-ontonotes, batch 125 (157): mcc: 0.6141, acc: 0.4139, precision: 0.8861, recall: 0.4327, f1: 0.5814, edges-pos-ontonotes_loss: 0.0394
09/16 07:03:43 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:03:43 AM: Best result seen so far for macro.
09/16 07:03:43 AM: Updating LR scheduler:
09/16 07:03:43 AM: 	Best result seen so far for macro_avg: 0.574
09/16 07:03:43 AM: 	# validation passes without improvement: 0
09/16 07:03:43 AM: edges-pos-ontonotes_loss: training: 0.048634 validation: 0.039694
09/16 07:03:43 AM: macro_avg: validation: 0.573746
09/16 07:03:43 AM: micro_avg: validation: 0.000000
09/16 07:03:43 AM: edges-pos-ontonotes_mcc: training: 0.495744 validation: 0.607533
09/16 07:03:43 AM: edges-pos-ontonotes_acc: training: 0.305914 validation: 0.405939
09/16 07:03:43 AM: edges-pos-ontonotes_precision: training: 0.751442 validation: 0.883322
09/16 07:03:43 AM: edges-pos-ontonotes_recall: training: 0.335611 validation: 0.424849
09/16 07:03:43 AM: edges-pos-ontonotes_f1: training: 0.463993 validation: 0.573746
09/16 07:03:43 AM: Global learning rate: 0.0001
09/16 07:03:43 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:03:46 AM: Update 5021: task edges-pos-ontonotes, batch 21 (5021): mcc: 0.5324, acc: 0.3422, precision: 0.7802, recall: 0.3718, f1: 0.5036, edges-pos-ontonotes_loss: 0.0455
09/16 07:03:57 AM: Update 5026: task edges-pos-ontonotes, batch 26 (5026): mcc: 0.5225, acc: 0.3313, precision: 0.7740, recall: 0.3612, f1: 0.4926, edges-pos-ontonotes_loss: 0.0464
09/16 07:04:07 AM: Update 5088: task edges-pos-ontonotes, batch 88 (5088): mcc: 0.5216, acc: 0.3307, precision: 0.7714, recall: 0.3612, f1: 0.4920, edges-pos-ontonotes_loss: 0.0467
09/16 07:04:17 AM: Update 5142: task edges-pos-ontonotes, batch 142 (5142): mcc: 0.5177, acc: 0.3275, precision: 0.7673, recall: 0.3578, f1: 0.4880, edges-pos-ontonotes_loss: 0.0471
09/16 07:04:27 AM: Update 5202: task edges-pos-ontonotes, batch 202 (5202): mcc: 0.5199, acc: 0.3300, precision: 0.7687, recall: 0.3602, f1: 0.4905, edges-pos-ontonotes_loss: 0.0468
09/16 07:04:37 AM: Update 5259: task edges-pos-ontonotes, batch 259 (5259): mcc: 0.5210, acc: 0.3314, precision: 0.7694, recall: 0.3614, f1: 0.4918, edges-pos-ontonotes_loss: 0.0469
09/16 07:04:47 AM: Update 5313: task edges-pos-ontonotes, batch 313 (5313): mcc: 0.5201, acc: 0.3305, precision: 0.7685, recall: 0.3606, f1: 0.4909, edges-pos-ontonotes_loss: 0.0469
09/16 07:04:57 AM: Update 5352: task edges-pos-ontonotes, batch 352 (5352): mcc: 0.5148, acc: 0.3264, precision: 0.7619, recall: 0.3565, f1: 0.4858, edges-pos-ontonotes_loss: 0.0471
09/16 07:05:07 AM: Update 5421: task edges-pos-ontonotes, batch 421 (5421): mcc: 0.5171, acc: 0.3287, precision: 0.7629, recall: 0.3592, f1: 0.4885, edges-pos-ontonotes_loss: 0.0464
09/16 07:05:17 AM: Update 5494: task edges-pos-ontonotes, batch 494 (5494): mcc: 0.5213, acc: 0.3328, precision: 0.7653, recall: 0.3638, f1: 0.4931, edges-pos-ontonotes_loss: 0.0457
09/16 07:05:28 AM: Update 5571: task edges-pos-ontonotes, batch 571 (5571): mcc: 0.5239, acc: 0.3356, precision: 0.7663, recall: 0.3669, f1: 0.4963, edges-pos-ontonotes_loss: 0.0452
09/16 07:05:38 AM: Update 5630: task edges-pos-ontonotes, batch 630 (5630): mcc: 0.5225, acc: 0.3345, precision: 0.7640, recall: 0.3661, f1: 0.4950, edges-pos-ontonotes_loss: 0.0451
09/16 07:05:48 AM: Update 5700: task edges-pos-ontonotes, batch 700 (5700): mcc: 0.5280, acc: 0.3398, precision: 0.7680, recall: 0.3718, f1: 0.5010, edges-pos-ontonotes_loss: 0.0445
09/16 07:05:58 AM: Update 5797: task edges-pos-ontonotes, batch 797 (5797): mcc: 0.5377, acc: 0.3493, precision: 0.7758, recall: 0.3814, f1: 0.5114, edges-pos-ontonotes_loss: 0.0434
09/16 07:06:08 AM: Update 5886: task edges-pos-ontonotes, batch 886 (5886): mcc: 0.5460, acc: 0.3574, precision: 0.7823, recall: 0.3898, f1: 0.5203, edges-pos-ontonotes_loss: 0.0426
09/16 07:06:18 AM: Update 5966: task edges-pos-ontonotes, batch 966 (5966): mcc: 0.5524, acc: 0.3640, precision: 0.7870, recall: 0.3964, f1: 0.5272, edges-pos-ontonotes_loss: 0.0420
09/16 07:06:21 AM: ***** Step 6000 / Validation 6 *****
09/16 07:06:21 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:06:21 AM: Validating...
09/16 07:06:28 AM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.6151, acc: 0.4151, precision: 0.9014, recall: 0.4264, f1: 0.5789, edges-pos-ontonotes_loss: 0.0387
09/16 07:06:38 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.6312, acc: 0.4351, precision: 0.9043, recall: 0.4473, f1: 0.5986, edges-pos-ontonotes_loss: 0.0370
09/16 07:06:48 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.6125, acc: 0.4123, precision: 0.8974, recall: 0.4249, f1: 0.5767, edges-pos-ontonotes_loss: 0.0384
09/16 07:06:48 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:06:48 AM: Best result seen so far for macro.
09/16 07:06:48 AM: Updating LR scheduler:
09/16 07:06:48 AM: 	Best result seen so far for macro_avg: 0.577
09/16 07:06:48 AM: 	# validation passes without improvement: 0
09/16 07:06:48 AM: edges-pos-ontonotes_loss: training: 0.041863 validation: 0.038425
09/16 07:06:48 AM: macro_avg: validation: 0.577172
09/16 07:06:48 AM: micro_avg: validation: 0.000000
09/16 07:06:48 AM: edges-pos-ontonotes_mcc: training: 0.554505 validation: 0.612935
09/16 07:06:48 AM: edges-pos-ontonotes_acc: training: 0.366166 validation: 0.412690
09/16 07:06:48 AM: edges-pos-ontonotes_precision: training: 0.788563 validation: 0.897452
09/16 07:06:48 AM: edges-pos-ontonotes_recall: training: 0.398556 validation: 0.425368
09/16 07:06:48 AM: edges-pos-ontonotes_f1: training: 0.529495 validation: 0.577172
09/16 07:06:48 AM: Global learning rate: 0.0001
09/16 07:06:48 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:06:58 AM: Update 6113: task edges-pos-ontonotes, batch 113 (6113): mcc: 0.6553, acc: 0.4812, precision: 0.8502, recall: 0.5131, f1: 0.6400, edges-pos-ontonotes_loss: 0.0356
09/16 07:07:08 AM: Update 6214: task edges-pos-ontonotes, batch 214 (6214): mcc: 0.6580, acc: 0.4857, precision: 0.8483, recall: 0.5185, f1: 0.6436, edges-pos-ontonotes_loss: 0.0353
09/16 07:07:18 AM: Update 6320: task edges-pos-ontonotes, batch 320 (6320): mcc: 0.6452, acc: 0.4714, precision: 0.8384, recall: 0.5049, f1: 0.6302, edges-pos-ontonotes_loss: 0.0365
09/16 07:07:28 AM: Update 6448: task edges-pos-ontonotes, batch 448 (6448): mcc: 0.6224, acc: 0.4445, precision: 0.8265, recall: 0.4771, f1: 0.6050, edges-pos-ontonotes_loss: 0.0380
09/16 07:07:40 AM: Update 6591: task edges-pos-ontonotes, batch 591 (6591): mcc: 0.6095, acc: 0.4308, precision: 0.8178, recall: 0.4628, f1: 0.5911, edges-pos-ontonotes_loss: 0.0388
09/16 07:07:50 AM: Update 6662: task edges-pos-ontonotes, batch 662 (6662): mcc: 0.5915, acc: 0.4109, precision: 0.8032, recall: 0.4443, f1: 0.5722, edges-pos-ontonotes_loss: 0.0398
09/16 07:08:00 AM: Update 6719: task edges-pos-ontonotes, batch 719 (6719): mcc: 0.5746, acc: 0.3924, precision: 0.7910, recall: 0.4263, f1: 0.5540, edges-pos-ontonotes_loss: 0.0406
09/16 07:08:10 AM: Update 6775: task edges-pos-ontonotes, batch 775 (6775): mcc: 0.5633, acc: 0.3801, precision: 0.7829, recall: 0.4142, f1: 0.5417, edges-pos-ontonotes_loss: 0.0411
09/16 07:08:20 AM: Update 6835: task edges-pos-ontonotes, batch 835 (6835): mcc: 0.5559, acc: 0.3721, precision: 0.7774, recall: 0.4065, f1: 0.5338, edges-pos-ontonotes_loss: 0.0417
09/16 07:08:30 AM: Update 6900: task edges-pos-ontonotes, batch 900 (6900): mcc: 0.5521, acc: 0.3677, precision: 0.7751, recall: 0.4022, f1: 0.5296, edges-pos-ontonotes_loss: 0.0420
09/16 07:08:41 AM: Update 6969: task edges-pos-ontonotes, batch 969 (6969): mcc: 0.5505, acc: 0.3659, precision: 0.7750, recall: 0.4000, f1: 0.5277, edges-pos-ontonotes_loss: 0.0421
09/16 07:08:44 AM: ***** Step 7000 / Validation 7 *****
09/16 07:08:44 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:08:44 AM: Validating...
09/16 07:08:51 AM: Evaluate: task edges-pos-ontonotes, batch 47 (157): mcc: 0.6481, acc: 0.4527, precision: 0.9130, recall: 0.4668, f1: 0.6177, edges-pos-ontonotes_loss: 0.0353
09/16 07:09:01 AM: Evaluate: task edges-pos-ontonotes, batch 107 (157): mcc: 0.6486, acc: 0.4546, precision: 0.9074, recall: 0.4704, f1: 0.6196, edges-pos-ontonotes_loss: 0.0345
09/16 07:09:11 AM: Evaluate: task edges-pos-ontonotes, batch 154 (157): mcc: 0.6209, acc: 0.4208, precision: 0.8941, recall: 0.4380, f1: 0.5880, edges-pos-ontonotes_loss: 0.0364
09/16 07:09:11 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:09:11 AM: Best result seen so far for macro.
09/16 07:09:11 AM: Updating LR scheduler:
09/16 07:09:11 AM: 	Best result seen so far for macro_avg: 0.588
09/16 07:09:11 AM: 	# validation passes without improvement: 0
09/16 07:09:11 AM: edges-pos-ontonotes_loss: training: 0.042059 validation: 0.036401
09/16 07:09:11 AM: macro_avg: validation: 0.588050
09/16 07:09:11 AM: micro_avg: validation: 0.000000
09/16 07:09:11 AM: edges-pos-ontonotes_mcc: training: 0.550283 validation: 0.620915
09/16 07:09:11 AM: edges-pos-ontonotes_acc: training: 0.365683 validation: 0.420722
09/16 07:09:11 AM: edges-pos-ontonotes_precision: training: 0.775136 validation: 0.894070
09/16 07:09:11 AM: edges-pos-ontonotes_recall: training: 0.399601 validation: 0.438099
09/16 07:09:11 AM: edges-pos-ontonotes_f1: training: 0.527344 validation: 0.588050
09/16 07:09:11 AM: Global learning rate: 0.0001
09/16 07:09:11 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:09:21 AM: Update 7091: task edges-pos-ontonotes, batch 91 (7091): mcc: 0.5689, acc: 0.3810, precision: 0.8083, recall: 0.4087, f1: 0.5429, edges-pos-ontonotes_loss: 0.0383
09/16 07:09:31 AM: Update 7183: task edges-pos-ontonotes, batch 183 (7183): mcc: 0.5735, acc: 0.3863, precision: 0.8101, recall: 0.4143, f1: 0.5483, edges-pos-ontonotes_loss: 0.0386
09/16 07:09:41 AM: Update 7254: task edges-pos-ontonotes, batch 254 (7254): mcc: 0.5711, acc: 0.3849, precision: 0.8027, recall: 0.4148, f1: 0.5470, edges-pos-ontonotes_loss: 0.0389
09/16 07:09:51 AM: Update 7326: task edges-pos-ontonotes, batch 326 (7326): mcc: 0.5693, acc: 0.3841, precision: 0.7933, recall: 0.4173, f1: 0.5469, edges-pos-ontonotes_loss: 0.0393
09/16 07:10:01 AM: Update 7403: task edges-pos-ontonotes, batch 403 (7403): mcc: 0.5723, acc: 0.3877, precision: 0.7912, recall: 0.4229, f1: 0.5511, edges-pos-ontonotes_loss: 0.0393
09/16 07:10:11 AM: Update 7474: task edges-pos-ontonotes, batch 474 (7474): mcc: 0.5721, acc: 0.3877, precision: 0.7881, recall: 0.4241, f1: 0.5515, edges-pos-ontonotes_loss: 0.0395
09/16 07:10:30 AM: Update 7547: task edges-pos-ontonotes, batch 547 (7547): mcc: 0.5735, acc: 0.3897, precision: 0.7870, recall: 0.4269, f1: 0.5535, edges-pos-ontonotes_loss: 0.0394
09/16 07:10:40 AM: Update 7600: task edges-pos-ontonotes, batch 600 (7600): mcc: 0.5638, acc: 0.3803, precision: 0.7781, recall: 0.4176, f1: 0.5435, edges-pos-ontonotes_loss: 0.0402
09/16 07:10:51 AM: Update 7658: task edges-pos-ontonotes, batch 658 (7658): mcc: 0.5577, acc: 0.3746, precision: 0.7723, recall: 0.4119, f1: 0.5373, edges-pos-ontonotes_loss: 0.0408
09/16 07:11:01 AM: Update 7720: task edges-pos-ontonotes, batch 720 (7720): mcc: 0.5541, acc: 0.3708, precision: 0.7689, recall: 0.4085, f1: 0.5335, edges-pos-ontonotes_loss: 0.0413
09/16 07:11:11 AM: Update 7787: task edges-pos-ontonotes, batch 787 (7787): mcc: 0.5529, acc: 0.3696, precision: 0.7676, recall: 0.4074, f1: 0.5323, edges-pos-ontonotes_loss: 0.0417
09/16 07:11:21 AM: Update 7841: task edges-pos-ontonotes, batch 841 (7841): mcc: 0.5494, acc: 0.3660, precision: 0.7646, recall: 0.4040, f1: 0.5287, edges-pos-ontonotes_loss: 0.0420
09/16 07:11:31 AM: Update 7884: task edges-pos-ontonotes, batch 884 (7884): mcc: 0.5473, acc: 0.3639, precision: 0.7625, recall: 0.4020, f1: 0.5265, edges-pos-ontonotes_loss: 0.0422
09/16 07:11:41 AM: Update 7940: task edges-pos-ontonotes, batch 940 (7940): mcc: 0.5462, acc: 0.3628, precision: 0.7615, recall: 0.4010, f1: 0.5254, edges-pos-ontonotes_loss: 0.0425
09/16 07:11:51 AM: Update 7993: task edges-pos-ontonotes, batch 993 (7993): mcc: 0.5451, acc: 0.3618, precision: 0.7605, recall: 0.4000, f1: 0.5243, edges-pos-ontonotes_loss: 0.0427
09/16 07:11:52 AM: ***** Step 8000 / Validation 8 *****
09/16 07:11:52 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:11:52 AM: Validating...
09/16 07:12:01 AM: Evaluate: task edges-pos-ontonotes, batch 64 (157): mcc: 0.6638, acc: 0.4745, precision: 0.9112, recall: 0.4903, f1: 0.6376, edges-pos-ontonotes_loss: 0.0347
09/16 07:12:11 AM: Evaluate: task edges-pos-ontonotes, batch 118 (157): mcc: 0.6577, acc: 0.4683, precision: 0.9027, recall: 0.4861, f1: 0.6319, edges-pos-ontonotes_loss: 0.0345
09/16 07:12:19 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:12:19 AM: Best result seen so far for macro.
09/16 07:12:19 AM: Updating LR scheduler:
09/16 07:12:19 AM: 	Best result seen so far for macro_avg: 0.617
09/16 07:12:19 AM: 	# validation passes without improvement: 0
09/16 07:12:19 AM: edges-pos-ontonotes_loss: training: 0.042661 validation: 0.035330
09/16 07:12:19 AM: macro_avg: validation: 0.616583
09/16 07:12:19 AM: micro_avg: validation: 0.000000
09/16 07:12:19 AM: edges-pos-ontonotes_mcc: training: 0.545129 validation: 0.644356
09/16 07:12:19 AM: edges-pos-ontonotes_acc: training: 0.361864 validation: 0.451147
09/16 07:12:19 AM: edges-pos-ontonotes_precision: training: 0.760448 validation: 0.897448
09/16 07:12:19 AM: edges-pos-ontonotes_recall: training: 0.400062 validation: 0.469613
09/16 07:12:19 AM: edges-pos-ontonotes_f1: training: 0.524297 validation: 0.616583
09/16 07:12:19 AM: Global learning rate: 0.0001
09/16 07:12:19 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:12:21 AM: Update 8010: task edges-pos-ontonotes, batch 10 (8010): mcc: 0.5316, acc: 0.3474, precision: 0.7491, recall: 0.3866, f1: 0.5100, edges-pos-ontonotes_loss: 0.0458
09/16 07:12:31 AM: Update 8069: task edges-pos-ontonotes, batch 69 (8069): mcc: 0.5420, acc: 0.3575, precision: 0.7597, recall: 0.3960, f1: 0.5206, edges-pos-ontonotes_loss: 0.0449
09/16 07:12:42 AM: Update 8121: task edges-pos-ontonotes, batch 121 (8121): mcc: 0.5356, acc: 0.3517, precision: 0.7538, recall: 0.3898, f1: 0.5139, edges-pos-ontonotes_loss: 0.0454
09/16 07:12:53 AM: Update 8173: task edges-pos-ontonotes, batch 173 (8173): mcc: 0.5355, acc: 0.3520, precision: 0.7526, recall: 0.3903, f1: 0.5141, edges-pos-ontonotes_loss: 0.0454
09/16 07:13:03 AM: Update 8227: task edges-pos-ontonotes, batch 227 (8227): mcc: 0.5332, acc: 0.3498, precision: 0.7519, recall: 0.3874, f1: 0.5113, edges-pos-ontonotes_loss: 0.0457
09/16 07:13:13 AM: Update 8283: task edges-pos-ontonotes, batch 283 (8283): mcc: 0.5350, acc: 0.3517, precision: 0.7533, recall: 0.3893, f1: 0.5133, edges-pos-ontonotes_loss: 0.0455
09/16 07:13:23 AM: Update 8343: task edges-pos-ontonotes, batch 343 (8343): mcc: 0.5366, acc: 0.3530, precision: 0.7551, recall: 0.3906, f1: 0.5149, edges-pos-ontonotes_loss: 0.0454
09/16 07:13:33 AM: Update 8401: task edges-pos-ontonotes, batch 401 (8401): mcc: 0.5386, acc: 0.3550, precision: 0.7566, recall: 0.3928, f1: 0.5171, edges-pos-ontonotes_loss: 0.0452
09/16 07:13:43 AM: Update 8459: task edges-pos-ontonotes, batch 459 (8459): mcc: 0.5403, acc: 0.3567, precision: 0.7579, recall: 0.3945, f1: 0.5189, edges-pos-ontonotes_loss: 0.0452
09/16 07:13:53 AM: Update 8503: task edges-pos-ontonotes, batch 503 (8503): mcc: 0.5400, acc: 0.3564, precision: 0.7572, recall: 0.3944, f1: 0.5186, edges-pos-ontonotes_loss: 0.0452
09/16 07:14:03 AM: Update 8557: task edges-pos-ontonotes, batch 557 (8557): mcc: 0.5400, acc: 0.3563, precision: 0.7572, recall: 0.3943, f1: 0.5186, edges-pos-ontonotes_loss: 0.0452
09/16 07:14:13 AM: Update 8614: task edges-pos-ontonotes, batch 614 (8614): mcc: 0.5415, acc: 0.3580, precision: 0.7583, recall: 0.3960, f1: 0.5203, edges-pos-ontonotes_loss: 0.0452
09/16 07:14:23 AM: Update 8671: task edges-pos-ontonotes, batch 671 (8671): mcc: 0.5429, acc: 0.3593, precision: 0.7594, recall: 0.3974, f1: 0.5218, edges-pos-ontonotes_loss: 0.0450
09/16 07:14:33 AM: Update 8732: task edges-pos-ontonotes, batch 732 (8732): mcc: 0.5443, acc: 0.3608, precision: 0.7606, recall: 0.3988, f1: 0.5233, edges-pos-ontonotes_loss: 0.0449
09/16 07:14:43 AM: Update 8787: task edges-pos-ontonotes, batch 787 (8787): mcc: 0.5435, acc: 0.3600, precision: 0.7598, recall: 0.3980, f1: 0.5224, edges-pos-ontonotes_loss: 0.0449
09/16 07:14:53 AM: Update 8833: task edges-pos-ontonotes, batch 833 (8833): mcc: 0.5418, acc: 0.3587, precision: 0.7576, recall: 0.3967, f1: 0.5207, edges-pos-ontonotes_loss: 0.0449
09/16 07:15:03 AM: Update 8897: task edges-pos-ontonotes, batch 897 (8897): mcc: 0.5420, acc: 0.3591, precision: 0.7574, recall: 0.3972, f1: 0.5211, edges-pos-ontonotes_loss: 0.0446
09/16 07:15:13 AM: Update 8965: task edges-pos-ontonotes, batch 965 (8965): mcc: 0.5430, acc: 0.3602, precision: 0.7578, recall: 0.3984, f1: 0.5223, edges-pos-ontonotes_loss: 0.0443
09/16 07:15:18 AM: ***** Step 9000 / Validation 9 *****
09/16 07:15:18 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:15:18 AM: Validating...
09/16 07:15:23 AM: Evaluate: task edges-pos-ontonotes, batch 37 (157): mcc: 0.6544, acc: 0.4645, precision: 0.9064, recall: 0.4793, f1: 0.6270, edges-pos-ontonotes_loss: 0.0354
09/16 07:15:34 AM: Evaluate: task edges-pos-ontonotes, batch 100 (157): mcc: 0.6650, acc: 0.4781, precision: 0.9088, recall: 0.4935, f1: 0.6396, edges-pos-ontonotes_loss: 0.0340
09/16 07:15:44 AM: Evaluate: task edges-pos-ontonotes, batch 146 (157): mcc: 0.6444, acc: 0.4533, precision: 0.8948, recall: 0.4711, f1: 0.6172, edges-pos-ontonotes_loss: 0.0351
09/16 07:15:46 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:15:46 AM: Best result seen so far for macro.
09/16 07:15:46 AM: Updating LR scheduler:
09/16 07:15:46 AM: 	Best result seen so far for macro_avg: 0.619
09/16 07:15:46 AM: 	# validation passes without improvement: 0
09/16 07:15:46 AM: edges-pos-ontonotes_loss: training: 0.044087 validation: 0.035017
09/16 07:15:46 AM: macro_avg: validation: 0.619496
09/16 07:15:46 AM: micro_avg: validation: 0.000000
09/16 07:15:46 AM: edges-pos-ontonotes_mcc: training: 0.543864 validation: 0.646318
09/16 07:15:46 AM: edges-pos-ontonotes_acc: training: 0.361079 validation: 0.455454
09/16 07:15:46 AM: edges-pos-ontonotes_precision: training: 0.758383 validation: 0.895329
09/16 07:15:46 AM: edges-pos-ontonotes_recall: training: 0.399349 validation: 0.473592
09/16 07:15:46 AM: edges-pos-ontonotes_f1: training: 0.523194 validation: 0.619496
09/16 07:15:46 AM: Global learning rate: 0.0001
09/16 07:15:46 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:15:54 AM: Update 9059: task edges-pos-ontonotes, batch 59 (9059): mcc: 0.5714, acc: 0.3929, precision: 0.7682, recall: 0.4346, f1: 0.5551, edges-pos-ontonotes_loss: 0.0392
09/16 07:16:11 AM: Update 9112: task edges-pos-ontonotes, batch 112 (9112): mcc: 0.5716, acc: 0.3933, precision: 0.7690, recall: 0.4343, f1: 0.5551, edges-pos-ontonotes_loss: 0.0393
09/16 07:16:21 AM: Update 9209: task edges-pos-ontonotes, batch 209 (9209): mcc: 0.6118, acc: 0.4340, precision: 0.8018, recall: 0.4759, f1: 0.5972, edges-pos-ontonotes_loss: 0.0364
09/16 07:16:31 AM: Update 9303: task edges-pos-ontonotes, batch 303 (9303): mcc: 0.6296, acc: 0.4531, precision: 0.8148, recall: 0.4953, f1: 0.6161, edges-pos-ontonotes_loss: 0.0352
09/16 07:16:41 AM: Update 9391: task edges-pos-ontonotes, batch 391 (9391): mcc: 0.6404, acc: 0.4650, precision: 0.8232, recall: 0.5069, f1: 0.6275, edges-pos-ontonotes_loss: 0.0346
09/16 07:16:51 AM: Update 9476: task edges-pos-ontonotes, batch 476 (9476): mcc: 0.6457, acc: 0.4714, precision: 0.8267, recall: 0.5130, f1: 0.6331, edges-pos-ontonotes_loss: 0.0344
09/16 07:17:01 AM: Update 9581: task edges-pos-ontonotes, batch 581 (9581): mcc: 0.6516, acc: 0.4787, precision: 0.8301, recall: 0.5200, f1: 0.6395, edges-pos-ontonotes_loss: 0.0341
09/16 07:17:11 AM: Update 9700: task edges-pos-ontonotes, batch 700 (9700): mcc: 0.6570, acc: 0.4856, precision: 0.8332, recall: 0.5265, f1: 0.6453, edges-pos-ontonotes_loss: 0.0338
09/16 07:17:21 AM: Update 9811: task edges-pos-ontonotes, batch 811 (9811): mcc: 0.6524, acc: 0.4808, precision: 0.8295, recall: 0.5217, f1: 0.6405, edges-pos-ontonotes_loss: 0.0344
09/16 07:17:31 AM: Update 9958: task edges-pos-ontonotes, batch 958 (9958): mcc: 0.6461, acc: 0.4738, precision: 0.8259, recall: 0.5142, f1: 0.6338, edges-pos-ontonotes_loss: 0.0352
09/16 07:17:34 AM: ***** Step 10000 / Validation 10 *****
09/16 07:17:34 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:17:34 AM: Validating...
09/16 07:17:41 AM: Evaluate: task edges-pos-ontonotes, batch 49 (157): mcc: 0.6543, acc: 0.4678, precision: 0.8799, recall: 0.4939, f1: 0.6327, edges-pos-ontonotes_loss: 0.0345
09/16 07:17:51 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.6630, acc: 0.4806, precision: 0.8765, recall: 0.5091, f1: 0.6441, edges-pos-ontonotes_loss: 0.0333
09/16 07:18:01 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.6405, acc: 0.4521, precision: 0.8611, recall: 0.4841, f1: 0.6198, edges-pos-ontonotes_loss: 0.0351
09/16 07:18:02 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:18:02 AM: Best result seen so far for macro.
09/16 07:18:02 AM: Updating LR scheduler:
09/16 07:18:02 AM: 	Best result seen so far for macro_avg: 0.620
09/16 07:18:02 AM: 	# validation passes without improvement: 0
09/16 07:18:02 AM: edges-pos-ontonotes_loss: training: 0.035320 validation: 0.035132
09/16 07:18:02 AM: macro_avg: validation: 0.620262
09/16 07:18:02 AM: micro_avg: validation: 0.000000
09/16 07:18:02 AM: edges-pos-ontonotes_mcc: training: 0.644766 validation: 0.640857
09/16 07:18:02 AM: edges-pos-ontonotes_acc: training: 0.472255 validation: 0.452448
09/16 07:18:02 AM: edges-pos-ontonotes_precision: training: 0.825028 validation: 0.861118
09/16 07:18:02 AM: edges-pos-ontonotes_recall: training: 0.512565 validation: 0.484693
09/16 07:18:02 AM: edges-pos-ontonotes_f1: training: 0.632300 validation: 0.620262
09/16 07:18:02 AM: Global learning rate: 0.0001
09/16 07:18:02 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:18:12 AM: Update 10066: task edges-pos-ontonotes, batch 66 (10066): mcc: 0.5097, acc: 0.3351, precision: 0.7038, recall: 0.3796, f1: 0.4932, edges-pos-ontonotes_loss: 0.0436
09/16 07:18:22 AM: Update 10127: task edges-pos-ontonotes, batch 127 (10127): mcc: 0.5138, acc: 0.3349, precision: 0.7157, recall: 0.3789, f1: 0.4955, edges-pos-ontonotes_loss: 0.0452
09/16 07:18:32 AM: Update 10187: task edges-pos-ontonotes, batch 187 (10187): mcc: 0.5140, acc: 0.3339, precision: 0.7183, recall: 0.3778, f1: 0.4952, edges-pos-ontonotes_loss: 0.0455
09/16 07:18:42 AM: Update 10252: task edges-pos-ontonotes, batch 252 (10252): mcc: 0.5170, acc: 0.3361, precision: 0.7234, recall: 0.3794, f1: 0.4977, edges-pos-ontonotes_loss: 0.0456
09/16 07:18:52 AM: Update 10315: task edges-pos-ontonotes, batch 315 (10315): mcc: 0.5181, acc: 0.3366, precision: 0.7265, recall: 0.3793, f1: 0.4984, edges-pos-ontonotes_loss: 0.0457
09/16 07:19:02 AM: Update 10373: task edges-pos-ontonotes, batch 373 (10373): mcc: 0.5203, acc: 0.3383, precision: 0.7296, recall: 0.3808, f1: 0.5004, edges-pos-ontonotes_loss: 0.0457
09/16 07:19:12 AM: Update 10445: task edges-pos-ontonotes, batch 445 (10445): mcc: 0.5259, acc: 0.3437, precision: 0.7365, recall: 0.3852, f1: 0.5058, edges-pos-ontonotes_loss: 0.0447
09/16 07:19:22 AM: Update 10533: task edges-pos-ontonotes, batch 533 (10533): mcc: 0.5337, acc: 0.3515, precision: 0.7448, recall: 0.3920, f1: 0.5136, edges-pos-ontonotes_loss: 0.0436
09/16 07:19:32 AM: Update 10619: task edges-pos-ontonotes, batch 619 (10619): mcc: 0.5391, acc: 0.3570, precision: 0.7502, recall: 0.3969, f1: 0.5191, edges-pos-ontonotes_loss: 0.0428
09/16 07:19:42 AM: Update 10690: task edges-pos-ontonotes, batch 690 (10690): mcc: 0.5429, acc: 0.3610, precision: 0.7538, recall: 0.4004, f1: 0.5230, edges-pos-ontonotes_loss: 0.0420
09/16 07:19:52 AM: Update 10737: task edges-pos-ontonotes, batch 737 (10737): mcc: 0.5451, acc: 0.3633, precision: 0.7546, recall: 0.4032, f1: 0.5256, edges-pos-ontonotes_loss: 0.0419
09/16 07:20:02 AM: Update 10785: task edges-pos-ontonotes, batch 785 (10785): mcc: 0.5470, acc: 0.3653, precision: 0.7553, recall: 0.4056, f1: 0.5278, edges-pos-ontonotes_loss: 0.0417
09/16 07:20:12 AM: Update 10844: task edges-pos-ontonotes, batch 844 (10844): mcc: 0.5505, acc: 0.3689, precision: 0.7570, recall: 0.4097, f1: 0.5317, edges-pos-ontonotes_loss: 0.0415
09/16 07:20:22 AM: Update 10897: task edges-pos-ontonotes, batch 897 (10897): mcc: 0.5530, acc: 0.3715, precision: 0.7582, recall: 0.4128, f1: 0.5346, edges-pos-ontonotes_loss: 0.0413
09/16 07:20:33 AM: Update 10950: task edges-pos-ontonotes, batch 950 (10950): mcc: 0.5552, acc: 0.3739, precision: 0.7592, recall: 0.4155, f1: 0.5371, edges-pos-ontonotes_loss: 0.0411
09/16 07:20:41 AM: ***** Step 11000 / Validation 11 *****
09/16 07:20:41 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:20:41 AM: Validating...
09/16 07:20:43 AM: Evaluate: task edges-pos-ontonotes, batch 11 (157): mcc: 0.6885, acc: 0.5152, precision: 0.8754, recall: 0.5491, f1: 0.6749, edges-pos-ontonotes_loss: 0.0320
09/16 07:20:53 AM: Evaluate: task edges-pos-ontonotes, batch 70 (157): mcc: 0.6973, acc: 0.5247, precision: 0.8952, recall: 0.5503, f1: 0.6816, edges-pos-ontonotes_loss: 0.0312
09/16 07:21:03 AM: Evaluate: task edges-pos-ontonotes, batch 115 (157): mcc: 0.6839, acc: 0.5082, precision: 0.8848, recall: 0.5361, f1: 0.6676, edges-pos-ontonotes_loss: 0.0319
09/16 07:21:13 AM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.6667, acc: 0.4854, precision: 0.8757, recall: 0.5151, f1: 0.6487, edges-pos-ontonotes_loss: 0.0333
09/16 07:21:13 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:21:13 AM: Best result seen so far for macro.
09/16 07:21:13 AM: Updating LR scheduler:
09/16 07:21:13 AM: 	Best result seen so far for macro_avg: 0.649
09/16 07:21:13 AM: 	# validation passes without improvement: 0
09/16 07:21:13 AM: edges-pos-ontonotes_loss: training: 0.040913 validation: 0.033277
09/16 07:21:13 AM: macro_avg: validation: 0.648730
09/16 07:21:13 AM: micro_avg: validation: 0.000000
09/16 07:21:13 AM: edges-pos-ontonotes_mcc: training: 0.557660 validation: 0.666750
09/16 07:21:13 AM: edges-pos-ontonotes_acc: training: 0.376556 validation: 0.485402
09/16 07:21:13 AM: edges-pos-ontonotes_precision: training: 0.760716 validation: 0.875751
09/16 07:21:13 AM: edges-pos-ontonotes_recall: training: 0.418301 validation: 0.515180
09/16 07:21:13 AM: edges-pos-ontonotes_f1: training: 0.539786 validation: 0.648730
09/16 07:21:13 AM: Global learning rate: 0.0001
09/16 07:21:13 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:21:23 AM: Update 11037: task edges-pos-ontonotes, batch 37 (11037): mcc: 0.5346, acc: 0.3582, precision: 0.7281, recall: 0.4026, f1: 0.5185, edges-pos-ontonotes_loss: 0.0448
09/16 07:21:33 AM: Update 11084: task edges-pos-ontonotes, batch 84 (11084): mcc: 0.5335, acc: 0.3557, precision: 0.7298, recall: 0.4001, f1: 0.5168, edges-pos-ontonotes_loss: 0.0452
09/16 07:21:44 AM: Update 11128: task edges-pos-ontonotes, batch 128 (11128): mcc: 0.5333, acc: 0.3545, precision: 0.7320, recall: 0.3985, f1: 0.5161, edges-pos-ontonotes_loss: 0.0454
09/16 07:21:54 AM: Update 11171: task edges-pos-ontonotes, batch 171 (11171): mcc: 0.5331, acc: 0.3541, precision: 0.7317, recall: 0.3984, f1: 0.5159, edges-pos-ontonotes_loss: 0.0454
09/16 07:22:04 AM: Update 11220: task edges-pos-ontonotes, batch 220 (11220): mcc: 0.5318, acc: 0.3528, precision: 0.7314, recall: 0.3966, f1: 0.5143, edges-pos-ontonotes_loss: 0.0456
09/16 07:22:14 AM: Update 11265: task edges-pos-ontonotes, batch 265 (11265): mcc: 0.5308, acc: 0.3518, precision: 0.7309, recall: 0.3954, f1: 0.5132, edges-pos-ontonotes_loss: 0.0454
09/16 07:22:24 AM: Update 11314: task edges-pos-ontonotes, batch 314 (11314): mcc: 0.5328, acc: 0.3536, precision: 0.7327, recall: 0.3974, f1: 0.5153, edges-pos-ontonotes_loss: 0.0453
09/16 07:22:36 AM: Update 11320: task edges-pos-ontonotes, batch 320 (11320): mcc: 0.5317, acc: 0.3526, precision: 0.7314, recall: 0.3965, f1: 0.5142, edges-pos-ontonotes_loss: 0.0453
09/16 07:22:46 AM: Update 11365: task edges-pos-ontonotes, batch 365 (11365): mcc: 0.5336, acc: 0.3544, precision: 0.7330, recall: 0.3983, f1: 0.5162, edges-pos-ontonotes_loss: 0.0452
09/16 07:22:56 AM: Update 11410: task edges-pos-ontonotes, batch 410 (11410): mcc: 0.5349, acc: 0.3555, precision: 0.7344, recall: 0.3995, f1: 0.5175, edges-pos-ontonotes_loss: 0.0452
09/16 07:23:06 AM: Update 11450: task edges-pos-ontonotes, batch 450 (11450): mcc: 0.5349, acc: 0.3556, precision: 0.7345, recall: 0.3994, f1: 0.5175, edges-pos-ontonotes_loss: 0.0452
09/16 07:23:16 AM: Update 11492: task edges-pos-ontonotes, batch 492 (11492): mcc: 0.5355, acc: 0.3562, precision: 0.7356, recall: 0.3997, f1: 0.5179, edges-pos-ontonotes_loss: 0.0452
09/16 07:23:26 AM: Update 11535: task edges-pos-ontonotes, batch 535 (11535): mcc: 0.5368, acc: 0.3574, precision: 0.7372, recall: 0.4007, f1: 0.5192, edges-pos-ontonotes_loss: 0.0451
09/16 07:23:36 AM: Update 11580: task edges-pos-ontonotes, batch 580 (11580): mcc: 0.5376, acc: 0.3583, precision: 0.7381, recall: 0.4014, f1: 0.5200, edges-pos-ontonotes_loss: 0.0450
09/16 07:23:47 AM: Update 11623: task edges-pos-ontonotes, batch 623 (11623): mcc: 0.5386, acc: 0.3592, precision: 0.7393, recall: 0.4022, f1: 0.5210, edges-pos-ontonotes_loss: 0.0449
09/16 07:23:57 AM: Update 11662: task edges-pos-ontonotes, batch 662 (11662): mcc: 0.5393, acc: 0.3597, precision: 0.7401, recall: 0.4027, f1: 0.5216, edges-pos-ontonotes_loss: 0.0449
09/16 07:24:07 AM: Update 11708: task edges-pos-ontonotes, batch 708 (11708): mcc: 0.5395, acc: 0.3599, precision: 0.7407, recall: 0.4028, f1: 0.5218, edges-pos-ontonotes_loss: 0.0449
09/16 07:24:17 AM: Update 11754: task edges-pos-ontonotes, batch 754 (11754): mcc: 0.5408, acc: 0.3611, precision: 0.7422, recall: 0.4039, f1: 0.5231, edges-pos-ontonotes_loss: 0.0448
09/16 07:24:27 AM: Update 11799: task edges-pos-ontonotes, batch 799 (11799): mcc: 0.5423, acc: 0.3625, precision: 0.7437, recall: 0.4052, f1: 0.5245, edges-pos-ontonotes_loss: 0.0447
09/16 07:24:38 AM: Update 11844: task edges-pos-ontonotes, batch 844 (11844): mcc: 0.5433, acc: 0.3635, precision: 0.7446, recall: 0.4061, f1: 0.5256, edges-pos-ontonotes_loss: 0.0446
09/16 07:24:48 AM: Update 11888: task edges-pos-ontonotes, batch 888 (11888): mcc: 0.5433, acc: 0.3635, precision: 0.7447, recall: 0.4061, f1: 0.5256, edges-pos-ontonotes_loss: 0.0446
09/16 07:24:58 AM: Update 11942: task edges-pos-ontonotes, batch 942 (11942): mcc: 0.5441, acc: 0.3643, precision: 0.7455, recall: 0.4069, f1: 0.5264, edges-pos-ontonotes_loss: 0.0446
09/16 07:25:08 AM: Update 11986: task edges-pos-ontonotes, batch 986 (11986): mcc: 0.5442, acc: 0.3644, precision: 0.7455, recall: 0.4069, f1: 0.5265, edges-pos-ontonotes_loss: 0.0446
09/16 07:25:11 AM: ***** Step 12000 / Validation 12 *****
09/16 07:25:11 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:25:11 AM: Validating...
09/16 07:25:18 AM: Evaluate: task edges-pos-ontonotes, batch 52 (157): mcc: 0.6765, acc: 0.4915, precision: 0.9101, recall: 0.5098, f1: 0.6535, edges-pos-ontonotes_loss: 0.0338
09/16 07:25:28 AM: Evaluate: task edges-pos-ontonotes, batch 111 (157): mcc: 0.6803, acc: 0.4989, precision: 0.9042, recall: 0.5188, f1: 0.6593, edges-pos-ontonotes_loss: 0.0324
09/16 07:25:38 AM: Updating LR scheduler:
09/16 07:25:38 AM: 	Best result seen so far for macro_avg: 0.649
09/16 07:25:38 AM: 	# validation passes without improvement: 1
09/16 07:25:38 AM: edges-pos-ontonotes_loss: training: 0.044553 validation: 0.033264
09/16 07:25:38 AM: macro_avg: validation: 0.643800
09/16 07:25:38 AM: micro_avg: validation: 0.000000
09/16 07:25:38 AM: edges-pos-ontonotes_mcc: training: 0.544168 validation: 0.666307
09/16 07:25:38 AM: edges-pos-ontonotes_acc: training: 0.364382 validation: 0.480735
09/16 07:25:38 AM: edges-pos-ontonotes_precision: training: 0.745560 validation: 0.896823
09/16 07:25:38 AM: edges-pos-ontonotes_recall: training: 0.406902 validation: 0.502132
09/16 07:25:38 AM: edges-pos-ontonotes_f1: training: 0.526473 validation: 0.643800
09/16 07:25:38 AM: Global learning rate: 0.0001
09/16 07:25:38 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:25:38 AM: Update 12001: task edges-pos-ontonotes, batch 1 (12001): mcc: 0.6773, acc: 0.5057, precision: 0.8670, recall: 0.5369, f1: 0.6632, edges-pos-ontonotes_loss: 0.0345
09/16 07:25:48 AM: Update 12058: task edges-pos-ontonotes, batch 58 (12058): mcc: 0.5562, acc: 0.3766, precision: 0.7585, recall: 0.4174, f1: 0.5385, edges-pos-ontonotes_loss: 0.0439
09/16 07:25:58 AM: Update 12115: task edges-pos-ontonotes, batch 115 (12115): mcc: 0.5590, acc: 0.3792, precision: 0.7616, recall: 0.4198, f1: 0.5412, edges-pos-ontonotes_loss: 0.0436
09/16 07:26:08 AM: Update 12176: task edges-pos-ontonotes, batch 176 (12176): mcc: 0.5608, acc: 0.3808, precision: 0.7631, recall: 0.4216, f1: 0.5431, edges-pos-ontonotes_loss: 0.0433
09/16 07:26:19 AM: Update 12234: task edges-pos-ontonotes, batch 234 (12234): mcc: 0.5632, acc: 0.3833, precision: 0.7660, recall: 0.4235, f1: 0.5455, edges-pos-ontonotes_loss: 0.0433
09/16 07:26:29 AM: Update 12277: task edges-pos-ontonotes, batch 277 (12277): mcc: 0.5568, acc: 0.3775, precision: 0.7588, recall: 0.4181, f1: 0.5391, edges-pos-ontonotes_loss: 0.0433
09/16 07:26:39 AM: Update 12347: task edges-pos-ontonotes, batch 347 (12347): mcc: 0.5594, acc: 0.3801, precision: 0.7597, recall: 0.4214, f1: 0.5421, edges-pos-ontonotes_loss: 0.0424
09/16 07:26:49 AM: Update 12422: task edges-pos-ontonotes, batch 422 (12422): mcc: 0.5626, acc: 0.3835, precision: 0.7616, recall: 0.4252, f1: 0.5457, edges-pos-ontonotes_loss: 0.0416
09/16 07:26:59 AM: Update 12492: task edges-pos-ontonotes, batch 492 (12492): mcc: 0.5640, acc: 0.3852, precision: 0.7620, recall: 0.4269, f1: 0.5473, edges-pos-ontonotes_loss: 0.0411
09/16 07:27:10 AM: Update 12557: task edges-pos-ontonotes, batch 557 (12557): mcc: 0.5638, acc: 0.3854, precision: 0.7609, recall: 0.4273, f1: 0.5473, edges-pos-ontonotes_loss: 0.0409
09/16 07:27:20 AM: Update 12627: task edges-pos-ontonotes, batch 627 (12627): mcc: 0.5714, acc: 0.3931, precision: 0.7668, recall: 0.4353, f1: 0.5554, edges-pos-ontonotes_loss: 0.0401
09/16 07:27:30 AM: Update 12722: task edges-pos-ontonotes, batch 722 (12722): mcc: 0.5826, acc: 0.4045, precision: 0.7754, recall: 0.4472, f1: 0.5672, edges-pos-ontonotes_loss: 0.0390
09/16 07:27:40 AM: Update 12811: task edges-pos-ontonotes, batch 811 (12811): mcc: 0.5917, acc: 0.4141, precision: 0.7822, recall: 0.4569, f1: 0.5769, edges-pos-ontonotes_loss: 0.0382
09/16 07:27:50 AM: Update 12896: task edges-pos-ontonotes, batch 896 (12896): mcc: 0.5984, acc: 0.4213, precision: 0.7871, recall: 0.4642, f1: 0.5840, edges-pos-ontonotes_loss: 0.0375
09/16 07:27:59 AM: ***** Step 13000 / Validation 13 *****
09/16 07:27:59 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:27:59 AM: Validating...
09/16 07:28:00 AM: Evaluate: task edges-pos-ontonotes, batch 6 (157): mcc: 0.6979, acc: 0.5257, precision: 0.8937, recall: 0.5523, f1: 0.6827, edges-pos-ontonotes_loss: 0.0299
09/16 07:28:10 AM: Evaluate: task edges-pos-ontonotes, batch 77 (157): mcc: 0.7041, acc: 0.5339, precision: 0.9071, recall: 0.5535, f1: 0.6875, edges-pos-ontonotes_loss: 0.0308
09/16 07:28:20 AM: Evaluate: task edges-pos-ontonotes, batch 128 (157): mcc: 0.6766, acc: 0.4984, precision: 0.8944, recall: 0.5190, f1: 0.6568, edges-pos-ontonotes_loss: 0.0324
09/16 07:28:26 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:28:26 AM: Best result seen so far for macro.
09/16 07:28:26 AM: Updating LR scheduler:
09/16 07:28:26 AM: 	Best result seen so far for macro_avg: 0.650
09/16 07:28:26 AM: 	# validation passes without improvement: 0
09/16 07:28:26 AM: edges-pos-ontonotes_loss: training: 0.036937 validation: 0.033054
09/16 07:28:26 AM: macro_avg: validation: 0.650043
09/16 07:28:26 AM: micro_avg: validation: 0.000000
09/16 07:28:26 AM: edges-pos-ontonotes_mcc: training: 0.605017 validation: 0.670516
09/16 07:28:26 AM: edges-pos-ontonotes_acc: training: 0.428680 validation: 0.490248
09/16 07:28:26 AM: edges-pos-ontonotes_precision: training: 0.791947 validation: 0.891588
09/16 07:28:26 AM: edges-pos-ontonotes_recall: training: 0.471445 validation: 0.511477
09/16 07:28:26 AM: edges-pos-ontonotes_f1: training: 0.591043 validation: 0.650043
09/16 07:28:26 AM: Global learning rate: 0.0001
09/16 07:28:26 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:28:30 AM: Update 13043: task edges-pos-ontonotes, batch 43 (13043): mcc: 0.6942, acc: 0.5374, precision: 0.8428, recall: 0.5802, f1: 0.6872, edges-pos-ontonotes_loss: 0.0316
09/16 07:28:40 AM: Update 13149: task edges-pos-ontonotes, batch 149 (13149): mcc: 0.6961, acc: 0.5375, precision: 0.8469, recall: 0.5804, f1: 0.6888, edges-pos-ontonotes_loss: 0.0319
09/16 07:28:50 AM: Update 13261: task edges-pos-ontonotes, batch 261 (13261): mcc: 0.6721, acc: 0.5089, precision: 0.8316, recall: 0.5518, f1: 0.6634, edges-pos-ontonotes_loss: 0.0336
09/16 07:29:00 AM: Update 13389: task edges-pos-ontonotes, batch 389 (13389): mcc: 0.6484, acc: 0.4813, precision: 0.8166, recall: 0.5237, f1: 0.6381, edges-pos-ontonotes_loss: 0.0356
09/16 07:29:19 AM: Update 13511: task edges-pos-ontonotes, batch 511 (13511): mcc: 0.6333, acc: 0.4651, precision: 0.8045, recall: 0.5077, f1: 0.6225, edges-pos-ontonotes_loss: 0.0363
09/16 07:29:29 AM: Update 13572: task edges-pos-ontonotes, batch 572 (13572): mcc: 0.6116, acc: 0.4399, precision: 0.7883, recall: 0.4839, f1: 0.5997, edges-pos-ontonotes_loss: 0.0373
09/16 07:29:39 AM: Update 13634: task edges-pos-ontonotes, batch 634 (13634): mcc: 0.5979, acc: 0.4240, precision: 0.7794, recall: 0.4682, f1: 0.5850, edges-pos-ontonotes_loss: 0.0381
09/16 07:29:49 AM: Update 13697: task edges-pos-ontonotes, batch 697 (13697): mcc: 0.5890, acc: 0.4137, precision: 0.7736, recall: 0.4580, f1: 0.5754, edges-pos-ontonotes_loss: 0.0387
09/16 07:29:59 AM: Update 13757: task edges-pos-ontonotes, batch 757 (13757): mcc: 0.5808, acc: 0.4044, precision: 0.7674, recall: 0.4491, f1: 0.5666, edges-pos-ontonotes_loss: 0.0392
09/16 07:30:09 AM: Update 13820: task edges-pos-ontonotes, batch 820 (13820): mcc: 0.5743, acc: 0.3973, precision: 0.7628, recall: 0.4421, f1: 0.5598, edges-pos-ontonotes_loss: 0.0397
09/16 07:30:19 AM: Update 13882: task edges-pos-ontonotes, batch 882 (13882): mcc: 0.5721, acc: 0.3947, precision: 0.7624, recall: 0.4390, f1: 0.5572, edges-pos-ontonotes_loss: 0.0398
09/16 07:30:29 AM: Update 13973: task edges-pos-ontonotes, batch 973 (13973): mcc: 0.5734, acc: 0.3958, precision: 0.7650, recall: 0.4394, f1: 0.5582, edges-pos-ontonotes_loss: 0.0396
09/16 07:30:32 AM: ***** Step 14000 / Validation 14 *****
09/16 07:30:32 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:30:32 AM: Validating...
09/16 07:30:39 AM: Evaluate: task edges-pos-ontonotes, batch 48 (157): mcc: 0.6892, acc: 0.5076, precision: 0.9143, recall: 0.5263, f1: 0.6680, edges-pos-ontonotes_loss: 0.0309
09/16 07:30:49 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.6848, acc: 0.5037, precision: 0.9067, recall: 0.5242, f1: 0.6643, edges-pos-ontonotes_loss: 0.0305
09/16 07:30:59 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.6597, acc: 0.4713, precision: 0.8937, recall: 0.4941, f1: 0.6364, edges-pos-ontonotes_loss: 0.0323
09/16 07:31:00 AM: Updating LR scheduler:
09/16 07:31:00 AM: 	Best result seen so far for macro_avg: 0.650
09/16 07:31:00 AM: 	# validation passes without improvement: 1
09/16 07:31:00 AM: edges-pos-ontonotes_loss: training: 0.039502 validation: 0.032364
09/16 07:31:00 AM: macro_avg: validation: 0.636772
09/16 07:31:00 AM: micro_avg: validation: 0.000000
09/16 07:31:00 AM: edges-pos-ontonotes_mcc: training: 0.573660 validation: 0.660045
09/16 07:31:00 AM: edges-pos-ontonotes_acc: training: 0.395949 validation: 0.471634
09/16 07:31:00 AM: edges-pos-ontonotes_precision: training: 0.765704 validation: 0.893798
09/16 07:31:00 AM: edges-pos-ontonotes_recall: training: 0.439381 validation: 0.494555
09/16 07:31:00 AM: edges-pos-ontonotes_f1: training: 0.558360 validation: 0.636772
09/16 07:31:00 AM: Global learning rate: 0.0001
09/16 07:31:00 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:31:10 AM: Update 14105: task edges-pos-ontonotes, batch 105 (14105): mcc: 0.6180, acc: 0.4416, precision: 0.8155, recall: 0.4771, f1: 0.6020, edges-pos-ontonotes_loss: 0.0352
09/16 07:31:20 AM: Update 14171: task edges-pos-ontonotes, batch 171 (14171): mcc: 0.6005, acc: 0.4235, precision: 0.7969, recall: 0.4616, f1: 0.5846, edges-pos-ontonotes_loss: 0.0363
09/16 07:31:30 AM: Update 14251: task edges-pos-ontonotes, batch 251 (14251): mcc: 0.5999, acc: 0.4233, precision: 0.7874, recall: 0.4663, f1: 0.5857, edges-pos-ontonotes_loss: 0.0367
09/16 07:31:40 AM: Update 14325: task edges-pos-ontonotes, batch 325 (14325): mcc: 0.5995, acc: 0.4234, precision: 0.7840, recall: 0.4679, f1: 0.5860, edges-pos-ontonotes_loss: 0.0370
09/16 07:31:50 AM: Update 14396: task edges-pos-ontonotes, batch 396 (14396): mcc: 0.5994, acc: 0.4235, precision: 0.7828, recall: 0.4684, f1: 0.5861, edges-pos-ontonotes_loss: 0.0371
09/16 07:32:00 AM: Update 14465: task edges-pos-ontonotes, batch 465 (14465): mcc: 0.5983, acc: 0.4224, precision: 0.7806, recall: 0.4681, f1: 0.5852, edges-pos-ontonotes_loss: 0.0372
09/16 07:32:10 AM: Update 14511: task edges-pos-ontonotes, batch 511 (14511): mcc: 0.5903, acc: 0.4140, precision: 0.7735, recall: 0.4601, f1: 0.5770, edges-pos-ontonotes_loss: 0.0379
09/16 07:32:20 AM: Update 14573: task edges-pos-ontonotes, batch 573 (14573): mcc: 0.5831, acc: 0.4063, precision: 0.7676, recall: 0.4527, f1: 0.5695, edges-pos-ontonotes_loss: 0.0387
09/16 07:32:30 AM: Update 14635: task edges-pos-ontonotes, batch 635 (14635): mcc: 0.5793, acc: 0.4021, precision: 0.7647, recall: 0.4486, f1: 0.5654, edges-pos-ontonotes_loss: 0.0392
09/16 07:32:40 AM: Update 14690: task edges-pos-ontonotes, batch 690 (14690): mcc: 0.5749, acc: 0.3976, precision: 0.7610, recall: 0.4442, f1: 0.5609, edges-pos-ontonotes_loss: 0.0396
09/16 07:32:51 AM: Update 14746: task edges-pos-ontonotes, batch 746 (14746): mcc: 0.5719, acc: 0.3942, precision: 0.7591, recall: 0.4406, f1: 0.5576, edges-pos-ontonotes_loss: 0.0400
09/16 07:33:01 AM: Update 14791: task edges-pos-ontonotes, batch 791 (14791): mcc: 0.5695, acc: 0.3916, precision: 0.7572, recall: 0.4381, f1: 0.5551, edges-pos-ontonotes_loss: 0.0402
09/16 07:33:11 AM: Update 14845: task edges-pos-ontonotes, batch 845 (14845): mcc: 0.5675, acc: 0.3895, precision: 0.7558, recall: 0.4360, f1: 0.5530, edges-pos-ontonotes_loss: 0.0405
09/16 07:33:21 AM: Update 14904: task edges-pos-ontonotes, batch 904 (14904): mcc: 0.5671, acc: 0.3890, precision: 0.7555, recall: 0.4354, f1: 0.5525, edges-pos-ontonotes_loss: 0.0407
09/16 07:33:31 AM: Update 14955: task edges-pos-ontonotes, batch 955 (14955): mcc: 0.5654, acc: 0.3873, precision: 0.7542, recall: 0.4337, f1: 0.5507, edges-pos-ontonotes_loss: 0.0409
09/16 07:33:39 AM: ***** Step 15000 / Validation 15 *****
09/16 07:33:39 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:33:39 AM: Validating...
09/16 07:33:41 AM: Evaluate: task edges-pos-ontonotes, batch 17 (157): mcc: 0.6889, acc: 0.5091, precision: 0.9036, recall: 0.5321, f1: 0.6698, edges-pos-ontonotes_loss: 0.0317
09/16 07:33:51 AM: Evaluate: task edges-pos-ontonotes, batch 87 (157): mcc: 0.7019, acc: 0.5274, precision: 0.9112, recall: 0.5475, f1: 0.6840, edges-pos-ontonotes_loss: 0.0307
09/16 07:34:02 AM: Evaluate: task edges-pos-ontonotes, batch 134 (157): mcc: 0.6751, acc: 0.4926, precision: 0.8996, recall: 0.5137, f1: 0.6540, edges-pos-ontonotes_loss: 0.0320
09/16 07:34:06 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:34:06 AM: Best result seen so far for macro.
09/16 07:34:06 AM: Updating LR scheduler:
09/16 07:34:06 AM: 	Best result seen so far for macro_avg: 0.653
09/16 07:34:06 AM: 	# validation passes without improvement: 0
09/16 07:34:06 AM: edges-pos-ontonotes_loss: training: 0.040965 validation: 0.032225
09/16 07:34:06 AM: macro_avg: validation: 0.652686
09/16 07:34:06 AM: micro_avg: validation: 0.000000
09/16 07:34:06 AM: edges-pos-ontonotes_mcc: training: 0.565564 validation: 0.673965
09/16 07:34:06 AM: edges-pos-ontonotes_acc: training: 0.387440 validation: 0.490841
09/16 07:34:06 AM: edges-pos-ontonotes_precision: training: 0.754497 validation: 0.899047
09/16 07:34:06 AM: edges-pos-ontonotes_recall: training: 0.433761 validation: 0.512302
09/16 07:34:06 AM: edges-pos-ontonotes_f1: training: 0.550842 validation: 0.652686
09/16 07:34:06 AM: Global learning rate: 0.0001
09/16 07:34:06 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:34:12 AM: Update 15028: task edges-pos-ontonotes, batch 28 (15028): mcc: 0.5443, acc: 0.3652, precision: 0.7368, recall: 0.4122, f1: 0.5286, edges-pos-ontonotes_loss: 0.0453
09/16 07:34:22 AM: Update 15084: task edges-pos-ontonotes, batch 84 (15084): mcc: 0.5492, acc: 0.3715, precision: 0.7431, recall: 0.4158, f1: 0.5332, edges-pos-ontonotes_loss: 0.0446
09/16 07:34:33 AM: Update 15093: task edges-pos-ontonotes, batch 93 (15093): mcc: 0.5479, acc: 0.3703, precision: 0.7414, recall: 0.4149, f1: 0.5320, edges-pos-ontonotes_loss: 0.0446
09/16 07:34:44 AM: Update 15148: task edges-pos-ontonotes, batch 148 (15148): mcc: 0.5513, acc: 0.3737, precision: 0.7462, recall: 0.4171, f1: 0.5351, edges-pos-ontonotes_loss: 0.0441
09/16 07:34:54 AM: Update 15202: task edges-pos-ontonotes, batch 202 (15202): mcc: 0.5535, acc: 0.3752, precision: 0.7493, recall: 0.4186, f1: 0.5371, edges-pos-ontonotes_loss: 0.0439
09/16 07:35:04 AM: Update 15255: task edges-pos-ontonotes, batch 255 (15255): mcc: 0.5546, acc: 0.3762, precision: 0.7505, recall: 0.4196, f1: 0.5382, edges-pos-ontonotes_loss: 0.0439
09/16 07:35:14 AM: Update 15311: task edges-pos-ontonotes, batch 311 (15311): mcc: 0.5563, acc: 0.3781, precision: 0.7519, recall: 0.4214, f1: 0.5401, edges-pos-ontonotes_loss: 0.0437
09/16 07:35:24 AM: Update 15372: task edges-pos-ontonotes, batch 372 (15372): mcc: 0.5587, acc: 0.3805, precision: 0.7538, recall: 0.4238, f1: 0.5426, edges-pos-ontonotes_loss: 0.0435
09/16 07:35:34 AM: Update 15415: task edges-pos-ontonotes, batch 415 (15415): mcc: 0.5580, acc: 0.3797, precision: 0.7538, recall: 0.4228, f1: 0.5417, edges-pos-ontonotes_loss: 0.0435
09/16 07:35:44 AM: Update 15470: task edges-pos-ontonotes, batch 470 (15470): mcc: 0.5588, acc: 0.3805, precision: 0.7542, recall: 0.4238, f1: 0.5426, edges-pos-ontonotes_loss: 0.0435
09/16 07:35:54 AM: Update 15525: task edges-pos-ontonotes, batch 525 (15525): mcc: 0.5602, acc: 0.3820, precision: 0.7553, recall: 0.4251, f1: 0.5440, edges-pos-ontonotes_loss: 0.0434
09/16 07:36:05 AM: Update 15580: task edges-pos-ontonotes, batch 580 (15580): mcc: 0.5605, acc: 0.3823, precision: 0.7557, recall: 0.4254, f1: 0.5444, edges-pos-ontonotes_loss: 0.0433
09/16 07:36:15 AM: Update 15634: task edges-pos-ontonotes, batch 634 (15634): mcc: 0.5608, acc: 0.3827, precision: 0.7559, recall: 0.4258, f1: 0.5448, edges-pos-ontonotes_loss: 0.0433
09/16 07:36:25 AM: Update 15681: task edges-pos-ontonotes, batch 681 (15681): mcc: 0.5614, acc: 0.3833, precision: 0.7564, recall: 0.4263, f1: 0.5453, edges-pos-ontonotes_loss: 0.0432
09/16 07:36:35 AM: Update 15719: task edges-pos-ontonotes, batch 719 (15719): mcc: 0.5611, acc: 0.3832, precision: 0.7560, recall: 0.4261, f1: 0.5451, edges-pos-ontonotes_loss: 0.0433
09/16 07:36:45 AM: Update 15764: task edges-pos-ontonotes, batch 764 (15764): mcc: 0.5597, acc: 0.3818, precision: 0.7546, recall: 0.4248, f1: 0.5436, edges-pos-ontonotes_loss: 0.0430
09/16 07:36:55 AM: Update 15816: task edges-pos-ontonotes, batch 816 (15816): mcc: 0.5606, acc: 0.3829, precision: 0.7551, recall: 0.4260, f1: 0.5447, edges-pos-ontonotes_loss: 0.0427
09/16 07:37:05 AM: Update 15864: task edges-pos-ontonotes, batch 864 (15864): mcc: 0.5607, acc: 0.3830, precision: 0.7548, recall: 0.4263, f1: 0.5449, edges-pos-ontonotes_loss: 0.0425
09/16 07:37:15 AM: Update 15915: task edges-pos-ontonotes, batch 915 (15915): mcc: 0.5618, acc: 0.3842, precision: 0.7556, recall: 0.4275, f1: 0.5460, edges-pos-ontonotes_loss: 0.0422
09/16 07:37:25 AM: Update 15972: task edges-pos-ontonotes, batch 972 (15972): mcc: 0.5635, acc: 0.3858, precision: 0.7568, recall: 0.4293, f1: 0.5478, edges-pos-ontonotes_loss: 0.0419
09/16 07:37:30 AM: ***** Step 16000 / Validation 16 *****
09/16 07:37:30 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:37:30 AM: Validating...
09/16 07:37:35 AM: Evaluate: task edges-pos-ontonotes, batch 31 (157): mcc: 0.6833, acc: 0.5005, precision: 0.9106, recall: 0.5196, f1: 0.6616, edges-pos-ontonotes_loss: 0.0326
09/16 07:37:45 AM: Evaluate: task edges-pos-ontonotes, batch 88 (157): mcc: 0.6979, acc: 0.5202, precision: 0.9162, recall: 0.5384, f1: 0.6782, edges-pos-ontonotes_loss: 0.0309
09/16 07:37:56 AM: Evaluate: task edges-pos-ontonotes, batch 129 (157): mcc: 0.6746, acc: 0.4910, precision: 0.9030, recall: 0.5110, f1: 0.6527, edges-pos-ontonotes_loss: 0.0320
09/16 07:38:02 AM: Updating LR scheduler:
09/16 07:38:02 AM: 	Best result seen so far for macro_avg: 0.653
09/16 07:38:02 AM: 	# validation passes without improvement: 1
09/16 07:38:02 AM: edges-pos-ontonotes_loss: training: 0.041755 validation: 0.032299
09/16 07:38:02 AM: macro_avg: validation: 0.649305
09/16 07:38:02 AM: micro_avg: validation: 0.000000
09/16 07:38:02 AM: edges-pos-ontonotes_mcc: training: 0.564395 validation: 0.671473
09/16 07:38:02 AM: edges-pos-ontonotes_acc: training: 0.386835 validation: 0.486904
09/16 07:38:02 AM: edges-pos-ontonotes_precision: training: 0.757326 validation: 0.900659
09/16 07:38:02 AM: edges-pos-ontonotes_recall: training: 0.430326 validation: 0.507635
09/16 07:38:02 AM: edges-pos-ontonotes_f1: training: 0.548809 validation: 0.649305
09/16 07:38:02 AM: Global learning rate: 0.0001
09/16 07:38:02 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:38:06 AM: Update 16020: task edges-pos-ontonotes, batch 20 (16020): mcc: 0.5854, acc: 0.4092, precision: 0.7756, recall: 0.4512, f1: 0.5705, edges-pos-ontonotes_loss: 0.0383
09/16 07:38:16 AM: Update 16077: task edges-pos-ontonotes, batch 77 (16077): mcc: 0.6268, acc: 0.4529, precision: 0.8029, recall: 0.4984, f1: 0.6150, edges-pos-ontonotes_loss: 0.0346
09/16 07:38:26 AM: Update 16143: task edges-pos-ontonotes, batch 143 (16143): mcc: 0.6550, acc: 0.4838, precision: 0.8242, recall: 0.5294, f1: 0.6447, edges-pos-ontonotes_loss: 0.0327
09/16 07:38:36 AM: Update 16211: task edges-pos-ontonotes, batch 211 (16211): mcc: 0.6694, acc: 0.5002, precision: 0.8334, recall: 0.5463, f1: 0.6600, edges-pos-ontonotes_loss: 0.0320
09/16 07:38:46 AM: Update 16282: task edges-pos-ontonotes, batch 282 (16282): mcc: 0.6773, acc: 0.5095, precision: 0.8387, recall: 0.5554, f1: 0.6683, edges-pos-ontonotes_loss: 0.0316
09/16 07:38:57 AM: Update 16345: task edges-pos-ontonotes, batch 345 (16345): mcc: 0.6790, acc: 0.5116, precision: 0.8394, recall: 0.5578, f1: 0.6702, edges-pos-ontonotes_loss: 0.0315
09/16 07:39:07 AM: Update 16420: task edges-pos-ontonotes, batch 420 (16420): mcc: 0.6801, acc: 0.5136, precision: 0.8395, recall: 0.5594, f1: 0.6714, edges-pos-ontonotes_loss: 0.0317
09/16 07:39:17 AM: Update 16500: task edges-pos-ontonotes, batch 500 (16500): mcc: 0.6835, acc: 0.5181, precision: 0.8419, recall: 0.5633, f1: 0.6750, edges-pos-ontonotes_loss: 0.0315
09/16 07:39:28 AM: Update 16571: task edges-pos-ontonotes, batch 571 (16571): mcc: 0.6858, acc: 0.5214, precision: 0.8423, recall: 0.5667, f1: 0.6776, edges-pos-ontonotes_loss: 0.0315
09/16 07:39:38 AM: Update 16651: task edges-pos-ontonotes, batch 651 (16651): mcc: 0.6884, acc: 0.5247, precision: 0.8439, recall: 0.5699, f1: 0.6803, edges-pos-ontonotes_loss: 0.0313
09/16 07:39:48 AM: Update 16729: task edges-pos-ontonotes, batch 729 (16729): mcc: 0.6804, acc: 0.5158, precision: 0.8382, recall: 0.5608, f1: 0.6720, edges-pos-ontonotes_loss: 0.0322
09/16 07:39:58 AM: Update 16823: task edges-pos-ontonotes, batch 823 (16823): mcc: 0.6752, acc: 0.5097, precision: 0.8352, recall: 0.5544, f1: 0.6664, edges-pos-ontonotes_loss: 0.0330
09/16 07:40:08 AM: Update 16926: task edges-pos-ontonotes, batch 926 (16926): mcc: 0.6702, acc: 0.5038, precision: 0.8322, recall: 0.5483, f1: 0.6611, edges-pos-ontonotes_loss: 0.0335
09/16 07:40:18 AM: Update 16981: task edges-pos-ontonotes, batch 981 (16981): mcc: 0.6593, acc: 0.4921, precision: 0.8234, recall: 0.5367, f1: 0.6498, edges-pos-ontonotes_loss: 0.0339
09/16 07:40:23 AM: ***** Step 17000 / Validation 17 *****
09/16 07:40:23 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:40:23 AM: Validating...
09/16 07:40:28 AM: Evaluate: task edges-pos-ontonotes, batch 35 (157): mcc: 0.6884, acc: 0.5066, precision: 0.9054, recall: 0.5303, f1: 0.6689, edges-pos-ontonotes_loss: 0.0316
09/16 07:40:39 AM: Evaluate: task edges-pos-ontonotes, batch 90 (157): mcc: 0.7045, acc: 0.5296, precision: 0.9099, recall: 0.5524, f1: 0.6875, edges-pos-ontonotes_loss: 0.0300
09/16 07:40:49 AM: Evaluate: task edges-pos-ontonotes, batch 130 (157): mcc: 0.6738, acc: 0.4895, precision: 0.8935, recall: 0.5152, f1: 0.6536, edges-pos-ontonotes_loss: 0.0318
09/16 07:40:55 AM: Updating LR scheduler:
09/16 07:40:55 AM: 	Best result seen so far for macro_avg: 0.653
09/16 07:40:55 AM: 	# validation passes without improvement: 2
09/16 07:40:55 AM: edges-pos-ontonotes_loss: training: 0.034102 validation: 0.032270
09/16 07:40:55 AM: macro_avg: validation: 0.648814
09/16 07:40:55 AM: micro_avg: validation: 0.000000
09/16 07:40:55 AM: edges-pos-ontonotes_mcc: training: 0.654451 validation: 0.669237
09/16 07:40:55 AM: edges-pos-ontonotes_acc: training: 0.486612 validation: 0.483962
09/16 07:40:55 AM: edges-pos-ontonotes_precision: training: 0.819418 validation: 0.890016
09/16 07:40:55 AM: edges-pos-ontonotes_recall: training: 0.531576 validation: 0.510471
09/16 07:40:55 AM: edges-pos-ontonotes_f1: training: 0.644833 validation: 0.648814
09/16 07:40:55 AM: Global learning rate: 0.0001
09/16 07:40:55 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:40:59 AM: Update 17019: task edges-pos-ontonotes, batch 19 (17019): mcc: 0.5504, acc: 0.3699, precision: 0.7402, recall: 0.4192, f1: 0.5353, edges-pos-ontonotes_loss: 0.0436
09/16 07:41:09 AM: Update 17069: task edges-pos-ontonotes, batch 69 (17069): mcc: 0.5451, acc: 0.3662, precision: 0.7365, recall: 0.4134, f1: 0.5296, edges-pos-ontonotes_loss: 0.0438
09/16 07:41:19 AM: Update 17116: task edges-pos-ontonotes, batch 116 (17116): mcc: 0.5356, acc: 0.3563, precision: 0.7279, recall: 0.4042, f1: 0.5198, edges-pos-ontonotes_loss: 0.0448
09/16 07:41:29 AM: Update 17164: task edges-pos-ontonotes, batch 164 (17164): mcc: 0.5326, acc: 0.3534, precision: 0.7259, recall: 0.4009, f1: 0.5165, edges-pos-ontonotes_loss: 0.0450
09/16 07:41:39 AM: Update 17216: task edges-pos-ontonotes, batch 216 (17216): mcc: 0.5366, acc: 0.3570, precision: 0.7297, recall: 0.4047, f1: 0.5207, edges-pos-ontonotes_loss: 0.0447
09/16 07:41:49 AM: Update 17264: task edges-pos-ontonotes, batch 264 (17264): mcc: 0.5355, acc: 0.3557, precision: 0.7294, recall: 0.4033, f1: 0.5194, edges-pos-ontonotes_loss: 0.0448
09/16 07:42:05 AM: Update 17301: task edges-pos-ontonotes, batch 301 (17301): mcc: 0.5371, acc: 0.3570, precision: 0.7313, recall: 0.4045, f1: 0.5209, edges-pos-ontonotes_loss: 0.0447
09/16 07:42:15 AM: Update 17367: task edges-pos-ontonotes, batch 367 (17367): mcc: 0.5420, acc: 0.3619, precision: 0.7376, recall: 0.4082, f1: 0.5256, edges-pos-ontonotes_loss: 0.0434
09/16 07:42:26 AM: Update 17440: task edges-pos-ontonotes, batch 440 (17440): mcc: 0.5501, acc: 0.3702, precision: 0.7461, recall: 0.4155, f1: 0.5337, edges-pos-ontonotes_loss: 0.0421
09/16 07:42:36 AM: Update 17506: task edges-pos-ontonotes, batch 506 (17506): mcc: 0.5541, acc: 0.3746, precision: 0.7502, recall: 0.4191, f1: 0.5378, edges-pos-ontonotes_loss: 0.0415
09/16 07:42:46 AM: Update 17574: task edges-pos-ontonotes, batch 574 (17574): mcc: 0.5589, acc: 0.3796, precision: 0.7546, recall: 0.4237, f1: 0.5427, edges-pos-ontonotes_loss: 0.0407
09/16 07:42:56 AM: Update 17630: task edges-pos-ontonotes, batch 630 (17630): mcc: 0.5605, acc: 0.3814, precision: 0.7558, recall: 0.4253, f1: 0.5443, edges-pos-ontonotes_loss: 0.0403
09/16 07:43:06 AM: Update 17690: task edges-pos-ontonotes, batch 690 (17690): mcc: 0.5652, acc: 0.3864, precision: 0.7583, recall: 0.4310, f1: 0.5496, edges-pos-ontonotes_loss: 0.0400
09/16 07:43:16 AM: Update 17747: task edges-pos-ontonotes, batch 747 (17747): mcc: 0.5683, acc: 0.3897, precision: 0.7600, recall: 0.4346, f1: 0.5530, edges-pos-ontonotes_loss: 0.0397
09/16 07:43:26 AM: Update 17801: task edges-pos-ontonotes, batch 801 (17801): mcc: 0.5710, acc: 0.3925, precision: 0.7613, recall: 0.4379, f1: 0.5560, edges-pos-ontonotes_loss: 0.0396
09/16 07:43:36 AM: Update 17848: task edges-pos-ontonotes, batch 848 (17848): mcc: 0.5718, acc: 0.3935, precision: 0.7612, recall: 0.4392, f1: 0.5570, edges-pos-ontonotes_loss: 0.0396
09/16 07:43:46 AM: Update 17908: task edges-pos-ontonotes, batch 908 (17908): mcc: 0.5743, acc: 0.3961, precision: 0.7628, recall: 0.4421, f1: 0.5597, edges-pos-ontonotes_loss: 0.0394
09/16 07:43:56 AM: Update 17945: task edges-pos-ontonotes, batch 945 (17945): mcc: 0.5725, acc: 0.3945, precision: 0.7608, recall: 0.4405, f1: 0.5579, edges-pos-ontonotes_loss: 0.0395
09/16 07:44:07 AM: Update 17995: task edges-pos-ontonotes, batch 995 (17995): mcc: 0.5705, acc: 0.3926, precision: 0.7588, recall: 0.4388, f1: 0.5560, edges-pos-ontonotes_loss: 0.0398
09/16 07:44:07 AM: ***** Step 18000 / Validation 18 *****
09/16 07:44:07 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:44:07 AM: Validating...
09/16 07:44:17 AM: Evaluate: task edges-pos-ontonotes, batch 67 (157): mcc: 0.7094, acc: 0.5348, precision: 0.9215, recall: 0.5527, f1: 0.6910, edges-pos-ontonotes_loss: 0.0295
09/16 07:44:27 AM: Evaluate: task edges-pos-ontonotes, batch 120 (157): mcc: 0.6932, acc: 0.5142, precision: 0.9117, recall: 0.5339, f1: 0.6734, edges-pos-ontonotes_loss: 0.0301
09/16 07:44:34 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:44:34 AM: Best result seen so far for macro.
09/16 07:44:34 AM: Updating LR scheduler:
09/16 07:44:34 AM: 	Best result seen so far for macro_avg: 0.656
09/16 07:44:34 AM: 	# validation passes without improvement: 0
09/16 07:44:34 AM: edges-pos-ontonotes_loss: training: 0.039815 validation: 0.031231
09/16 07:44:34 AM: macro_avg: validation: 0.656185
09/16 07:44:34 AM: micro_avg: validation: 0.000000
09/16 07:44:34 AM: edges-pos-ontonotes_mcc: training: 0.570623 validation: 0.678055
09/16 07:44:34 AM: edges-pos-ontonotes_acc: training: 0.392671 validation: 0.494301
09/16 07:44:34 AM: edges-pos-ontonotes_precision: training: 0.758900 validation: 0.906143
09/16 07:44:34 AM: edges-pos-ontonotes_recall: training: 0.438819 validation: 0.514313
09/16 07:44:34 AM: edges-pos-ontonotes_f1: training: 0.556090 validation: 0.656185
09/16 07:44:34 AM: Global learning rate: 0.0001
09/16 07:44:34 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:44:37 AM: Update 18012: task edges-pos-ontonotes, batch 12 (18012): mcc: 0.5380, acc: 0.3609, precision: 0.7287, recall: 0.4073, f1: 0.5226, edges-pos-ontonotes_loss: 0.0436
09/16 07:44:47 AM: Update 18067: task edges-pos-ontonotes, batch 67 (18067): mcc: 0.5447, acc: 0.3673, precision: 0.7352, recall: 0.4136, f1: 0.5294, edges-pos-ontonotes_loss: 0.0437
09/16 07:44:57 AM: Update 18128: task edges-pos-ontonotes, batch 128 (18128): mcc: 0.5419, acc: 0.3637, precision: 0.7317, recall: 0.4114, f1: 0.5267, edges-pos-ontonotes_loss: 0.0443
09/16 07:45:07 AM: Update 18187: task edges-pos-ontonotes, batch 187 (18187): mcc: 0.5483, acc: 0.3699, precision: 0.7372, recall: 0.4179, f1: 0.5334, edges-pos-ontonotes_loss: 0.0438
09/16 07:45:17 AM: Update 18240: task edges-pos-ontonotes, batch 240 (18240): mcc: 0.5491, acc: 0.3711, precision: 0.7372, recall: 0.4191, f1: 0.5344, edges-pos-ontonotes_loss: 0.0437
09/16 07:45:27 AM: Update 18296: task edges-pos-ontonotes, batch 296 (18296): mcc: 0.5507, acc: 0.3726, precision: 0.7388, recall: 0.4205, f1: 0.5360, edges-pos-ontonotes_loss: 0.0436
09/16 07:45:37 AM: Update 18349: task edges-pos-ontonotes, batch 349 (18349): mcc: 0.5510, acc: 0.3728, precision: 0.7398, recall: 0.4204, f1: 0.5361, edges-pos-ontonotes_loss: 0.0438
09/16 07:45:48 AM: Update 18405: task edges-pos-ontonotes, batch 405 (18405): mcc: 0.5515, acc: 0.3735, precision: 0.7407, recall: 0.4207, f1: 0.5366, edges-pos-ontonotes_loss: 0.0437
09/16 07:45:58 AM: Update 18457: task edges-pos-ontonotes, batch 457 (18457): mcc: 0.5524, acc: 0.3746, precision: 0.7417, recall: 0.4214, f1: 0.5375, edges-pos-ontonotes_loss: 0.0437
09/16 07:46:08 AM: Update 18517: task edges-pos-ontonotes, batch 517 (18517): mcc: 0.5541, acc: 0.3762, precision: 0.7435, recall: 0.4228, f1: 0.5391, edges-pos-ontonotes_loss: 0.0435
09/16 07:46:18 AM: Update 18557: task edges-pos-ontonotes, batch 557 (18557): mcc: 0.5535, acc: 0.3758, precision: 0.7431, recall: 0.4223, f1: 0.5386, edges-pos-ontonotes_loss: 0.0436
09/16 07:46:28 AM: Update 18620: task edges-pos-ontonotes, batch 620 (18620): mcc: 0.5556, acc: 0.3779, precision: 0.7452, recall: 0.4241, f1: 0.5406, edges-pos-ontonotes_loss: 0.0434
09/16 07:46:38 AM: Update 18678: task edges-pos-ontonotes, batch 678 (18678): mcc: 0.5564, acc: 0.3788, precision: 0.7463, recall: 0.4248, f1: 0.5414, edges-pos-ontonotes_loss: 0.0434
09/16 07:46:48 AM: Update 18737: task edges-pos-ontonotes, batch 737 (18737): mcc: 0.5575, acc: 0.3799, precision: 0.7473, recall: 0.4258, f1: 0.5425, edges-pos-ontonotes_loss: 0.0433
09/16 07:46:58 AM: Update 18794: task edges-pos-ontonotes, batch 794 (18794): mcc: 0.5580, acc: 0.3803, precision: 0.7481, recall: 0.4262, f1: 0.5430, edges-pos-ontonotes_loss: 0.0433
09/16 07:47:08 AM: Update 18850: task edges-pos-ontonotes, batch 850 (18850): mcc: 0.5585, acc: 0.3808, precision: 0.7488, recall: 0.4264, f1: 0.5434, edges-pos-ontonotes_loss: 0.0433
09/16 07:47:19 AM: Update 18893: task edges-pos-ontonotes, batch 893 (18893): mcc: 0.5579, acc: 0.3802, precision: 0.7486, recall: 0.4257, f1: 0.5427, edges-pos-ontonotes_loss: 0.0433
09/16 07:47:29 AM: Update 18952: task edges-pos-ontonotes, batch 952 (18952): mcc: 0.5591, acc: 0.3813, precision: 0.7496, recall: 0.4268, f1: 0.5439, edges-pos-ontonotes_loss: 0.0432
09/16 07:47:37 AM: ***** Step 19000 / Validation 19 *****
09/16 07:47:37 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:47:37 AM: Validating...
09/16 07:47:39 AM: Evaluate: task edges-pos-ontonotes, batch 11 (157): mcc: 0.7046, acc: 0.5295, precision: 0.9066, recall: 0.5545, f1: 0.6881, edges-pos-ontonotes_loss: 0.0305
09/16 07:47:49 AM: Evaluate: task edges-pos-ontonotes, batch 82 (157): mcc: 0.7091, acc: 0.5357, precision: 0.9133, recall: 0.5575, f1: 0.6923, edges-pos-ontonotes_loss: 0.0303
09/16 07:47:59 AM: Evaluate: task edges-pos-ontonotes, batch 130 (157): mcc: 0.6833, acc: 0.5031, precision: 0.9006, recall: 0.5255, f1: 0.6637, edges-pos-ontonotes_loss: 0.0315
09/16 07:48:04 AM: Best result seen so far for edges-pos-ontonotes.
09/16 07:48:04 AM: Best result seen so far for macro.
09/16 07:48:04 AM: Updating LR scheduler:
09/16 07:48:04 AM: 	Best result seen so far for macro_avg: 0.663
09/16 07:48:04 AM: 	# validation passes without improvement: 0
09/16 07:48:04 AM: edges-pos-ontonotes_loss: training: 0.043198 validation: 0.031601
09/16 07:48:04 AM: macro_avg: validation: 0.662887
09/16 07:48:04 AM: micro_avg: validation: 0.000000
09/16 07:48:04 AM: edges-pos-ontonotes_mcc: training: 0.559607 validation: 0.682523
09/16 07:48:04 AM: edges-pos-ontonotes_acc: training: 0.381934 validation: 0.502016
09/16 07:48:04 AM: edges-pos-ontonotes_precision: training: 0.750263 validation: 0.900027
09/16 07:48:04 AM: edges-pos-ontonotes_recall: training: 0.427255 validation: 0.524652
09/16 07:48:04 AM: edges-pos-ontonotes_f1: training: 0.544456 validation: 0.662887
09/16 07:48:04 AM: Global learning rate: 0.0001
09/16 07:48:04 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:48:09 AM: Update 19023: task edges-pos-ontonotes, batch 23 (19023): mcc: 0.5599, acc: 0.3830, precision: 0.7510, recall: 0.4272, f1: 0.5446, edges-pos-ontonotes_loss: 0.0436
09/16 07:48:19 AM: Update 19082: task edges-pos-ontonotes, batch 82 (19082): mcc: 0.5662, acc: 0.3885, precision: 0.7569, recall: 0.4333, f1: 0.5512, edges-pos-ontonotes_loss: 0.0429
09/16 07:48:29 AM: Update 19142: task edges-pos-ontonotes, batch 142 (19142): mcc: 0.5716, acc: 0.3930, precision: 0.7630, recall: 0.4378, f1: 0.5564, edges-pos-ontonotes_loss: 0.0424
09/16 07:48:46 AM: Update 19179: task edges-pos-ontonotes, batch 179 (19179): mcc: 0.5671, acc: 0.3903, precision: 0.7572, recall: 0.4344, f1: 0.5521, edges-pos-ontonotes_loss: 0.0425
09/16 07:48:56 AM: Update 19237: task edges-pos-ontonotes, batch 237 (19237): mcc: 0.5596, acc: 0.3831, precision: 0.7494, recall: 0.4277, f1: 0.5446, edges-pos-ontonotes_loss: 0.0421
09/16 07:49:06 AM: Update 19311: task edges-pos-ontonotes, batch 311 (19311): mcc: 0.5675, acc: 0.3911, precision: 0.7558, recall: 0.4360, f1: 0.5530, edges-pos-ontonotes_loss: 0.0409
09/16 07:49:17 AM: Update 19377: task edges-pos-ontonotes, batch 377 (19377): mcc: 0.5690, acc: 0.3930, precision: 0.7561, recall: 0.4381, f1: 0.5547, edges-pos-ontonotes_loss: 0.0403
09/16 07:49:27 AM: Update 19449: task edges-pos-ontonotes, batch 449 (19449): mcc: 0.5726, acc: 0.3968, precision: 0.7585, recall: 0.4421, f1: 0.5586, edges-pos-ontonotes_loss: 0.0398
09/16 07:49:37 AM: Update 19519: task edges-pos-ontonotes, batch 519 (19519): mcc: 0.5772, acc: 0.4016, precision: 0.7619, recall: 0.4471, f1: 0.5635, edges-pos-ontonotes_loss: 0.0390
09/16 07:49:47 AM: Update 19616: task edges-pos-ontonotes, batch 616 (19616): mcc: 0.5915, acc: 0.4163, precision: 0.7729, recall: 0.4623, f1: 0.5785, edges-pos-ontonotes_loss: 0.0376
09/16 07:49:57 AM: Update 19706: task edges-pos-ontonotes, batch 706 (19706): mcc: 0.6027, acc: 0.4281, precision: 0.7813, recall: 0.4745, f1: 0.5904, edges-pos-ontonotes_loss: 0.0367
09/16 07:50:07 AM: Update 19801: task edges-pos-ontonotes, batch 801 (19801): mcc: 0.6120, acc: 0.4380, precision: 0.7882, recall: 0.4847, f1: 0.6003, edges-pos-ontonotes_loss: 0.0360
09/16 07:50:17 AM: Update 19896: task edges-pos-ontonotes, batch 896 (19896): mcc: 0.6182, acc: 0.4449, precision: 0.7926, recall: 0.4914, f1: 0.6067, edges-pos-ontonotes_loss: 0.0355
09/16 07:50:25 AM: ***** Step 20000 / Validation 20 *****
09/16 07:50:25 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:50:25 AM: Validating...
09/16 07:50:27 AM: Evaluate: task edges-pos-ontonotes, batch 10 (157): mcc: 0.6877, acc: 0.5086, precision: 0.9004, recall: 0.5323, f1: 0.6691, edges-pos-ontonotes_loss: 0.0313
09/16 07:50:37 AM: Evaluate: task edges-pos-ontonotes, batch 80 (157): mcc: 0.7071, acc: 0.5360, precision: 0.9112, recall: 0.5556, f1: 0.6903, edges-pos-ontonotes_loss: 0.0298
09/16 07:50:47 AM: Evaluate: task edges-pos-ontonotes, batch 129 (157): mcc: 0.6791, acc: 0.5004, precision: 0.8959, recall: 0.5219, f1: 0.6596, edges-pos-ontonotes_loss: 0.0315
09/16 07:50:53 AM: Updating LR scheduler:
09/16 07:50:53 AM: 	Best result seen so far for macro_avg: 0.663
09/16 07:50:53 AM: 	# validation passes without improvement: 1
09/16 07:50:53 AM: edges-pos-ontonotes_loss: training: 0.034901 validation: 0.032075
09/16 07:50:53 AM: macro_avg: validation: 0.654509
09/16 07:50:53 AM: micro_avg: validation: 0.000000
09/16 07:50:53 AM: edges-pos-ontonotes_mcc: training: 0.623882 validation: 0.674439
09/16 07:50:53 AM: edges-pos-ontonotes_acc: training: 0.451447 validation: 0.494450
09/16 07:50:53 AM: edges-pos-ontonotes_precision: training: 0.796699 validation: 0.893058
09/16 07:50:53 AM: edges-pos-ontonotes_recall: training: 0.497842 validation: 0.516535
09/16 07:50:53 AM: edges-pos-ontonotes_f1: training: 0.612774 validation: 0.654509
09/16 07:50:53 AM: Global learning rate: 0.0001
09/16 07:50:53 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:50:57 AM: Update 20049: task edges-pos-ontonotes, batch 49 (20049): mcc: 0.7151, acc: 0.5631, precision: 0.8523, recall: 0.6081, f1: 0.7098, edges-pos-ontonotes_loss: 0.0300
09/16 07:51:07 AM: Update 20135: task edges-pos-ontonotes, batch 135 (20135): mcc: 0.6807, acc: 0.5225, precision: 0.8267, recall: 0.5693, f1: 0.6743, edges-pos-ontonotes_loss: 0.0318
09/16 07:51:17 AM: Update 20285: task edges-pos-ontonotes, batch 285 (20285): mcc: 0.6515, acc: 0.4860, precision: 0.8130, recall: 0.5312, f1: 0.6426, edges-pos-ontonotes_loss: 0.0350
09/16 07:51:27 AM: Update 20419: task edges-pos-ontonotes, batch 419 (20419): mcc: 0.6375, acc: 0.4690, precision: 0.8048, recall: 0.5141, f1: 0.6274, edges-pos-ontonotes_loss: 0.0362
09/16 07:51:37 AM: Update 20475: task edges-pos-ontonotes, batch 475 (20475): mcc: 0.6115, acc: 0.4406, precision: 0.7842, recall: 0.4864, f1: 0.6004, edges-pos-ontonotes_loss: 0.0371
09/16 07:51:48 AM: Update 20537: task edges-pos-ontonotes, batch 537 (20537): mcc: 0.5968, acc: 0.4234, precision: 0.7727, recall: 0.4706, f1: 0.5850, edges-pos-ontonotes_loss: 0.0379
09/16 07:51:58 AM: Update 20604: task edges-pos-ontonotes, batch 604 (20604): mcc: 0.5877, acc: 0.4128, precision: 0.7668, recall: 0.4602, f1: 0.5752, edges-pos-ontonotes_loss: 0.0387
09/16 07:52:08 AM: Update 20668: task edges-pos-ontonotes, batch 668 (20668): mcc: 0.5813, acc: 0.4058, precision: 0.7621, recall: 0.4533, f1: 0.5684, edges-pos-ontonotes_loss: 0.0391
09/16 07:52:18 AM: Update 20724: task edges-pos-ontonotes, batch 724 (20724): mcc: 0.5746, acc: 0.3984, precision: 0.7576, recall: 0.4456, f1: 0.5612, edges-pos-ontonotes_loss: 0.0396
09/16 07:52:28 AM: Update 20782: task edges-pos-ontonotes, batch 782 (20782): mcc: 0.5730, acc: 0.3964, precision: 0.7573, recall: 0.4435, f1: 0.5594, edges-pos-ontonotes_loss: 0.0397
09/16 07:52:38 AM: Update 20876: task edges-pos-ontonotes, batch 876 (20876): mcc: 0.5760, acc: 0.3994, precision: 0.7613, recall: 0.4456, f1: 0.5622, edges-pos-ontonotes_loss: 0.0394
09/16 07:52:48 AM: Update 20973: task edges-pos-ontonotes, batch 973 (20973): mcc: 0.5788, acc: 0.4021, precision: 0.7647, recall: 0.4478, f1: 0.5648, edges-pos-ontonotes_loss: 0.0389
09/16 07:52:51 AM: ***** Step 21000 / Validation 21 *****
09/16 07:52:51 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 07:52:51 AM: Validating...
09/16 07:52:58 AM: Evaluate: task edges-pos-ontonotes, batch 48 (157): mcc: 0.7043, acc: 0.5275, precision: 0.9187, recall: 0.5467, f1: 0.6855, edges-pos-ontonotes_loss: 0.0296
09/16 07:53:08 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.6984, acc: 0.5215, precision: 0.9106, recall: 0.5425, f1: 0.6799, edges-pos-ontonotes_loss: 0.0292
09/16 07:53:18 AM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.6728, acc: 0.4881, precision: 0.8977, recall: 0.5114, f1: 0.6516, edges-pos-ontonotes_loss: 0.0311
09/16 07:53:18 AM: Updating LR scheduler:
09/16 07:53:18 AM: 	Best result seen so far for macro_avg: 0.663
09/16 07:53:18 AM: 	# validation passes without improvement: 2
09/16 07:53:18 AM: edges-pos-ontonotes_loss: training: 0.038835 validation: 0.031096
09/16 07:53:18 AM: macro_avg: validation: 0.651920
09/16 07:53:18 AM: micro_avg: validation: 0.000000
09/16 07:53:18 AM: edges-pos-ontonotes_mcc: training: 0.579356 validation: 0.673111
09/16 07:53:18 AM: edges-pos-ontonotes_acc: training: 0.402638 validation: 0.488492
09/16 07:53:18 AM: edges-pos-ontonotes_precision: training: 0.765437 validation: 0.897737
09/16 07:53:18 AM: edges-pos-ontonotes_recall: training: 0.448204 validation: 0.511783
09/16 07:53:18 AM: edges-pos-ontonotes_f1: training: 0.565360 validation: 0.651920
09/16 07:53:18 AM: Global learning rate: 0.0001
09/16 07:53:18 AM: Saving checkpoints to: ./experiments/pos-ontonotes-hotpot-top/run
09/16 07:53:28 AM: Update 21074: task edges-pos-ontonotes, batch 74 (21074): mcc: 0.5983, acc: 0.4228, precision: 0.7872, recall: 0.4641, f1: 0.5839, edges-pos-ontonotes_loss: 0.0359
09/16 07:53:38 AM: Update 21140: task edges-pos-ontonotes, batch 140 (21140): mcc: 0.5973, acc: 0.4206, precision: 0.7777, recall: 0.4683, f1: 0.5846, edges-pos-ontonotes_loss: 0.0371
09/16 07:53:48 AM: Update 21211: task edges-pos-ontonotes, batch 211 (21211): mcc: 0.5968, acc: 0.4205, precision: 0.7738, recall: 0.4700, f1: 0.5848, edges-pos-ontonotes_loss: 0.0372
09/16 07:53:59 AM: Update 21291: task edges-pos-ontonotes, batch 291 (21291): mcc: 0.6007, acc: 0.4248, precision: 0.7762, recall: 0.4745, f1: 0.5889, edges-pos-ontonotes_loss: 0.0368
09/16 07:54:09 AM: Update 21363: task edges-pos-ontonotes, batch 363 (21363): mcc: 0.6020, acc: 0.4265, precision: 0.7767, recall: 0.4762, f1: 0.5904, edges-pos-ontonotes_loss: 0.0370
09/16 07:54:21 AM: Update 21387: task edges-pos-ontonotes, batch 387 (21387): mcc: 0.6019, acc: 0.4267, precision: 0.7762, recall: 0.4764, f1: 0.5904, edges-pos-ontonotes_loss: 0.0369
09/16 07:54:31 AM: Update 21443: task edges-pos-ontonotes, batch 443 (21443): mcc: 0.5924, acc: 0.4166, precision: 0.7683, recall: 0.4666, f1: 0.5806, edges-pos-ontonotes_loss: 0.0378
09/16 07:54:42 AM: Update 21497: task edges-pos-ontonotes, batch 497 (21497): mcc: 0.5851, acc: 0.4088, precision: 0.7626, recall: 0.4587, f1: 0.5729, edges-pos-ontonotes_loss: 0.0386
09/16 07:54:52 AM: Update 21554: task edges-pos-ontonotes, batch 554 (21554): mcc: 0.5795, acc: 0.4029, precision: 0.7586, recall: 0.4525, f1: 0.5669, edges-pos-ontonotes_loss: 0.0391
09/16 07:55:02 AM: Update 21619: task edges-pos-ontonotes, batch 619 (21619): mcc: 0.5777, acc: 0.4010, precision: 0.7572, recall: 0.4507, f1: 0.5650, edges-pos-ontonotes_loss: 0.0396
09/16 07:55:12 AM: Update 21677: task edges-pos-ontonotes, batch 677 (21677): mcc: 0.5757, acc: 0.3986, precision: 0.7562, recall: 0.4483, f1: 0.5629, edges-pos-ontonotes_loss: 0.0399
09/16 07:55:22 AM: Update 21722: task edges-pos-ontonotes, batch 722 (21722): mcc: 0.5734, acc: 0.3964, precision: 0.7541, recall: 0.4460, f1: 0.5605, edges-pos-ontonotes_loss: 0.0401
09/16 07:55:32 AM: Update 21773: task edges-pos-ontonotes, batch 773 (21773): mcc: 0.5716, acc: 0.3946, precision: 0.7528, recall: 0.4440, f1: 0.5585, edges-pos-ontonotes_loss: 0.0404
09/16 07:55:42 AM: Update 21828: task edges-pos-ontonotes, batch 828 (21828): mcc: 0.5714, acc: 0.3943, precision: 0.7530, recall: 0.4436, f1: 0.5583, edges-pos-ontonotes_loss: 0.0405
09/16 07:55:52 AM: Update 21880: task edges-pos-ontonotes, batch 880 (21880): mcc: 0.5701, acc: 0.3930, precision: 0.7522, recall: 0.4420, f1: 0.5568, edges-pos-ontonotes_loss: 0.0407
