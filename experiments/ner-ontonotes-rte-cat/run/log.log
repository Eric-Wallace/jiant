09/09 03:49:21 PM: Git branch: master
09/09 03:49:21 PM: Git SHA: e26b1963b48bdbc7f001ca620a782c5a262276eb
09/09 03:49:23 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes- models/RTE-cat/",
  "exp_name": "experiments/ner-ontonotes- models/RTE-cat",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes- models/RTE-cat/run/log.log",
  "lr_patience": 5,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 20,
  "pretrain_tasks": "",
  "pretrained_dir": "models/RTE",
  "pytorch_transformers_output_mode": "cat",
  "remote_log_name": "experiments/ner-ontonotes- models/RTE-cat__run",
  "run_dir": "./experiments/ner-ontonotes- models/RTE-cat/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/09 03:49:23 PM: Saved config to ./experiments/ner-ontonotes- models/RTE-cat/run/params.conf
09/09 03:49:23 PM: Using random seed 1234
09/09 03:49:23 PM: Using GPU 0
09/09 03:49:23 PM: Loading tasks...
09/09 03:49:23 PM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes- models/RTE-cat/
09/09 03:49:23 PM: 	Creating task edges-ner-ontonotes from scratch.
09/09 03:49:25 PM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/09 03:49:25 PM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/09 03:49:25 PM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/09 03:49:26 PM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/09 03:49:26 PM: 	Finished loading tasks: edges-ner-ontonotes.
09/09 03:49:26 PM: 	Building vocab from scratch.
09/09 03:49:26 PM: 	Counting units for task edges-ner-ontonotes.
09/09 03:49:28 PM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/09 03:49:28 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /cliphomes/ewallac2/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:29 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/09 03:49:29 PM: 	Saved vocab to ./experiments/ner-ontonotes- models/RTE-cat/vocab
09/09 03:49:29 PM: Loading token dictionary from ./experiments/ner-ontonotes- models/RTE-cat/vocab.
09/09 03:49:29 PM: 	Loaded vocab from ./experiments/ner-ontonotes- models/RTE-cat/vocab
09/09 03:49:29 PM: 	Vocab namespace tokens: size 22840
09/09 03:49:29 PM: 	Vocab namespace chars: size 77
09/09 03:49:29 PM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/09 03:49:29 PM: 	Vocab namespace bert_uncased: size 30524
09/09 03:49:29 PM: 	Finished building vocab.
09/09 03:49:29 PM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/09 03:49:42 PM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes- models/RTE-cat/preproc/edges-ner-ontonotes__train_data
09/09 03:49:42 PM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/09 03:49:44 PM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes- models/RTE-cat/preproc/edges-ner-ontonotes__val_data
09/09 03:49:44 PM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/09 03:49:45 PM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes- models/RTE-cat/preproc/edges-ner-ontonotes__test_data
09/09 03:49:45 PM: 	Finished indexing tasks
09/09 03:49:45 PM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/09 03:49:45 PM: 	  Training on 
09/09 03:49:45 PM: 	  Evaluating on edges-ner-ontonotes
09/09 03:49:45 PM: 	Finished loading tasks in 22.538s
09/09 03:49:45 PM: 	 Tasks: ['edges-ner-ontonotes']
09/09 03:49:45 PM: Building model...
09/09 03:49:45 PM: Using BERT model (bert-base-uncased).
09/09 03:49:46 PM: LOADING A FUNETUNED MODEL from: 
09/09 03:49:46 PM: models/RTE
09/09 03:49:46 PM: loading configuration file models/RTE/config.json
09/09 03:49:46 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/09 03:49:46 PM: loading weights file models/RTE/pytorch_model.bin
09/09 03:49:53 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmp1t587rdk
09/09 03:49:53 PM: copying /tmp/tmp1t587rdk to cache at ./experiments/ner-ontonotes- models/RTE-cat/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:53 PM: creating metadata file for ./experiments/ner-ontonotes- models/RTE-cat/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:53 PM: removing temp file /tmp/tmp1t587rdk
09/09 03:49:53 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes- models/RTE-cat/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:53 PM: Initializing parameters
09/09 03:49:53 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/09 03:49:53 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/09 03:49:53 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/09 03:49:53 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/09 03:49:53 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/09 03:49:54 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/09 03:49:54 PM:    _text_field_embedder.model.pooler.dense.bias
09/09 03:49:54 PM:    _text_field_embedder.model.pooler.dense.weight
09/09 03:49:54 PM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/09 03:50:00 PM: Model specification:
09/09 03:50:00 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(1536, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/09 03:50:00 PM: Model parameters:
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:50:00 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:50:00 PM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 393216 with torch.Size([256, 1536, 1])
09/09 03:50:00 PM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/09 03:50:00 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/09 03:50:00 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/09 03:50:00 PM: Total number of parameters: 109884946 (1.09885e+08)
09/09 03:50:00 PM: Number of trainable parameters: 402706 (402706)
09/09 03:50:00 PM: Finished building model in 15.010s
09/09 03:50:00 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/09 03:50:06 PM: patience = 20
09/09 03:50:06 PM: val_interval = 1000
09/09 03:50:06 PM: max_vals = 250
09/09 03:50:06 PM: cuda_device = 0
09/09 03:50:06 PM: grad_norm = 5.0
09/09 03:50:06 PM: grad_clipping = None
09/09 03:50:06 PM: lr_decay = 0.99
09/09 03:50:06 PM: min_lr = 1e-06
09/09 03:50:06 PM: keep_all_checkpoints = 0
09/09 03:50:06 PM: val_data_limit = 5000
09/09 03:50:06 PM: max_epochs = -1
09/09 03:50:06 PM: dec_val_scale = 250
09/09 03:50:06 PM: training_data_fraction = 1
09/09 03:50:06 PM: type = adam
09/09 03:50:06 PM: parameter_groups = None
09/09 03:50:06 PM: Number of trainable parameters: 402706
09/09 03:50:06 PM: infer_type_and_cast = True
09/09 03:50:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:50:06 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:50:06 PM: lr = 0.0001
09/09 03:50:06 PM: amsgrad = True
09/09 03:50:06 PM: type = reduce_on_plateau
09/09 03:50:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:50:06 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:50:06 PM: mode = max
09/09 03:50:06 PM: factor = 0.5
09/09 03:50:06 PM: patience = 5
09/09 03:50:06 PM: threshold = 0.0001
09/09 03:50:06 PM: threshold_mode = abs
09/09 03:50:06 PM: verbose = True
09/09 03:50:06 PM: type = adam
09/09 03:50:06 PM: parameter_groups = None
09/09 03:50:06 PM: Number of trainable parameters: 402706
09/09 03:50:06 PM: infer_type_and_cast = True
09/09 03:50:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:50:06 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:50:06 PM: lr = 0.0001
09/09 03:50:06 PM: amsgrad = True
09/09 03:50:06 PM: type = reduce_on_plateau
09/09 03:50:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:50:06 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:50:06 PM: mode = max
09/09 03:50:06 PM: factor = 0.5
09/09 03:50:06 PM: patience = 5
09/09 03:50:06 PM: threshold = 0.0001
09/09 03:50:06 PM: threshold_mode = abs
09/09 03:50:06 PM: verbose = True
09/09 03:50:06 PM: Starting training without restoring from a checkpoint.
09/09 03:50:06 PM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/09 03:50:06 PM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/09 03:50:16 PM: Update 46: task edges-ner-ontonotes, batch 46 (46): mcc: 0.0518, acc: 0.0407, precision: 0.0951, recall: 0.1411, f1: 0.1136, edges-ner-ontonotes_loss: 0.3647
09/09 03:50:26 PM: Update 121: task edges-ner-ontonotes, batch 121 (121): mcc: 0.2595, acc: 0.2266, precision: 0.3298, recall: 0.2663, f1: 0.2947, edges-ner-ontonotes_loss: 0.2112
09/09 03:50:36 PM: Update 192: task edges-ner-ontonotes, batch 192 (192): mcc: 0.4324, acc: 0.3735, precision: 0.5292, recall: 0.3990, f1: 0.4550, edges-ner-ontonotes_loss: 0.1635
09/09 03:50:47 PM: Update 259: task edges-ner-ontonotes, batch 259 (259): mcc: 0.5301, acc: 0.4613, precision: 0.6346, recall: 0.4812, f1: 0.5474, edges-ner-ontonotes_loss: 0.1379
09/09 03:50:57 PM: Update 314: task edges-ner-ontonotes, batch 314 (314): mcc: 0.5854, acc: 0.5129, precision: 0.6920, recall: 0.5295, f1: 0.5999, edges-ner-ontonotes_loss: 0.1236
09/09 03:51:07 PM: Update 372: task edges-ner-ontonotes, batch 372 (372): mcc: 0.6341, acc: 0.5589, precision: 0.7408, recall: 0.5733, f1: 0.6464, edges-ner-ontonotes_loss: 0.1132
09/09 03:51:17 PM: Update 435: task edges-ner-ontonotes, batch 435 (435): mcc: 0.6724, acc: 0.5977, precision: 0.7757, recall: 0.6108, f1: 0.6835, edges-ner-ontonotes_loss: 0.1038
09/09 03:51:27 PM: Update 500: task edges-ner-ontonotes, batch 500 (500): mcc: 0.7042, acc: 0.6309, precision: 0.8031, recall: 0.6431, f1: 0.7143, edges-ner-ontonotes_loss: 0.0958
09/09 03:51:37 PM: Update 558: task edges-ner-ontonotes, batch 558 (558): mcc: 0.7261, acc: 0.6542, precision: 0.8216, recall: 0.6657, f1: 0.7355, edges-ner-ontonotes_loss: 0.0900
09/09 03:51:47 PM: Update 621: task edges-ner-ontonotes, batch 621 (621): mcc: 0.7452, acc: 0.6755, precision: 0.8368, recall: 0.6864, f1: 0.7541, edges-ner-ontonotes_loss: 0.0846
09/09 03:51:58 PM: Update 670: task edges-ner-ontonotes, batch 670 (670): mcc: 0.7585, acc: 0.6900, precision: 0.8469, recall: 0.7011, f1: 0.7671, edges-ner-ontonotes_loss: 0.0811
09/09 03:52:08 PM: Update 727: task edges-ner-ontonotes, batch 727 (727): mcc: 0.7727, acc: 0.7056, precision: 0.8577, recall: 0.7168, f1: 0.7809, edges-ner-ontonotes_loss: 0.0774
09/09 03:52:18 PM: Update 798: task edges-ner-ontonotes, batch 798 (798): mcc: 0.7860, acc: 0.7207, precision: 0.8670, recall: 0.7323, f1: 0.7940, edges-ner-ontonotes_loss: 0.0732
09/09 03:52:28 PM: Update 860: task edges-ner-ontonotes, batch 860 (860): mcc: 0.7963, acc: 0.7328, precision: 0.8741, recall: 0.7444, f1: 0.8040, edges-ner-ontonotes_loss: 0.0700
09/09 03:52:38 PM: Update 907: task edges-ner-ontonotes, batch 907 (907): mcc: 0.8049, acc: 0.7428, precision: 0.8802, recall: 0.7543, f1: 0.8124, edges-ner-ontonotes_loss: 0.0677
09/09 03:52:48 PM: Update 943: task edges-ner-ontonotes, batch 943 (943): mcc: 0.8100, acc: 0.7486, precision: 0.8838, recall: 0.7602, f1: 0.8173, edges-ner-ontonotes_loss: 0.0661
09/09 03:52:58 PM: Update 991: task edges-ner-ontonotes, batch 991 (991): mcc: 0.8157, acc: 0.7553, precision: 0.8878, recall: 0.7667, f1: 0.8229, edges-ner-ontonotes_loss: 0.0643
09/09 03:53:00 PM: ***** Step 1000 / Validation 1 *****
09/09 03:53:00 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:53:00 PM: Validating...
09/09 03:53:08 PM: Evaluate: task edges-ner-ontonotes, batch 36 (157): mcc: 0.8554, acc: 0.8056, precision: 0.9110, recall: 0.8175, f1: 0.8617, edges-ner-ontonotes_loss: 0.0462
09/09 03:53:18 PM: Evaluate: task edges-ner-ontonotes, batch 78 (157): mcc: 0.8866, acc: 0.8394, precision: 0.9396, recall: 0.8477, f1: 0.8913, edges-ner-ontonotes_loss: 0.0396
09/09 03:53:29 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.8977, acc: 0.8545, precision: 0.9451, recall: 0.8630, f1: 0.9022, edges-ner-ontonotes_loss: 0.0355
09/09 03:53:39 PM: Evaluate: task edges-ner-ontonotes, batch 157 (157): mcc: 0.9085, acc: 0.8684, precision: 0.9509, recall: 0.8774, f1: 0.9126, edges-ner-ontonotes_loss: 0.0319
09/09 03:53:39 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:53:39 PM: Best result seen so far for micro.
09/09 03:53:39 PM: Best result seen so far for macro.
09/09 03:53:39 PM: Updating LR scheduler:
09/09 03:53:39 PM: 	Best result seen so far for macro_avg: 0.913
09/09 03:53:39 PM: 	# validation passes without improvement: 0
09/09 03:53:39 PM: edges-ner-ontonotes_loss: training: 0.063956 validation: 0.031927
09/09 03:53:39 PM: macro_avg: validation: 0.912647
09/09 03:53:39 PM: micro_avg: validation: 0.000000
09/09 03:53:39 PM: edges-ner-ontonotes_mcc: training: 0.816574 validation: 0.908546
09/09 03:53:39 PM: edges-ner-ontonotes_acc: training: 0.756352 validation: 0.868365
09/09 03:53:39 PM: edges-ner-ontonotes_precision: training: 0.888428 validation: 0.950859
09/09 03:53:39 PM: edges-ner-ontonotes_recall: training: 0.767828 validation: 0.877389
09/09 03:53:39 PM: edges-ner-ontonotes_f1: training: 0.823737 validation: 0.912647
09/09 03:53:39 PM: Global learning rate: 0.0001
09/09 03:53:39 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 03:53:49 PM: Update 1059: task edges-ner-ontonotes, batch 59 (1059): mcc: 0.9217, acc: 0.8868, precision: 0.9528, recall: 0.8998, f1: 0.9256, edges-ner-ontonotes_loss: 0.0257
09/09 03:53:59 PM: Update 1128: task edges-ner-ontonotes, batch 128 (1128): mcc: 0.9224, acc: 0.8869, precision: 0.9546, recall: 0.8993, f1: 0.9261, edges-ner-ontonotes_loss: 0.0260
09/09 03:54:09 PM: Update 1184: task edges-ner-ontonotes, batch 184 (1184): mcc: 0.9270, acc: 0.8923, precision: 0.9590, recall: 0.9036, f1: 0.9305, edges-ner-ontonotes_loss: 0.0248
09/09 03:54:19 PM: Update 1243: task edges-ner-ontonotes, batch 243 (1243): mcc: 0.9272, acc: 0.8923, precision: 0.9595, recall: 0.9036, f1: 0.9307, edges-ner-ontonotes_loss: 0.0245
09/09 03:54:29 PM: Update 1284: task edges-ner-ontonotes, batch 284 (1284): mcc: 0.9221, acc: 0.8853, precision: 0.9571, recall: 0.8965, f1: 0.9258, edges-ner-ontonotes_loss: 0.0264
09/09 03:54:39 PM: Update 1346: task edges-ner-ontonotes, batch 346 (1346): mcc: 0.9153, acc: 0.8765, precision: 0.9533, recall: 0.8875, f1: 0.9192, edges-ner-ontonotes_loss: 0.0293
09/09 03:54:49 PM: Update 1410: task edges-ner-ontonotes, batch 410 (1410): mcc: 0.9124, acc: 0.8732, precision: 0.9518, recall: 0.8836, f1: 0.9164, edges-ner-ontonotes_loss: 0.0304
09/09 03:54:59 PM: Update 1478: task edges-ner-ontonotes, batch 478 (1478): mcc: 0.9111, acc: 0.8721, precision: 0.9509, recall: 0.8821, f1: 0.9152, edges-ner-ontonotes_loss: 0.0310
09/09 03:55:10 PM: Update 1546: task edges-ner-ontonotes, batch 546 (1546): mcc: 0.9089, acc: 0.8690, precision: 0.9493, recall: 0.8794, f1: 0.9130, edges-ner-ontonotes_loss: 0.0318
09/09 03:55:20 PM: Update 1602: task edges-ner-ontonotes, batch 602 (1602): mcc: 0.9086, acc: 0.8689, precision: 0.9489, recall: 0.8793, f1: 0.9128, edges-ner-ontonotes_loss: 0.0318
09/09 03:55:30 PM: Update 1672: task edges-ner-ontonotes, batch 672 (1672): mcc: 0.9092, acc: 0.8700, precision: 0.9492, recall: 0.8801, f1: 0.9134, edges-ner-ontonotes_loss: 0.0314
09/09 03:55:40 PM: Update 1752: task edges-ner-ontonotes, batch 752 (1752): mcc: 0.9097, acc: 0.8711, precision: 0.9490, recall: 0.8813, f1: 0.9139, edges-ner-ontonotes_loss: 0.0312
09/09 03:55:50 PM: Update 1822: task edges-ner-ontonotes, batch 822 (1822): mcc: 0.9106, acc: 0.8725, precision: 0.9494, recall: 0.8826, f1: 0.9148, edges-ner-ontonotes_loss: 0.0307
09/09 03:56:00 PM: Update 1871: task edges-ner-ontonotes, batch 871 (1871): mcc: 0.9113, acc: 0.8735, precision: 0.9498, recall: 0.8834, f1: 0.9154, edges-ner-ontonotes_loss: 0.0304
09/09 03:56:10 PM: Update 1923: task edges-ner-ontonotes, batch 923 (1923): mcc: 0.9119, acc: 0.8746, precision: 0.9500, recall: 0.8845, f1: 0.9161, edges-ner-ontonotes_loss: 0.0301
09/09 03:56:21 PM: Update 1975: task edges-ner-ontonotes, batch 975 (1975): mcc: 0.9129, acc: 0.8761, precision: 0.9504, recall: 0.8859, f1: 0.9170, edges-ner-ontonotes_loss: 0.0297
09/09 03:56:26 PM: ***** Step 2000 / Validation 2 *****
09/09 03:56:26 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:56:26 PM: Validating...
09/09 03:56:31 PM: Evaluate: task edges-ner-ontonotes, batch 21 (157): mcc: 0.8763, acc: 0.8300, precision: 0.9202, recall: 0.8470, f1: 0.8821, edges-ner-ontonotes_loss: 0.0343
09/09 03:56:41 PM: Evaluate: task edges-ner-ontonotes, batch 62 (157): mcc: 0.9182, acc: 0.8884, precision: 0.9481, recall: 0.8977, f1: 0.9222, edges-ner-ontonotes_loss: 0.0272
09/09 03:56:51 PM: Evaluate: task edges-ner-ontonotes, batch 106 (157): mcc: 0.9193, acc: 0.8860, precision: 0.9541, recall: 0.8941, f1: 0.9231, edges-ner-ontonotes_loss: 0.0272
09/09 03:57:01 PM: Evaluate: task edges-ner-ontonotes, batch 146 (157): mcc: 0.9290, acc: 0.8979, precision: 0.9605, recall: 0.9060, f1: 0.9324, edges-ner-ontonotes_loss: 0.0248
09/09 03:57:04 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:57:04 PM: Best result seen so far for macro.
09/09 03:57:04 PM: Updating LR scheduler:
09/09 03:57:04 PM: 	Best result seen so far for macro_avg: 0.934
09/09 03:57:04 PM: 	# validation passes without improvement: 0
09/09 03:57:04 PM: edges-ner-ontonotes_loss: training: 0.029611 validation: 0.024225
09/09 03:57:04 PM: macro_avg: validation: 0.933806
09/09 03:57:04 PM: micro_avg: validation: 0.000000
09/09 03:57:04 PM: edges-ner-ontonotes_mcc: training: 0.913242 validation: 0.930443
09/09 03:57:04 PM: edges-ner-ontonotes_acc: training: 0.876624 validation: 0.899909
09/09 03:57:04 PM: edges-ner-ontonotes_precision: training: 0.950580 validation: 0.960927
09/09 03:57:04 PM: edges-ner-ontonotes_recall: training: 0.886275 validation: 0.908174
09/09 03:57:04 PM: edges-ner-ontonotes_f1: training: 0.917302 validation: 0.933806
09/09 03:57:04 PM: Global learning rate: 0.0001
09/09 03:57:04 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 03:57:11 PM: Update 2041: task edges-ner-ontonotes, batch 41 (2041): mcc: 0.9361, acc: 0.9102, precision: 0.9603, recall: 0.9192, f1: 0.9393, edges-ner-ontonotes_loss: 0.0210
09/09 03:57:21 PM: Update 2110: task edges-ner-ontonotes, batch 110 (2110): mcc: 0.9370, acc: 0.9111, precision: 0.9613, recall: 0.9200, f1: 0.9402, edges-ner-ontonotes_loss: 0.0213
09/09 03:57:31 PM: Update 2164: task edges-ner-ontonotes, batch 164 (2164): mcc: 0.9353, acc: 0.9088, precision: 0.9603, recall: 0.9178, f1: 0.9386, edges-ner-ontonotes_loss: 0.0216
09/09 03:57:41 PM: Update 2208: task edges-ner-ontonotes, batch 208 (2208): mcc: 0.9348, acc: 0.9078, precision: 0.9599, recall: 0.9173, f1: 0.9381, edges-ner-ontonotes_loss: 0.0215
09/09 03:57:52 PM: Update 2267: task edges-ner-ontonotes, batch 267 (2267): mcc: 0.9382, acc: 0.9119, precision: 0.9618, recall: 0.9217, f1: 0.9414, edges-ner-ontonotes_loss: 0.0206
09/09 03:58:02 PM: Update 2324: task edges-ner-ontonotes, batch 324 (2324): mcc: 0.9392, acc: 0.9130, precision: 0.9623, recall: 0.9231, f1: 0.9423, edges-ner-ontonotes_loss: 0.0200
09/09 03:58:12 PM: Update 2388: task edges-ner-ontonotes, batch 388 (2388): mcc: 0.9401, acc: 0.9140, precision: 0.9625, recall: 0.9245, f1: 0.9431, edges-ner-ontonotes_loss: 0.0197
09/09 03:58:22 PM: Update 2447: task edges-ner-ontonotes, batch 447 (2447): mcc: 0.9417, acc: 0.9163, precision: 0.9634, recall: 0.9267, f1: 0.9447, edges-ner-ontonotes_loss: 0.0192
09/09 03:58:33 PM: Update 2496: task edges-ner-ontonotes, batch 496 (2496): mcc: 0.9430, acc: 0.9178, precision: 0.9642, recall: 0.9284, f1: 0.9460, edges-ner-ontonotes_loss: 0.0188
09/09 03:58:43 PM: Update 2556: task edges-ner-ontonotes, batch 556 (2556): mcc: 0.9430, acc: 0.9176, precision: 0.9638, recall: 0.9287, f1: 0.9459, edges-ner-ontonotes_loss: 0.0188
09/09 03:58:53 PM: Update 2613: task edges-ner-ontonotes, batch 613 (2613): mcc: 0.9434, acc: 0.9179, precision: 0.9639, recall: 0.9293, f1: 0.9463, edges-ner-ontonotes_loss: 0.0187
09/09 03:59:03 PM: Update 2674: task edges-ner-ontonotes, batch 674 (2674): mcc: 0.9438, acc: 0.9185, precision: 0.9641, recall: 0.9299, f1: 0.9467, edges-ner-ontonotes_loss: 0.0186
09/09 03:59:14 PM: Update 2735: task edges-ner-ontonotes, batch 735 (2735): mcc: 0.9445, acc: 0.9194, precision: 0.9645, recall: 0.9307, f1: 0.9473, edges-ner-ontonotes_loss: 0.0184
09/09 03:59:24 PM: Update 2795: task edges-ner-ontonotes, batch 795 (2795): mcc: 0.9454, acc: 0.9206, precision: 0.9650, recall: 0.9320, f1: 0.9482, edges-ner-ontonotes_loss: 0.0181
09/09 03:59:34 PM: Update 2838: task edges-ner-ontonotes, batch 838 (2838): mcc: 0.9441, acc: 0.9187, precision: 0.9643, recall: 0.9302, f1: 0.9469, edges-ner-ontonotes_loss: 0.0187
09/09 03:59:44 PM: Update 2893: task edges-ner-ontonotes, batch 893 (2893): mcc: 0.9421, acc: 0.9162, precision: 0.9631, recall: 0.9278, f1: 0.9451, edges-ner-ontonotes_loss: 0.0195
09/09 03:59:54 PM: Update 2943: task edges-ner-ontonotes, batch 943 (2943): mcc: 0.9407, acc: 0.9145, precision: 0.9622, recall: 0.9261, f1: 0.9438, edges-ner-ontonotes_loss: 0.0201
09/09 04:00:04 PM: Update 2992: task edges-ner-ontonotes, batch 992 (2992): mcc: 0.9392, acc: 0.9126, precision: 0.9613, recall: 0.9242, f1: 0.9424, edges-ner-ontonotes_loss: 0.0207
09/09 04:00:05 PM: ***** Step 3000 / Validation 3 *****
09/09 04:00:05 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:00:05 PM: Validating...
09/09 04:00:14 PM: Evaluate: task edges-ner-ontonotes, batch 37 (157): mcc: 0.8991, acc: 0.8696, precision: 0.9275, recall: 0.8822, f1: 0.9043, edges-ner-ontonotes_loss: 0.0329
09/09 04:00:24 PM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.9162, acc: 0.8854, precision: 0.9473, recall: 0.8949, f1: 0.9203, edges-ner-ontonotes_loss: 0.0292
09/09 04:00:34 PM: Evaluate: task edges-ner-ontonotes, batch 116 (157): mcc: 0.9266, acc: 0.8967, precision: 0.9557, recall: 0.9060, f1: 0.9302, edges-ner-ontonotes_loss: 0.0258
09/09 04:00:44 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:00:44 PM: Best result seen so far for macro.
09/09 04:00:44 PM: Updating LR scheduler:
09/09 04:00:44 PM: 	Best result seen so far for macro_avg: 0.938
09/09 04:00:44 PM: 	# validation passes without improvement: 0
09/09 04:00:44 PM: edges-ner-ontonotes_loss: training: 0.020676 validation: 0.023041
09/09 04:00:44 PM: macro_avg: validation: 0.937964
09/09 04:00:44 PM: micro_avg: validation: 0.000000
09/09 04:00:44 PM: edges-ner-ontonotes_mcc: training: 0.939217 validation: 0.934725
09/09 04:00:44 PM: edges-ner-ontonotes_acc: training: 0.912597 validation: 0.906733
09/09 04:00:44 PM: edges-ner-ontonotes_precision: training: 0.961264 validation: 0.961609
09/09 04:00:44 PM: edges-ner-ontonotes_recall: training: 0.924137 validation: 0.915453
09/09 04:00:44 PM: edges-ner-ontonotes_f1: training: 0.942335 validation: 0.937964
09/09 04:00:44 PM: Global learning rate: 0.0001
09/09 04:00:44 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:00:44 PM: Update 3008: task edges-ner-ontonotes, batch 8 (3008): mcc: 0.9163, acc: 0.8791, precision: 0.9576, recall: 0.8854, f1: 0.9201, edges-ner-ontonotes_loss: 0.0286
09/09 04:00:55 PM: Update 3080: task edges-ner-ontonotes, batch 80 (3080): mcc: 0.9124, acc: 0.8755, precision: 0.9459, recall: 0.8891, f1: 0.9166, edges-ner-ontonotes_loss: 0.0316
09/09 04:01:05 PM: Update 3133: task edges-ner-ontonotes, batch 133 (3133): mcc: 0.9121, acc: 0.8761, precision: 0.9450, recall: 0.8896, f1: 0.9164, edges-ner-ontonotes_loss: 0.0302
09/09 04:01:15 PM: Update 3202: task edges-ner-ontonotes, batch 202 (3202): mcc: 0.9157, acc: 0.8822, precision: 0.9455, recall: 0.8956, f1: 0.9199, edges-ner-ontonotes_loss: 0.0288
09/09 04:01:25 PM: Update 3279: task edges-ner-ontonotes, batch 279 (3279): mcc: 0.9197, acc: 0.8880, precision: 0.9479, recall: 0.9007, f1: 0.9237, edges-ner-ontonotes_loss: 0.0270
09/09 04:01:35 PM: Update 3359: task edges-ner-ontonotes, batch 359 (3359): mcc: 0.9224, acc: 0.8920, precision: 0.9493, recall: 0.9045, f1: 0.9264, edges-ner-ontonotes_loss: 0.0259
09/09 04:01:48 PM: Update 3426: task edges-ner-ontonotes, batch 426 (3426): mcc: 0.9239, acc: 0.8942, precision: 0.9500, recall: 0.9066, f1: 0.9278, edges-ner-ontonotes_loss: 0.0253
09/09 04:01:58 PM: Update 3485: task edges-ner-ontonotes, batch 485 (3485): mcc: 0.9250, acc: 0.8960, precision: 0.9505, recall: 0.9082, f1: 0.9289, edges-ner-ontonotes_loss: 0.0249
09/09 04:02:08 PM: Update 3544: task edges-ner-ontonotes, batch 544 (3544): mcc: 0.9271, acc: 0.8983, precision: 0.9522, recall: 0.9104, f1: 0.9308, edges-ner-ontonotes_loss: 0.0244
09/09 04:02:18 PM: Update 3604: task edges-ner-ontonotes, batch 604 (3604): mcc: 0.9287, acc: 0.9003, precision: 0.9532, recall: 0.9123, f1: 0.9323, edges-ner-ontonotes_loss: 0.0240
09/09 04:02:28 PM: Update 3673: task edges-ner-ontonotes, batch 673 (3673): mcc: 0.9303, acc: 0.9026, precision: 0.9542, recall: 0.9144, f1: 0.9339, edges-ner-ontonotes_loss: 0.0234
09/09 04:02:38 PM: Update 3732: task edges-ner-ontonotes, batch 732 (3732): mcc: 0.9318, acc: 0.9046, precision: 0.9552, recall: 0.9163, f1: 0.9353, edges-ner-ontonotes_loss: 0.0230
09/09 04:02:48 PM: Update 3776: task edges-ner-ontonotes, batch 776 (3776): mcc: 0.9328, acc: 0.9059, precision: 0.9557, recall: 0.9176, f1: 0.9363, edges-ner-ontonotes_loss: 0.0227
09/09 04:02:58 PM: Update 3833: task edges-ner-ontonotes, batch 833 (3833): mcc: 0.9345, acc: 0.9078, precision: 0.9567, recall: 0.9197, f1: 0.9379, edges-ner-ontonotes_loss: 0.0223
09/09 04:03:08 PM: Update 3899: task edges-ner-ontonotes, batch 899 (3899): mcc: 0.9359, acc: 0.9097, precision: 0.9573, recall: 0.9218, f1: 0.9392, edges-ner-ontonotes_loss: 0.0217
09/09 04:03:18 PM: Update 3950: task edges-ner-ontonotes, batch 950 (3950): mcc: 0.9371, acc: 0.9112, precision: 0.9581, recall: 0.9233, f1: 0.9403, edges-ner-ontonotes_loss: 0.0213
09/09 04:03:28 PM: Update 4000: task edges-ner-ontonotes, batch 1000 (4000): mcc: 0.9384, acc: 0.9129, precision: 0.9589, recall: 0.9249, f1: 0.9416, edges-ner-ontonotes_loss: 0.0209
09/09 04:03:28 PM: ***** Step 4000 / Validation 4 *****
09/09 04:03:28 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:03:28 PM: Validating...
09/09 04:03:38 PM: Evaluate: task edges-ner-ontonotes, batch 44 (157): mcc: 0.9117, acc: 0.8861, precision: 0.9332, recall: 0.9001, f1: 0.9163, edges-ner-ontonotes_loss: 0.0304
09/09 04:03:48 PM: Evaluate: task edges-ner-ontonotes, batch 79 (157): mcc: 0.9241, acc: 0.8963, precision: 0.9494, recall: 0.9074, f1: 0.9280, edges-ner-ontonotes_loss: 0.0271
09/09 04:03:59 PM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.9322, acc: 0.9055, precision: 0.9554, recall: 0.9167, f1: 0.9357, edges-ner-ontonotes_loss: 0.0241
09/09 04:04:08 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:04:08 PM: Best result seen so far for macro.
09/09 04:04:08 PM: Updating LR scheduler:
09/09 04:04:08 PM: 	Best result seen so far for macro_avg: 0.944
09/09 04:04:08 PM: 	# validation passes without improvement: 0
09/09 04:04:08 PM: edges-ner-ontonotes_loss: training: 0.020865 validation: 0.020998
09/09 04:04:08 PM: macro_avg: validation: 0.944131
09/09 04:04:08 PM: micro_avg: validation: 0.000000
09/09 04:04:08 PM: edges-ner-ontonotes_mcc: training: 0.938402 validation: 0.941071
09/09 04:04:08 PM: edges-ner-ontonotes_acc: training: 0.912892 validation: 0.917046
09/09 04:04:08 PM: edges-ner-ontonotes_precision: training: 0.958921 validation: 0.961115
09/09 04:04:08 PM: edges-ner-ontonotes_recall: training: 0.924893 validation: 0.927737
09/09 04:04:08 PM: edges-ner-ontonotes_f1: training: 0.941600 validation: 0.944131
09/09 04:04:08 PM: Global learning rate: 0.0001
09/09 04:04:08 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:04:09 PM: Update 4004: task edges-ner-ontonotes, batch 4 (4004): mcc: 0.9505, acc: 0.9246, precision: 0.9647, recall: 0.9418, f1: 0.9531, edges-ner-ontonotes_loss: 0.0183
09/09 04:04:19 PM: Update 4053: task edges-ner-ontonotes, batch 53 (4053): mcc: 0.9596, acc: 0.9391, precision: 0.9723, recall: 0.9514, f1: 0.9617, edges-ner-ontonotes_loss: 0.0136
09/09 04:04:29 PM: Update 4114: task edges-ner-ontonotes, batch 114 (4114): mcc: 0.9578, acc: 0.9365, precision: 0.9715, recall: 0.9488, f1: 0.9600, edges-ner-ontonotes_loss: 0.0140
09/09 04:04:39 PM: Update 4172: task edges-ner-ontonotes, batch 172 (4172): mcc: 0.9559, acc: 0.9344, precision: 0.9693, recall: 0.9475, f1: 0.9583, edges-ner-ontonotes_loss: 0.0144
09/09 04:04:49 PM: Update 4233: task edges-ner-ontonotes, batch 233 (4233): mcc: 0.9555, acc: 0.9342, precision: 0.9690, recall: 0.9471, f1: 0.9579, edges-ner-ontonotes_loss: 0.0146
09/09 04:04:59 PM: Update 4295: task edges-ner-ontonotes, batch 295 (4295): mcc: 0.9562, acc: 0.9352, precision: 0.9693, recall: 0.9481, f1: 0.9586, edges-ner-ontonotes_loss: 0.0144
09/09 04:05:09 PM: Update 4354: task edges-ner-ontonotes, batch 354 (4354): mcc: 0.9558, acc: 0.9346, precision: 0.9689, recall: 0.9477, f1: 0.9582, edges-ner-ontonotes_loss: 0.0145
09/09 04:05:20 PM: Update 4400: task edges-ner-ontonotes, batch 400 (4400): mcc: 0.9518, acc: 0.9295, precision: 0.9666, recall: 0.9425, f1: 0.9544, edges-ner-ontonotes_loss: 0.0160
09/09 04:05:30 PM: Update 4462: task edges-ner-ontonotes, batch 462 (4462): mcc: 0.9489, acc: 0.9259, precision: 0.9648, recall: 0.9387, f1: 0.9516, edges-ner-ontonotes_loss: 0.0171
09/09 04:05:40 PM: Update 4524: task edges-ner-ontonotes, batch 524 (4524): mcc: 0.9457, acc: 0.9219, precision: 0.9630, recall: 0.9346, f1: 0.9486, edges-ner-ontonotes_loss: 0.0187
09/09 04:05:50 PM: Update 4586: task edges-ner-ontonotes, batch 586 (4586): mcc: 0.9427, acc: 0.9183, precision: 0.9608, recall: 0.9311, f1: 0.9457, edges-ner-ontonotes_loss: 0.0201
09/09 04:06:00 PM: Update 4657: task edges-ner-ontonotes, batch 657 (4657): mcc: 0.9403, acc: 0.9150, precision: 0.9597, recall: 0.9277, f1: 0.9434, edges-ner-ontonotes_loss: 0.0209
09/09 04:06:10 PM: Update 4717: task edges-ner-ontonotes, batch 717 (4717): mcc: 0.9393, acc: 0.9139, precision: 0.9587, recall: 0.9268, f1: 0.9425, edges-ner-ontonotes_loss: 0.0212
09/09 04:06:20 PM: Update 4788: task edges-ner-ontonotes, batch 788 (4788): mcc: 0.9391, acc: 0.9139, precision: 0.9585, recall: 0.9267, f1: 0.9423, edges-ner-ontonotes_loss: 0.0212
09/09 04:06:30 PM: Update 4858: task edges-ner-ontonotes, batch 858 (4858): mcc: 0.9388, acc: 0.9136, precision: 0.9583, recall: 0.9263, f1: 0.9420, edges-ner-ontonotes_loss: 0.0212
09/09 04:06:40 PM: Update 4944: task edges-ner-ontonotes, batch 944 (4944): mcc: 0.9389, acc: 0.9140, precision: 0.9581, recall: 0.9266, f1: 0.9421, edges-ner-ontonotes_loss: 0.0211
09/09 04:06:50 PM: Update 4983: task edges-ner-ontonotes, batch 983 (4983): mcc: 0.9385, acc: 0.9136, precision: 0.9578, recall: 0.9262, f1: 0.9417, edges-ner-ontonotes_loss: 0.0211
09/09 04:06:54 PM: ***** Step 5000 / Validation 5 *****
09/09 04:06:54 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:06:54 PM: Validating...
09/09 04:07:01 PM: Evaluate: task edges-ner-ontonotes, batch 29 (157): mcc: 0.8986, acc: 0.8679, precision: 0.9168, recall: 0.8916, f1: 0.9040, edges-ner-ontonotes_loss: 0.0322
09/09 04:07:11 PM: Evaluate: task edges-ner-ontonotes, batch 65 (157): mcc: 0.9310, acc: 0.9053, precision: 0.9497, recall: 0.9201, f1: 0.9347, edges-ner-ontonotes_loss: 0.0251
09/09 04:07:21 PM: Evaluate: task edges-ner-ontonotes, batch 102 (157): mcc: 0.9342, acc: 0.9080, precision: 0.9566, recall: 0.9194, f1: 0.9376, edges-ner-ontonotes_loss: 0.0239
09/09 04:07:31 PM: Evaluate: task edges-ner-ontonotes, batch 141 (157): mcc: 0.9434, acc: 0.9194, precision: 0.9624, recall: 0.9308, f1: 0.9463, edges-ner-ontonotes_loss: 0.0210
09/09 04:07:35 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:07:35 PM: Best result seen so far for macro.
09/09 04:07:35 PM: Updating LR scheduler:
09/09 04:07:35 PM: 	Best result seen so far for macro_avg: 0.947
09/09 04:07:35 PM: 	# validation passes without improvement: 0
09/09 04:07:35 PM: edges-ner-ontonotes_loss: training: 0.021096 validation: 0.020418
09/09 04:07:35 PM: macro_avg: validation: 0.946983
09/09 04:07:35 PM: micro_avg: validation: 0.000000
09/09 04:07:35 PM: edges-ner-ontonotes_mcc: training: 0.938618 validation: 0.944058
09/09 04:07:35 PM: edges-ner-ontonotes_acc: training: 0.913764 validation: 0.920306
09/09 04:07:35 PM: edges-ner-ontonotes_precision: training: 0.957839 validation: 0.962635
09/09 04:07:35 PM: edges-ner-ontonotes_recall: training: 0.926353 validation: 0.931832
09/09 04:07:35 PM: edges-ner-ontonotes_f1: training: 0.941833 validation: 0.946983
09/09 04:07:35 PM: Global learning rate: 0.0001
09/09 04:07:35 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:07:41 PM: Update 5036: task edges-ner-ontonotes, batch 36 (5036): mcc: 0.9333, acc: 0.9111, precision: 0.9489, recall: 0.9253, f1: 0.9369, edges-ner-ontonotes_loss: 0.0214
09/09 04:07:51 PM: Update 5094: task edges-ner-ontonotes, batch 94 (5094): mcc: 0.9398, acc: 0.9181, precision: 0.9555, recall: 0.9308, f1: 0.9430, edges-ner-ontonotes_loss: 0.0203
09/09 04:08:01 PM: Update 5158: task edges-ner-ontonotes, batch 158 (5158): mcc: 0.9430, acc: 0.9218, precision: 0.9588, recall: 0.9336, f1: 0.9460, edges-ner-ontonotes_loss: 0.0195
09/09 04:08:11 PM: Update 5224: task edges-ner-ontonotes, batch 224 (5224): mcc: 0.9433, acc: 0.9222, precision: 0.9594, recall: 0.9336, f1: 0.9463, edges-ner-ontonotes_loss: 0.0190
09/09 04:08:23 PM: Update 5295: task edges-ner-ontonotes, batch 295 (5295): mcc: 0.9457, acc: 0.9249, precision: 0.9613, recall: 0.9363, f1: 0.9486, edges-ner-ontonotes_loss: 0.0182
09/09 04:08:33 PM: Update 5352: task edges-ner-ontonotes, batch 352 (5352): mcc: 0.9469, acc: 0.9259, precision: 0.9622, recall: 0.9375, f1: 0.9497, edges-ner-ontonotes_loss: 0.0177
09/09 04:08:43 PM: Update 5414: task edges-ner-ontonotes, batch 414 (5414): mcc: 0.9488, acc: 0.9278, precision: 0.9634, recall: 0.9399, f1: 0.9515, edges-ner-ontonotes_loss: 0.0170
09/09 04:08:53 PM: Update 5471: task edges-ner-ontonotes, batch 471 (5471): mcc: 0.9496, acc: 0.9289, precision: 0.9640, recall: 0.9409, f1: 0.9523, edges-ner-ontonotes_loss: 0.0167
09/09 04:09:03 PM: Update 5532: task edges-ner-ontonotes, batch 532 (5532): mcc: 0.9511, acc: 0.9307, precision: 0.9648, recall: 0.9428, f1: 0.9537, edges-ner-ontonotes_loss: 0.0163
09/09 04:09:13 PM: Update 5600: task edges-ner-ontonotes, batch 600 (5600): mcc: 0.9524, acc: 0.9324, precision: 0.9661, recall: 0.9440, f1: 0.9549, edges-ner-ontonotes_loss: 0.0159
09/09 04:09:24 PM: Update 5641: task edges-ner-ontonotes, batch 641 (5641): mcc: 0.9527, acc: 0.9326, precision: 0.9662, recall: 0.9445, f1: 0.9552, edges-ner-ontonotes_loss: 0.0157
09/09 04:09:34 PM: Update 5696: task edges-ner-ontonotes, batch 696 (5696): mcc: 0.9530, acc: 0.9330, precision: 0.9663, recall: 0.9451, f1: 0.9556, edges-ner-ontonotes_loss: 0.0156
09/09 04:09:44 PM: Update 5758: task edges-ner-ontonotes, batch 758 (5758): mcc: 0.9537, acc: 0.9337, precision: 0.9670, recall: 0.9457, f1: 0.9562, edges-ner-ontonotes_loss: 0.0154
09/09 04:09:54 PM: Update 5819: task edges-ner-ontonotes, batch 819 (5819): mcc: 0.9541, acc: 0.9342, precision: 0.9672, recall: 0.9462, f1: 0.9566, edges-ner-ontonotes_loss: 0.0153
09/09 04:10:04 PM: Update 5876: task edges-ner-ontonotes, batch 876 (5876): mcc: 0.9544, acc: 0.9345, precision: 0.9673, recall: 0.9466, f1: 0.9568, edges-ner-ontonotes_loss: 0.0152
09/09 04:10:14 PM: Update 5924: task edges-ner-ontonotes, batch 924 (5924): mcc: 0.9543, acc: 0.9343, precision: 0.9673, recall: 0.9464, f1: 0.9568, edges-ner-ontonotes_loss: 0.0152
09/09 04:10:24 PM: Update 5977: task edges-ner-ontonotes, batch 977 (5977): mcc: 0.9527, acc: 0.9322, precision: 0.9663, recall: 0.9445, f1: 0.9553, edges-ner-ontonotes_loss: 0.0159
09/09 04:10:29 PM: ***** Step 6000 / Validation 6 *****
09/09 04:10:29 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:10:29 PM: Validating...
09/09 04:10:34 PM: Evaluate: task edges-ner-ontonotes, batch 23 (157): mcc: 0.8767, acc: 0.8459, precision: 0.9028, recall: 0.8643, f1: 0.8832, edges-ner-ontonotes_loss: 0.0391
09/09 04:10:45 PM: Evaluate: task edges-ner-ontonotes, batch 62 (157): mcc: 0.9298, acc: 0.9080, precision: 0.9458, recall: 0.9218, f1: 0.9336, edges-ner-ontonotes_loss: 0.0253
09/09 04:10:55 PM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.9308, acc: 0.9053, precision: 0.9527, recall: 0.9168, f1: 0.9344, edges-ner-ontonotes_loss: 0.0250
09/09 04:11:05 PM: Evaluate: task edges-ner-ontonotes, batch 132 (157): mcc: 0.9393, acc: 0.9149, precision: 0.9590, recall: 0.9266, f1: 0.9425, edges-ner-ontonotes_loss: 0.0221
09/09 04:11:11 PM: Updating LR scheduler:
09/09 04:11:11 PM: 	Best result seen so far for macro_avg: 0.947
09/09 04:11:11 PM: 	# validation passes without improvement: 1
09/09 04:11:11 PM: edges-ner-ontonotes_loss: training: 0.016110 validation: 0.020624
09/09 04:11:11 PM: macro_avg: validation: 0.945787
09/09 04:11:11 PM: micro_avg: validation: 0.000000
09/09 04:11:11 PM: edges-ner-ontonotes_mcc: training: 0.952154 validation: 0.942793
09/09 04:11:11 PM: edges-ner-ontonotes_acc: training: 0.931464 validation: 0.919245
09/09 04:11:11 PM: edges-ner-ontonotes_precision: training: 0.966036 validation: 0.961457
09/09 04:11:11 PM: edges-ner-ontonotes_recall: training: 0.943649 validation: 0.930619
09/09 04:11:11 PM: edges-ner-ontonotes_f1: training: 0.954711 validation: 0.945787
09/09 04:11:11 PM: Global learning rate: 0.0001
09/09 04:11:11 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:11:15 PM: Update 6027: task edges-ner-ontonotes, batch 27 (6027): mcc: 0.9312, acc: 0.9020, precision: 0.9524, recall: 0.9179, f1: 0.9348, edges-ner-ontonotes_loss: 0.0250
09/09 04:11:25 PM: Update 6093: task edges-ner-ontonotes, batch 93 (6093): mcc: 0.9229, acc: 0.8924, precision: 0.9475, recall: 0.9071, f1: 0.9268, edges-ner-ontonotes_loss: 0.0265
09/09 04:11:36 PM: Update 6153: task edges-ner-ontonotes, batch 153 (6153): mcc: 0.9235, acc: 0.8945, precision: 0.9471, recall: 0.9085, f1: 0.9274, edges-ner-ontonotes_loss: 0.0268
09/09 04:11:47 PM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.9235, acc: 0.8936, precision: 0.9478, recall: 0.9079, f1: 0.9275, edges-ner-ontonotes_loss: 0.0267
09/09 04:11:58 PM: Update 6297: task edges-ner-ontonotes, batch 297 (6297): mcc: 0.9266, acc: 0.8980, precision: 0.9497, recall: 0.9119, f1: 0.9304, edges-ner-ontonotes_loss: 0.0252
09/09 04:12:08 PM: Update 6368: task edges-ner-ontonotes, batch 368 (6368): mcc: 0.9279, acc: 0.9001, precision: 0.9505, recall: 0.9135, f1: 0.9317, edges-ner-ontonotes_loss: 0.0245
09/09 04:12:18 PM: Update 6443: task edges-ner-ontonotes, batch 443 (6443): mcc: 0.9296, acc: 0.9028, precision: 0.9513, recall: 0.9160, f1: 0.9333, edges-ner-ontonotes_loss: 0.0239
09/09 04:12:28 PM: Update 6515: task edges-ner-ontonotes, batch 515 (6515): mcc: 0.9309, acc: 0.9048, precision: 0.9517, recall: 0.9180, f1: 0.9345, edges-ner-ontonotes_loss: 0.0234
09/09 04:12:38 PM: Update 6572: task edges-ner-ontonotes, batch 572 (6572): mcc: 0.9322, acc: 0.9066, precision: 0.9526, recall: 0.9195, f1: 0.9358, edges-ner-ontonotes_loss: 0.0228
09/09 04:12:48 PM: Update 6634: task edges-ner-ontonotes, batch 634 (6634): mcc: 0.9347, acc: 0.9096, precision: 0.9543, recall: 0.9225, f1: 0.9381, edges-ner-ontonotes_loss: 0.0221
09/09 04:12:58 PM: Update 6695: task edges-ner-ontonotes, batch 695 (6695): mcc: 0.9354, acc: 0.9108, precision: 0.9549, recall: 0.9233, f1: 0.9388, edges-ner-ontonotes_loss: 0.0218
09/09 04:13:08 PM: Update 6762: task edges-ner-ontonotes, batch 762 (6762): mcc: 0.9372, acc: 0.9132, precision: 0.9560, recall: 0.9255, f1: 0.9405, edges-ner-ontonotes_loss: 0.0213
09/09 04:13:18 PM: Update 6818: task edges-ner-ontonotes, batch 818 (6818): mcc: 0.9384, acc: 0.9148, precision: 0.9568, recall: 0.9269, f1: 0.9417, edges-ner-ontonotes_loss: 0.0210
09/09 04:13:28 PM: Update 6873: task edges-ner-ontonotes, batch 873 (6873): mcc: 0.9393, acc: 0.9160, precision: 0.9575, recall: 0.9279, f1: 0.9425, edges-ner-ontonotes_loss: 0.0207
09/09 04:13:38 PM: Update 6934: task edges-ner-ontonotes, batch 934 (6934): mcc: 0.9406, acc: 0.9177, precision: 0.9582, recall: 0.9297, f1: 0.9438, edges-ner-ontonotes_loss: 0.0202
09/09 04:13:48 PM: Update 6992: task edges-ner-ontonotes, batch 992 (6992): mcc: 0.9422, acc: 0.9196, precision: 0.9593, recall: 0.9316, f1: 0.9452, edges-ner-ontonotes_loss: 0.0198
09/09 04:13:50 PM: ***** Step 7000 / Validation 7 *****
09/09 04:13:50 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:13:50 PM: Validating...
09/09 04:13:58 PM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.9056, acc: 0.8748, precision: 0.9258, recall: 0.8960, f1: 0.9106, edges-ner-ontonotes_loss: 0.0317
09/09 04:14:09 PM: Evaluate: task edges-ner-ontonotes, batch 71 (157): mcc: 0.9269, acc: 0.8979, precision: 0.9488, recall: 0.9132, f1: 0.9307, edges-ner-ontonotes_loss: 0.0266
09/09 04:14:19 PM: Evaluate: task edges-ner-ontonotes, batch 108 (157): mcc: 0.9330, acc: 0.9061, precision: 0.9536, recall: 0.9199, f1: 0.9365, edges-ner-ontonotes_loss: 0.0236
09/09 04:14:29 PM: Evaluate: task edges-ner-ontonotes, batch 139 (157): mcc: 0.9436, acc: 0.9205, precision: 0.9605, recall: 0.9330, f1: 0.9466, edges-ner-ontonotes_loss: 0.0207
09/09 04:14:34 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:14:34 PM: Best result seen so far for macro.
09/09 04:14:34 PM: Updating LR scheduler:
09/09 04:14:34 PM: 	Best result seen so far for macro_avg: 0.949
09/09 04:14:34 PM: 	# validation passes without improvement: 0
09/09 04:14:34 PM: edges-ner-ontonotes_loss: training: 0.019736 validation: 0.019776
09/09 04:14:34 PM: macro_avg: validation: 0.948542
09/09 04:14:34 PM: micro_avg: validation: 0.000000
09/09 04:14:34 PM: edges-ner-ontonotes_mcc: training: 0.942397 validation: 0.945657
09/09 04:14:34 PM: edges-ner-ontonotes_acc: training: 0.919822 validation: 0.923491
09/09 04:14:34 PM: edges-ner-ontonotes_precision: training: 0.959425 validation: 0.961661
09/09 04:14:34 PM: edges-ner-ontonotes_recall: training: 0.931866 validation: 0.935775
09/09 04:14:34 PM: edges-ner-ontonotes_f1: training: 0.945445 validation: 0.948542
09/09 04:14:34 PM: Global learning rate: 0.0001
09/09 04:14:34 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:14:39 PM: Update 7031: task edges-ner-ontonotes, batch 31 (7031): mcc: 0.9594, acc: 0.9401, precision: 0.9668, recall: 0.9566, f1: 0.9617, edges-ner-ontonotes_loss: 0.0138
09/09 04:14:49 PM: Update 7092: task edges-ner-ontonotes, batch 92 (7092): mcc: 0.9600, acc: 0.9416, precision: 0.9696, recall: 0.9549, f1: 0.9622, edges-ner-ontonotes_loss: 0.0135
09/09 04:14:59 PM: Update 7145: task edges-ner-ontonotes, batch 145 (7145): mcc: 0.9620, acc: 0.9442, precision: 0.9715, recall: 0.9568, f1: 0.9641, edges-ner-ontonotes_loss: 0.0128
09/09 04:15:09 PM: Update 7189: task edges-ner-ontonotes, batch 189 (7189): mcc: 0.9618, acc: 0.9441, precision: 0.9711, recall: 0.9568, f1: 0.9639, edges-ner-ontonotes_loss: 0.0129
09/09 04:15:19 PM: Update 7248: task edges-ner-ontonotes, batch 248 (7248): mcc: 0.9604, acc: 0.9420, precision: 0.9703, recall: 0.9550, f1: 0.9626, edges-ner-ontonotes_loss: 0.0135
09/09 04:15:29 PM: Update 7311: task edges-ner-ontonotes, batch 311 (7311): mcc: 0.9606, acc: 0.9419, precision: 0.9704, recall: 0.9552, f1: 0.9628, edges-ner-ontonotes_loss: 0.0132
09/09 04:15:39 PM: Update 7366: task edges-ner-ontonotes, batch 366 (7366): mcc: 0.9613, acc: 0.9425, precision: 0.9714, recall: 0.9556, f1: 0.9634, edges-ner-ontonotes_loss: 0.0130
09/09 04:15:49 PM: Update 7426: task edges-ner-ontonotes, batch 426 (7426): mcc: 0.9607, acc: 0.9417, precision: 0.9708, recall: 0.9550, f1: 0.9629, edges-ner-ontonotes_loss: 0.0131
09/09 04:16:00 PM: Update 7478: task edges-ner-ontonotes, batch 478 (7478): mcc: 0.9600, acc: 0.9408, precision: 0.9704, recall: 0.9542, f1: 0.9622, edges-ner-ontonotes_loss: 0.0131
09/09 04:16:10 PM: Update 7545: task edges-ner-ontonotes, batch 545 (7545): mcc: 0.9555, acc: 0.9351, precision: 0.9674, recall: 0.9486, f1: 0.9579, edges-ner-ontonotes_loss: 0.0152
09/09 04:16:20 PM: Update 7600: task edges-ner-ontonotes, batch 600 (7600): mcc: 0.9532, acc: 0.9321, precision: 0.9657, recall: 0.9459, f1: 0.9557, edges-ner-ontonotes_loss: 0.0160
09/09 04:16:30 PM: Update 7665: task edges-ner-ontonotes, batch 665 (7665): mcc: 0.9509, acc: 0.9292, precision: 0.9642, recall: 0.9430, f1: 0.9535, edges-ner-ontonotes_loss: 0.0170
09/09 04:16:40 PM: Update 7720: task edges-ner-ontonotes, batch 720 (7720): mcc: 0.9490, acc: 0.9267, precision: 0.9629, recall: 0.9409, f1: 0.9518, edges-ner-ontonotes_loss: 0.0176
09/09 04:16:52 PM: Update 7781: task edges-ner-ontonotes, batch 781 (7781): mcc: 0.9477, acc: 0.9251, precision: 0.9620, recall: 0.9393, f1: 0.9505, edges-ner-ontonotes_loss: 0.0182
09/09 04:17:02 PM: Update 7857: task edges-ner-ontonotes, batch 857 (7857): mcc: 0.9468, acc: 0.9241, precision: 0.9611, recall: 0.9384, f1: 0.9496, edges-ner-ontonotes_loss: 0.0186
09/09 04:17:12 PM: Update 7933: task edges-ner-ontonotes, batch 933 (7933): mcc: 0.9466, acc: 0.9240, precision: 0.9610, recall: 0.9381, f1: 0.9494, edges-ner-ontonotes_loss: 0.0185
09/09 04:17:21 PM: ***** Step 8000 / Validation 8 *****
09/09 04:17:21 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:17:21 PM: Validating...
09/09 04:17:22 PM: Evaluate: task edges-ner-ontonotes, batch 4 (157): mcc: 0.8032, acc: 0.7355, precision: 0.8498, recall: 0.7790, f1: 0.8129, edges-ner-ontonotes_loss: 0.0525
09/09 04:17:32 PM: Evaluate: task edges-ner-ontonotes, batch 50 (157): mcc: 0.9247, acc: 0.9009, precision: 0.9427, recall: 0.9151, f1: 0.9287, edges-ner-ontonotes_loss: 0.0258
09/09 04:17:42 PM: Evaluate: task edges-ner-ontonotes, batch 85 (157): mcc: 0.9383, acc: 0.9148, precision: 0.9576, recall: 0.9260, f1: 0.9415, edges-ner-ontonotes_loss: 0.0232
09/09 04:17:52 PM: Evaluate: task edges-ner-ontonotes, batch 117 (157): mcc: 0.9404, acc: 0.9170, precision: 0.9590, recall: 0.9286, f1: 0.9435, edges-ner-ontonotes_loss: 0.0219
09/09 04:18:02 PM: Evaluate: task edges-ner-ontonotes, batch 153 (157): mcc: 0.9463, acc: 0.9244, precision: 0.9628, recall: 0.9359, f1: 0.9492, edges-ner-ontonotes_loss: 0.0197
09/09 04:18:03 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:18:03 PM: Best result seen so far for macro.
09/09 04:18:03 PM: Updating LR scheduler:
09/09 04:18:03 PM: 	Best result seen so far for macro_avg: 0.950
09/09 04:18:03 PM: 	# validation passes without improvement: 0
09/09 04:18:03 PM: edges-ner-ontonotes_loss: training: 0.018591 validation: 0.019407
09/09 04:18:03 PM: macro_avg: validation: 0.949748
09/09 04:18:03 PM: micro_avg: validation: 0.000000
09/09 04:18:03 PM: edges-ner-ontonotes_mcc: training: 0.946292 validation: 0.946942
09/09 04:18:03 PM: edges-ner-ontonotes_acc: training: 0.923804 validation: 0.925235
09/09 04:18:03 PM: edges-ner-ontonotes_precision: training: 0.960811 validation: 0.963341
09/09 04:18:03 PM: edges-ner-ontonotes_recall: training: 0.937800 validation: 0.936533
09/09 04:18:03 PM: edges-ner-ontonotes_f1: training: 0.949166 validation: 0.949748
09/09 04:18:03 PM: Global learning rate: 0.0001
09/09 04:18:03 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:18:12 PM: Update 8062: task edges-ner-ontonotes, batch 62 (8062): mcc: 0.9443, acc: 0.9219, precision: 0.9578, recall: 0.9370, f1: 0.9473, edges-ner-ontonotes_loss: 0.0188
09/09 04:18:22 PM: Update 8116: task edges-ner-ontonotes, batch 116 (8116): mcc: 0.9450, acc: 0.9237, precision: 0.9589, recall: 0.9373, f1: 0.9479, edges-ner-ontonotes_loss: 0.0185
09/09 04:18:32 PM: Update 8178: task edges-ner-ontonotes, batch 178 (8178): mcc: 0.9471, acc: 0.9259, precision: 0.9612, recall: 0.9389, f1: 0.9499, edges-ner-ontonotes_loss: 0.0178
09/09 04:18:42 PM: Update 8240: task edges-ner-ontonotes, batch 240 (8240): mcc: 0.9478, acc: 0.9268, precision: 0.9618, recall: 0.9398, f1: 0.9506, edges-ner-ontonotes_loss: 0.0174
09/09 04:18:52 PM: Update 8300: task edges-ner-ontonotes, batch 300 (8300): mcc: 0.9472, acc: 0.9265, precision: 0.9615, recall: 0.9387, f1: 0.9500, edges-ner-ontonotes_loss: 0.0176
09/09 04:19:02 PM: Update 8358: task edges-ner-ontonotes, batch 358 (8358): mcc: 0.9481, acc: 0.9277, precision: 0.9627, recall: 0.9393, f1: 0.9508, edges-ner-ontonotes_loss: 0.0173
09/09 04:19:13 PM: Update 8407: task edges-ner-ontonotes, batch 407 (8407): mcc: 0.9487, acc: 0.9287, precision: 0.9630, recall: 0.9402, f1: 0.9514, edges-ner-ontonotes_loss: 0.0172
09/09 04:19:23 PM: Update 8475: task edges-ner-ontonotes, batch 475 (8475): mcc: 0.9506, acc: 0.9307, precision: 0.9642, recall: 0.9425, f1: 0.9532, edges-ner-ontonotes_loss: 0.0166
09/09 04:19:34 PM: Update 8536: task edges-ner-ontonotes, batch 536 (8536): mcc: 0.9516, acc: 0.9320, precision: 0.9646, recall: 0.9441, f1: 0.9542, edges-ner-ontonotes_loss: 0.0163
09/09 04:19:44 PM: Update 8594: task edges-ner-ontonotes, batch 594 (8594): mcc: 0.9528, acc: 0.9335, precision: 0.9654, recall: 0.9454, f1: 0.9553, edges-ner-ontonotes_loss: 0.0159
09/09 04:19:54 PM: Update 8650: task edges-ner-ontonotes, batch 650 (8650): mcc: 0.9540, acc: 0.9349, precision: 0.9664, recall: 0.9468, f1: 0.9565, edges-ner-ontonotes_loss: 0.0155
09/09 04:20:04 PM: Update 8709: task edges-ner-ontonotes, batch 709 (8709): mcc: 0.9549, acc: 0.9360, precision: 0.9671, recall: 0.9476, f1: 0.9573, edges-ner-ontonotes_loss: 0.0152
09/09 04:20:14 PM: Update 8764: task edges-ner-ontonotes, batch 764 (8764): mcc: 0.9555, acc: 0.9366, precision: 0.9676, recall: 0.9484, f1: 0.9579, edges-ner-ontonotes_loss: 0.0149
09/09 04:20:24 PM: Update 8822: task edges-ner-ontonotes, batch 822 (8822): mcc: 0.9562, acc: 0.9376, precision: 0.9679, recall: 0.9493, f1: 0.9585, edges-ner-ontonotes_loss: 0.0148
09/09 04:20:34 PM: Update 8884: task edges-ner-ontonotes, batch 884 (8884): mcc: 0.9566, acc: 0.9380, precision: 0.9682, recall: 0.9498, f1: 0.9589, edges-ner-ontonotes_loss: 0.0147
09/09 04:20:44 PM: Update 8942: task edges-ner-ontonotes, batch 942 (8942): mcc: 0.9567, acc: 0.9380, precision: 0.9683, recall: 0.9498, f1: 0.9590, edges-ner-ontonotes_loss: 0.0146
09/09 04:20:54 PM: ***** Step 9000 / Validation 9 *****
09/09 04:20:54 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:20:54 PM: Validating...
09/09 04:20:54 PM: Evaluate: task edges-ner-ontonotes, batch 1 (157): mcc: 0.7906, acc: 0.7377, precision: 0.8519, recall: 0.7541, f1: 0.8000, edges-ner-ontonotes_loss: 0.0595
09/09 04:21:05 PM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.9172, acc: 0.8906, precision: 0.9380, recall: 0.9058, f1: 0.9216, edges-ner-ontonotes_loss: 0.0294
09/09 04:21:15 PM: Evaluate: task edges-ner-ontonotes, batch 84 (157): mcc: 0.9339, acc: 0.9081, precision: 0.9542, recall: 0.9211, f1: 0.9374, edges-ner-ontonotes_loss: 0.0258
09/09 04:21:25 PM: Evaluate: task edges-ner-ontonotes, batch 116 (157): mcc: 0.9385, acc: 0.9149, precision: 0.9567, recall: 0.9271, f1: 0.9417, edges-ner-ontonotes_loss: 0.0233
09/09 04:21:35 PM: Evaluate: task edges-ner-ontonotes, batch 150 (157): mcc: 0.9461, acc: 0.9253, precision: 0.9616, recall: 0.9366, f1: 0.9489, edges-ner-ontonotes_loss: 0.0204
09/09 04:21:37 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:21:37 PM: Best result seen so far for macro.
09/09 04:21:37 PM: Updating LR scheduler:
09/09 04:21:37 PM: 	Best result seen so far for macro_avg: 0.950
09/09 04:21:37 PM: 	# validation passes without improvement: 1
09/09 04:21:37 PM: edges-ner-ontonotes_loss: training: 0.014442 validation: 0.019989
09/09 04:21:37 PM: macro_avg: validation: 0.949781
09/09 04:21:37 PM: micro_avg: validation: 0.000000
09/09 04:21:37 PM: edges-ner-ontonotes_mcc: training: 0.957005 validation: 0.946952
09/09 04:21:37 PM: edges-ner-ontonotes_acc: training: 0.938554 validation: 0.926448
09/09 04:21:37 PM: edges-ner-ontonotes_precision: training: 0.968601 validation: 0.961969
09/09 04:21:37 PM: edges-ner-ontonotes_recall: training: 0.950224 validation: 0.937898
09/09 04:21:37 PM: edges-ner-ontonotes_f1: training: 0.959324 validation: 0.949781
09/09 04:21:37 PM: Global learning rate: 0.0001
09/09 04:21:37 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:21:46 PM: Update 9033: task edges-ner-ontonotes, batch 33 (9033): mcc: 0.9651, acc: 0.9452, precision: 0.9748, recall: 0.9594, f1: 0.9670, edges-ner-ontonotes_loss: 0.0117
09/09 04:21:56 PM: Update 9099: task edges-ner-ontonotes, batch 99 (9099): mcc: 0.9392, acc: 0.9134, precision: 0.9567, recall: 0.9286, f1: 0.9424, edges-ner-ontonotes_loss: 0.0223
09/09 04:22:06 PM: Update 9164: task edges-ner-ontonotes, batch 164 (9164): mcc: 0.9361, acc: 0.9096, precision: 0.9545, recall: 0.9249, f1: 0.9394, edges-ner-ontonotes_loss: 0.0227
09/09 04:22:16 PM: Update 9227: task edges-ner-ontonotes, batch 227 (9227): mcc: 0.9350, acc: 0.9080, precision: 0.9546, recall: 0.9227, f1: 0.9384, edges-ner-ontonotes_loss: 0.0233
09/09 04:22:26 PM: Update 9292: task edges-ner-ontonotes, batch 292 (9292): mcc: 0.9324, acc: 0.9044, precision: 0.9524, recall: 0.9200, f1: 0.9359, edges-ner-ontonotes_loss: 0.0240
09/09 04:22:36 PM: Update 9346: task edges-ner-ontonotes, batch 346 (9346): mcc: 0.9316, acc: 0.9035, precision: 0.9517, recall: 0.9191, f1: 0.9351, edges-ner-ontonotes_loss: 0.0240
09/09 04:22:46 PM: Update 9416: task edges-ner-ontonotes, batch 416 (9416): mcc: 0.9318, acc: 0.9042, precision: 0.9515, recall: 0.9199, f1: 0.9354, edges-ner-ontonotes_loss: 0.0237
09/09 04:22:56 PM: Update 9488: task edges-ner-ontonotes, batch 488 (9488): mcc: 0.9338, acc: 0.9071, precision: 0.9527, recall: 0.9225, f1: 0.9373, edges-ner-ontonotes_loss: 0.0228
09/09 04:23:06 PM: Update 9560: task edges-ner-ontonotes, batch 560 (9560): mcc: 0.9348, acc: 0.9086, precision: 0.9532, recall: 0.9237, f1: 0.9382, edges-ner-ontonotes_loss: 0.0223
09/09 04:23:16 PM: Update 9633: task edges-ner-ontonotes, batch 633 (9633): mcc: 0.9356, acc: 0.9101, precision: 0.9537, recall: 0.9247, f1: 0.9390, edges-ner-ontonotes_loss: 0.0219
09/09 04:23:27 PM: Update 9683: task edges-ner-ontonotes, batch 683 (9683): mcc: 0.9366, acc: 0.9113, precision: 0.9546, recall: 0.9257, f1: 0.9399, edges-ner-ontonotes_loss: 0.0216
09/09 04:23:37 PM: Update 9748: task edges-ner-ontonotes, batch 748 (9748): mcc: 0.9382, acc: 0.9136, precision: 0.9557, recall: 0.9276, f1: 0.9415, edges-ner-ontonotes_loss: 0.0211
09/09 04:23:47 PM: Update 9809: task edges-ner-ontonotes, batch 809 (9809): mcc: 0.9390, acc: 0.9151, precision: 0.9561, recall: 0.9288, f1: 0.9423, edges-ner-ontonotes_loss: 0.0208
09/09 04:23:57 PM: Update 9872: task edges-ner-ontonotes, batch 872 (9872): mcc: 0.9402, acc: 0.9166, precision: 0.9570, recall: 0.9302, f1: 0.9434, edges-ner-ontonotes_loss: 0.0204
09/09 04:24:07 PM: Update 9935: task edges-ner-ontonotes, batch 935 (9935): mcc: 0.9409, acc: 0.9176, precision: 0.9574, recall: 0.9310, f1: 0.9440, edges-ner-ontonotes_loss: 0.0201
09/09 04:24:17 PM: Update 9980: task edges-ner-ontonotes, batch 980 (9980): mcc: 0.9417, acc: 0.9186, precision: 0.9579, recall: 0.9320, f1: 0.9448, edges-ner-ontonotes_loss: 0.0199
09/09 04:24:20 PM: ***** Step 10000 / Validation 10 *****
09/09 04:24:20 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:24:20 PM: Validating...
09/09 04:24:27 PM: Evaluate: task edges-ner-ontonotes, batch 41 (157): mcc: 0.9178, acc: 0.8908, precision: 0.9379, recall: 0.9069, f1: 0.9222, edges-ner-ontonotes_loss: 0.0283
09/09 04:24:37 PM: Evaluate: task edges-ner-ontonotes, batch 81 (157): mcc: 0.9365, acc: 0.9102, precision: 0.9576, recall: 0.9226, f1: 0.9398, edges-ner-ontonotes_loss: 0.0237
09/09 04:24:48 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9390, acc: 0.9141, precision: 0.9589, recall: 0.9261, f1: 0.9422, edges-ner-ontonotes_loss: 0.0220
09/09 04:24:58 PM: Evaluate: task edges-ner-ontonotes, batch 148 (157): mcc: 0.9466, acc: 0.9245, precision: 0.9636, recall: 0.9357, f1: 0.9494, edges-ner-ontonotes_loss: 0.0196
09/09 04:25:00 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:25:00 PM: Best result seen so far for macro.
09/09 04:25:00 PM: Updating LR scheduler:
09/09 04:25:00 PM: 	Best result seen so far for macro_avg: 0.950
09/09 04:25:00 PM: 	# validation passes without improvement: 0
09/09 04:25:00 PM: edges-ner-ontonotes_loss: training: 0.019702 validation: 0.019167
09/09 04:25:00 PM: macro_avg: validation: 0.950246
09/09 04:25:00 PM: micro_avg: validation: 0.000000
09/09 04:25:00 PM: edges-ner-ontonotes_mcc: training: 0.942161 validation: 0.947469
09/09 04:25:00 PM: edges-ner-ontonotes_acc: training: 0.919240 validation: 0.925766
09/09 04:25:00 PM: edges-ner-ontonotes_precision: training: 0.958264 validation: 0.963885
09/09 04:25:00 PM: edges-ner-ontonotes_recall: training: 0.932562 validation: 0.936988
09/09 04:25:00 PM: edges-ner-ontonotes_f1: training: 0.945238 validation: 0.950246
09/09 04:25:00 PM: Global learning rate: 0.0001
09/09 04:25:00 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:25:08 PM: Update 10033: task edges-ner-ontonotes, batch 33 (10033): mcc: 0.9650, acc: 0.9446, precision: 0.9742, recall: 0.9597, f1: 0.9669, edges-ner-ontonotes_loss: 0.0116
09/09 04:25:18 PM: Update 10082: task edges-ner-ontonotes, batch 82 (10082): mcc: 0.9626, acc: 0.9431, precision: 0.9715, recall: 0.9579, f1: 0.9646, edges-ner-ontonotes_loss: 0.0122
09/09 04:25:28 PM: Update 10142: task edges-ner-ontonotes, batch 142 (10142): mcc: 0.9623, acc: 0.9432, precision: 0.9709, recall: 0.9578, f1: 0.9643, edges-ner-ontonotes_loss: 0.0121
09/09 04:25:38 PM: Update 10197: task edges-ner-ontonotes, batch 197 (10197): mcc: 0.9630, acc: 0.9447, precision: 0.9716, recall: 0.9584, f1: 0.9650, edges-ner-ontonotes_loss: 0.0120
09/09 04:25:48 PM: Update 10256: task edges-ner-ontonotes, batch 256 (10256): mcc: 0.9633, acc: 0.9448, precision: 0.9721, recall: 0.9585, f1: 0.9653, edges-ner-ontonotes_loss: 0.0119
09/09 04:25:58 PM: Update 10299: task edges-ner-ontonotes, batch 299 (10299): mcc: 0.9632, acc: 0.9450, precision: 0.9718, recall: 0.9587, f1: 0.9652, edges-ner-ontonotes_loss: 0.0120
09/09 04:26:09 PM: Update 10366: task edges-ner-ontonotes, batch 366 (10366): mcc: 0.9628, acc: 0.9447, precision: 0.9714, recall: 0.9584, f1: 0.9648, edges-ner-ontonotes_loss: 0.0121
09/09 04:26:19 PM: Update 10425: task edges-ner-ontonotes, batch 425 (10425): mcc: 0.9625, acc: 0.9444, precision: 0.9710, recall: 0.9581, f1: 0.9645, edges-ner-ontonotes_loss: 0.0120
09/09 04:26:29 PM: Update 10482: task edges-ner-ontonotes, batch 482 (10482): mcc: 0.9627, acc: 0.9446, precision: 0.9711, recall: 0.9585, f1: 0.9647, edges-ner-ontonotes_loss: 0.0121
09/09 04:26:39 PM: Update 10547: task edges-ner-ontonotes, batch 547 (10547): mcc: 0.9631, acc: 0.9453, precision: 0.9715, recall: 0.9589, f1: 0.9652, edges-ner-ontonotes_loss: 0.0122
09/09 04:26:49 PM: Update 10590: task edges-ner-ontonotes, batch 590 (10590): mcc: 0.9629, acc: 0.9449, precision: 0.9715, recall: 0.9584, f1: 0.9649, edges-ner-ontonotes_loss: 0.0121
09/09 04:26:59 PM: Update 10653: task edges-ner-ontonotes, batch 653 (10653): mcc: 0.9601, acc: 0.9414, precision: 0.9698, recall: 0.9549, f1: 0.9623, edges-ner-ontonotes_loss: 0.0133
09/09 04:27:09 PM: Update 10717: task edges-ner-ontonotes, batch 717 (10717): mcc: 0.9575, acc: 0.9380, precision: 0.9681, recall: 0.9516, f1: 0.9598, edges-ner-ontonotes_loss: 0.0143
09/09 04:27:19 PM: Update 10783: task edges-ner-ontonotes, batch 783 (10783): mcc: 0.9558, acc: 0.9358, precision: 0.9670, recall: 0.9495, f1: 0.9582, edges-ner-ontonotes_loss: 0.0150
09/09 04:27:29 PM: Update 10843: task edges-ner-ontonotes, batch 843 (10843): mcc: 0.9537, acc: 0.9331, precision: 0.9656, recall: 0.9469, f1: 0.9562, edges-ner-ontonotes_loss: 0.0158
09/09 04:27:40 PM: Update 10893: task edges-ner-ontonotes, batch 893 (10893): mcc: 0.9524, acc: 0.9313, precision: 0.9649, recall: 0.9452, f1: 0.9549, edges-ner-ontonotes_loss: 0.0164
09/09 04:27:50 PM: Update 10954: task edges-ner-ontonotes, batch 954 (10954): mcc: 0.9513, acc: 0.9298, precision: 0.9640, recall: 0.9440, f1: 0.9539, edges-ner-ontonotes_loss: 0.0168
09/09 04:27:55 PM: ***** Step 11000 / Validation 11 *****
09/09 04:27:55 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:27:55 PM: Validating...
09/09 04:28:00 PM: Evaluate: task edges-ner-ontonotes, batch 24 (157): mcc: 0.8921, acc: 0.8570, precision: 0.9155, recall: 0.8808, f1: 0.8978, edges-ner-ontonotes_loss: 0.0340
09/09 04:28:10 PM: Evaluate: task edges-ner-ontonotes, batch 71 (157): mcc: 0.9312, acc: 0.9032, precision: 0.9525, recall: 0.9178, f1: 0.9348, edges-ner-ontonotes_loss: 0.0251
09/09 04:28:20 PM: Evaluate: task edges-ner-ontonotes, batch 111 (157): mcc: 0.9376, acc: 0.9122, precision: 0.9581, recall: 0.9241, f1: 0.9408, edges-ner-ontonotes_loss: 0.0225
09/09 04:28:30 PM: Evaluate: task edges-ner-ontonotes, batch 143 (157): mcc: 0.9451, acc: 0.9216, precision: 0.9632, recall: 0.9332, f1: 0.9480, edges-ner-ontonotes_loss: 0.0201
09/09 04:28:34 PM: Updating LR scheduler:
09/09 04:28:34 PM: 	Best result seen so far for macro_avg: 0.950
09/09 04:28:34 PM: 	# validation passes without improvement: 1
09/09 04:28:34 PM: edges-ner-ontonotes_loss: training: 0.016882 validation: 0.019663
09/09 04:28:34 PM: macro_avg: validation: 0.948589
09/09 04:28:34 PM: micro_avg: validation: 0.000000
09/09 04:28:34 PM: edges-ner-ontonotes_mcc: training: 0.950946 validation: 0.945746
09/09 04:28:34 PM: edges-ner-ontonotes_acc: training: 0.929522 validation: 0.922505
09/09 04:28:34 PM: edges-ner-ontonotes_precision: training: 0.963606 validation: 0.963769
09/09 04:28:34 PM: edges-ner-ontonotes_recall: training: 0.943777 validation: 0.933879
09/09 04:28:34 PM: edges-ner-ontonotes_f1: training: 0.953589 validation: 0.948589
09/09 04:28:34 PM: Global learning rate: 0.0001
09/09 04:28:34 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:28:40 PM: Update 11034: task edges-ner-ontonotes, batch 34 (11034): mcc: 0.9448, acc: 0.9231, precision: 0.9573, recall: 0.9386, f1: 0.9478, edges-ner-ontonotes_loss: 0.0185
09/09 04:28:50 PM: Update 11089: task edges-ner-ontonotes, batch 89 (11089): mcc: 0.9495, acc: 0.9299, precision: 0.9616, recall: 0.9431, f1: 0.9522, edges-ner-ontonotes_loss: 0.0172
09/09 04:29:00 PM: Update 11155: task edges-ner-ontonotes, batch 155 (11155): mcc: 0.9467, acc: 0.9259, precision: 0.9600, recall: 0.9394, f1: 0.9496, edges-ner-ontonotes_loss: 0.0178
09/09 04:29:10 PM: Update 11232: task edges-ner-ontonotes, batch 232 (11232): mcc: 0.9454, acc: 0.9241, precision: 0.9590, recall: 0.9380, f1: 0.9484, edges-ner-ontonotes_loss: 0.0180
09/09 04:29:20 PM: Update 11325: task edges-ner-ontonotes, batch 325 (11325): mcc: 0.9478, acc: 0.9272, precision: 0.9614, recall: 0.9401, f1: 0.9506, edges-ner-ontonotes_loss: 0.0174
09/09 04:29:31 PM: Update 11412: task edges-ner-ontonotes, batch 412 (11412): mcc: 0.9481, acc: 0.9279, precision: 0.9613, recall: 0.9407, f1: 0.9509, edges-ner-ontonotes_loss: 0.0172
09/09 04:29:41 PM: Update 11508: task edges-ner-ontonotes, batch 508 (11508): mcc: 0.9488, acc: 0.9287, precision: 0.9616, recall: 0.9416, f1: 0.9515, edges-ner-ontonotes_loss: 0.0170
09/09 04:29:51 PM: Update 11576: task edges-ner-ontonotes, batch 576 (11576): mcc: 0.9505, acc: 0.9306, precision: 0.9630, recall: 0.9435, f1: 0.9532, edges-ner-ontonotes_loss: 0.0165
09/09 04:30:01 PM: Update 11644: task edges-ner-ontonotes, batch 644 (11644): mcc: 0.9515, acc: 0.9318, precision: 0.9636, recall: 0.9449, f1: 0.9542, edges-ner-ontonotes_loss: 0.0161
09/09 04:30:11 PM: Update 11708: task edges-ner-ontonotes, batch 708 (11708): mcc: 0.9530, acc: 0.9334, precision: 0.9647, recall: 0.9465, f1: 0.9555, edges-ner-ontonotes_loss: 0.0156
09/09 04:30:21 PM: Update 11771: task edges-ner-ontonotes, batch 771 (11771): mcc: 0.9539, acc: 0.9345, precision: 0.9652, recall: 0.9477, f1: 0.9564, edges-ner-ontonotes_loss: 0.0154
09/09 04:30:35 PM: Update 11832: task edges-ner-ontonotes, batch 832 (11832): mcc: 0.9549, acc: 0.9357, precision: 0.9660, recall: 0.9488, f1: 0.9573, edges-ner-ontonotes_loss: 0.0150
09/09 04:30:45 PM: Update 11902: task edges-ner-ontonotes, batch 902 (11902): mcc: 0.9556, acc: 0.9366, precision: 0.9666, recall: 0.9497, f1: 0.9581, edges-ner-ontonotes_loss: 0.0148
09/09 04:30:55 PM: Update 11959: task edges-ner-ontonotes, batch 959 (11959): mcc: 0.9559, acc: 0.9369, precision: 0.9667, recall: 0.9499, f1: 0.9583, edges-ner-ontonotes_loss: 0.0146
09/09 04:31:02 PM: ***** Step 12000 / Validation 12 *****
09/09 04:31:02 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:31:02 PM: Validating...
09/09 04:31:05 PM: Evaluate: task edges-ner-ontonotes, batch 18 (157): mcc: 0.8772, acc: 0.8417, precision: 0.9040, recall: 0.8642, f1: 0.8837, edges-ner-ontonotes_loss: 0.0347
09/09 04:31:15 PM: Evaluate: task edges-ner-ontonotes, batch 62 (157): mcc: 0.9300, acc: 0.9089, precision: 0.9452, recall: 0.9227, f1: 0.9338, edges-ner-ontonotes_loss: 0.0252
09/09 04:31:25 PM: Evaluate: task edges-ner-ontonotes, batch 105 (157): mcc: 0.9334, acc: 0.9105, precision: 0.9513, recall: 0.9230, f1: 0.9369, edges-ner-ontonotes_loss: 0.0242
09/09 04:31:35 PM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.9452, acc: 0.9254, precision: 0.9589, recall: 0.9375, f1: 0.9481, edges-ner-ontonotes_loss: 0.0204
09/09 04:31:38 PM: Updating LR scheduler:
09/09 04:31:38 PM: 	Best result seen so far for macro_avg: 0.950
09/09 04:31:38 PM: 	# validation passes without improvement: 2
09/09 04:31:38 PM: edges-ner-ontonotes_loss: training: 0.014581 validation: 0.019766
09/09 04:31:38 PM: macro_avg: validation: 0.949414
09/09 04:31:38 PM: micro_avg: validation: 0.000000
09/09 04:31:38 PM: edges-ner-ontonotes_mcc: training: 0.956183 validation: 0.946534
09/09 04:31:38 PM: edges-ner-ontonotes_acc: training: 0.937269 validation: 0.927358
09/09 04:31:38 PM: edges-ner-ontonotes_precision: training: 0.966986 validation: 0.959786
09/09 04:31:38 PM: edges-ner-ontonotes_recall: training: 0.950275 validation: 0.939263
09/09 04:31:38 PM: edges-ner-ontonotes_f1: training: 0.958558 validation: 0.949414
09/09 04:31:38 PM: Global learning rate: 0.0001
09/09 04:31:38 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/RTE-cat/run
09/09 04:31:45 PM: Update 12042: task edges-ner-ontonotes, batch 42 (12042): mcc: 0.9637, acc: 0.9457, precision: 0.9735, recall: 0.9580, f1: 0.9657, edges-ner-ontonotes_loss: 0.0114
09/09 04:31:56 PM: Update 12105: task edges-ner-ontonotes, batch 105 (12105): mcc: 0.9652, acc: 0.9482, precision: 0.9738, recall: 0.9605, f1: 0.9671, edges-ner-ontonotes_loss: 0.0110
09/09 04:32:06 PM: Update 12153: task edges-ner-ontonotes, batch 153 (12153): mcc: 0.9609, acc: 0.9417, precision: 0.9710, recall: 0.9552, f1: 0.9630, edges-ner-ontonotes_loss: 0.0123
09/09 04:32:16 PM: Update 12215: task edges-ner-ontonotes, batch 215 (12215): mcc: 0.9517, acc: 0.9310, precision: 0.9643, recall: 0.9445, f1: 0.9543, edges-ner-ontonotes_loss: 0.0160
09/09 04:32:26 PM: Update 12276: task edges-ner-ontonotes, batch 276 (12276): mcc: 0.9469, acc: 0.9245, precision: 0.9613, recall: 0.9385, f1: 0.9497, edges-ner-ontonotes_loss: 0.0179
09/09 04:32:36 PM: Update 12335: task edges-ner-ontonotes, batch 335 (12335): mcc: 0.9441, acc: 0.9211, precision: 0.9593, recall: 0.9353, f1: 0.9471, edges-ner-ontonotes_loss: 0.0191
09/09 04:32:46 PM: Update 12401: task edges-ner-ontonotes, batch 401 (12401): mcc: 0.9420, acc: 0.9183, precision: 0.9578, recall: 0.9327, f1: 0.9450, edges-ner-ontonotes_loss: 0.0199
09/09 04:32:53 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/scratch0/ewallac2/jiant/jiant/__main__.py", line 589, in main
    phase="target_train",
  File "/scratch0/ewallac2/jiant/jiant/trainer.py", line 594, in train
    for batch in itertools.islice(tr_generator, 1):
  File "/fs/clip-ml/ewallac2/anaconda/envs/jiant/lib/python3.6/site-packages/allennlp/data/iterators/data_iterator.py", line 143, in __call__
    for batch in batches:
  File "/fs/clip-ml/ewallac2/anaconda/envs/jiant/lib/python3.6/site-packages/allennlp/data/iterators/bucket_iterator.py", line 112, in _create_batches
    for instance_list in self._memory_sized_lists(instances):
  File "/fs/clip-ml/ewallac2/anaconda/envs/jiant/lib/python3.6/site-packages/allennlp/data/iterators/data_iterator.py", line 219, in _memory_sized_lists
    yield from lazy_groups_of(iterator, self._max_instances_in_memory)
  File "/fs/clip-ml/ewallac2/anaconda/envs/jiant/lib/python3.6/site-packages/allennlp/common/util.py", line 104, in <lambda>
    return iter(lambda: list(islice(iterator, 0, group_size)), [])
  File "/fs/clip-ml/ewallac2/anaconda/envs/jiant/lib/python3.6/site-packages/allennlp/data/iterators/data_iterator.py", line 174, in _take_instances
    yield from iter(instances)
  File "/scratch0/ewallac2/jiant/jiant/utils/serialize.py", line 76, in _iter_fn
    with open(filename, "rb") as fd:
FileNotFoundError: [Errno 2] No such file or directory: './experiments/ner-ontonotes- models/RTE-cat/preproc/edges-ner-ontonotes__train_data'
