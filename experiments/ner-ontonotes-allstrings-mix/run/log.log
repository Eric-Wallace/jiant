10/01 04:33:43 AM: Git branch: master
10/01 04:33:43 AM: Git SHA: 8a5d6bbc81dc2562b6a149e8b00815e7e9113c4c
10/01 04:33:43 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-allstrings-mix/",
  "exp_name": "experiments/ner-ontonotes-allstrings-mix",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-allstrings-mix/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/allstrings",
  "pytorch_transformers_output_mode": "mix",
  "remote_log_name": "experiments/ner-ontonotes-allstrings-mix__run",
  "run_dir": "./experiments/ner-ontonotes-allstrings-mix/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 04:33:43 AM: Saved config to ./experiments/ner-ontonotes-allstrings-mix/run/params.conf
10/01 04:33:43 AM: Using random seed 1234
10/01 04:33:45 AM: Using GPU 0
10/01 04:33:45 AM: Loading tasks...
10/01 04:33:45 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-allstrings-mix/
10/01 04:33:45 AM: 	Creating task edges-ner-ontonotes from scratch.
10/01 04:33:46 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
10/01 04:33:46 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
10/01 04:33:46 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
10/01 04:33:47 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
10/01 04:33:47 AM: 	Finished loading tasks: edges-ner-ontonotes.
10/01 04:33:47 AM: 	Building vocab from scratch.
10/01 04:33:47 AM: 	Counting units for task edges-ner-ontonotes.
10/01 04:33:48 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
10/01 04:33:49 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:33:49 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 04:33:49 AM: 	Saved vocab to ./experiments/ner-ontonotes-allstrings-mix/vocab
10/01 04:33:49 AM: Loading token dictionary from ./experiments/ner-ontonotes-allstrings-mix/vocab.
10/01 04:33:49 AM: 	Loaded vocab from ./experiments/ner-ontonotes-allstrings-mix/vocab
10/01 04:33:49 AM: 	Vocab namespace bert_uncased: size 30524
10/01 04:33:49 AM: 	Vocab namespace tokens: size 22840
10/01 04:33:49 AM: 	Vocab namespace chars: size 77
10/01 04:33:49 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
10/01 04:33:49 AM: 	Finished building vocab.
10/01 04:33:49 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
10/01 04:33:55 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-allstrings-mix/preproc/edges-ner-ontonotes__train_data
10/01 04:33:55 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
10/01 04:33:56 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-allstrings-mix/preproc/edges-ner-ontonotes__val_data
10/01 04:33:56 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
10/01 04:33:56 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-allstrings-mix/preproc/edges-ner-ontonotes__test_data
10/01 04:33:56 AM: 	Finished indexing tasks
10/01 04:33:56 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
10/01 04:33:56 AM: 	  Training on 
10/01 04:33:56 AM: 	  Evaluating on edges-ner-ontonotes
10/01 04:33:56 AM: 	Finished loading tasks in 11.194s
10/01 04:33:56 AM: 	 Tasks: ['edges-ner-ontonotes']
10/01 04:33:56 AM: Building model...
10/01 04:33:56 AM: Using BERT model (bert-base-uncased).
10/01 04:33:56 AM: LOADING A FUNETUNED MODEL from: 
10/01 04:33:56 AM: models/allstrings
10/01 04:33:56 AM: loading configuration file models/allstrings/config.json
10/01 04:33:56 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "random-memorize-all-binary",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 04:33:56 AM: loading weights file models/allstrings/pytorch_model.bin
10/01 04:33:59 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpw2v0klz1
10/01 04:34:01 AM: copying /tmp/tmpw2v0klz1 to cache at ./experiments/ner-ontonotes-allstrings-mix/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:34:01 AM: creating metadata file for ./experiments/ner-ontonotes-allstrings-mix/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:34:01 AM: removing temp file /tmp/tmpw2v0klz1
10/01 04:34:01 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-allstrings-mix/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:34:01 AM: NOTE: pytorch_transformers_output_mode='mix', so scalar mixing weights will be fine-tuned even if BERT model is frozen.
10/01 04:34:01 AM: Initializing parameters
10/01 04:34:01 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 04:34:01 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 04:34:01 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 04:34:01 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 04:34:01 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.gamma
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.0
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.1
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.10
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.11
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.12
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.2
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.3
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.4
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.5
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.6
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.7
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.8
10/01 04:34:01 AM:    _text_field_embedder.scalar_mix.scalar_parameters.9
10/01 04:34:01 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
10/01 04:34:06 AM: Model specification:
10/01 04:34:06 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
      (scalar_mix): ScalarMix(
        (scalar_parameters): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
            (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
10/01 04:34:06 AM: Model parameters:
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.gamma: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.0: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.1: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.2: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.3: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.4: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.5: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.6: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.7: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.8: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.9: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.10: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.11: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	sent_encoder._text_field_embedder.scalar_mix.scalar_parameters.12: Trainable parameter, count 1 with torch.Size([1])
10/01 04:34:06 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 04:34:06 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 04:34:06 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
10/01 04:34:06 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
10/01 04:34:06 AM: Total number of parameters: 109688352 (1.09688e+08)
10/01 04:34:06 AM: Number of trainable parameters: 206112 (206112)
10/01 04:34:06 AM: Finished building model in 9.589s
10/01 04:34:06 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

10/01 04:34:08 AM: patience = 9
10/01 04:34:08 AM: val_interval = 1000
10/01 04:34:08 AM: max_vals = 250
10/01 04:34:08 AM: cuda_device = 0
10/01 04:34:08 AM: grad_norm = 5.0
10/01 04:34:08 AM: grad_clipping = None
10/01 04:34:08 AM: lr_decay = 0.99
10/01 04:34:08 AM: min_lr = 1e-06
10/01 04:34:08 AM: keep_all_checkpoints = 0
10/01 04:34:08 AM: val_data_limit = 5000
10/01 04:34:08 AM: max_epochs = -1
10/01 04:34:08 AM: dec_val_scale = 250
10/01 04:34:08 AM: training_data_fraction = 1
10/01 04:34:08 AM: type = adam
10/01 04:34:08 AM: parameter_groups = None
10/01 04:34:08 AM: Number of trainable parameters: 206112
10/01 04:34:08 AM: infer_type_and_cast = True
10/01 04:34:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:08 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:08 AM: lr = 0.0001
10/01 04:34:08 AM: amsgrad = True
10/01 04:34:08 AM: type = reduce_on_plateau
10/01 04:34:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:08 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:08 AM: mode = max
10/01 04:34:08 AM: factor = 0.5
10/01 04:34:08 AM: patience = 3
10/01 04:34:08 AM: threshold = 0.0001
10/01 04:34:08 AM: threshold_mode = abs
10/01 04:34:08 AM: verbose = True
10/01 04:34:08 AM: type = adam
10/01 04:34:08 AM: parameter_groups = None
10/01 04:34:08 AM: Number of trainable parameters: 206112
10/01 04:34:08 AM: infer_type_and_cast = True
10/01 04:34:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:08 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:08 AM: lr = 0.0001
10/01 04:34:08 AM: amsgrad = True
10/01 04:34:08 AM: type = reduce_on_plateau
10/01 04:34:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:08 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:08 AM: mode = max
10/01 04:34:08 AM: factor = 0.5
10/01 04:34:08 AM: patience = 3
10/01 04:34:08 AM: threshold = 0.0001
10/01 04:34:08 AM: threshold_mode = abs
10/01 04:34:08 AM: verbose = True
10/01 04:34:08 AM: Starting training without restoring from a checkpoint.
10/01 04:34:08 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
10/01 04:34:08 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
10/01 04:34:18 AM: Update 185: task edges-ner-ontonotes, batch 185 (185): mcc: 0.1417, acc: 0.1122, precision: 0.2287, recall: 0.1397, f1: 0.1735, edges-ner-ontonotes_loss: 0.2204
10/01 04:34:28 AM: Update 338: task edges-ner-ontonotes, batch 338 (338): mcc: 0.4049, acc: 0.3182, precision: 0.5538, recall: 0.3339, f1: 0.4166, edges-ner-ontonotes_loss: 0.1618
10/01 04:34:38 AM: Update 503: task edges-ner-ontonotes, batch 503 (503): mcc: 0.5594, acc: 0.4589, precision: 0.7124, recall: 0.4700, f1: 0.5664, edges-ner-ontonotes_loss: 0.1312
10/01 04:34:48 AM: Update 653: task edges-ner-ontonotes, batch 653 (653): mcc: 0.6356, acc: 0.5374, precision: 0.7772, recall: 0.5471, f1: 0.6422, edges-ner-ontonotes_loss: 0.1137
10/01 04:34:58 AM: Update 814: task edges-ner-ontonotes, batch 814 (814): mcc: 0.6910, acc: 0.5974, precision: 0.8183, recall: 0.6081, f1: 0.6977, edges-ner-ontonotes_loss: 0.1003
10/01 04:35:08 AM: Update 953: task edges-ner-ontonotes, batch 953 (953): mcc: 0.7236, acc: 0.6349, precision: 0.8402, recall: 0.6461, f1: 0.7305, edges-ner-ontonotes_loss: 0.0915
10/01 04:35:11 AM: ***** Step 1000 / Validation 1 *****
10/01 04:35:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:35:11 AM: Validating...
10/01 04:35:19 AM: Evaluate: task edges-ner-ontonotes, batch 95 (157): mcc: 0.8622, acc: 0.8081, precision: 0.9252, recall: 0.8167, f1: 0.8676, edges-ner-ontonotes_loss: 0.0492
10/01 04:35:25 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:35:25 AM: Best result seen so far for micro.
10/01 04:35:25 AM: Best result seen so far for macro.
10/01 04:35:25 AM: Updating LR scheduler:
10/01 04:35:25 AM: 	Best result seen so far for macro_avg: 0.882
10/01 04:35:25 AM: 	# validation passes without improvement: 0
10/01 04:35:25 AM: edges-ner-ontonotes_loss: training: 0.089146 validation: 0.043478
10/01 04:35:25 AM: macro_avg: validation: 0.882276
10/01 04:35:25 AM: micro_avg: validation: 0.000000
10/01 04:35:25 AM: edges-ner-ontonotes_mcc: training: 0.732091 validation: 0.876904
10/01 04:35:25 AM: edges-ner-ontonotes_acc: training: 0.644744 validation: 0.827646
10/01 04:35:25 AM: edges-ner-ontonotes_precision: training: 0.845609 validation: 0.928099
10/01 04:35:25 AM: edges-ner-ontonotes_recall: training: 0.656182 validation: 0.840764
10/01 04:35:25 AM: edges-ner-ontonotes_f1: training: 0.738949 validation: 0.882276
10/01 04:35:25 AM: Global learning rate: 0.0001
10/01 04:35:25 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:35:29 AM: Update 1067: task edges-ner-ontonotes, batch 67 (1067): mcc: 0.8831, acc: 0.8326, precision: 0.9325, recall: 0.8479, f1: 0.8882, edges-ner-ontonotes_loss: 0.0378
10/01 04:35:39 AM: Update 1226: task edges-ner-ontonotes, batch 226 (1226): mcc: 0.8897, acc: 0.8405, precision: 0.9364, recall: 0.8563, f1: 0.8946, edges-ner-ontonotes_loss: 0.0367
10/01 04:35:49 AM: Update 1368: task edges-ner-ontonotes, batch 368 (1368): mcc: 0.8795, acc: 0.8259, precision: 0.9337, recall: 0.8402, f1: 0.8845, edges-ner-ontonotes_loss: 0.0408
10/01 04:35:59 AM: Update 1537: task edges-ner-ontonotes, batch 537 (1537): mcc: 0.8762, acc: 0.8220, precision: 0.9334, recall: 0.8346, f1: 0.8812, edges-ner-ontonotes_loss: 0.0423
10/01 04:36:09 AM: Update 1719: task edges-ner-ontonotes, batch 719 (1719): mcc: 0.8781, acc: 0.8248, precision: 0.9348, recall: 0.8367, f1: 0.8830, edges-ner-ontonotes_loss: 0.0413
10/01 04:36:19 AM: Update 1879: task edges-ner-ontonotes, batch 879 (1879): mcc: 0.8811, acc: 0.8294, precision: 0.9365, recall: 0.8407, f1: 0.8860, edges-ner-ontonotes_loss: 0.0401
10/01 04:36:26 AM: ***** Step 2000 / Validation 2 *****
10/01 04:36:26 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:36:26 AM: Validating...
10/01 04:36:29 AM: Evaluate: task edges-ner-ontonotes, batch 41 (157): mcc: 0.8720, acc: 0.8353, precision: 0.9188, recall: 0.8405, f1: 0.8779, edges-ner-ontonotes_loss: 0.0401
10/01 04:36:39 AM: Evaluate: task edges-ner-ontonotes, batch 155 (157): mcc: 0.9117, acc: 0.8754, precision: 0.9503, recall: 0.8837, f1: 0.9158, edges-ner-ontonotes_loss: 0.0302
10/01 04:36:39 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:36:39 AM: Best result seen so far for macro.
10/01 04:36:39 AM: Updating LR scheduler:
10/01 04:36:39 AM: 	Best result seen so far for macro_avg: 0.916
10/01 04:36:39 AM: 	# validation passes without improvement: 0
10/01 04:36:39 AM: edges-ner-ontonotes_loss: training: 0.039229 validation: 0.029948
10/01 04:36:39 AM: macro_avg: validation: 0.916212
10/01 04:36:39 AM: micro_avg: validation: 0.000000
10/01 04:36:39 AM: edges-ner-ontonotes_mcc: training: 0.883558 validation: 0.912135
10/01 04:36:39 AM: edges-ner-ontonotes_acc: training: 0.833243 validation: 0.876024
10/01 04:36:39 AM: edges-ner-ontonotes_precision: training: 0.937497 validation: 0.950526
10/01 04:36:39 AM: edges-ner-ontonotes_recall: training: 0.844205 validation: 0.884289
10/01 04:36:39 AM: edges-ner-ontonotes_f1: training: 0.888409 validation: 0.916212
10/01 04:36:39 AM: Global learning rate: 0.0001
10/01 04:36:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:36:49 AM: Update 2166: task edges-ner-ontonotes, batch 166 (2166): mcc: 0.9091, acc: 0.8723, precision: 0.9479, recall: 0.8811, f1: 0.9133, edges-ner-ontonotes_loss: 0.0296
10/01 04:36:59 AM: Update 2303: task edges-ner-ontonotes, batch 303 (2303): mcc: 0.9126, acc: 0.8745, precision: 0.9496, recall: 0.8860, f1: 0.9167, edges-ner-ontonotes_loss: 0.0283
10/01 04:37:09 AM: Update 2462: task edges-ner-ontonotes, batch 462 (2462): mcc: 0.9167, acc: 0.8797, precision: 0.9506, recall: 0.8927, f1: 0.9207, edges-ner-ontonotes_loss: 0.0270
10/01 04:37:19 AM: Update 2607: task edges-ner-ontonotes, batch 607 (2607): mcc: 0.9182, acc: 0.8813, precision: 0.9510, recall: 0.8950, f1: 0.9221, edges-ner-ontonotes_loss: 0.0266
10/01 04:37:29 AM: Update 2777: task edges-ner-ontonotes, batch 777 (2777): mcc: 0.9198, acc: 0.8835, precision: 0.9519, recall: 0.8972, f1: 0.9237, edges-ner-ontonotes_loss: 0.0259
10/01 04:37:39 AM: Update 2918: task edges-ner-ontonotes, batch 918 (2918): mcc: 0.9171, acc: 0.8798, precision: 0.9508, recall: 0.8931, f1: 0.9211, edges-ner-ontonotes_loss: 0.0271
10/01 04:37:44 AM: ***** Step 3000 / Validation 3 *****
10/01 04:37:44 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:37:44 AM: Validating...
10/01 04:37:49 AM: Evaluate: task edges-ner-ontonotes, batch 66 (157): mcc: 0.9004, acc: 0.8627, precision: 0.9320, recall: 0.8803, f1: 0.9054, edges-ner-ontonotes_loss: 0.0335
10/01 04:37:57 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:37:57 AM: Best result seen so far for macro.
10/01 04:37:57 AM: Updating LR scheduler:
10/01 04:37:57 AM: 	Best result seen so far for macro_avg: 0.929
10/01 04:37:57 AM: 	# validation passes without improvement: 0
10/01 04:37:57 AM: edges-ner-ontonotes_loss: training: 0.028079 validation: 0.025986
10/01 04:37:57 AM: macro_avg: validation: 0.928699
10/01 04:37:57 AM: micro_avg: validation: 0.000000
10/01 04:37:57 AM: edges-ner-ontonotes_mcc: training: 0.915136 validation: 0.924918
10/01 04:37:57 AM: edges-ner-ontonotes_acc: training: 0.877570 validation: 0.894980
10/01 04:37:57 AM: edges-ner-ontonotes_precision: training: 0.949560 validation: 0.951839
10/01 04:37:57 AM: edges-ner-ontonotes_recall: training: 0.890732 validation: 0.906658
10/01 04:37:57 AM: edges-ner-ontonotes_f1: training: 0.919206 validation: 0.928699
10/01 04:37:57 AM: Global learning rate: 0.0001
10/01 04:37:57 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:37:59 AM: Update 3033: task edges-ner-ontonotes, batch 33 (3033): mcc: 0.8951, acc: 0.8517, precision: 0.9406, recall: 0.8624, f1: 0.8998, edges-ner-ontonotes_loss: 0.0373
10/01 04:38:09 AM: Update 3193: task edges-ner-ontonotes, batch 193 (3193): mcc: 0.8965, acc: 0.8543, precision: 0.9384, recall: 0.8670, f1: 0.9013, edges-ner-ontonotes_loss: 0.0355
10/01 04:38:19 AM: Update 3393: task edges-ner-ontonotes, batch 393 (3393): mcc: 0.9044, acc: 0.8649, precision: 0.9432, recall: 0.8769, f1: 0.9088, edges-ner-ontonotes_loss: 0.0323
10/01 04:38:29 AM: Update 3558: task edges-ner-ontonotes, batch 558 (3558): mcc: 0.9075, acc: 0.8701, precision: 0.9439, recall: 0.8820, f1: 0.9119, edges-ner-ontonotes_loss: 0.0310
10/01 04:38:40 AM: Update 3739: task edges-ner-ontonotes, batch 739 (3739): mcc: 0.9123, acc: 0.8762, precision: 0.9468, recall: 0.8882, f1: 0.9166, edges-ner-ontonotes_loss: 0.0295
10/01 04:38:51 AM: Update 3912: task edges-ner-ontonotes, batch 912 (3912): mcc: 0.9174, acc: 0.8826, precision: 0.9491, recall: 0.8953, f1: 0.9214, edges-ner-ontonotes_loss: 0.0280
10/01 04:38:55 AM: ***** Step 4000 / Validation 4 *****
10/01 04:38:55 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:38:55 AM: Validating...
10/01 04:39:01 AM: Evaluate: task edges-ner-ontonotes, batch 69 (157): mcc: 0.9066, acc: 0.8723, precision: 0.9403, recall: 0.8837, f1: 0.9111, edges-ner-ontonotes_loss: 0.0316
10/01 04:39:08 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:39:08 AM: Best result seen so far for macro.
10/01 04:39:08 AM: Updating LR scheduler:
10/01 04:39:08 AM: 	Best result seen so far for macro_avg: 0.933
10/01 04:39:08 AM: 	# validation passes without improvement: 0
10/01 04:39:08 AM: edges-ner-ontonotes_loss: training: 0.027229 validation: 0.023771
10/01 04:39:08 AM: macro_avg: validation: 0.933313
10/01 04:39:08 AM: micro_avg: validation: 0.000000
10/01 04:39:08 AM: edges-ner-ontonotes_mcc: training: 0.919656 validation: 0.929767
10/01 04:39:08 AM: edges-ner-ontonotes_acc: training: 0.885600 validation: 0.903094
10/01 04:39:08 AM: edges-ner-ontonotes_precision: training: 0.950205 validation: 0.955517
10/01 04:39:08 AM: edges-ner-ontonotes_recall: training: 0.898471 validation: 0.912117
10/01 04:39:08 AM: edges-ner-ontonotes_f1: training: 0.923614 validation: 0.933313
10/01 04:39:08 AM: Global learning rate: 0.0001
10/01 04:39:08 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:39:11 AM: Update 4036: task edges-ner-ontonotes, batch 36 (4036): mcc: 0.9396, acc: 0.9096, precision: 0.9600, recall: 0.9261, f1: 0.9427, edges-ner-ontonotes_loss: 0.0189
10/01 04:39:21 AM: Update 4191: task edges-ner-ontonotes, batch 191 (4191): mcc: 0.9379, acc: 0.9086, precision: 0.9587, recall: 0.9242, f1: 0.9411, edges-ner-ontonotes_loss: 0.0197
10/01 04:39:31 AM: Update 4363: task edges-ner-ontonotes, batch 363 (4363): mcc: 0.9373, acc: 0.9079, precision: 0.9582, recall: 0.9236, f1: 0.9406, edges-ner-ontonotes_loss: 0.0201
10/01 04:39:41 AM: Update 4516: task edges-ner-ontonotes, batch 516 (4516): mcc: 0.9276, acc: 0.8953, precision: 0.9533, recall: 0.9102, f1: 0.9312, edges-ner-ontonotes_loss: 0.0243
10/01 04:39:51 AM: Update 4669: task edges-ner-ontonotes, batch 669 (4669): mcc: 0.9215, acc: 0.8878, precision: 0.9496, recall: 0.9024, f1: 0.9254, edges-ner-ontonotes_loss: 0.0268
10/01 04:40:01 AM: Update 4870: task edges-ner-ontonotes, batch 870 (4870): mcc: 0.9206, acc: 0.8872, precision: 0.9490, recall: 0.9015, f1: 0.9246, edges-ner-ontonotes_loss: 0.0270
10/01 04:40:10 AM: ***** Step 5000 / Validation 5 *****
10/01 04:40:10 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:40:10 AM: Validating...
10/01 04:40:11 AM: Evaluate: task edges-ner-ontonotes, batch 25 (157): mcc: 0.8610, acc: 0.8262, precision: 0.9006, recall: 0.8374, f1: 0.8678, edges-ner-ontonotes_loss: 0.0420
10/01 04:40:22 AM: Evaluate: task edges-ner-ontonotes, batch 142 (157): mcc: 0.9341, acc: 0.9086, precision: 0.9595, recall: 0.9164, f1: 0.9374, edges-ner-ontonotes_loss: 0.0230
10/01 04:40:23 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:40:23 AM: Best result seen so far for macro.
10/01 04:40:23 AM: Updating LR scheduler:
10/01 04:40:23 AM: 	Best result seen so far for macro_avg: 0.939
10/01 04:40:23 AM: 	# validation passes without improvement: 0
10/01 04:40:23 AM: edges-ner-ontonotes_loss: training: 0.026853 validation: 0.022518
10/01 04:40:23 AM: macro_avg: validation: 0.938891
10/01 04:40:23 AM: micro_avg: validation: 0.000000
10/01 04:40:23 AM: edges-ner-ontonotes_mcc: training: 0.920939 validation: 0.935653
10/01 04:40:23 AM: edges-ner-ontonotes_acc: training: 0.887740 validation: 0.910601
10/01 04:40:23 AM: edges-ner-ontonotes_precision: training: 0.949088 validation: 0.960721
10/01 04:40:23 AM: edges-ner-ontonotes_recall: training: 0.901920 validation: 0.918032
10/01 04:40:23 AM: edges-ner-ontonotes_f1: training: 0.924903 validation: 0.938891
10/01 04:40:23 AM: Global learning rate: 0.0001
10/01 04:40:23 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:40:32 AM: Update 5140: task edges-ner-ontonotes, batch 140 (5140): mcc: 0.9243, acc: 0.8952, precision: 0.9492, recall: 0.9081, f1: 0.9282, edges-ner-ontonotes_loss: 0.0254
10/01 04:40:42 AM: Update 5295: task edges-ner-ontonotes, batch 295 (5295): mcc: 0.9285, acc: 0.9000, precision: 0.9529, recall: 0.9122, f1: 0.9321, edges-ner-ontonotes_loss: 0.0237
10/01 04:40:52 AM: Update 5468: task edges-ner-ontonotes, batch 468 (5468): mcc: 0.9332, acc: 0.9055, precision: 0.9548, recall: 0.9192, f1: 0.9366, edges-ner-ontonotes_loss: 0.0219
10/01 04:41:02 AM: Update 5630: task edges-ner-ontonotes, batch 630 (5630): mcc: 0.9363, acc: 0.9090, precision: 0.9564, recall: 0.9234, f1: 0.9396, edges-ner-ontonotes_loss: 0.0209
10/01 04:41:12 AM: Update 5806: task edges-ner-ontonotes, batch 806 (5806): mcc: 0.9377, acc: 0.9105, precision: 0.9573, recall: 0.9252, f1: 0.9410, edges-ner-ontonotes_loss: 0.0203
10/01 04:41:22 AM: Update 5959: task edges-ner-ontonotes, batch 959 (5959): mcc: 0.9371, acc: 0.9093, precision: 0.9571, recall: 0.9242, f1: 0.9404, edges-ner-ontonotes_loss: 0.0208
10/01 04:41:25 AM: ***** Step 6000 / Validation 6 *****
10/01 04:41:25 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:41:25 AM: Validating...
10/01 04:41:32 AM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.9227, acc: 0.8955, precision: 0.9479, recall: 0.9062, f1: 0.9266, edges-ner-ontonotes_loss: 0.0270
10/01 04:41:38 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:41:38 AM: Best result seen so far for macro.
10/01 04:41:38 AM: Updating LR scheduler:
10/01 04:41:38 AM: 	Best result seen so far for macro_avg: 0.941
10/01 04:41:38 AM: 	# validation passes without improvement: 0
10/01 04:41:38 AM: edges-ner-ontonotes_loss: training: 0.021354 validation: 0.022198
10/01 04:41:38 AM: macro_avg: validation: 0.940972
10/01 04:41:38 AM: micro_avg: validation: 0.000000
10/01 04:41:38 AM: edges-ner-ontonotes_mcc: training: 0.935754 validation: 0.937727
10/01 04:41:38 AM: edges-ner-ontonotes_acc: training: 0.907601 validation: 0.915074
10/01 04:41:38 AM: edges-ner-ontonotes_precision: training: 0.956260 validation: 0.957823
10/01 04:41:38 AM: edges-ner-ontonotes_recall: training: 0.922545 validation: 0.924704
10/01 04:41:38 AM: edges-ner-ontonotes_f1: training: 0.939100 validation: 0.940972
10/01 04:41:38 AM: Global learning rate: 0.0001
10/01 04:41:38 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:41:42 AM: Update 6092: task edges-ner-ontonotes, batch 92 (6092): mcc: 0.9097, acc: 0.8721, precision: 0.9416, recall: 0.8883, f1: 0.9142, edges-ner-ontonotes_loss: 0.0321
10/01 04:41:52 AM: Update 6239: task edges-ner-ontonotes, batch 239 (6239): mcc: 0.9084, acc: 0.8714, precision: 0.9394, recall: 0.8879, f1: 0.9129, edges-ner-ontonotes_loss: 0.0321
10/01 04:42:03 AM: Update 6433: task edges-ner-ontonotes, batch 433 (6433): mcc: 0.9126, acc: 0.8773, precision: 0.9418, recall: 0.8934, f1: 0.9170, edges-ner-ontonotes_loss: 0.0298
10/01 04:42:13 AM: Update 6591: task edges-ner-ontonotes, batch 591 (6591): mcc: 0.9171, acc: 0.8834, precision: 0.9447, recall: 0.8991, f1: 0.9213, edges-ner-ontonotes_loss: 0.0282
10/01 04:42:23 AM: Update 6760: task edges-ner-ontonotes, batch 760 (6760): mcc: 0.9211, acc: 0.8890, precision: 0.9472, recall: 0.9040, f1: 0.9251, edges-ner-ontonotes_loss: 0.0268
10/01 04:42:33 AM: Update 6897: task edges-ner-ontonotes, batch 897 (6897): mcc: 0.9240, acc: 0.8932, precision: 0.9486, recall: 0.9081, f1: 0.9279, edges-ner-ontonotes_loss: 0.0259
10/01 04:42:39 AM: ***** Step 7000 / Validation 7 *****
10/01 04:42:39 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:42:39 AM: Validating...
10/01 04:42:43 AM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.9013, acc: 0.8737, precision: 0.9313, recall: 0.8826, f1: 0.9063, edges-ner-ontonotes_loss: 0.0328
10/01 04:42:52 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:42:52 AM: Best result seen so far for macro.
10/01 04:42:52 AM: Updating LR scheduler:
10/01 04:42:52 AM: 	Best result seen so far for macro_avg: 0.942
10/01 04:42:52 AM: 	# validation passes without improvement: 0
10/01 04:42:52 AM: edges-ner-ontonotes_loss: training: 0.024972 validation: 0.022011
10/01 04:42:52 AM: macro_avg: validation: 0.942181
10/01 04:42:52 AM: micro_avg: validation: 0.000000
10/01 04:42:52 AM: edges-ner-ontonotes_mcc: training: 0.926946 validation: 0.939031
10/01 04:42:52 AM: edges-ner-ontonotes_acc: training: 0.896763 validation: 0.916894
10/01 04:42:52 AM: edges-ner-ontonotes_precision: training: 0.950192 validation: 0.960167
10/01 04:42:52 AM: edges-ner-ontonotes_recall: training: 0.912021 validation: 0.924856
10/01 04:42:52 AM: edges-ner-ontonotes_f1: training: 0.930715 validation: 0.942181
10/01 04:42:52 AM: Global learning rate: 0.0001
10/01 04:42:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:42:53 AM: Update 7008: task edges-ner-ontonotes, batch 8 (7008): mcc: 0.9515, acc: 0.9287, precision: 0.9587, recall: 0.9498, f1: 0.9542, edges-ner-ontonotes_loss: 0.0154
10/01 04:43:03 AM: Update 7164: task edges-ner-ontonotes, batch 164 (7164): mcc: 0.9483, acc: 0.9233, precision: 0.9618, recall: 0.9406, f1: 0.9511, edges-ner-ontonotes_loss: 0.0174
10/01 04:43:13 AM: Update 7328: task edges-ner-ontonotes, batch 328 (7328): mcc: 0.9474, acc: 0.9222, precision: 0.9622, recall: 0.9384, f1: 0.9502, edges-ner-ontonotes_loss: 0.0175
10/01 04:43:24 AM: Update 7477: task edges-ner-ontonotes, batch 477 (7477): mcc: 0.9464, acc: 0.9210, precision: 0.9614, recall: 0.9374, f1: 0.9493, edges-ner-ontonotes_loss: 0.0177
10/01 04:43:34 AM: Update 7639: task edges-ner-ontonotes, batch 639 (7639): mcc: 0.9370, acc: 0.9090, precision: 0.9558, recall: 0.9252, f1: 0.9403, edges-ner-ontonotes_loss: 0.0214
10/01 04:43:44 AM: Update 7787: task edges-ner-ontonotes, batch 787 (7787): mcc: 0.9324, acc: 0.9034, precision: 0.9529, recall: 0.9195, f1: 0.9359, edges-ner-ontonotes_loss: 0.0234
10/01 04:43:54 AM: Update 7983: task edges-ner-ontonotes, batch 983 (7983): mcc: 0.9309, acc: 0.9018, precision: 0.9519, recall: 0.9178, f1: 0.9345, edges-ner-ontonotes_loss: 0.0239
10/01 04:43:55 AM: ***** Step 8000 / Validation 8 *****
10/01 04:43:55 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:43:55 AM: Validating...
10/01 04:44:04 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9305, acc: 0.9049, precision: 0.9532, recall: 0.9158, f1: 0.9341, edges-ner-ontonotes_loss: 0.0241
10/01 04:44:08 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:44:08 AM: Best result seen so far for macro.
10/01 04:44:08 AM: Updating LR scheduler:
10/01 04:44:08 AM: 	Best result seen so far for macro_avg: 0.942
10/01 04:44:08 AM: 	# validation passes without improvement: 0
10/01 04:44:08 AM: edges-ner-ontonotes_loss: training: 0.023831 validation: 0.021381
10/01 04:44:08 AM: macro_avg: validation: 0.942398
10/01 04:44:08 AM: micro_avg: validation: 0.000000
10/01 04:44:08 AM: edges-ner-ontonotes_mcc: training: 0.930945 validation: 0.939236
10/01 04:44:08 AM: edges-ner-ontonotes_acc: training: 0.901740 validation: 0.916363
10/01 04:44:08 AM: edges-ner-ontonotes_precision: training: 0.952013 validation: 0.959312
10/01 04:44:08 AM: edges-ner-ontonotes_recall: training: 0.917707 validation: 0.926069
10/01 04:44:08 AM: edges-ner-ontonotes_f1: training: 0.934545 validation: 0.942398
10/01 04:44:08 AM: Global learning rate: 0.0001
10/01 04:44:08 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:44:14 AM: Update 8100: task edges-ner-ontonotes, batch 100 (8100): mcc: 0.9292, acc: 0.9020, precision: 0.9496, recall: 0.9169, f1: 0.9329, edges-ner-ontonotes_loss: 0.0236
10/01 04:44:24 AM: Update 8282: task edges-ner-ontonotes, batch 282 (8282): mcc: 0.9330, acc: 0.9062, precision: 0.9522, recall: 0.9213, f1: 0.9365, edges-ner-ontonotes_loss: 0.0223
10/01 04:44:34 AM: Update 8438: task edges-ner-ontonotes, batch 438 (8438): mcc: 0.9342, acc: 0.9076, precision: 0.9533, recall: 0.9226, f1: 0.9377, edges-ner-ontonotes_loss: 0.0218
10/01 04:44:44 AM: Update 8609: task edges-ner-ontonotes, batch 609 (8609): mcc: 0.9388, acc: 0.9128, precision: 0.9561, recall: 0.9283, f1: 0.9420, edges-ner-ontonotes_loss: 0.0202
10/01 04:44:54 AM: Update 8764: task edges-ner-ontonotes, batch 764 (8764): mcc: 0.9415, acc: 0.9161, precision: 0.9577, recall: 0.9319, f1: 0.9446, edges-ner-ontonotes_loss: 0.0194
10/01 04:45:04 AM: Update 8941: task edges-ner-ontonotes, batch 941 (8941): mcc: 0.9425, acc: 0.9171, precision: 0.9582, recall: 0.9332, f1: 0.9455, edges-ner-ontonotes_loss: 0.0191
10/01 04:45:08 AM: ***** Step 9000 / Validation 9 *****
10/01 04:45:08 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:45:08 AM: Validating...
10/01 04:45:14 AM: Evaluate: task edges-ner-ontonotes, batch 78 (157): mcc: 0.9187, acc: 0.8900, precision: 0.9461, recall: 0.9007, f1: 0.9228, edges-ner-ontonotes_loss: 0.0298
10/01 04:45:21 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:45:21 AM: Best result seen so far for macro.
10/01 04:45:21 AM: Updating LR scheduler:
10/01 04:45:21 AM: 	Best result seen so far for macro_avg: 0.943
10/01 04:45:21 AM: 	# validation passes without improvement: 0
10/01 04:45:21 AM: edges-ner-ontonotes_loss: training: 0.018969 validation: 0.021809
10/01 04:45:21 AM: macro_avg: validation: 0.943051
10/01 04:45:21 AM: micro_avg: validation: 0.000000
10/01 04:45:21 AM: edges-ner-ontonotes_mcc: training: 0.942824 validation: 0.939928
10/01 04:45:21 AM: edges-ner-ontonotes_acc: training: 0.917462 validation: 0.918032
10/01 04:45:21 AM: edges-ner-ontonotes_precision: training: 0.958390 validation: 0.960016
10/01 04:45:21 AM: edges-ner-ontonotes_recall: training: 0.933681 validation: 0.926676
10/01 04:45:21 AM: edges-ner-ontonotes_f1: training: 0.945874 validation: 0.943051
10/01 04:45:21 AM: Global learning rate: 0.0001
10/01 04:45:21 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:45:24 AM: Update 9033: task edges-ner-ontonotes, batch 33 (9033): mcc: 0.9491, acc: 0.9243, precision: 0.9652, recall: 0.9388, f1: 0.9518, edges-ner-ontonotes_loss: 0.0161
10/01 04:45:34 AM: Update 9210: task edges-ner-ontonotes, batch 210 (9210): mcc: 0.9169, acc: 0.8840, precision: 0.9443, recall: 0.8991, f1: 0.9211, edges-ner-ontonotes_loss: 0.0291
10/01 04:45:45 AM: Update 9389: task edges-ner-ontonotes, batch 389 (9389): mcc: 0.9141, acc: 0.8808, precision: 0.9415, recall: 0.8966, f1: 0.9185, edges-ner-ontonotes_loss: 0.0297
10/01 04:45:55 AM: Update 9584: task edges-ner-ontonotes, batch 584 (9584): mcc: 0.9188, acc: 0.8869, precision: 0.9448, recall: 0.9022, f1: 0.9230, edges-ner-ontonotes_loss: 0.0277
10/01 04:46:05 AM: Update 9737: task edges-ner-ontonotes, batch 737 (9737): mcc: 0.9217, acc: 0.8908, precision: 0.9463, recall: 0.9060, f1: 0.9257, edges-ner-ontonotes_loss: 0.0267
10/01 04:46:15 AM: Update 9903: task edges-ner-ontonotes, batch 903 (9903): mcc: 0.9248, acc: 0.8951, precision: 0.9480, recall: 0.9102, f1: 0.9287, edges-ner-ontonotes_loss: 0.0255
10/01 04:46:22 AM: ***** Step 10000 / Validation 10 *****
10/01 04:46:22 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:46:22 AM: Validating...
10/01 04:46:25 AM: Evaluate: task edges-ner-ontonotes, batch 42 (157): mcc: 0.9063, acc: 0.8818, precision: 0.9324, recall: 0.8909, f1: 0.9112, edges-ner-ontonotes_loss: 0.0324
10/01 04:46:35 AM: Evaluate: task edges-ner-ontonotes, batch 155 (157): mcc: 0.9411, acc: 0.9197, precision: 0.9609, recall: 0.9281, f1: 0.9442, edges-ner-ontonotes_loss: 0.0211
10/01 04:46:35 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:46:35 AM: Best result seen so far for macro.
10/01 04:46:35 AM: Updating LR scheduler:
10/01 04:46:35 AM: 	Best result seen so far for macro_avg: 0.944
10/01 04:46:35 AM: 	# validation passes without improvement: 0
10/01 04:46:35 AM: edges-ner-ontonotes_loss: training: 0.024984 validation: 0.021024
10/01 04:46:35 AM: macro_avg: validation: 0.944462
10/01 04:46:35 AM: micro_avg: validation: 0.000000
10/01 04:46:35 AM: edges-ner-ontonotes_mcc: training: 0.926603 validation: 0.941412
10/01 04:46:35 AM: edges-ner-ontonotes_acc: training: 0.897394 validation: 0.920079
10/01 04:46:35 AM: edges-ner-ontonotes_precision: training: 0.948907 validation: 0.961067
10/01 04:46:35 AM: edges-ner-ontonotes_recall: training: 0.912630 validation: 0.928420
10/01 04:46:35 AM: edges-ner-ontonotes_f1: training: 0.930415 validation: 0.944462
10/01 04:46:35 AM: Global learning rate: 0.0001
10/01 04:46:35 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:46:45 AM: Update 10166: task edges-ner-ontonotes, batch 166 (10166): mcc: 0.9495, acc: 0.9266, precision: 0.9616, recall: 0.9431, f1: 0.9522, edges-ner-ontonotes_loss: 0.0167
10/01 04:46:55 AM: Update 10304: task edges-ner-ontonotes, batch 304 (10304): mcc: 0.9494, acc: 0.9262, precision: 0.9615, recall: 0.9430, f1: 0.9522, edges-ner-ontonotes_loss: 0.0164
10/01 04:47:05 AM: Update 10465: task edges-ner-ontonotes, batch 465 (10465): mcc: 0.9496, acc: 0.9264, precision: 0.9617, recall: 0.9432, f1: 0.9523, edges-ner-ontonotes_loss: 0.0162
10/01 04:47:15 AM: Update 10603: task edges-ner-ontonotes, batch 603 (10603): mcc: 0.9487, acc: 0.9248, precision: 0.9612, recall: 0.9419, f1: 0.9515, edges-ner-ontonotes_loss: 0.0167
10/01 04:47:25 AM: Update 10771: task edges-ner-ontonotes, batch 771 (10771): mcc: 0.9417, acc: 0.9159, precision: 0.9571, recall: 0.9329, f1: 0.9448, edges-ner-ontonotes_loss: 0.0196
10/01 04:47:35 AM: Update 10914: task edges-ner-ontonotes, batch 914 (10914): mcc: 0.9380, acc: 0.9114, precision: 0.9549, recall: 0.9280, f1: 0.9413, edges-ner-ontonotes_loss: 0.0214
10/01 04:47:39 AM: ***** Step 11000 / Validation 11 *****
10/01 04:47:39 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:47:39 AM: Validating...
10/01 04:47:45 AM: Evaluate: task edges-ner-ontonotes, batch 74 (157): mcc: 0.9227, acc: 0.8934, precision: 0.9451, recall: 0.9090, f1: 0.9267, edges-ner-ontonotes_loss: 0.0272
10/01 04:47:52 AM: Updating LR scheduler:
10/01 04:47:52 AM: 	Best result seen so far for macro_avg: 0.944
10/01 04:47:52 AM: 	# validation passes without improvement: 1
10/01 04:47:52 AM: edges-ner-ontonotes_loss: training: 0.021685 validation: 0.020880
10/01 04:47:52 AM: macro_avg: validation: 0.944228
10/01 04:47:52 AM: micro_avg: validation: 0.000000
10/01 04:47:52 AM: edges-ner-ontonotes_mcc: training: 0.937191 validation: 0.941150
10/01 04:47:52 AM: edges-ner-ontonotes_acc: training: 0.910342 validation: 0.918259
10/01 04:47:52 AM: edges-ner-ontonotes_precision: training: 0.954322 validation: 0.960179
10/01 04:47:52 AM: edges-ner-ontonotes_recall: training: 0.927124 validation: 0.928799
10/01 04:47:52 AM: edges-ner-ontonotes_f1: training: 0.940526 validation: 0.944228
10/01 04:47:52 AM: Global learning rate: 0.0001
10/01 04:47:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:47:55 AM: Update 11055: task edges-ner-ontonotes, batch 55 (11055): mcc: 0.9292, acc: 0.8996, precision: 0.9495, recall: 0.9170, f1: 0.9330, edges-ner-ontonotes_loss: 0.0234
10/01 04:48:05 AM: Update 11227: task edges-ner-ontonotes, batch 227 (11227): mcc: 0.9314, acc: 0.9044, precision: 0.9508, recall: 0.9198, f1: 0.9351, edges-ner-ontonotes_loss: 0.0229
10/01 04:48:15 AM: Update 11393: task edges-ner-ontonotes, batch 393 (11393): mcc: 0.9337, acc: 0.9073, precision: 0.9525, recall: 0.9224, f1: 0.9372, edges-ner-ontonotes_loss: 0.0220
10/01 04:48:25 AM: Update 11547: task edges-ner-ontonotes, batch 547 (11547): mcc: 0.9354, acc: 0.9094, precision: 0.9535, recall: 0.9246, f1: 0.9388, edges-ner-ontonotes_loss: 0.0213
10/01 04:48:35 AM: Update 11714: task edges-ner-ontonotes, batch 714 (11714): mcc: 0.9399, acc: 0.9150, precision: 0.9564, recall: 0.9302, f1: 0.9431, edges-ner-ontonotes_loss: 0.0201
10/01 04:48:45 AM: Update 11858: task edges-ner-ontonotes, batch 858 (11858): mcc: 0.9424, acc: 0.9180, precision: 0.9581, recall: 0.9331, f1: 0.9455, edges-ner-ontonotes_loss: 0.0193
10/01 04:48:54 AM: ***** Step 12000 / Validation 12 *****
10/01 04:48:54 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:48:54 AM: Validating...
10/01 04:48:55 AM: Evaluate: task edges-ner-ontonotes, batch 23 (157): mcc: 0.8559, acc: 0.8281, precision: 0.8893, recall: 0.8388, f1: 0.8633, edges-ner-ontonotes_loss: 0.0482
10/01 04:49:05 AM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.9393, acc: 0.9186, precision: 0.9570, recall: 0.9284, f1: 0.9425, edges-ner-ontonotes_loss: 0.0223
10/01 04:49:06 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:49:06 AM: Best result seen so far for macro.
10/01 04:49:06 AM: Updating LR scheduler:
10/01 04:49:06 AM: 	Best result seen so far for macro_avg: 0.945
10/01 04:49:06 AM: 	# validation passes without improvement: 0
10/01 04:49:06 AM: edges-ner-ontonotes_loss: training: 0.018877 validation: 0.021260
10/01 04:49:06 AM: macro_avg: validation: 0.944724
10/01 04:49:06 AM: micro_avg: validation: 0.000000
10/01 04:49:06 AM: edges-ner-ontonotes_mcc: training: 0.943630 validation: 0.941635
10/01 04:49:06 AM: edges-ner-ontonotes_acc: training: 0.919452 validation: 0.921368
10/01 04:49:06 AM: edges-ner-ontonotes_precision: training: 0.958795 validation: 0.958701
10/01 04:49:06 AM: edges-ner-ontonotes_recall: training: 0.934792 validation: 0.931150
10/01 04:49:06 AM: edges-ner-ontonotes_f1: training: 0.946642 validation: 0.944724
10/01 04:49:06 AM: Global learning rate: 0.0001
10/01 04:49:06 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:49:15 AM: Update 12142: task edges-ner-ontonotes, batch 142 (12142): mcc: 0.9534, acc: 0.9300, precision: 0.9644, recall: 0.9476, f1: 0.9559, edges-ner-ontonotes_loss: 0.0155
10/01 04:49:25 AM: Update 12292: task edges-ner-ontonotes, batch 292 (12292): mcc: 0.9327, acc: 0.9039, precision: 0.9524, recall: 0.9206, f1: 0.9362, edges-ner-ontonotes_loss: 0.0231
10/01 04:49:35 AM: Update 12449: task edges-ner-ontonotes, batch 449 (12449): mcc: 0.9269, acc: 0.8967, precision: 0.9480, recall: 0.9142, f1: 0.9308, edges-ner-ontonotes_loss: 0.0256
10/01 04:49:45 AM: Update 12649: task edges-ner-ontonotes, batch 649 (12649): mcc: 0.9280, acc: 0.8979, precision: 0.9488, recall: 0.9153, f1: 0.9318, edges-ner-ontonotes_loss: 0.0248
10/01 04:49:56 AM: Update 12816: task edges-ner-ontonotes, batch 816 (12816): mcc: 0.9290, acc: 0.8998, precision: 0.9493, recall: 0.9167, f1: 0.9327, edges-ner-ontonotes_loss: 0.0243
10/01 04:50:06 AM: Update 12982: task edges-ner-ontonotes, batch 982 (12982): mcc: 0.9312, acc: 0.9030, precision: 0.9505, recall: 0.9196, f1: 0.9348, edges-ner-ontonotes_loss: 0.0236
10/01 04:50:07 AM: ***** Step 13000 / Validation 13 *****
10/01 04:50:07 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:50:07 AM: Validating...
10/01 04:50:16 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9333, acc: 0.9086, precision: 0.9541, recall: 0.9201, f1: 0.9368, edges-ner-ontonotes_loss: 0.0232
10/01 04:50:19 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:50:19 AM: Best result seen so far for macro.
10/01 04:50:19 AM: Updating LR scheduler:
10/01 04:50:19 AM: 	Best result seen so far for macro_avg: 0.945
10/01 04:50:19 AM: 	# validation passes without improvement: 0
10/01 04:50:19 AM: edges-ner-ontonotes_loss: training: 0.023556 validation: 0.020349
10/01 04:50:19 AM: macro_avg: validation: 0.945056
10/01 04:50:19 AM: micro_avg: validation: 0.000000
10/01 04:50:19 AM: edges-ner-ontonotes_mcc: training: 0.931375 validation: 0.941991
10/01 04:50:19 AM: edges-ner-ontonotes_acc: training: 0.903258 validation: 0.920534
10/01 04:50:19 AM: edges-ner-ontonotes_precision: training: 0.950624 validation: 0.959303
10/01 04:50:19 AM: edges-ner-ontonotes_recall: training: 0.919865 validation: 0.931225
10/01 04:50:19 AM: edges-ner-ontonotes_f1: training: 0.934991 validation: 0.945056
10/01 04:50:19 AM: Global learning rate: 0.0001
10/01 04:50:19 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:50:26 AM: Update 13095: task edges-ner-ontonotes, batch 95 (13095): mcc: 0.9389, acc: 0.9127, precision: 0.9538, recall: 0.9309, f1: 0.9422, edges-ner-ontonotes_loss: 0.0201
10/01 04:50:36 AM: Update 13260: task edges-ner-ontonotes, batch 260 (13260): mcc: 0.9498, acc: 0.9262, precision: 0.9617, recall: 0.9434, f1: 0.9525, edges-ner-ontonotes_loss: 0.0167
10/01 04:50:46 AM: Update 13408: task edges-ner-ontonotes, batch 408 (13408): mcc: 0.9496, acc: 0.9259, precision: 0.9615, recall: 0.9433, f1: 0.9523, edges-ner-ontonotes_loss: 0.0166
10/01 04:50:56 AM: Update 13577: task edges-ner-ontonotes, batch 577 (13577): mcc: 0.9495, acc: 0.9259, precision: 0.9612, recall: 0.9435, f1: 0.9523, edges-ner-ontonotes_loss: 0.0164
10/01 04:51:06 AM: Update 13719: task edges-ner-ontonotes, batch 719 (13719): mcc: 0.9490, acc: 0.9251, precision: 0.9611, recall: 0.9425, f1: 0.9518, edges-ner-ontonotes_loss: 0.0167
10/01 04:51:16 AM: Update 13896: task edges-ner-ontonotes, batch 896 (13896): mcc: 0.9428, acc: 0.9174, precision: 0.9573, recall: 0.9348, f1: 0.9459, edges-ner-ontonotes_loss: 0.0193
10/01 04:51:23 AM: ***** Step 14000 / Validation 14 *****
10/01 04:51:23 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:51:23 AM: Validating...
10/01 04:51:26 AM: Evaluate: task edges-ner-ontonotes, batch 51 (157): mcc: 0.9128, acc: 0.8881, precision: 0.9290, recall: 0.9063, f1: 0.9175, edges-ner-ontonotes_loss: 0.0295
10/01 04:51:36 AM: Updating LR scheduler:
10/01 04:51:36 AM: 	Best result seen so far for macro_avg: 0.945
10/01 04:51:36 AM: 	# validation passes without improvement: 1
10/01 04:51:36 AM: edges-ner-ontonotes_loss: training: 0.020507 validation: 0.021524
10/01 04:51:36 AM: macro_avg: validation: 0.942377
10/01 04:51:36 AM: micro_avg: validation: 0.000000
10/01 04:51:36 AM: edges-ner-ontonotes_mcc: training: 0.939924 validation: 0.939164
10/01 04:51:36 AM: edges-ner-ontonotes_acc: training: 0.913685 validation: 0.916439
10/01 04:51:36 AM: edges-ner-ontonotes_precision: training: 0.955459 validation: 0.957001
10/01 04:51:36 AM: edges-ner-ontonotes_recall: training: 0.931126 validation: 0.928192
10/01 04:51:36 AM: edges-ner-ontonotes_f1: training: 0.943136 validation: 0.942377
10/01 04:51:36 AM: Global learning rate: 0.0001
10/01 04:51:36 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:51:37 AM: Update 14005: task edges-ner-ontonotes, batch 5 (14005): mcc: 0.9234, acc: 0.8916, precision: 0.9453, recall: 0.9102, f1: 0.9274, edges-ner-ontonotes_loss: 0.0241
10/01 04:51:47 AM: Update 14202: task edges-ner-ontonotes, batch 202 (14202): mcc: 0.9302, acc: 0.9023, precision: 0.9495, recall: 0.9187, f1: 0.9338, edges-ner-ontonotes_loss: 0.0238
10/01 04:51:57 AM: Update 14359: task edges-ner-ontonotes, batch 359 (14359): mcc: 0.9316, acc: 0.9046, precision: 0.9504, recall: 0.9204, f1: 0.9352, edges-ner-ontonotes_loss: 0.0232
10/01 04:52:07 AM: Update 14524: task edges-ner-ontonotes, batch 524 (14524): mcc: 0.9335, acc: 0.9072, precision: 0.9515, recall: 0.9230, f1: 0.9371, edges-ner-ontonotes_loss: 0.0223
10/01 04:52:17 AM: Update 14664: task edges-ner-ontonotes, batch 664 (14664): mcc: 0.9363, acc: 0.9106, precision: 0.9533, recall: 0.9264, f1: 0.9397, edges-ner-ontonotes_loss: 0.0215
10/01 04:52:27 AM: Update 14830: task edges-ner-ontonotes, batch 830 (14830): mcc: 0.9402, acc: 0.9153, precision: 0.9558, recall: 0.9314, f1: 0.9434, edges-ner-ontonotes_loss: 0.0202
10/01 04:52:37 AM: Update 14976: task edges-ner-ontonotes, batch 976 (14976): mcc: 0.9422, acc: 0.9179, precision: 0.9569, recall: 0.9341, f1: 0.9453, edges-ner-ontonotes_loss: 0.0195
10/01 04:52:39 AM: ***** Step 15000 / Validation 15 *****
10/01 04:52:39 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:52:39 AM: Validating...
10/01 04:52:47 AM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.9297, acc: 0.9051, precision: 0.9536, recall: 0.9138, f1: 0.9333, edges-ner-ontonotes_loss: 0.0254
10/01 04:52:52 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:52:52 AM: Best result seen so far for macro.
10/01 04:52:52 AM: Updating LR scheduler:
10/01 04:52:52 AM: 	Best result seen so far for macro_avg: 0.945
10/01 04:52:52 AM: 	# validation passes without improvement: 0
10/01 04:52:52 AM: edges-ner-ontonotes_loss: training: 0.019417 validation: 0.021063
10/01 04:52:52 AM: macro_avg: validation: 0.945378
10/01 04:52:52 AM: micro_avg: validation: 0.000000
10/01 04:52:52 AM: edges-ner-ontonotes_mcc: training: 0.942420 validation: 0.942354
10/01 04:52:52 AM: edges-ner-ontonotes_acc: training: 0.918089 validation: 0.921747
10/01 04:52:52 AM: edges-ner-ontonotes_precision: training: 0.956926 validation: 0.960774
10/01 04:52:52 AM: edges-ner-ontonotes_recall: training: 0.934364 validation: 0.930467
10/01 04:52:52 AM: edges-ner-ontonotes_f1: training: 0.945510 validation: 0.945378
10/01 04:52:52 AM: Global learning rate: 0.0001
10/01 04:52:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:52:58 AM: Update 15092: task edges-ner-ontonotes, batch 92 (15092): mcc: 0.9501, acc: 0.9263, precision: 0.9623, recall: 0.9436, f1: 0.9528, edges-ner-ontonotes_loss: 0.0161
10/01 04:53:08 AM: Update 15253: task edges-ner-ontonotes, batch 253 (15253): mcc: 0.9518, acc: 0.9289, precision: 0.9638, recall: 0.9452, f1: 0.9544, edges-ner-ontonotes_loss: 0.0155
10/01 04:53:18 AM: Update 15410: task edges-ner-ontonotes, batch 410 (15410): mcc: 0.9386, acc: 0.9121, precision: 0.9551, recall: 0.9291, f1: 0.9419, edges-ner-ontonotes_loss: 0.0208
10/01 04:53:28 AM: Update 15561: task edges-ner-ontonotes, batch 561 (15561): mcc: 0.9320, acc: 0.9043, precision: 0.9506, recall: 0.9211, f1: 0.9356, edges-ner-ontonotes_loss: 0.0235
10/01 04:53:38 AM: Update 15762: task edges-ner-ontonotes, batch 762 (15762): mcc: 0.9316, acc: 0.9038, precision: 0.9501, recall: 0.9208, f1: 0.9352, edges-ner-ontonotes_loss: 0.0234
10/01 04:53:48 AM: Update 15926: task edges-ner-ontonotes, batch 926 (15926): mcc: 0.9318, acc: 0.9040, precision: 0.9502, recall: 0.9212, f1: 0.9354, edges-ner-ontonotes_loss: 0.0231
10/01 04:53:52 AM: ***** Step 16000 / Validation 16 *****
10/01 04:53:52 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:53:52 AM: Validating...
10/01 04:53:58 AM: Evaluate: task edges-ner-ontonotes, batch 74 (157): mcc: 0.9272, acc: 0.9005, precision: 0.9506, recall: 0.9121, f1: 0.9310, edges-ner-ontonotes_loss: 0.0263
10/01 04:54:05 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:54:05 AM: Best result seen so far for macro.
10/01 04:54:05 AM: Updating LR scheduler:
10/01 04:54:05 AM: 	Best result seen so far for macro_avg: 0.947
10/01 04:54:05 AM: 	# validation passes without improvement: 0
10/01 04:54:05 AM: edges-ner-ontonotes_loss: training: 0.022710 validation: 0.019869
10/01 04:54:05 AM: macro_avg: validation: 0.946858
10/01 04:54:05 AM: micro_avg: validation: 0.000000
10/01 04:54:05 AM: edges-ner-ontonotes_mcc: training: 0.933027 validation: 0.943900
10/01 04:54:05 AM: edges-ner-ontonotes_acc: training: 0.905642 validation: 0.922733
10/01 04:54:05 AM: edges-ner-ontonotes_precision: training: 0.951156 validation: 0.961247
10/01 04:54:05 AM: edges-ner-ontonotes_recall: training: 0.922433 validation: 0.932894
10/01 04:54:05 AM: edges-ner-ontonotes_f1: training: 0.936575 validation: 0.946858
10/01 04:54:05 AM: Global learning rate: 0.0001
10/01 04:54:05 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:54:08 AM: Update 16048: task edges-ner-ontonotes, batch 48 (16048): mcc: 0.9417, acc: 0.9205, precision: 0.9560, recall: 0.9340, f1: 0.9449, edges-ner-ontonotes_loss: 0.0191
10/01 04:54:18 AM: Update 16187: task edges-ner-ontonotes, batch 187 (16187): mcc: 0.9385, acc: 0.9138, precision: 0.9538, recall: 0.9300, f1: 0.9418, edges-ner-ontonotes_loss: 0.0198
10/01 04:54:28 AM: Update 16361: task edges-ner-ontonotes, batch 361 (16361): mcc: 0.9447, acc: 0.9206, precision: 0.9574, recall: 0.9382, f1: 0.9477, edges-ner-ontonotes_loss: 0.0180
10/01 04:54:38 AM: Update 16506: task edges-ner-ontonotes, batch 506 (16506): mcc: 0.9484, acc: 0.9252, precision: 0.9600, recall: 0.9426, f1: 0.9512, edges-ner-ontonotes_loss: 0.0169
10/01 04:54:48 AM: Update 16675: task edges-ner-ontonotes, batch 675 (16675): mcc: 0.9493, acc: 0.9259, precision: 0.9609, recall: 0.9433, f1: 0.9520, edges-ner-ontonotes_loss: 0.0166
10/01 04:54:58 AM: Update 16820: task edges-ner-ontonotes, batch 820 (16820): mcc: 0.9496, acc: 0.9261, precision: 0.9612, recall: 0.9436, f1: 0.9523, edges-ner-ontonotes_loss: 0.0164
10/01 04:55:08 AM: Update 16979: task edges-ner-ontonotes, batch 979 (16979): mcc: 0.9441, acc: 0.9192, precision: 0.9578, recall: 0.9367, f1: 0.9471, edges-ner-ontonotes_loss: 0.0186
10/01 04:55:10 AM: ***** Step 17000 / Validation 17 *****
10/01 04:55:10 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:55:10 AM: Validating...
10/01 04:55:19 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.9313, acc: 0.9063, precision: 0.9495, recall: 0.9209, f1: 0.9350, edges-ner-ontonotes_loss: 0.0243
10/01 04:55:23 AM: Updating LR scheduler:
10/01 04:55:23 AM: 	Best result seen so far for macro_avg: 0.947
10/01 04:55:23 AM: 	# validation passes without improvement: 1
10/01 04:55:23 AM: edges-ner-ontonotes_loss: training: 0.018858 validation: 0.020927
10/01 04:55:23 AM: macro_avg: validation: 0.944664
10/01 04:55:23 AM: micro_avg: validation: 0.000000
10/01 04:55:23 AM: edges-ner-ontonotes_mcc: training: 0.943519 validation: 0.941541
10/01 04:55:23 AM: edges-ner-ontonotes_acc: training: 0.918486 validation: 0.920155
10/01 04:55:23 AM: edges-ner-ontonotes_precision: training: 0.957391 validation: 0.956975
10/01 04:55:23 AM: edges-ner-ontonotes_recall: training: 0.935967 validation: 0.932666
10/01 04:55:23 AM: edges-ner-ontonotes_f1: training: 0.946558 validation: 0.944664
10/01 04:55:23 AM: Global learning rate: 0.0001
10/01 04:55:23 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:55:29 AM: Update 17109: task edges-ner-ontonotes, batch 109 (17109): mcc: 0.9173, acc: 0.8842, precision: 0.9418, recall: 0.9023, f1: 0.9216, edges-ner-ontonotes_loss: 0.0286
10/01 04:55:39 AM: Update 17284: task edges-ner-ontonotes, batch 284 (17284): mcc: 0.9247, acc: 0.8948, precision: 0.9456, recall: 0.9123, f1: 0.9287, edges-ner-ontonotes_loss: 0.0255
10/01 04:55:49 AM: Update 17455: task edges-ner-ontonotes, batch 455 (17455): mcc: 0.9282, acc: 0.8997, precision: 0.9477, recall: 0.9167, f1: 0.9320, edges-ner-ontonotes_loss: 0.0242
10/01 04:55:59 AM: Update 17618: task edges-ner-ontonotes, batch 618 (17618): mcc: 0.9316, acc: 0.9042, precision: 0.9498, recall: 0.9210, f1: 0.9352, edges-ner-ontonotes_loss: 0.0230
10/01 04:56:09 AM: Update 17762: task edges-ner-ontonotes, batch 762 (17762): mcc: 0.9337, acc: 0.9069, precision: 0.9511, recall: 0.9238, f1: 0.9372, edges-ner-ontonotes_loss: 0.0221
10/01 04:56:19 AM: Update 17927: task edges-ner-ontonotes, batch 927 (17927): mcc: 0.9379, acc: 0.9120, precision: 0.9536, recall: 0.9292, f1: 0.9412, edges-ner-ontonotes_loss: 0.0209
10/01 04:56:23 AM: ***** Step 18000 / Validation 18 *****
10/01 04:56:23 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:56:23 AM: Validating...
10/01 04:56:29 AM: Evaluate: task edges-ner-ontonotes, batch 74 (157): mcc: 0.9226, acc: 0.8947, precision: 0.9474, recall: 0.9067, f1: 0.9266, edges-ner-ontonotes_loss: 0.0286
10/01 04:56:36 AM: Updating LR scheduler:
10/01 04:56:36 AM: 	Best result seen so far for macro_avg: 0.947
10/01 04:56:36 AM: 	# validation passes without improvement: 2
10/01 04:56:36 AM: edges-ner-ontonotes_loss: training: 0.020307 validation: 0.020620
10/01 04:56:36 AM: macro_avg: validation: 0.945886
10/01 04:56:36 AM: micro_avg: validation: 0.000000
10/01 04:56:36 AM: edges-ner-ontonotes_mcc: training: 0.939703 validation: 0.942875
10/01 04:56:36 AM: edges-ner-ontonotes_acc: training: 0.914318 validation: 0.922202
10/01 04:56:36 AM: edges-ner-ontonotes_precision: training: 0.954856 validation: 0.960450
10/01 04:56:36 AM: edges-ner-ontonotes_recall: training: 0.931306 validation: 0.931756
10/01 04:56:36 AM: edges-ner-ontonotes_f1: training: 0.942934 validation: 0.945886
10/01 04:56:36 AM: Global learning rate: 0.0001
10/01 04:56:36 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:56:39 AM: Update 18045: task edges-ner-ontonotes, batch 45 (18045): mcc: 0.9550, acc: 0.9338, precision: 0.9657, recall: 0.9493, f1: 0.9575, edges-ner-ontonotes_loss: 0.0145
10/01 04:56:49 AM: Update 18200: task edges-ner-ontonotes, batch 200 (18200): mcc: 0.9525, acc: 0.9295, precision: 0.9643, recall: 0.9461, f1: 0.9551, edges-ner-ontonotes_loss: 0.0152
10/01 04:56:59 AM: Update 18367: task edges-ner-ontonotes, batch 367 (18367): mcc: 0.9532, acc: 0.9306, precision: 0.9648, recall: 0.9469, f1: 0.9558, edges-ner-ontonotes_loss: 0.0152
10/01 04:57:09 AM: Update 18505: task edges-ner-ontonotes, batch 505 (18505): mcc: 0.9434, acc: 0.9181, precision: 0.9588, recall: 0.9344, f1: 0.9464, edges-ner-ontonotes_loss: 0.0192
10/01 04:57:20 AM: Update 18673: task edges-ner-ontonotes, batch 673 (18673): mcc: 0.9382, acc: 0.9118, precision: 0.9555, recall: 0.9279, f1: 0.9415, edges-ner-ontonotes_loss: 0.0215
10/01 04:57:30 AM: Update 18867: task edges-ner-ontonotes, batch 867 (18867): mcc: 0.9373, acc: 0.9107, precision: 0.9546, recall: 0.9270, f1: 0.9406, edges-ner-ontonotes_loss: 0.0217
10/01 04:57:38 AM: ***** Step 19000 / Validation 19 *****
10/01 04:57:38 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:57:38 AM: Validating...
10/01 04:57:40 AM: Evaluate: task edges-ner-ontonotes, batch 25 (157): mcc: 0.8765, acc: 0.8439, precision: 0.9033, recall: 0.8636, f1: 0.8830, edges-ner-ontonotes_loss: 0.0398
10/01 04:57:50 AM: Evaluate: task edges-ner-ontonotes, batch 142 (157): mcc: 0.9422, acc: 0.9201, precision: 0.9601, recall: 0.9308, f1: 0.9452, edges-ner-ontonotes_loss: 0.0204
10/01 04:57:51 AM: Updating LR scheduler:
10/01 04:57:51 AM: 	Best result seen so far for macro_avg: 0.947
10/01 04:57:51 AM: 	# validation passes without improvement: 3
10/01 04:57:51 AM: edges-ner-ontonotes_loss: training: 0.021818 validation: 0.019828
10/01 04:57:51 AM: macro_avg: validation: 0.946663
10/01 04:57:51 AM: micro_avg: validation: 0.000000
10/01 04:57:51 AM: edges-ner-ontonotes_mcc: training: 0.936766 validation: 0.943695
10/01 04:57:51 AM: edges-ner-ontonotes_acc: training: 0.910271 validation: 0.921975
10/01 04:57:51 AM: edges-ner-ontonotes_precision: training: 0.953980 validation: 0.961088
10/01 04:57:51 AM: edges-ner-ontonotes_recall: training: 0.926663 validation: 0.932666
10/01 04:57:51 AM: edges-ner-ontonotes_f1: training: 0.940123 validation: 0.946663
10/01 04:57:51 AM: Global learning rate: 0.0001
10/01 04:57:51 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:58:00 AM: Update 19149: task edges-ner-ontonotes, batch 149 (19149): mcc: 0.9405, acc: 0.9157, precision: 0.9555, recall: 0.9322, f1: 0.9437, edges-ner-ontonotes_loss: 0.0194
10/01 04:58:10 AM: Update 19299: task edges-ner-ontonotes, batch 299 (19299): mcc: 0.9413, acc: 0.9166, precision: 0.9560, recall: 0.9332, f1: 0.9444, edges-ner-ontonotes_loss: 0.0191
10/01 04:58:20 AM: Update 19454: task edges-ner-ontonotes, batch 454 (19454): mcc: 0.9458, acc: 0.9220, precision: 0.9584, recall: 0.9393, f1: 0.9487, edges-ner-ontonotes_loss: 0.0177
10/01 04:58:30 AM: Update 19610: task edges-ner-ontonotes, batch 610 (19610): mcc: 0.9484, acc: 0.9251, precision: 0.9600, recall: 0.9425, f1: 0.9512, edges-ner-ontonotes_loss: 0.0169
10/01 04:58:40 AM: Update 19767: task edges-ner-ontonotes, batch 767 (19767): mcc: 0.9494, acc: 0.9263, precision: 0.9607, recall: 0.9437, f1: 0.9521, edges-ner-ontonotes_loss: 0.0166
10/01 04:58:51 AM: Update 19925: task edges-ner-ontonotes, batch 925 (19925): mcc: 0.9505, acc: 0.9276, precision: 0.9617, recall: 0.9448, f1: 0.9532, edges-ner-ontonotes_loss: 0.0162
10/01 04:58:56 AM: ***** Step 20000 / Validation 20 *****
10/01 04:58:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:58:56 AM: Validating...
10/01 04:59:01 AM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.9233, acc: 0.8947, precision: 0.9409, recall: 0.9143, f1: 0.9274, edges-ner-ontonotes_loss: 0.0274
10/01 04:59:09 AM: Updating LR scheduler:
10/01 04:59:09 AM: 	Best result seen so far for macro_avg: 0.947
10/01 04:59:09 AM: 	# validation passes without improvement: 0
10/01 04:59:09 AM: edges-ner-ontonotes_loss: training: 0.017493 validation: 0.020596
10/01 04:59:09 AM: macro_avg: validation: 0.944231
10/01 04:59:09 AM: micro_avg: validation: 0.000000
10/01 04:59:09 AM: edges-ner-ontonotes_mcc: training: 0.947656 validation: 0.941086
10/01 04:59:09 AM: edges-ner-ontonotes_acc: training: 0.923910 validation: 0.918562
10/01 04:59:09 AM: edges-ner-ontonotes_precision: training: 0.960052 validation: 0.956725
10/01 04:59:09 AM: edges-ner-ontonotes_recall: training: 0.941107 validation: 0.932059
10/01 04:59:09 AM: edges-ner-ontonotes_f1: training: 0.950485 validation: 0.944231
10/01 04:59:09 AM: Global learning rate: 5e-05
10/01 04:59:09 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 04:59:11 AM: Update 20041: task edges-ner-ontonotes, batch 41 (20041): mcc: 0.9165, acc: 0.8825, precision: 0.9433, recall: 0.8992, f1: 0.9207, edges-ner-ontonotes_loss: 0.0287
10/01 04:59:21 AM: Update 20201: task edges-ner-ontonotes, batch 201 (20201): mcc: 0.9191, acc: 0.8877, precision: 0.9426, recall: 0.9048, f1: 0.9233, edges-ner-ontonotes_loss: 0.0273
10/01 04:59:32 AM: Update 20373: task edges-ner-ontonotes, batch 373 (20373): mcc: 0.9230, acc: 0.8931, precision: 0.9442, recall: 0.9105, f1: 0.9271, edges-ner-ontonotes_loss: 0.0258
10/01 04:59:42 AM: Update 20544: task edges-ner-ontonotes, batch 544 (20544): mcc: 0.9264, acc: 0.8975, precision: 0.9462, recall: 0.9148, f1: 0.9303, edges-ner-ontonotes_loss: 0.0246
10/01 04:59:52 AM: Update 20707: task edges-ner-ontonotes, batch 707 (20707): mcc: 0.9308, acc: 0.9035, precision: 0.9492, recall: 0.9201, f1: 0.9344, edges-ner-ontonotes_loss: 0.0232
10/01 05:00:02 AM: Update 20856: task edges-ner-ontonotes, batch 856 (20856): mcc: 0.9322, acc: 0.9053, precision: 0.9498, recall: 0.9223, f1: 0.9358, edges-ner-ontonotes_loss: 0.0225
10/01 05:00:10 AM: ***** Step 21000 / Validation 21 *****
10/01 05:00:10 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:00:10 AM: Validating...
10/01 05:00:12 AM: Evaluate: task edges-ner-ontonotes, batch 19 (157): mcc: 0.8785, acc: 0.8484, precision: 0.9122, recall: 0.8586, f1: 0.8846, edges-ner-ontonotes_loss: 0.0364
10/01 05:00:22 AM: Evaluate: task edges-ner-ontonotes, batch 130 (157): mcc: 0.9404, acc: 0.9190, precision: 0.9589, recall: 0.9287, f1: 0.9435, edges-ner-ontonotes_loss: 0.0220
10/01 05:00:24 AM: Best result seen so far for edges-ner-ontonotes.
10/01 05:00:24 AM: Best result seen so far for macro.
10/01 05:00:24 AM: Updating LR scheduler:
10/01 05:00:24 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:00:24 AM: 	# validation passes without improvement: 0
10/01 05:00:24 AM: edges-ner-ontonotes_loss: training: 0.021300 validation: 0.020161
10/01 05:00:24 AM: macro_avg: validation: 0.947874
10/01 05:00:24 AM: micro_avg: validation: 0.000000
10/01 05:00:24 AM: edges-ner-ontonotes_mcc: training: 0.936296 validation: 0.944956
10/01 05:00:24 AM: edges-ner-ontonotes_acc: training: 0.910273 validation: 0.924932
10/01 05:00:24 AM: edges-ner-ontonotes_precision: training: 0.952287 validation: 0.961251
10/01 05:00:24 AM: edges-ner-ontonotes_recall: training: 0.927446 validation: 0.934865
10/01 05:00:24 AM: edges-ner-ontonotes_f1: training: 0.939702 validation: 0.947874
10/01 05:00:24 AM: Global learning rate: 5e-05
10/01 05:00:24 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:00:32 AM: Update 21127: task edges-ner-ontonotes, batch 127 (21127): mcc: 0.9553, acc: 0.9340, precision: 0.9653, recall: 0.9503, f1: 0.9577, edges-ner-ontonotes_loss: 0.0143
10/01 05:00:42 AM: Update 21270: task edges-ner-ontonotes, batch 270 (21270): mcc: 0.9553, acc: 0.9331, precision: 0.9657, recall: 0.9499, f1: 0.9577, edges-ner-ontonotes_loss: 0.0144
10/01 05:00:52 AM: Update 21436: task edges-ner-ontonotes, batch 436 (21436): mcc: 0.9546, acc: 0.9318, precision: 0.9651, recall: 0.9491, f1: 0.9570, edges-ner-ontonotes_loss: 0.0147
10/01 05:01:02 AM: Update 21590: task edges-ner-ontonotes, batch 590 (21590): mcc: 0.9480, acc: 0.9231, precision: 0.9611, recall: 0.9407, f1: 0.9508, edges-ner-ontonotes_loss: 0.0175
10/01 05:01:12 AM: Update 21761: task edges-ner-ontonotes, batch 761 (21761): mcc: 0.9424, acc: 0.9162, precision: 0.9580, recall: 0.9334, f1: 0.9455, edges-ner-ontonotes_loss: 0.0199
10/01 05:01:22 AM: Update 21933: task edges-ner-ontonotes, batch 933 (21933): mcc: 0.9404, acc: 0.9137, precision: 0.9562, recall: 0.9314, f1: 0.9436, edges-ner-ontonotes_loss: 0.0206
10/01 05:01:25 AM: ***** Step 22000 / Validation 22 *****
10/01 05:01:25 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:01:25 AM: Validating...
10/01 05:01:32 AM: Evaluate: task edges-ner-ontonotes, batch 85 (157): mcc: 0.9344, acc: 0.9101, precision: 0.9534, recall: 0.9227, f1: 0.9378, edges-ner-ontonotes_loss: 0.0239
10/01 05:01:38 AM: Updating LR scheduler:
10/01 05:01:38 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:01:38 AM: 	# validation passes without improvement: 1
10/01 05:01:38 AM: edges-ner-ontonotes_loss: training: 0.020619 validation: 0.019676
10/01 05:01:38 AM: macro_avg: validation: 0.946907
10/01 05:01:38 AM: micro_avg: validation: 0.000000
10/01 05:01:38 AM: edges-ner-ontonotes_mcc: training: 0.940239 validation: 0.943934
10/01 05:01:38 AM: edges-ner-ontonotes_acc: training: 0.913660 validation: 0.923264
10/01 05:01:38 AM: edges-ner-ontonotes_precision: training: 0.956123 validation: 0.960384
10/01 05:01:38 AM: edges-ner-ontonotes_recall: training: 0.931064 validation: 0.933803
10/01 05:01:38 AM: edges-ner-ontonotes_f1: training: 0.943427 validation: 0.946907
10/01 05:01:38 AM: Global learning rate: 5e-05
10/01 05:01:38 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:01:42 AM: Update 22078: task edges-ner-ontonotes, batch 78 (22078): mcc: 0.9394, acc: 0.9164, precision: 0.9546, recall: 0.9310, f1: 0.9427, edges-ner-ontonotes_loss: 0.0207
10/01 05:01:52 AM: Update 22238: task edges-ner-ontonotes, batch 238 (22238): mcc: 0.9392, acc: 0.9148, precision: 0.9548, recall: 0.9304, f1: 0.9424, edges-ner-ontonotes_loss: 0.0200
10/01 05:02:02 AM: Update 22406: task edges-ner-ontonotes, batch 406 (22406): mcc: 0.9409, acc: 0.9174, precision: 0.9556, recall: 0.9328, f1: 0.9441, edges-ner-ontonotes_loss: 0.0195
10/01 05:02:12 AM: Update 22547: task edges-ner-ontonotes, batch 547 (22547): mcc: 0.9450, acc: 0.9221, precision: 0.9583, recall: 0.9379, f1: 0.9480, edges-ner-ontonotes_loss: 0.0182
10/01 05:02:22 AM: Update 22713: task edges-ner-ontonotes, batch 713 (22713): mcc: 0.9481, acc: 0.9258, precision: 0.9602, recall: 0.9418, f1: 0.9509, edges-ner-ontonotes_loss: 0.0172
10/01 05:02:32 AM: Update 22864: task edges-ner-ontonotes, batch 864 (22864): mcc: 0.9497, acc: 0.9273, precision: 0.9615, recall: 0.9434, f1: 0.9524, edges-ner-ontonotes_loss: 0.0167
10/01 05:02:41 AM: ***** Step 23000 / Validation 23 *****
10/01 05:02:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:02:41 AM: Validating...
10/01 05:02:42 AM: Evaluate: task edges-ner-ontonotes, batch 22 (157): mcc: 0.8823, acc: 0.8526, precision: 0.9112, recall: 0.8667, f1: 0.8884, edges-ner-ontonotes_loss: 0.0381
10/01 05:02:52 AM: Evaluate: task edges-ner-ontonotes, batch 139 (157): mcc: 0.9420, acc: 0.9216, precision: 0.9587, recall: 0.9318, f1: 0.9450, edges-ner-ontonotes_loss: 0.0216
10/01 05:02:54 AM: Updating LR scheduler:
10/01 05:02:54 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:02:54 AM: 	# validation passes without improvement: 2
10/01 05:02:54 AM: edges-ner-ontonotes_loss: training: 0.016531 validation: 0.020487
10/01 05:02:54 AM: macro_avg: validation: 0.946948
10/01 05:02:54 AM: micro_avg: validation: 0.000000
10/01 05:02:54 AM: edges-ner-ontonotes_mcc: training: 0.950289 validation: 0.943964
10/01 05:02:54 AM: edges-ner-ontonotes_acc: training: 0.927793 validation: 0.924173
10/01 05:02:54 AM: edges-ner-ontonotes_precision: training: 0.961960 validation: 0.959667
10/01 05:02:54 AM: edges-ner-ontonotes_recall: training: 0.944168 validation: 0.934562
10/01 05:02:54 AM: edges-ner-ontonotes_f1: training: 0.952981 validation: 0.946948
10/01 05:02:54 AM: Global learning rate: 5e-05
10/01 05:02:54 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:03:02 AM: Update 23125: task edges-ner-ontonotes, batch 125 (23125): mcc: 0.9246, acc: 0.8932, precision: 0.9467, recall: 0.9110, f1: 0.9285, edges-ner-ontonotes_loss: 0.0258
10/01 05:03:12 AM: Update 23302: task edges-ner-ontonotes, batch 302 (23302): mcc: 0.9200, acc: 0.8881, precision: 0.9420, recall: 0.9071, f1: 0.9242, edges-ner-ontonotes_loss: 0.0271
10/01 05:03:22 AM: Update 23467: task edges-ner-ontonotes, batch 467 (23467): mcc: 0.9236, acc: 0.8931, precision: 0.9445, recall: 0.9114, f1: 0.9276, edges-ner-ontonotes_loss: 0.0257
10/01 05:03:33 AM: Update 23654: task edges-ner-ontonotes, batch 654 (23654): mcc: 0.9265, acc: 0.8969, precision: 0.9464, recall: 0.9148, f1: 0.9304, edges-ner-ontonotes_loss: 0.0247
10/01 05:03:43 AM: Update 23821: task edges-ner-ontonotes, batch 821 (23821): mcc: 0.9302, acc: 0.9020, precision: 0.9490, recall: 0.9192, f1: 0.9339, edges-ner-ontonotes_loss: 0.0235
10/01 05:03:53 AM: Update 23967: task edges-ner-ontonotes, batch 967 (23967): mcc: 0.9318, acc: 0.9042, precision: 0.9497, recall: 0.9215, f1: 0.9354, edges-ner-ontonotes_loss: 0.0228
10/01 05:03:56 AM: ***** Step 24000 / Validation 24 *****
10/01 05:03:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:03:56 AM: Validating...
10/01 05:04:03 AM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.9299, acc: 0.9048, precision: 0.9519, recall: 0.9158, f1: 0.9335, edges-ner-ontonotes_loss: 0.0244
10/01 05:04:08 AM: Updating LR scheduler:
10/01 05:04:08 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:04:08 AM: 	# validation passes without improvement: 3
10/01 05:04:08 AM: edges-ner-ontonotes_loss: training: 0.022517 validation: 0.019661
10/01 05:04:08 AM: macro_avg: validation: 0.947869
10/01 05:04:08 AM: micro_avg: validation: 0.000000
10/01 05:04:08 AM: edges-ner-ontonotes_mcc: training: 0.932929 validation: 0.944926
10/01 05:04:08 AM: edges-ner-ontonotes_acc: training: 0.905716 validation: 0.924704
10/01 05:04:08 AM: edges-ner-ontonotes_precision: training: 0.950457 validation: 0.959882
10/01 05:04:08 AM: edges-ner-ontonotes_recall: training: 0.922935 validation: 0.936154
10/01 05:04:08 AM: edges-ner-ontonotes_f1: training: 0.936494 validation: 0.947869
10/01 05:04:08 AM: Global learning rate: 5e-05
10/01 05:04:08 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:04:13 AM: Update 24076: task edges-ner-ontonotes, batch 76 (24076): mcc: 0.9549, acc: 0.9333, precision: 0.9636, recall: 0.9513, f1: 0.9574, edges-ner-ontonotes_loss: 0.0148
10/01 05:04:23 AM: Update 24236: task edges-ner-ontonotes, batch 236 (24236): mcc: 0.9554, acc: 0.9333, precision: 0.9650, recall: 0.9506, f1: 0.9578, edges-ner-ontonotes_loss: 0.0144
10/01 05:04:34 AM: Update 24384: task edges-ner-ontonotes, batch 384 (24384): mcc: 0.9552, acc: 0.9335, precision: 0.9653, recall: 0.9501, f1: 0.9577, edges-ner-ontonotes_loss: 0.0144
10/01 05:04:44 AM: Update 24554: task edges-ner-ontonotes, batch 554 (24554): mcc: 0.9553, acc: 0.9336, precision: 0.9655, recall: 0.9500, f1: 0.9577, edges-ner-ontonotes_loss: 0.0145
10/01 05:04:54 AM: Update 24709: task edges-ner-ontonotes, batch 709 (24709): mcc: 0.9492, acc: 0.9251, precision: 0.9619, recall: 0.9422, f1: 0.9519, edges-ner-ontonotes_loss: 0.0170
10/01 05:05:04 AM: Update 24876: task edges-ner-ontonotes, batch 876 (24876): mcc: 0.9438, acc: 0.9182, precision: 0.9583, recall: 0.9356, f1: 0.9468, edges-ner-ontonotes_loss: 0.0191
10/01 05:05:12 AM: ***** Step 25000 / Validation 25 *****
10/01 05:05:12 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:05:12 AM: Validating...
10/01 05:05:14 AM: Evaluate: task edges-ner-ontonotes, batch 28 (157): mcc: 0.8859, acc: 0.8558, precision: 0.9076, recall: 0.8769, f1: 0.8920, edges-ner-ontonotes_loss: 0.0371
10/01 05:05:24 AM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.9429, acc: 0.9218, precision: 0.9582, recall: 0.9340, f1: 0.9459, edges-ner-ontonotes_loss: 0.0203
10/01 05:05:25 AM: Updating LR scheduler:
10/01 05:05:25 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:05:25 AM: 	# validation passes without improvement: 0
10/01 05:05:25 AM: edges-ner-ontonotes_loss: training: 0.019638 validation: 0.019740
10/01 05:05:25 AM: macro_avg: validation: 0.946896
10/01 05:05:25 AM: micro_avg: validation: 0.000000
10/01 05:05:25 AM: edges-ner-ontonotes_mcc: training: 0.942167 validation: 0.943900
10/01 05:05:25 AM: edges-ner-ontonotes_acc: training: 0.916347 validation: 0.922884
10/01 05:05:25 AM: edges-ner-ontonotes_precision: training: 0.957018 validation: 0.959160
10/01 05:05:25 AM: edges-ner-ontonotes_recall: training: 0.933797 validation: 0.934941
10/01 05:05:25 AM: edges-ner-ontonotes_f1: training: 0.945265 validation: 0.946896
10/01 05:05:25 AM: Global learning rate: 2.5e-05
10/01 05:05:25 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:05:34 AM: Update 25181: task edges-ner-ontonotes, batch 181 (25181): mcc: 0.9373, acc: 0.9123, precision: 0.9546, recall: 0.9270, f1: 0.9406, edges-ner-ontonotes_loss: 0.0216
10/01 05:05:44 AM: Update 25328: task edges-ner-ontonotes, batch 328 (25328): mcc: 0.9366, acc: 0.9109, precision: 0.9530, recall: 0.9272, f1: 0.9399, edges-ner-ontonotes_loss: 0.0211
10/01 05:05:54 AM: Update 25505: task edges-ner-ontonotes, batch 505 (25505): mcc: 0.9395, acc: 0.9149, precision: 0.9552, recall: 0.9305, f1: 0.9427, edges-ner-ontonotes_loss: 0.0201
10/01 05:06:04 AM: Update 25646: task edges-ner-ontonotes, batch 646 (25646): mcc: 0.9422, acc: 0.9181, precision: 0.9562, recall: 0.9347, f1: 0.9454, edges-ner-ontonotes_loss: 0.0192
10/01 05:06:14 AM: Update 25814: task edges-ner-ontonotes, batch 814 (25814): mcc: 0.9457, acc: 0.9224, precision: 0.9584, recall: 0.9390, f1: 0.9486, edges-ner-ontonotes_loss: 0.0181
10/01 05:06:24 AM: Update 25956: task edges-ner-ontonotes, batch 956 (25956): mcc: 0.9472, acc: 0.9241, precision: 0.9595, recall: 0.9408, f1: 0.9500, edges-ner-ontonotes_loss: 0.0176
10/01 05:06:26 AM: ***** Step 26000 / Validation 26 *****
10/01 05:06:26 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:06:26 AM: Validating...
10/01 05:06:34 AM: Evaluate: task edges-ner-ontonotes, batch 98 (157): mcc: 0.9327, acc: 0.9089, precision: 0.9526, recall: 0.9204, f1: 0.9362, edges-ner-ontonotes_loss: 0.0247
10/01 05:06:39 AM: Updating LR scheduler:
10/01 05:06:39 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:06:39 AM: 	# validation passes without improvement: 1
10/01 05:06:39 AM: edges-ner-ontonotes_loss: training: 0.017513 validation: 0.020232
10/01 05:06:39 AM: macro_avg: validation: 0.947376
10/01 05:06:39 AM: micro_avg: validation: 0.000000
10/01 05:06:39 AM: edges-ner-ontonotes_mcc: training: 0.947557 validation: 0.944403
10/01 05:06:39 AM: edges-ner-ontonotes_acc: training: 0.924439 validation: 0.924477
10/01 05:06:39 AM: edges-ner-ontonotes_precision: training: 0.959879 validation: 0.959269
10/01 05:06:39 AM: edges-ner-ontonotes_recall: training: 0.941092 validation: 0.935775
10/01 05:06:39 AM: edges-ner-ontonotes_f1: training: 0.950393 validation: 0.947376
10/01 05:06:39 AM: Global learning rate: 2.5e-05
10/01 05:06:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:06:44 AM: Update 26073: task edges-ner-ontonotes, batch 73 (26073): mcc: 0.9543, acc: 0.9312, precision: 0.9640, recall: 0.9497, f1: 0.9568, edges-ner-ontonotes_loss: 0.0151
10/01 05:06:54 AM: Update 26230: task edges-ner-ontonotes, batch 230 (26230): mcc: 0.9427, acc: 0.9159, precision: 0.9589, recall: 0.9330, f1: 0.9458, edges-ner-ontonotes_loss: 0.0198
10/01 05:07:04 AM: Update 26402: task edges-ner-ontonotes, batch 402 (26402): mcc: 0.9340, acc: 0.9054, precision: 0.9528, recall: 0.9226, f1: 0.9375, edges-ner-ontonotes_loss: 0.0228
10/01 05:07:14 AM: Update 26569: task edges-ner-ontonotes, batch 569 (26569): mcc: 0.9324, acc: 0.9039, precision: 0.9512, recall: 0.9213, f1: 0.9360, edges-ner-ontonotes_loss: 0.0233
10/01 05:07:24 AM: Update 26764: task edges-ner-ontonotes, batch 764 (26764): mcc: 0.9334, acc: 0.9055, precision: 0.9516, recall: 0.9227, f1: 0.9370, edges-ner-ontonotes_loss: 0.0228
10/01 05:07:34 AM: Update 26915: task edges-ner-ontonotes, batch 915 (26915): mcc: 0.9348, acc: 0.9075, precision: 0.9524, recall: 0.9246, f1: 0.9383, edges-ner-ontonotes_loss: 0.0221
10/01 05:07:39 AM: ***** Step 27000 / Validation 27 *****
10/01 05:07:39 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:07:39 AM: Validating...
10/01 05:07:44 AM: Evaluate: task edges-ner-ontonotes, batch 67 (157): mcc: 0.9257, acc: 0.9001, precision: 0.9473, recall: 0.9125, f1: 0.9295, edges-ner-ontonotes_loss: 0.0261
10/01 05:07:52 AM: Updating LR scheduler:
10/01 05:07:52 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:07:52 AM: 	# validation passes without improvement: 2
10/01 05:07:52 AM: edges-ner-ontonotes_loss: training: 0.021853 validation: 0.019367
10/01 05:07:52 AM: macro_avg: validation: 0.947668
10/01 05:07:52 AM: micro_avg: validation: 0.000000
10/01 05:07:52 AM: edges-ner-ontonotes_mcc: training: 0.935503 validation: 0.944729
10/01 05:07:52 AM: edges-ner-ontonotes_acc: training: 0.908573 validation: 0.924553
10/01 05:07:52 AM: edges-ner-ontonotes_precision: training: 0.952834 validation: 0.960586
10/01 05:07:52 AM: edges-ner-ontonotes_recall: training: 0.925422 validation: 0.935093
10/01 05:07:52 AM: edges-ner-ontonotes_f1: training: 0.938928 validation: 0.947668
10/01 05:07:52 AM: Global learning rate: 2.5e-05
10/01 05:07:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:07:54 AM: Update 27033: task edges-ner-ontonotes, batch 33 (27033): mcc: 0.9395, acc: 0.9174, precision: 0.9507, recall: 0.9351, f1: 0.9428, edges-ner-ontonotes_loss: 0.0202
10/01 05:08:04 AM: Update 27173: task edges-ner-ontonotes, batch 173 (27173): mcc: 0.9489, acc: 0.9267, precision: 0.9596, recall: 0.9439, f1: 0.9517, edges-ner-ontonotes_loss: 0.0169
10/01 05:08:14 AM: Update 27339: task edges-ner-ontonotes, batch 339 (27339): mcc: 0.9536, acc: 0.9324, precision: 0.9629, recall: 0.9495, f1: 0.9561, edges-ner-ontonotes_loss: 0.0155
10/01 05:08:24 AM: Update 27494: task edges-ner-ontonotes, batch 494 (27494): mcc: 0.9547, acc: 0.9335, precision: 0.9642, recall: 0.9502, f1: 0.9572, edges-ner-ontonotes_loss: 0.0151
10/01 05:08:34 AM: Update 27662: task edges-ner-ontonotes, batch 662 (27662): mcc: 0.9547, acc: 0.9333, precision: 0.9642, recall: 0.9503, f1: 0.9572, edges-ner-ontonotes_loss: 0.0150
10/01 05:08:44 AM: Update 27804: task edges-ner-ontonotes, batch 804 (27804): mcc: 0.9509, acc: 0.9283, precision: 0.9620, recall: 0.9452, f1: 0.9535, edges-ner-ontonotes_loss: 0.0164
10/01 05:08:54 AM: Update 27982: task edges-ner-ontonotes, batch 982 (27982): mcc: 0.9459, acc: 0.9216, precision: 0.9590, recall: 0.9388, f1: 0.9488, edges-ner-ontonotes_loss: 0.0186
10/01 05:08:56 AM: ***** Step 28000 / Validation 28 *****
10/01 05:08:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:08:56 AM: Validating...
10/01 05:09:04 AM: Evaluate: task edges-ner-ontonotes, batch 107 (157): mcc: 0.9321, acc: 0.9069, precision: 0.9524, recall: 0.9195, f1: 0.9357, edges-ner-ontonotes_loss: 0.0240
10/01 05:09:09 AM: Updating LR scheduler:
10/01 05:09:09 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:09:09 AM: 	# validation passes without improvement: 3
10/01 05:09:09 AM: edges-ner-ontonotes_loss: training: 0.018786 validation: 0.020184
10/01 05:09:09 AM: macro_avg: validation: 0.946159
10/01 05:09:09 AM: micro_avg: validation: 0.000000
10/01 05:09:09 AM: edges-ner-ontonotes_mcc: training: 0.945475 validation: 0.943136
10/01 05:09:09 AM: edges-ner-ontonotes_acc: training: 0.921152 validation: 0.922126
10/01 05:09:09 AM: edges-ner-ontonotes_precision: training: 0.958778 validation: 0.959246
10/01 05:09:09 AM: edges-ner-ontonotes_recall: training: 0.938269 validation: 0.933424
10/01 05:09:09 AM: edges-ner-ontonotes_f1: training: 0.948413 validation: 0.946159
10/01 05:09:09 AM: Global learning rate: 2.5e-05
10/01 05:09:09 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:09:14 AM: Update 28081: task edges-ner-ontonotes, batch 81 (28081): mcc: 0.9294, acc: 0.9028, precision: 0.9465, recall: 0.9202, f1: 0.9332, edges-ner-ontonotes_loss: 0.0235
10/01 05:09:25 AM: Update 28274: task edges-ner-ontonotes, batch 274 (28274): mcc: 0.9338, acc: 0.9080, precision: 0.9499, recall: 0.9252, f1: 0.9374, edges-ner-ontonotes_loss: 0.0224
10/01 05:09:35 AM: Update 28428: task edges-ner-ontonotes, batch 428 (28428): mcc: 0.9366, acc: 0.9111, precision: 0.9520, recall: 0.9282, f1: 0.9400, edges-ner-ontonotes_loss: 0.0213
10/01 05:09:45 AM: Update 28597: task edges-ner-ontonotes, batch 597 (28597): mcc: 0.9378, acc: 0.9131, precision: 0.9530, recall: 0.9296, f1: 0.9411, edges-ner-ontonotes_loss: 0.0206
10/01 05:09:55 AM: Update 28749: task edges-ner-ontonotes, batch 749 (28749): mcc: 0.9415, acc: 0.9176, precision: 0.9551, recall: 0.9345, f1: 0.9447, edges-ner-ontonotes_loss: 0.0195
10/01 05:10:05 AM: Update 28919: task edges-ner-ontonotes, batch 919 (28919): mcc: 0.9447, acc: 0.9213, precision: 0.9573, recall: 0.9383, f1: 0.9477, edges-ner-ontonotes_loss: 0.0185
10/01 05:10:11 AM: ***** Step 29000 / Validation 29 *****
10/01 05:10:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:10:11 AM: Validating...
10/01 05:10:15 AM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.9126, acc: 0.8872, precision: 0.9341, recall: 0.9009, f1: 0.9172, edges-ner-ontonotes_loss: 0.0308
10/01 05:10:24 AM: Updating LR scheduler:
10/01 05:10:24 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:10:24 AM: 	# validation passes without improvement: 0
10/01 05:10:24 AM: edges-ner-ontonotes_loss: training: 0.018143 validation: 0.020369
10/01 05:10:24 AM: macro_avg: validation: 0.947126
10/01 05:10:24 AM: micro_avg: validation: 0.000000
10/01 05:10:24 AM: edges-ner-ontonotes_mcc: training: 0.945680 validation: 0.944144
10/01 05:10:24 AM: edges-ner-ontonotes_acc: training: 0.922447 validation: 0.923567
10/01 05:10:24 AM: edges-ner-ontonotes_precision: training: 0.958188 validation: 0.959393
10/01 05:10:24 AM: edges-ner-ontonotes_recall: training: 0.939238 validation: 0.935168
10/01 05:10:24 AM: edges-ner-ontonotes_f1: training: 0.948618 validation: 0.947126
10/01 05:10:24 AM: Global learning rate: 1.25e-05
10/01 05:10:24 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:10:25 AM: Update 29009: task edges-ner-ontonotes, batch 9 (29009): mcc: 0.9427, acc: 0.9177, precision: 0.9585, recall: 0.9332, f1: 0.9457, edges-ner-ontonotes_loss: 0.0176
10/01 05:10:35 AM: Update 29178: task edges-ner-ontonotes, batch 178 (29178): mcc: 0.9554, acc: 0.9328, precision: 0.9650, recall: 0.9507, f1: 0.9578, edges-ner-ontonotes_loss: 0.0152
10/01 05:10:45 AM: Update 29328: task edges-ner-ontonotes, batch 328 (29328): mcc: 0.9491, acc: 0.9246, precision: 0.9626, recall: 0.9412, f1: 0.9518, edges-ner-ontonotes_loss: 0.0173
10/01 05:10:55 AM: Update 29489: task edges-ner-ontonotes, batch 489 (29489): mcc: 0.9396, acc: 0.9127, precision: 0.9569, recall: 0.9292, f1: 0.9428, edges-ner-ontonotes_loss: 0.0210
10/01 05:11:05 AM: Update 29649: task edges-ner-ontonotes, batch 649 (29649): mcc: 0.9374, acc: 0.9098, precision: 0.9554, recall: 0.9264, f1: 0.9407, edges-ner-ontonotes_loss: 0.0218
10/01 05:11:15 AM: Update 29841: task edges-ner-ontonotes, batch 841 (29841): mcc: 0.9366, acc: 0.9094, precision: 0.9541, recall: 0.9262, f1: 0.9399, edges-ner-ontonotes_loss: 0.0219
10/01 05:11:25 AM: Update 29988: task edges-ner-ontonotes, batch 988 (29988): mcc: 0.9374, acc: 0.9109, precision: 0.9546, recall: 0.9273, f1: 0.9408, edges-ner-ontonotes_loss: 0.0216
10/01 05:11:26 AM: ***** Step 30000 / Validation 30 *****
10/01 05:11:26 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:11:26 AM: Validating...
10/01 05:11:35 AM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.9368, acc: 0.9144, precision: 0.9543, recall: 0.9264, f1: 0.9401, edges-ner-ontonotes_loss: 0.0223
10/01 05:11:38 AM: Best result seen so far for edges-ner-ontonotes.
10/01 05:11:38 AM: Best result seen so far for macro.
10/01 05:11:38 AM: Updating LR scheduler:
10/01 05:11:38 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:11:38 AM: 	# validation passes without improvement: 0
10/01 05:11:38 AM: edges-ner-ontonotes_loss: training: 0.021476 validation: 0.019388
10/01 05:11:38 AM: macro_avg: validation: 0.948128
10/01 05:11:38 AM: micro_avg: validation: 0.000000
10/01 05:11:38 AM: edges-ner-ontonotes_mcc: training: 0.937633 validation: 0.945191
10/01 05:11:38 AM: edges-ner-ontonotes_acc: training: 0.911124 validation: 0.925311
10/01 05:11:38 AM: edges-ner-ontonotes_precision: training: 0.954780 validation: 0.959615
10/01 05:11:38 AM: edges-ner-ontonotes_recall: training: 0.927503 validation: 0.936912
10/01 05:11:38 AM: edges-ner-ontonotes_f1: training: 0.940944 validation: 0.948128
10/01 05:11:38 AM: Global learning rate: 1.25e-05
10/01 05:11:38 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:11:45 AM: Update 30122: task edges-ner-ontonotes, batch 122 (30122): mcc: 0.9412, acc: 0.9174, precision: 0.9554, recall: 0.9335, f1: 0.9443, edges-ner-ontonotes_loss: 0.0194
10/01 05:11:55 AM: Update 30267: task edges-ner-ontonotes, batch 267 (30267): mcc: 0.9445, acc: 0.9205, precision: 0.9570, recall: 0.9382, f1: 0.9475, edges-ner-ontonotes_loss: 0.0179
10/01 05:12:05 AM: Update 30420: task edges-ner-ontonotes, batch 420 (30420): mcc: 0.9492, acc: 0.9268, precision: 0.9597, recall: 0.9443, f1: 0.9519, edges-ner-ontonotes_loss: 0.0164
10/01 05:12:15 AM: Update 30568: task edges-ner-ontonotes, batch 568 (30568): mcc: 0.9504, acc: 0.9281, precision: 0.9607, recall: 0.9456, f1: 0.9531, edges-ner-ontonotes_loss: 0.0161
10/01 05:12:25 AM: Update 30736: task edges-ner-ontonotes, batch 736 (30736): mcc: 0.9517, acc: 0.9294, precision: 0.9617, recall: 0.9470, f1: 0.9543, edges-ner-ontonotes_loss: 0.0158
10/01 05:12:35 AM: Update 30872: task edges-ner-ontonotes, batch 872 (30872): mcc: 0.9496, acc: 0.9268, precision: 0.9609, recall: 0.9440, f1: 0.9524, edges-ner-ontonotes_loss: 0.0165
10/01 05:12:43 AM: ***** Step 31000 / Validation 31 *****
10/01 05:12:43 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:12:43 AM: Validating...
10/01 05:12:45 AM: Evaluate: task edges-ner-ontonotes, batch 38 (157): mcc: 0.9097, acc: 0.8850, precision: 0.9270, recall: 0.9025, f1: 0.9146, edges-ner-ontonotes_loss: 0.0320
10/01 05:12:55 AM: Evaluate: task edges-ner-ontonotes, batch 152 (157): mcc: 0.9430, acc: 0.9222, precision: 0.9594, recall: 0.9330, f1: 0.9460, edges-ner-ontonotes_loss: 0.0203
10/01 05:12:56 AM: Updating LR scheduler:
10/01 05:12:56 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:12:56 AM: 	# validation passes without improvement: 1
10/01 05:12:56 AM: edges-ner-ontonotes_loss: training: 0.017934 validation: 0.019963
10/01 05:12:56 AM: macro_avg: validation: 0.946883
10/01 05:12:56 AM: micro_avg: validation: 0.000000
10/01 05:12:56 AM: edges-ner-ontonotes_mcc: training: 0.946376 validation: 0.943904
10/01 05:12:56 AM: edges-ner-ontonotes_acc: training: 0.922667 validation: 0.923264
10/01 05:12:56 AM: edges-ner-ontonotes_precision: training: 0.958913 validation: 0.960094
10/01 05:12:56 AM: edges-ner-ontonotes_recall: training: 0.939829 validation: 0.934031
10/01 05:12:56 AM: edges-ner-ontonotes_f1: training: 0.949275 validation: 0.946883
10/01 05:12:56 AM: Global learning rate: 1.25e-05
10/01 05:12:56 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:13:05 AM: Update 31142: task edges-ner-ontonotes, batch 142 (31142): mcc: 0.9223, acc: 0.8927, precision: 0.9438, recall: 0.9096, f1: 0.9264, edges-ner-ontonotes_loss: 0.0272
10/01 05:13:15 AM: Update 31343: task edges-ner-ontonotes, batch 343 (31343): mcc: 0.9287, acc: 0.9007, precision: 0.9486, recall: 0.9169, f1: 0.9324, edges-ner-ontonotes_loss: 0.0242
10/01 05:13:25 AM: Update 31497: task edges-ner-ontonotes, batch 497 (31497): mcc: 0.9322, acc: 0.9051, precision: 0.9505, recall: 0.9215, f1: 0.9358, edges-ner-ontonotes_loss: 0.0230
10/01 05:13:35 AM: Update 31664: task edges-ner-ontonotes, batch 664 (31664): mcc: 0.9350, acc: 0.9090, precision: 0.9524, recall: 0.9249, f1: 0.9385, edges-ner-ontonotes_loss: 0.0218
10/01 05:13:45 AM: Update 31801: task edges-ner-ontonotes, batch 801 (31801): mcc: 0.9379, acc: 0.9124, precision: 0.9543, recall: 0.9284, f1: 0.9412, edges-ner-ontonotes_loss: 0.0211
10/01 05:13:55 AM: Update 31964: task edges-ner-ontonotes, batch 964 (31964): mcc: 0.9415, acc: 0.9168, precision: 0.9565, recall: 0.9331, f1: 0.9447, edges-ner-ontonotes_loss: 0.0198
10/01 05:13:58 AM: ***** Step 32000 / Validation 32 *****
10/01 05:13:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:13:58 AM: Validating...
10/01 05:14:06 AM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.9297, acc: 0.9055, precision: 0.9509, recall: 0.9165, f1: 0.9334, edges-ner-ontonotes_loss: 0.0247
10/01 05:14:11 AM: Best result seen so far for edges-ner-ontonotes.
10/01 05:14:11 AM: Best result seen so far for macro.
10/01 05:14:11 AM: Updating LR scheduler:
10/01 05:14:11 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:14:11 AM: 	# validation passes without improvement: 0
10/01 05:14:11 AM: edges-ner-ontonotes_loss: training: 0.019638 validation: 0.019741
10/01 05:14:11 AM: macro_avg: validation: 0.948277
10/01 05:14:11 AM: micro_avg: validation: 0.000000
10/01 05:14:11 AM: edges-ner-ontonotes_mcc: training: 0.942130 validation: 0.945350
10/01 05:14:11 AM: edges-ner-ontonotes_acc: training: 0.917517 validation: 0.925614
10/01 05:14:11 AM: edges-ner-ontonotes_precision: training: 0.956781 validation: 0.959842
10/01 05:14:11 AM: edges-ner-ontonotes_recall: training: 0.933961 validation: 0.936988
10/01 05:14:11 AM: edges-ner-ontonotes_f1: training: 0.945233 validation: 0.948277
10/01 05:14:11 AM: Global learning rate: 1.25e-05
10/01 05:14:11 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:14:16 AM: Update 32069: task edges-ner-ontonotes, batch 69 (32069): mcc: 0.9569, acc: 0.9374, precision: 0.9612, recall: 0.9574, f1: 0.9593, edges-ner-ontonotes_loss: 0.0143
10/01 05:14:26 AM: Update 32230: task edges-ner-ontonotes, batch 230 (32230): mcc: 0.9559, acc: 0.9350, precision: 0.9646, recall: 0.9522, f1: 0.9584, edges-ner-ontonotes_loss: 0.0144
10/01 05:14:36 AM: Update 32373: task edges-ner-ontonotes, batch 373 (32373): mcc: 0.9553, acc: 0.9333, precision: 0.9645, recall: 0.9511, f1: 0.9577, edges-ner-ontonotes_loss: 0.0146
10/01 05:14:46 AM: Update 32546: task edges-ner-ontonotes, batch 546 (32546): mcc: 0.9441, acc: 0.9189, precision: 0.9579, recall: 0.9365, f1: 0.9471, edges-ner-ontonotes_loss: 0.0194
10/01 05:14:56 AM: Update 32693: task edges-ner-ontonotes, batch 693 (32693): mcc: 0.9397, acc: 0.9134, precision: 0.9553, recall: 0.9310, f1: 0.9430, edges-ner-ontonotes_loss: 0.0209
10/01 05:15:06 AM: Update 32896: task edges-ner-ontonotes, batch 896 (32896): mcc: 0.9391, acc: 0.9126, precision: 0.9548, recall: 0.9303, f1: 0.9424, edges-ner-ontonotes_loss: 0.0209
10/01 05:15:13 AM: ***** Step 33000 / Validation 33 *****
10/01 05:15:13 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:15:13 AM: Validating...
10/01 05:15:16 AM: Evaluate: task edges-ner-ontonotes, batch 46 (157): mcc: 0.9181, acc: 0.8964, precision: 0.9337, recall: 0.9116, f1: 0.9225, edges-ner-ontonotes_loss: 0.0285
10/01 05:15:26 AM: Updating LR scheduler:
10/01 05:15:26 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:15:26 AM: 	# validation passes without improvement: 1
10/01 05:15:26 AM: edges-ner-ontonotes_loss: training: 0.021009 validation: 0.019528
10/01 05:15:26 AM: macro_avg: validation: 0.948007
10/01 05:15:26 AM: micro_avg: validation: 0.000000
10/01 05:15:26 AM: edges-ner-ontonotes_mcc: training: 0.938913 validation: 0.945077
10/01 05:15:26 AM: edges-ner-ontonotes_acc: training: 0.912394 validation: 0.925387
10/01 05:15:26 AM: edges-ner-ontonotes_precision: training: 0.954596 validation: 0.960324
10/01 05:15:26 AM: edges-ner-ontonotes_recall: training: 0.930079 validation: 0.936002
10/01 05:15:26 AM: edges-ner-ontonotes_f1: training: 0.942178 validation: 0.948007
10/01 05:15:26 AM: Global learning rate: 1.25e-05
10/01 05:15:26 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:15:26 AM: Update 33005: task edges-ner-ontonotes, batch 5 (33005): mcc: 0.9441, acc: 0.9256, precision: 0.9569, recall: 0.9376, f1: 0.9472, edges-ner-ontonotes_loss: 0.0191
10/01 05:15:36 AM: Update 33177: task edges-ner-ontonotes, batch 177 (33177): mcc: 0.9409, acc: 0.9177, precision: 0.9550, recall: 0.9334, f1: 0.9441, edges-ner-ontonotes_loss: 0.0189
10/01 05:15:46 AM: Update 33325: task edges-ner-ontonotes, batch 325 (33325): mcc: 0.9426, acc: 0.9199, precision: 0.9558, recall: 0.9359, f1: 0.9457, edges-ner-ontonotes_loss: 0.0185
10/01 05:15:56 AM: Update 33483: task edges-ner-ontonotes, batch 483 (33483): mcc: 0.9471, acc: 0.9252, precision: 0.9590, recall: 0.9412, f1: 0.9500, edges-ner-ontonotes_loss: 0.0172
10/01 05:16:06 AM: Update 33619: task edges-ner-ontonotes, batch 619 (33619): mcc: 0.9496, acc: 0.9278, precision: 0.9604, recall: 0.9444, f1: 0.9523, edges-ner-ontonotes_loss: 0.0164
10/01 05:16:16 AM: Update 33787: task edges-ner-ontonotes, batch 787 (33787): mcc: 0.9511, acc: 0.9294, precision: 0.9616, recall: 0.9461, f1: 0.9537, edges-ner-ontonotes_loss: 0.0160
10/01 05:16:26 AM: Update 33930: task edges-ner-ontonotes, batch 930 (33930): mcc: 0.9512, acc: 0.9293, precision: 0.9618, recall: 0.9460, f1: 0.9538, edges-ner-ontonotes_loss: 0.0160
10/01 05:16:30 AM: ***** Step 34000 / Validation 34 *****
10/01 05:16:30 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:16:30 AM: Validating...
10/01 05:16:36 AM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.9278, acc: 0.9018, precision: 0.9489, recall: 0.9150, f1: 0.9316, edges-ner-ontonotes_loss: 0.0266
10/01 05:16:43 AM: Updating LR scheduler:
10/01 05:16:43 AM: 	Best result seen so far for macro_avg: 0.948
10/01 05:16:43 AM: 	# validation passes without improvement: 2
10/01 05:16:43 AM: edges-ner-ontonotes_loss: training: 0.016836 validation: 0.019696
10/01 05:16:43 AM: macro_avg: validation: 0.947850
10/01 05:16:43 AM: micro_avg: validation: 0.000000
10/01 05:16:43 AM: edges-ner-ontonotes_mcc: training: 0.949009 validation: 0.944925
10/01 05:16:43 AM: edges-ner-ontonotes_acc: training: 0.926290 validation: 0.924856
10/01 05:16:43 AM: edges-ner-ontonotes_precision: training: 0.960724 validation: 0.960960
10/01 05:16:43 AM: edges-ner-ontonotes_recall: training: 0.942985 validation: 0.935093
10/01 05:16:43 AM: edges-ner-ontonotes_f1: training: 0.951772 validation: 0.947850
10/01 05:16:43 AM: Global learning rate: 1.25e-05
10/01 05:16:43 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:16:46 AM: Update 34044: task edges-ner-ontonotes, batch 44 (34044): mcc: 0.9163, acc: 0.8836, precision: 0.9398, recall: 0.9022, f1: 0.9206, edges-ner-ontonotes_loss: 0.0312
10/01 05:16:56 AM: Update 34215: task edges-ner-ontonotes, batch 215 (34215): mcc: 0.9179, acc: 0.8867, precision: 0.9404, recall: 0.9046, f1: 0.9222, edges-ner-ontonotes_loss: 0.0283
10/01 05:17:06 AM: Update 34382: task edges-ner-ontonotes, batch 382 (34382): mcc: 0.9244, acc: 0.8952, precision: 0.9450, recall: 0.9124, f1: 0.9284, edges-ner-ontonotes_loss: 0.0256
10/01 05:17:16 AM: Update 34547: task edges-ner-ontonotes, batch 547 (34547): mcc: 0.9276, acc: 0.8994, precision: 0.9467, recall: 0.9167, f1: 0.9314, edges-ner-ontonotes_loss: 0.0245
10/01 05:17:27 AM: Update 34713: task edges-ner-ontonotes, batch 713 (34713): mcc: 0.9313, acc: 0.9047, precision: 0.9489, recall: 0.9215, f1: 0.9350, edges-ner-ontonotes_loss: 0.0232
10/01 05:17:37 AM: Update 34865: task edges-ner-ontonotes, batch 865 (34865): mcc: 0.9335, acc: 0.9075, precision: 0.9505, recall: 0.9241, f1: 0.9371, edges-ner-ontonotes_loss: 0.0224
10/01 05:17:44 AM: ***** Step 35000 / Validation 35 *****
10/01 05:17:44 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:17:44 AM: Validating...
10/01 05:17:47 AM: Evaluate: task edges-ner-ontonotes, batch 33 (157): mcc: 0.8984, acc: 0.8720, precision: 0.9212, recall: 0.8869, f1: 0.9037, edges-ner-ontonotes_loss: 0.0343
10/01 05:17:57 AM: Evaluate: task edges-ner-ontonotes, batch 143 (157): mcc: 0.9443, acc: 0.9246, precision: 0.9598, recall: 0.9352, f1: 0.9473, edges-ner-ontonotes_loss: 0.0202
10/01 05:17:58 AM: Best result seen so far for edges-ner-ontonotes.
10/01 05:17:58 AM: Best result seen so far for macro.
10/01 05:17:58 AM: Updating LR scheduler:
10/01 05:17:58 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:17:58 AM: 	# validation passes without improvement: 0
10/01 05:17:58 AM: edges-ner-ontonotes_loss: training: 0.021283 validation: 0.019559
10/01 05:17:58 AM: macro_avg: validation: 0.948564
10/01 05:17:58 AM: micro_avg: validation: 0.000000
10/01 05:17:58 AM: edges-ner-ontonotes_mcc: training: 0.936856 validation: 0.945660
10/01 05:17:58 AM: edges-ner-ontonotes_acc: training: 0.911417 validation: 0.925918
10/01 05:17:58 AM: edges-ner-ontonotes_precision: training: 0.952623 validation: 0.960510
10/01 05:17:58 AM: edges-ner-ontonotes_recall: training: 0.928164 validation: 0.936912
10/01 05:17:58 AM: edges-ner-ontonotes_f1: training: 0.940235 validation: 0.948564
10/01 05:17:58 AM: Global learning rate: 1.25e-05
10/01 05:17:58 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:18:07 AM: Update 35147: task edges-ner-ontonotes, batch 147 (35147): mcc: 0.9600, acc: 0.9396, precision: 0.9668, recall: 0.9577, f1: 0.9622, edges-ner-ontonotes_loss: 0.0137
10/01 05:18:17 AM: Update 35284: task edges-ner-ontonotes, batch 284 (35284): mcc: 0.9575, acc: 0.9359, precision: 0.9660, recall: 0.9537, f1: 0.9598, edges-ner-ontonotes_loss: 0.0144
10/01 05:18:27 AM: Update 35446: task edges-ner-ontonotes, batch 446 (35446): mcc: 0.9574, acc: 0.9354, precision: 0.9666, recall: 0.9529, f1: 0.9597, edges-ner-ontonotes_loss: 0.0143
10/01 05:18:37 AM: Update 35602: task edges-ner-ontonotes, batch 602 (35602): mcc: 0.9505, acc: 0.9266, precision: 0.9624, recall: 0.9441, f1: 0.9531, edges-ner-ontonotes_loss: 0.0172
10/01 05:18:47 AM: Update 35765: task edges-ner-ontonotes, batch 765 (35765): mcc: 0.9446, acc: 0.9194, precision: 0.9587, recall: 0.9366, f1: 0.9475, edges-ner-ontonotes_loss: 0.0193
10/01 05:18:57 AM: Update 35936: task edges-ner-ontonotes, batch 936 (35936): mcc: 0.9425, acc: 0.9171, precision: 0.9572, recall: 0.9343, f1: 0.9456, edges-ner-ontonotes_loss: 0.0201
10/01 05:19:00 AM: ***** Step 36000 / Validation 36 *****
10/01 05:19:00 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:19:00 AM: Validating...
10/01 05:19:07 AM: Evaluate: task edges-ner-ontonotes, batch 90 (157): mcc: 0.9364, acc: 0.9129, precision: 0.9548, recall: 0.9252, f1: 0.9398, edges-ner-ontonotes_loss: 0.0235
10/01 05:19:13 AM: Updating LR scheduler:
10/01 05:19:13 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:19:13 AM: 	# validation passes without improvement: 1
10/01 05:19:13 AM: edges-ner-ontonotes_loss: training: 0.020126 validation: 0.019532
10/01 05:19:13 AM: macro_avg: validation: 0.948189
10/01 05:19:13 AM: micro_avg: validation: 0.000000
10/01 05:19:13 AM: edges-ner-ontonotes_mcc: training: 0.942281 validation: 0.945273
10/01 05:19:13 AM: edges-ner-ontonotes_acc: training: 0.916818 validation: 0.925690
10/01 05:19:13 AM: edges-ner-ontonotes_precision: training: 0.957002 validation: 0.960697
10/01 05:19:13 AM: edges-ner-ontonotes_recall: training: 0.934028 validation: 0.936002
10/01 05:19:13 AM: edges-ner-ontonotes_f1: training: 0.945375 validation: 0.948189
10/01 05:19:13 AM: Global learning rate: 1.25e-05
10/01 05:19:13 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:19:17 AM: Update 36080: task edges-ner-ontonotes, batch 80 (36080): mcc: 0.9383, acc: 0.9096, precision: 0.9555, recall: 0.9281, f1: 0.9416, edges-ner-ontonotes_loss: 0.0211
10/01 05:19:27 AM: Update 36234: task edges-ner-ontonotes, batch 234 (36234): mcc: 0.9421, acc: 0.9168, precision: 0.9568, recall: 0.9337, f1: 0.9452, edges-ner-ontonotes_loss: 0.0195
10/01 05:19:37 AM: Update 36407: task edges-ner-ontonotes, batch 407 (36407): mcc: 0.9415, acc: 0.9174, precision: 0.9554, recall: 0.9341, f1: 0.9447, edges-ner-ontonotes_loss: 0.0193
10/01 05:19:47 AM: Update 36542: task edges-ner-ontonotes, batch 542 (36542): mcc: 0.9460, acc: 0.9227, precision: 0.9583, recall: 0.9397, f1: 0.9489, edges-ner-ontonotes_loss: 0.0179
10/01 05:19:57 AM: Update 36701: task edges-ner-ontonotes, batch 701 (36701): mcc: 0.9487, acc: 0.9260, precision: 0.9603, recall: 0.9428, f1: 0.9515, edges-ner-ontonotes_loss: 0.0170
10/01 05:20:07 AM: Update 36848: task edges-ner-ontonotes, batch 848 (36848): mcc: 0.9497, acc: 0.9271, precision: 0.9610, recall: 0.9441, f1: 0.9524, edges-ner-ontonotes_loss: 0.0166
10/01 05:20:17 AM: ***** Step 37000 / Validation 37 *****
10/01 05:20:17 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:20:17 AM: Validating...
10/01 05:20:17 AM: Evaluate: task edges-ner-ontonotes, batch 5 (157): mcc: 0.8220, acc: 0.7761, precision: 0.8549, recall: 0.8090, f1: 0.8313, edges-ner-ontonotes_loss: 0.0527
10/01 05:20:27 AM: Evaluate: task edges-ner-ontonotes, batch 120 (157): mcc: 0.9372, acc: 0.9151, precision: 0.9559, recall: 0.9257, f1: 0.9405, edges-ner-ontonotes_loss: 0.0228
10/01 05:20:30 AM: Updating LR scheduler:
10/01 05:20:30 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:20:30 AM: 	# validation passes without improvement: 2
10/01 05:20:30 AM: edges-ner-ontonotes_loss: training: 0.016408 validation: 0.020049
10/01 05:20:30 AM: macro_avg: validation: 0.947599
10/01 05:20:30 AM: micro_avg: validation: 0.000000
10/01 05:20:30 AM: edges-ner-ontonotes_mcc: training: 0.950267 validation: 0.944653
10/01 05:20:30 AM: edges-ner-ontonotes_acc: training: 0.927551 validation: 0.924553
10/01 05:20:30 AM: edges-ner-ontonotes_precision: training: 0.961507 validation: 0.960364
10/01 05:20:30 AM: edges-ner-ontonotes_recall: training: 0.944574 validation: 0.935168
10/01 05:20:30 AM: edges-ner-ontonotes_f1: training: 0.952966 validation: 0.947599
10/01 05:20:30 AM: Global learning rate: 1.25e-05
10/01 05:20:30 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:20:37 AM: Update 37101: task edges-ner-ontonotes, batch 101 (37101): mcc: 0.9307, acc: 0.9027, precision: 0.9487, recall: 0.9205, f1: 0.9344, edges-ner-ontonotes_loss: 0.0239
10/01 05:20:47 AM: Update 37266: task edges-ner-ontonotes, batch 266 (37266): mcc: 0.9232, acc: 0.8928, precision: 0.9450, recall: 0.9102, f1: 0.9273, edges-ner-ontonotes_loss: 0.0263
10/01 05:20:57 AM: Update 37437: task edges-ner-ontonotes, batch 437 (37437): mcc: 0.9268, acc: 0.8979, precision: 0.9471, recall: 0.9148, f1: 0.9307, edges-ner-ontonotes_loss: 0.0252
10/01 05:21:07 AM: Update 37638: task edges-ner-ontonotes, batch 638 (37638): mcc: 0.9288, acc: 0.9006, precision: 0.9478, recall: 0.9178, f1: 0.9326, edges-ner-ontonotes_loss: 0.0243
10/01 05:21:17 AM: Update 37798: task edges-ner-ontonotes, batch 798 (37798): mcc: 0.9310, acc: 0.9036, precision: 0.9490, recall: 0.9208, f1: 0.9347, edges-ner-ontonotes_loss: 0.0233
10/01 05:21:28 AM: Update 37971: task edges-ner-ontonotes, batch 971 (37971): mcc: 0.9338, acc: 0.9071, precision: 0.9509, recall: 0.9241, f1: 0.9373, edges-ner-ontonotes_loss: 0.0224
10/01 05:21:30 AM: ***** Step 38000 / Validation 38 *****
10/01 05:21:30 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:21:30 AM: Validating...
10/01 05:21:38 AM: Evaluate: task edges-ner-ontonotes, batch 106 (157): mcc: 0.9349, acc: 0.9117, precision: 0.9545, recall: 0.9226, f1: 0.9383, edges-ner-ontonotes_loss: 0.0231
10/01 05:21:43 AM: Best result seen so far for edges-ner-ontonotes.
10/01 05:21:43 AM: Best result seen so far for macro.
10/01 05:21:43 AM: Updating LR scheduler:
10/01 05:21:43 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:21:43 AM: 	# validation passes without improvement: 0
10/01 05:21:43 AM: edges-ner-ontonotes_loss: training: 0.022144 validation: 0.019307
10/01 05:21:43 AM: macro_avg: validation: 0.948883
10/01 05:21:43 AM: micro_avg: validation: 0.000000
10/01 05:21:43 AM: edges-ner-ontonotes_mcc: training: 0.934475 validation: 0.945994
10/01 05:21:43 AM: edges-ner-ontonotes_acc: training: 0.907938 validation: 0.926221
10/01 05:21:43 AM: edges-ner-ontonotes_precision: training: 0.951353 validation: 0.960606
10/01 05:21:43 AM: edges-ner-ontonotes_recall: training: 0.924950 validation: 0.937443
10/01 05:21:43 AM: edges-ner-ontonotes_f1: training: 0.937966 validation: 0.948883
10/01 05:21:43 AM: Global learning rate: 1.25e-05
10/01 05:21:43 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:21:48 AM: Update 38092: task edges-ner-ontonotes, batch 92 (38092): mcc: 0.9557, acc: 0.9342, precision: 0.9640, recall: 0.9523, f1: 0.9581, edges-ner-ontonotes_loss: 0.0151
10/01 05:21:58 AM: Update 38257: task edges-ner-ontonotes, batch 257 (38257): mcc: 0.9579, acc: 0.9376, precision: 0.9661, recall: 0.9544, f1: 0.9602, edges-ner-ontonotes_loss: 0.0141
10/01 05:22:08 AM: Update 38401: task edges-ner-ontonotes, batch 401 (38401): mcc: 0.9571, acc: 0.9366, precision: 0.9653, recall: 0.9536, f1: 0.9594, edges-ner-ontonotes_loss: 0.0144
10/01 05:22:18 AM: Update 38557: task edges-ner-ontonotes, batch 557 (38557): mcc: 0.9563, acc: 0.9351, precision: 0.9654, recall: 0.9520, f1: 0.9587, edges-ner-ontonotes_loss: 0.0146
10/01 05:22:28 AM: Update 38700: task edges-ner-ontonotes, batch 700 (38700): mcc: 0.9504, acc: 0.9277, precision: 0.9618, recall: 0.9444, f1: 0.9531, edges-ner-ontonotes_loss: 0.0169
10/01 05:22:38 AM: Update 38870: task edges-ner-ontonotes, batch 870 (38870): mcc: 0.9456, acc: 0.9216, precision: 0.9590, recall: 0.9384, f1: 0.9486, edges-ner-ontonotes_loss: 0.0188
10/01 05:22:47 AM: ***** Step 39000 / Validation 39 *****
10/01 05:22:47 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:22:47 AM: Validating...
10/01 05:22:49 AM: Evaluate: task edges-ner-ontonotes, batch 28 (157): mcc: 0.8881, acc: 0.8603, precision: 0.9089, recall: 0.8798, f1: 0.8941, edges-ner-ontonotes_loss: 0.0374
10/01 05:22:59 AM: Evaluate: task edges-ner-ontonotes, batch 145 (157): mcc: 0.9436, acc: 0.9231, precision: 0.9596, recall: 0.9339, f1: 0.9466, edges-ner-ontonotes_loss: 0.0202
10/01 05:23:00 AM: Updating LR scheduler:
10/01 05:23:00 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:23:00 AM: 	# validation passes without improvement: 1
10/01 05:23:00 AM: edges-ner-ontonotes_loss: training: 0.019622 validation: 0.019647
10/01 05:23:00 AM: macro_avg: validation: 0.947914
10/01 05:23:00 AM: micro_avg: validation: 0.000000
10/01 05:23:00 AM: edges-ner-ontonotes_mcc: training: 0.943293 validation: 0.944984
10/01 05:23:00 AM: edges-ner-ontonotes_acc: training: 0.918529 validation: 0.924780
10/01 05:23:00 AM: edges-ner-ontonotes_precision: training: 0.957325 validation: 0.960532
10/01 05:23:00 AM: edges-ner-ontonotes_recall: training: 0.935607 validation: 0.935623
10/01 05:23:00 AM: edges-ner-ontonotes_f1: training: 0.946341 validation: 0.947914
10/01 05:23:00 AM: Global learning rate: 1.25e-05
10/01 05:23:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:23:09 AM: Update 39175: task edges-ner-ontonotes, batch 175 (39175): mcc: 0.9372, acc: 0.9122, precision: 0.9531, recall: 0.9283, f1: 0.9405, edges-ner-ontonotes_loss: 0.0210
10/01 05:23:19 AM: Update 39329: task edges-ner-ontonotes, batch 329 (39329): mcc: 0.9400, acc: 0.9151, precision: 0.9553, recall: 0.9314, f1: 0.9432, edges-ner-ontonotes_loss: 0.0199
10/01 05:23:29 AM: Update 39501: task edges-ner-ontonotes, batch 501 (39501): mcc: 0.9413, acc: 0.9173, precision: 0.9556, recall: 0.9335, f1: 0.9444, edges-ner-ontonotes_loss: 0.0194
10/01 05:23:39 AM: Update 39651: task edges-ner-ontonotes, batch 651 (39651): mcc: 0.9445, acc: 0.9213, precision: 0.9576, recall: 0.9376, f1: 0.9475, edges-ner-ontonotes_loss: 0.0184
10/01 05:23:49 AM: Update 39807: task edges-ner-ontonotes, batch 807 (39807): mcc: 0.9472, acc: 0.9247, precision: 0.9593, recall: 0.9411, f1: 0.9501, edges-ner-ontonotes_loss: 0.0175
10/01 05:23:59 AM: Update 39964: task edges-ner-ontonotes, batch 964 (39964): mcc: 0.9484, acc: 0.9259, precision: 0.9602, recall: 0.9425, f1: 0.9512, edges-ner-ontonotes_loss: 0.0171
10/01 05:24:01 AM: ***** Step 40000 / Validation 40 *****
10/01 05:24:01 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:24:01 AM: Validating...
10/01 05:24:09 AM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.9288, acc: 0.9038, precision: 0.9509, recall: 0.9148, f1: 0.9325, edges-ner-ontonotes_loss: 0.0252
10/01 05:24:14 AM: Updating LR scheduler:
10/01 05:24:14 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:24:14 AM: 	# validation passes without improvement: 2
10/01 05:24:14 AM: edges-ner-ontonotes_loss: training: 0.017045 validation: 0.019989
10/01 05:24:14 AM: macro_avg: validation: 0.947457
10/01 05:24:14 AM: micro_avg: validation: 0.000000
10/01 05:24:14 AM: edges-ner-ontonotes_mcc: training: 0.948679 validation: 0.944499
10/01 05:24:14 AM: edges-ner-ontonotes_acc: training: 0.926110 validation: 0.924249
10/01 05:24:14 AM: edges-ner-ontonotes_precision: training: 0.960316 validation: 0.959994
10/01 05:24:14 AM: edges-ner-ontonotes_recall: training: 0.942769 validation: 0.935244
10/01 05:24:14 AM: edges-ner-ontonotes_f1: training: 0.951461 validation: 0.947457
10/01 05:24:14 AM: Global learning rate: 1.25e-05
10/01 05:24:14 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:24:19 AM: Update 40082: task edges-ner-ontonotes, batch 82 (40082): mcc: 0.9569, acc: 0.9344, precision: 0.9672, recall: 0.9514, f1: 0.9592, edges-ner-ontonotes_loss: 0.0143
10/01 05:24:29 AM: Update 40226: task edges-ner-ontonotes, batch 226 (40226): mcc: 0.9432, acc: 0.9182, precision: 0.9577, recall: 0.9351, f1: 0.9463, edges-ner-ontonotes_loss: 0.0195
10/01 05:24:39 AM: Update 40394: task edges-ner-ontonotes, batch 394 (40394): mcc: 0.9333, acc: 0.9054, precision: 0.9511, recall: 0.9230, f1: 0.9368, edges-ner-ontonotes_loss: 0.0230
10/01 05:24:49 AM: Update 40550: task edges-ner-ontonotes, batch 550 (40550): mcc: 0.9322, acc: 0.9043, precision: 0.9503, recall: 0.9217, f1: 0.9358, edges-ner-ontonotes_loss: 0.0234
10/01 05:24:59 AM: Update 40739: task edges-ner-ontonotes, batch 739 (40739): mcc: 0.9331, acc: 0.9056, precision: 0.9505, recall: 0.9231, f1: 0.9366, edges-ner-ontonotes_loss: 0.0230
10/01 05:25:09 AM: Update 40894: task edges-ner-ontonotes, batch 894 (40894): mcc: 0.9344, acc: 0.9077, precision: 0.9510, recall: 0.9252, f1: 0.9379, edges-ner-ontonotes_loss: 0.0223
10/01 05:25:15 AM: ***** Step 41000 / Validation 41 *****
10/01 05:25:15 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:25:15 AM: Validating...
10/01 05:25:19 AM: Evaluate: task edges-ner-ontonotes, batch 50 (157): mcc: 0.9191, acc: 0.8962, precision: 0.9373, recall: 0.9101, f1: 0.9235, edges-ner-ontonotes_loss: 0.0274
10/01 05:25:28 AM: Updating LR scheduler:
10/01 05:25:28 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:25:28 AM: 	# validation passes without improvement: 3
10/01 05:25:28 AM: edges-ner-ontonotes_loss: training: 0.021934 validation: 0.019259
10/01 05:25:28 AM: macro_avg: validation: 0.948598
10/01 05:25:28 AM: micro_avg: validation: 0.000000
10/01 05:25:28 AM: edges-ner-ontonotes_mcc: training: 0.935444 validation: 0.945710
10/01 05:25:28 AM: edges-ner-ontonotes_acc: training: 0.909202 validation: 0.925311
10/01 05:25:28 AM: edges-ner-ontonotes_precision: training: 0.951517 validation: 0.961377
10/01 05:25:28 AM: edges-ner-ontonotes_recall: training: 0.926606 validation: 0.936154
10/01 05:25:28 AM: edges-ner-ontonotes_f1: training: 0.938896 validation: 0.948598
10/01 05:25:28 AM: Global learning rate: 1.25e-05
10/01 05:25:28 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:25:29 AM: Update 41011: task edges-ner-ontonotes, batch 11 (41011): mcc: 0.9463, acc: 0.9260, precision: 0.9572, recall: 0.9414, f1: 0.9492, edges-ner-ontonotes_loss: 0.0170
10/01 05:25:39 AM: Update 41163: task edges-ner-ontonotes, batch 163 (41163): mcc: 0.9480, acc: 0.9245, precision: 0.9607, recall: 0.9411, f1: 0.9508, edges-ner-ontonotes_loss: 0.0171
10/01 05:25:49 AM: Update 41332: task edges-ner-ontonotes, batch 332 (41332): mcc: 0.9524, acc: 0.9302, precision: 0.9627, recall: 0.9474, f1: 0.9550, edges-ner-ontonotes_loss: 0.0154
10/01 05:25:59 AM: Update 41468: task edges-ner-ontonotes, batch 468 (41468): mcc: 0.9538, acc: 0.9319, precision: 0.9635, recall: 0.9493, f1: 0.9563, edges-ner-ontonotes_loss: 0.0151
10/01 05:26:09 AM: Update 41638: task edges-ner-ontonotes, batch 638 (41638): mcc: 0.9540, acc: 0.9317, precision: 0.9639, recall: 0.9492, f1: 0.9565, edges-ner-ontonotes_loss: 0.0151
10/01 05:26:19 AM: Update 41788: task edges-ner-ontonotes, batch 788 (41788): mcc: 0.9502, acc: 0.9269, precision: 0.9616, recall: 0.9443, f1: 0.9529, edges-ner-ontonotes_loss: 0.0163
10/01 05:26:29 AM: Update 41959: task edges-ner-ontonotes, batch 959 (41959): mcc: 0.9456, acc: 0.9208, precision: 0.9592, recall: 0.9380, f1: 0.9485, edges-ner-ontonotes_loss: 0.0183
10/01 05:26:32 AM: ***** Step 42000 / Validation 42 *****
10/01 05:26:32 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:26:32 AM: Validating...
10/01 05:26:39 AM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.9322, acc: 0.9073, precision: 0.9528, recall: 0.9193, f1: 0.9358, edges-ner-ontonotes_loss: 0.0245
10/01 05:26:44 AM: Updating LR scheduler:
10/01 05:26:44 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:26:44 AM: 	# validation passes without improvement: 0
10/01 05:26:44 AM: edges-ner-ontonotes_loss: training: 0.018764 validation: 0.019913
10/01 05:26:44 AM: macro_avg: validation: 0.947105
10/01 05:26:44 AM: micro_avg: validation: 0.000000
10/01 05:26:44 AM: edges-ner-ontonotes_mcc: training: 0.944726 validation: 0.944142
10/01 05:26:44 AM: edges-ner-ontonotes_acc: training: 0.919792 validation: 0.923339
10/01 05:26:44 AM: edges-ner-ontonotes_precision: training: 0.958754 validation: 0.960471
10/01 05:26:44 AM: edges-ner-ontonotes_recall: training: 0.936888 validation: 0.934107
10/01 05:26:44 AM: edges-ner-ontonotes_f1: training: 0.947695 validation: 0.947105
10/01 05:26:44 AM: Global learning rate: 6.25e-06
10/01 05:26:44 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:26:49 AM: Update 42069: task edges-ner-ontonotes, batch 69 (42069): mcc: 0.9304, acc: 0.9052, precision: 0.9504, recall: 0.9182, f1: 0.9340, edges-ner-ontonotes_loss: 0.0247
10/01 05:26:59 AM: Update 42272: task edges-ner-ontonotes, batch 272 (42272): mcc: 0.9324, acc: 0.9071, precision: 0.9494, recall: 0.9229, f1: 0.9360, edges-ner-ontonotes_loss: 0.0222
10/01 05:27:09 AM: Update 42437: task edges-ner-ontonotes, batch 437 (42437): mcc: 0.9355, acc: 0.9108, precision: 0.9515, recall: 0.9267, f1: 0.9389, edges-ner-ontonotes_loss: 0.0211
10/01 05:27:19 AM: Update 42608: task edges-ner-ontonotes, batch 608 (42608): mcc: 0.9371, acc: 0.9129, precision: 0.9526, recall: 0.9287, f1: 0.9405, edges-ner-ontonotes_loss: 0.0206
10/01 05:27:29 AM: Update 42763: task edges-ner-ontonotes, batch 763 (42763): mcc: 0.9402, acc: 0.9167, precision: 0.9546, recall: 0.9325, f1: 0.9434, edges-ner-ontonotes_loss: 0.0195
10/01 05:27:39 AM: Update 42935: task edges-ner-ontonotes, batch 935 (42935): mcc: 0.9438, acc: 0.9206, precision: 0.9571, recall: 0.9368, f1: 0.9468, edges-ner-ontonotes_loss: 0.0184
10/01 05:27:45 AM: ***** Step 43000 / Validation 43 *****
10/01 05:27:45 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:27:45 AM: Validating...
10/01 05:27:49 AM: Evaluate: task edges-ner-ontonotes, batch 64 (157): mcc: 0.9275, acc: 0.9054, precision: 0.9428, recall: 0.9202, f1: 0.9314, edges-ner-ontonotes_loss: 0.0257
10/01 05:27:58 AM: Updating LR scheduler:
10/01 05:27:58 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:27:58 AM: 	# validation passes without improvement: 1
10/01 05:27:58 AM: edges-ner-ontonotes_loss: training: 0.018207 validation: 0.019638
10/01 05:27:58 AM: macro_avg: validation: 0.948011
10/01 05:27:58 AM: micro_avg: validation: 0.000000
10/01 05:27:58 AM: edges-ner-ontonotes_mcc: training: 0.944604 validation: 0.945068
10/01 05:27:58 AM: edges-ner-ontonotes_acc: training: 0.921547 validation: 0.925766
10/01 05:27:58 AM: edges-ner-ontonotes_precision: training: 0.957494 validation: 0.959534
10/01 05:27:58 AM: edges-ner-ontonotes_recall: training: 0.937901 validation: 0.936761
10/01 05:27:58 AM: edges-ner-ontonotes_f1: training: 0.947596 validation: 0.948011
10/01 05:27:58 AM: Global learning rate: 6.25e-06
10/01 05:27:58 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:27:59 AM: Update 43035: task edges-ner-ontonotes, batch 35 (43035): mcc: 0.9563, acc: 0.9350, precision: 0.9648, recall: 0.9527, f1: 0.9587, edges-ner-ontonotes_loss: 0.0143
10/01 05:28:10 AM: Update 43215: task edges-ner-ontonotes, batch 215 (43215): mcc: 0.9549, acc: 0.9329, precision: 0.9652, recall: 0.9496, f1: 0.9574, edges-ner-ontonotes_loss: 0.0152
10/01 05:28:20 AM: Update 43376: task edges-ner-ontonotes, batch 376 (43376): mcc: 0.9446, acc: 0.9193, precision: 0.9584, recall: 0.9371, f1: 0.9476, edges-ner-ontonotes_loss: 0.0186
10/01 05:28:30 AM: Update 43548: task edges-ner-ontonotes, batch 548 (43548): mcc: 0.9377, acc: 0.9104, precision: 0.9547, recall: 0.9276, f1: 0.9410, edges-ner-ontonotes_loss: 0.0215
10/01 05:28:40 AM: Update 43711: task edges-ner-ontonotes, batch 711 (43711): mcc: 0.9366, acc: 0.9094, precision: 0.9537, recall: 0.9267, f1: 0.9400, edges-ner-ontonotes_loss: 0.0217
10/01 05:28:50 AM: Update 43890: task edges-ner-ontonotes, batch 890 (43890): mcc: 0.9368, acc: 0.9100, precision: 0.9538, recall: 0.9269, f1: 0.9402, edges-ner-ontonotes_loss: 0.0216
10/01 05:28:56 AM: ***** Step 44000 / Validation 44 *****
10/01 05:28:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:28:56 AM: Validating...
10/01 05:29:00 AM: Evaluate: task edges-ner-ontonotes, batch 48 (157): mcc: 0.9181, acc: 0.8959, precision: 0.9352, recall: 0.9100, f1: 0.9225, edges-ner-ontonotes_loss: 0.0283
10/01 05:29:09 AM: Updating LR scheduler:
10/01 05:29:09 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:29:09 AM: 	# validation passes without improvement: 2
10/01 05:29:09 AM: edges-ner-ontonotes_loss: training: 0.021431 validation: 0.019411
10/01 05:29:09 AM: macro_avg: validation: 0.948302
10/01 05:29:09 AM: micro_avg: validation: 0.000000
10/01 05:29:09 AM: edges-ner-ontonotes_mcc: training: 0.936929 validation: 0.945394
10/01 05:29:09 AM: edges-ner-ontonotes_acc: training: 0.910307 validation: 0.925614
10/01 05:29:09 AM: edges-ner-ontonotes_precision: training: 0.953610 validation: 0.960850
10/01 05:29:09 AM: edges-ner-ontonotes_recall: training: 0.927331 validation: 0.936078
10/01 05:29:09 AM: edges-ner-ontonotes_f1: training: 0.940287 validation: 0.948302
10/01 05:29:09 AM: Global learning rate: 6.25e-06
10/01 05:29:09 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:29:10 AM: Update 44009: task edges-ner-ontonotes, batch 9 (44009): mcc: 0.9590, acc: 0.9413, precision: 0.9701, recall: 0.9525, f1: 0.9612, edges-ner-ontonotes_loss: 0.0134
10/01 05:29:20 AM: Update 44183: task edges-ner-ontonotes, batch 183 (44183): mcc: 0.9433, acc: 0.9196, precision: 0.9573, recall: 0.9356, f1: 0.9463, edges-ner-ontonotes_loss: 0.0178
10/01 05:29:30 AM: Update 44324: task edges-ner-ontonotes, batch 324 (44324): mcc: 0.9477, acc: 0.9251, precision: 0.9600, recall: 0.9412, f1: 0.9505, edges-ner-ontonotes_loss: 0.0167
10/01 05:29:40 AM: Update 44479: task edges-ner-ontonotes, batch 479 (44479): mcc: 0.9517, acc: 0.9296, precision: 0.9625, recall: 0.9463, f1: 0.9543, edges-ner-ontonotes_loss: 0.0156
10/01 05:29:50 AM: Update 44630: task edges-ner-ontonotes, batch 630 (44630): mcc: 0.9522, acc: 0.9302, precision: 0.9626, recall: 0.9472, f1: 0.9548, edges-ner-ontonotes_loss: 0.0155
10/01 05:30:00 AM: Update 44787: task edges-ner-ontonotes, batch 787 (44787): mcc: 0.9528, acc: 0.9305, precision: 0.9633, recall: 0.9475, f1: 0.9553, edges-ner-ontonotes_loss: 0.0153
10/01 05:30:10 AM: Update 44927: task edges-ner-ontonotes, batch 927 (44927): mcc: 0.9491, acc: 0.9258, precision: 0.9611, recall: 0.9427, f1: 0.9518, edges-ner-ontonotes_loss: 0.0166
10/01 05:30:14 AM: ***** Step 45000 / Validation 45 *****
10/01 05:30:14 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:30:14 AM: Validating...
10/01 05:30:20 AM: Evaluate: task edges-ner-ontonotes, batch 76 (157): mcc: 0.9291, acc: 0.9031, precision: 0.9493, recall: 0.9169, f1: 0.9328, edges-ner-ontonotes_loss: 0.0259
10/01 05:30:27 AM: Updating LR scheduler:
10/01 05:30:27 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:30:27 AM: 	# validation passes without improvement: 3
10/01 05:30:27 AM: edges-ner-ontonotes_loss: training: 0.017501 validation: 0.019576
10/01 05:30:27 AM: macro_avg: validation: 0.948015
10/01 05:30:27 AM: micro_avg: validation: 0.000000
10/01 05:30:27 AM: edges-ner-ontonotes_mcc: training: 0.946875 validation: 0.945096
10/01 05:30:27 AM: edges-ner-ontonotes_acc: training: 0.923068 validation: 0.924780
10/01 05:30:27 AM: edges-ner-ontonotes_precision: training: 0.959805 validation: 0.960900
10/01 05:30:27 AM: edges-ner-ontonotes_recall: training: 0.939884 validation: 0.935472
10/01 05:30:27 AM: edges-ner-ontonotes_f1: training: 0.949740 validation: 0.948015
10/01 05:30:27 AM: Global learning rate: 6.25e-06
10/01 05:30:27 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:30:30 AM: Update 45053: task edges-ner-ontonotes, batch 53 (45053): mcc: 0.9254, acc: 0.8947, precision: 0.9493, recall: 0.9100, f1: 0.9293, edges-ner-ontonotes_loss: 0.0274
10/01 05:30:40 AM: Update 45220: task edges-ner-ontonotes, batch 220 (45220): mcc: 0.9283, acc: 0.9000, precision: 0.9485, recall: 0.9162, f1: 0.9321, edges-ner-ontonotes_loss: 0.0253
10/01 05:30:50 AM: Update 45411: task edges-ner-ontonotes, batch 411 (45411): mcc: 0.9308, acc: 0.9039, precision: 0.9488, recall: 0.9206, f1: 0.9345, edges-ner-ontonotes_loss: 0.0237
10/01 05:31:00 AM: Update 45559: task edges-ner-ontonotes, batch 559 (45559): mcc: 0.9335, acc: 0.9074, precision: 0.9504, recall: 0.9240, f1: 0.9370, edges-ner-ontonotes_loss: 0.0227
10/01 05:31:10 AM: Update 45736: task edges-ner-ontonotes, batch 736 (45736): mcc: 0.9362, acc: 0.9110, precision: 0.9526, recall: 0.9270, f1: 0.9396, edges-ner-ontonotes_loss: 0.0216
10/01 05:31:20 AM: Update 45880: task edges-ner-ontonotes, batch 880 (45880): mcc: 0.9400, acc: 0.9156, precision: 0.9549, recall: 0.9318, f1: 0.9432, edges-ner-ontonotes_loss: 0.0204
10/01 05:31:27 AM: ***** Step 46000 / Validation 46 *****
10/01 05:31:27 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:31:27 AM: Validating...
10/01 05:31:30 AM: Evaluate: task edges-ner-ontonotes, batch 40 (157): mcc: 0.9081, acc: 0.8854, precision: 0.9270, recall: 0.8994, f1: 0.9130, edges-ner-ontonotes_loss: 0.0316
10/01 05:31:40 AM: Evaluate: task edges-ner-ontonotes, batch 154 (157): mcc: 0.9451, acc: 0.9258, precision: 0.9598, recall: 0.9364, f1: 0.9480, edges-ner-ontonotes_loss: 0.0198
10/01 05:31:40 AM: Updating LR scheduler:
10/01 05:31:40 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:31:40 AM: 	# validation passes without improvement: 0
10/01 05:31:40 AM: edges-ner-ontonotes_loss: training: 0.019690 validation: 0.019584
10/01 05:31:40 AM: macro_avg: validation: 0.948613
10/01 05:31:40 AM: micro_avg: validation: 0.000000
10/01 05:31:40 AM: edges-ner-ontonotes_mcc: training: 0.942336 validation: 0.945708
10/01 05:31:40 AM: edges-ner-ontonotes_acc: training: 0.918358 validation: 0.926676
10/01 05:31:40 AM: edges-ner-ontonotes_precision: training: 0.956334 validation: 0.960370
10/01 05:31:40 AM: edges-ner-ontonotes_recall: training: 0.934787 validation: 0.937140
10/01 05:31:40 AM: edges-ner-ontonotes_f1: training: 0.945438 validation: 0.948613
10/01 05:31:40 AM: Global learning rate: 3.125e-06
10/01 05:31:40 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:31:50 AM: Update 46152: task edges-ner-ontonotes, batch 152 (46152): mcc: 0.9560, acc: 0.9340, precision: 0.9660, recall: 0.9509, f1: 0.9584, edges-ner-ontonotes_loss: 0.0143
10/01 05:32:00 AM: Update 46321: task edges-ner-ontonotes, batch 321 (46321): mcc: 0.9553, acc: 0.9333, precision: 0.9649, recall: 0.9507, f1: 0.9578, edges-ner-ontonotes_loss: 0.0146
10/01 05:32:10 AM: Update 46466: task edges-ner-ontonotes, batch 466 (46466): mcc: 0.9476, acc: 0.9231, precision: 0.9599, recall: 0.9411, f1: 0.9504, edges-ner-ontonotes_loss: 0.0174
10/01 05:32:20 AM: Update 46643: task edges-ner-ontonotes, batch 643 (46643): mcc: 0.9405, acc: 0.9142, precision: 0.9554, recall: 0.9323, f1: 0.9437, edges-ner-ontonotes_loss: 0.0202
10/01 05:32:30 AM: Update 46815: task edges-ner-ontonotes, batch 815 (46815): mcc: 0.9386, acc: 0.9121, precision: 0.9541, recall: 0.9300, f1: 0.9419, edges-ner-ontonotes_loss: 0.0208
10/01 05:32:40 AM: Update 46995: task edges-ner-ontonotes, batch 995 (46995): mcc: 0.9384, acc: 0.9123, precision: 0.9541, recall: 0.9296, f1: 0.9417, edges-ner-ontonotes_loss: 0.0209
10/01 05:32:41 AM: ***** Step 47000 / Validation 47 *****
10/01 05:32:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:32:41 AM: Validating...
10/01 05:32:50 AM: Evaluate: task edges-ner-ontonotes, batch 118 (157): mcc: 0.9379, acc: 0.9158, precision: 0.9562, recall: 0.9266, f1: 0.9412, edges-ner-ontonotes_loss: 0.0222
10/01 05:32:54 AM: Updating LR scheduler:
10/01 05:32:54 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:32:54 AM: 	# validation passes without improvement: 1
10/01 05:32:54 AM: edges-ner-ontonotes_loss: training: 0.020885 validation: 0.019483
10/01 05:32:54 AM: macro_avg: validation: 0.948145
10/01 05:32:54 AM: micro_avg: validation: 0.000000
10/01 05:32:54 AM: edges-ner-ontonotes_mcc: training: 0.938424 validation: 0.945228
10/01 05:32:54 AM: edges-ner-ontonotes_acc: training: 0.912268 validation: 0.925311
10/01 05:32:54 AM: edges-ner-ontonotes_precision: training: 0.954106 validation: 0.960766
10/01 05:32:54 AM: edges-ner-ontonotes_recall: training: 0.929644 validation: 0.935851
10/01 05:32:54 AM: edges-ner-ontonotes_f1: training: 0.941717 validation: 0.948145
10/01 05:32:54 AM: Global learning rate: 3.125e-06
10/01 05:32:54 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:33:00 AM: Update 47120: task edges-ner-ontonotes, batch 120 (47120): mcc: 0.9441, acc: 0.9224, precision: 0.9568, recall: 0.9376, f1: 0.9471, edges-ner-ontonotes_loss: 0.0177
10/01 05:33:11 AM: Update 47285: task edges-ner-ontonotes, batch 285 (47285): mcc: 0.9415, acc: 0.9183, precision: 0.9556, recall: 0.9340, f1: 0.9447, edges-ner-ontonotes_loss: 0.0185
10/01 05:33:21 AM: Update 47429: task edges-ner-ontonotes, batch 429 (47429): mcc: 0.9453, acc: 0.9223, precision: 0.9585, recall: 0.9381, f1: 0.9482, edges-ner-ontonotes_loss: 0.0175
10/01 05:33:31 AM: Update 47595: task edges-ner-ontonotes, batch 595 (47595): mcc: 0.9490, acc: 0.9266, precision: 0.9610, recall: 0.9427, f1: 0.9518, edges-ner-ontonotes_loss: 0.0164
10/01 05:33:41 AM: Update 47742: task edges-ner-ontonotes, batch 742 (47742): mcc: 0.9500, acc: 0.9275, precision: 0.9616, recall: 0.9440, f1: 0.9527, edges-ner-ontonotes_loss: 0.0161
10/01 05:33:51 AM: Update 47917: task edges-ner-ontonotes, batch 917 (47917): mcc: 0.9511, acc: 0.9286, precision: 0.9624, recall: 0.9453, f1: 0.9537, edges-ner-ontonotes_loss: 0.0159
10/01 05:33:57 AM: ***** Step 48000 / Validation 48 *****
10/01 05:33:57 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 05:33:57 AM: Validating...
10/01 05:34:01 AM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.9225, acc: 0.9018, precision: 0.9385, recall: 0.9151, f1: 0.9267, edges-ner-ontonotes_loss: 0.0272
10/01 05:34:10 AM: Updating LR scheduler:
10/01 05:34:10 AM: 	Best result seen so far for macro_avg: 0.949
10/01 05:34:10 AM: 	# validation passes without improvement: 2
10/01 05:34:10 AM: Ran out of early stopping patience. Stopping training.
10/01 05:34:10 AM: edges-ner-ontonotes_loss: training: 0.016621 validation: 0.019559
10/01 05:34:10 AM: macro_avg: validation: 0.948511
10/01 05:34:10 AM: micro_avg: validation: 0.000000
10/01 05:34:10 AM: edges-ner-ontonotes_mcc: training: 0.949005 validation: 0.945597
10/01 05:34:10 AM: edges-ner-ontonotes_acc: training: 0.925868 validation: 0.926221
10/01 05:34:10 AM: edges-ner-ontonotes_precision: training: 0.960798 validation: 0.960003
10/01 05:34:10 AM: edges-ner-ontonotes_recall: training: 0.942905 validation: 0.937292
10/01 05:34:10 AM: edges-ner-ontonotes_f1: training: 0.951767 validation: 0.948511
10/01 05:34:10 AM: Global learning rate: 3.125e-06
10/01 05:34:10 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:34:10 AM: Stopped training after 48 validation checks
10/01 05:34:10 AM: Trained edges-ner-ontonotes for 48000 batches or 30.888 epochs
10/01 05:34:10 AM: ***** VALIDATION RESULTS *****
10/01 05:34:10 AM: edges-ner-ontonotes_f1 (for best val pass 38): edges-ner-ontonotes_loss: 0.01931, macro_avg: 0.94888, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.94599, edges-ner-ontonotes_acc: 0.92622, edges-ner-ontonotes_precision: 0.96061, edges-ner-ontonotes_recall: 0.93744, edges-ner-ontonotes_f1: 0.94888
10/01 05:34:10 AM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.04348, macro_avg: 0.88228, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.87690, edges-ner-ontonotes_acc: 0.82765, edges-ner-ontonotes_precision: 0.92810, edges-ner-ontonotes_recall: 0.84076, edges-ner-ontonotes_f1: 0.88228
10/01 05:34:10 AM: macro_avg (for best val pass 38): edges-ner-ontonotes_loss: 0.01931, macro_avg: 0.94888, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.94599, edges-ner-ontonotes_acc: 0.92622, edges-ner-ontonotes_precision: 0.96061, edges-ner-ontonotes_recall: 0.93744, edges-ner-ontonotes_f1: 0.94888
10/01 05:34:10 AM: Evaluating...
10/01 05:34:10 AM: Loaded model state from ./experiments/ner-ontonotes-allstrings-mix/run/edges-ner-ontonotes/model_state_target_train_val_38.best.th
10/01 05:34:10 AM: Evaluating on: edges-ner-ontonotes, split: val
10/01 05:34:31 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
10/01 05:34:31 AM: Finished evaluating on: edges-ner-ontonotes
10/01 05:34:31 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
10/01 05:34:31 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:34:31 AM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:34:31 AM: Evaluating on: edges-ner-ontonotes, split: test
10/01 05:34:44 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
10/01 05:34:44 AM: Finished evaluating on: edges-ner-ontonotes
10/01 05:34:44 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
10/01 05:34:45 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:34:45 AM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-allstrings-mix/run
10/01 05:34:45 AM: Writing results for split 'val' to ./experiments/ner-ontonotes-allstrings-mix/results.tsv
10/01 05:34:45 AM: micro_avg: 0.000, macro_avg: 0.946, edges-ner-ontonotes_mcc: 0.943, edges-ner-ontonotes_acc: 0.923, edges-ner-ontonotes_precision: 0.959, edges-ner-ontonotes_recall: 0.934, edges-ner-ontonotes_f1: 0.946
10/01 05:34:45 AM: Done!
10/01 05:34:45 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
