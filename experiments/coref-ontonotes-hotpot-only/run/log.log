09/17 12:27:20 AM: Git branch: master
09/17 12:27:20 AM: Git SHA: 4086cd8f278243816795989a620c769378a6ab56
09/17 12:27:20 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/coref-ontonotes-hotpot-only/",
  "exp_name": "experiments/coref-ontonotes-hotpot-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/coref-ontonotes-hotpot-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/hotpot",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/coref-ontonotes-hotpot-only__run",
  "run_dir": "./experiments/coref-ontonotes-hotpot-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-coref-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/17 12:27:20 AM: Saved config to ./experiments/coref-ontonotes-hotpot-only/run/params.conf
09/17 12:27:20 AM: Using random seed 1234
09/17 12:27:25 AM: Using GPU 0
09/17 12:27:25 AM: Loading tasks...
09/17 12:27:25 AM: Writing pre-preprocessed tasks to ./experiments/coref-ontonotes-hotpot-only/
09/17 12:27:25 AM: 	Creating task edges-coref-ontonotes from scratch.
09/17 12:27:27 AM: Read=41777, Skip=74035, Total=115812 from ./probing_data/edges/ontonotes/coref/train.json.retokenized.bert-base-uncased
09/17 12:27:27 AM: Read=5044, Skip=10636, Total=15680 from ./probing_data/edges/ontonotes/coref/development.json.retokenized.bert-base-uncased
09/17 12:27:27 AM: Read=5188, Skip=7029, Total=12217 from ./probing_data/edges/ontonotes/coref/test.json.retokenized.bert-base-uncased
09/17 12:27:28 AM: 	Task 'edges-coref-ontonotes': |train|=41777 |val|=5044 |test|=5188
09/17 12:27:28 AM: 	Finished loading tasks: edges-coref-ontonotes.
09/17 12:27:28 AM: 	Building vocab from scratch.
09/17 12:27:28 AM: 	Counting units for task edges-coref-ontonotes.
09/17 12:27:29 AM: 	Task 'edges-coref-ontonotes': adding vocab namespace 'edges-coref-ontonotes_labels'
09/17 12:27:30 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 12:27:30 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/17 12:27:30 AM: 	Saved vocab to ./experiments/coref-ontonotes-hotpot-only/vocab
09/17 12:27:30 AM: Loading token dictionary from ./experiments/coref-ontonotes-hotpot-only/vocab.
09/17 12:27:30 AM: 	Loaded vocab from ./experiments/coref-ontonotes-hotpot-only/vocab
09/17 12:27:30 AM: 	Vocab namespace tokens: size 20434
09/17 12:27:30 AM: 	Vocab namespace bert_uncased: size 30524
09/17 12:27:30 AM: 	Vocab namespace edges-coref-ontonotes_labels: size 2
09/17 12:27:30 AM: 	Vocab namespace chars: size 72
09/17 12:27:30 AM: 	Finished building vocab.
09/17 12:27:30 AM: 	Task edges-coref-ontonotes (train): Indexing from scratch.
09/17 12:27:39 AM: 	Task edges-coref-ontonotes (train): Saved 41777 instances to ./experiments/coref-ontonotes-hotpot-only/preproc/edges-coref-ontonotes__train_data
09/17 12:27:39 AM: 	Task edges-coref-ontonotes (val): Indexing from scratch.
09/17 12:27:41 AM: 	Task edges-coref-ontonotes (val): Saved 5044 instances to ./experiments/coref-ontonotes-hotpot-only/preproc/edges-coref-ontonotes__val_data
09/17 12:27:41 AM: 	Task edges-coref-ontonotes (test): Indexing from scratch.
09/17 12:27:42 AM: 	Task edges-coref-ontonotes (test): Saved 5188 instances to ./experiments/coref-ontonotes-hotpot-only/preproc/edges-coref-ontonotes__test_data
09/17 12:27:42 AM: 	Finished indexing tasks
09/17 12:27:42 AM: 	Creating trimmed target-only version of edges-coref-ontonotes train.
09/17 12:27:42 AM: 	  Training on 
09/17 12:27:42 AM: 	  Evaluating on edges-coref-ontonotes
09/17 12:27:42 AM: 	Finished loading tasks in 17.056s
09/17 12:27:42 AM: 	 Tasks: ['edges-coref-ontonotes']
09/17 12:27:42 AM: Building model...
09/17 12:27:42 AM: Using BERT model (bert-base-uncased).
09/17 12:27:42 AM: LOADING A FUNETUNED MODEL from: 
09/17 12:27:42 AM: models/hotpot
09/17 12:27:42 AM: loading configuration file models/hotpot/config.json
09/17 12:27:42 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/17 12:27:42 AM: loading weights file models/hotpot/pytorch_model.bin
09/17 12:27:46 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpzvo3hjgc
09/17 12:27:48 AM: copying /tmp/tmpzvo3hjgc to cache at ./experiments/coref-ontonotes-hotpot-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 12:27:48 AM: creating metadata file for ./experiments/coref-ontonotes-hotpot-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 12:27:48 AM: removing temp file /tmp/tmpzvo3hjgc
09/17 12:27:48 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/coref-ontonotes-hotpot-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 12:27:48 AM: Initializing parameters
09/17 12:27:48 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/17 12:27:48 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/17 12:27:48 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/17 12:27:48 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/17 12:27:48 AM:    _text_field_embedder.model.pooler.dense.bias
09/17 12:27:48 AM:    _text_field_embedder.model.pooler.dense.weight
09/17 12:27:48 AM: 	Task 'edges-coref-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-coref-ontonotes"
}
09/17 12:27:53 AM: Model specification:
09/17 12:27:53 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-coref-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=2, bias=True)
      )
    )
  )
)
09/17 12:27:53 AM: Model parameters:
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 12:27:53 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 512 with torch.Size([2, 256])
09/17 12:27:53 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
09/17 12:27:53 AM: Total number of parameters: 110139394 (1.10139e+08)
09/17 12:27:53 AM: Number of trainable parameters: 657154 (657154)
09/17 12:27:53 AM: Finished building model in 11.126s
09/17 12:27:53 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-coref-ontonotes 

09/17 12:27:58 AM: patience = 9
09/17 12:27:58 AM: val_interval = 1000
09/17 12:27:58 AM: max_vals = 250
09/17 12:27:58 AM: cuda_device = 0
09/17 12:27:58 AM: grad_norm = 5.0
09/17 12:27:58 AM: grad_clipping = None
09/17 12:27:58 AM: lr_decay = 0.99
09/17 12:27:58 AM: min_lr = 1e-06
09/17 12:27:58 AM: keep_all_checkpoints = 0
09/17 12:27:58 AM: val_data_limit = 5000
09/17 12:27:58 AM: max_epochs = -1
09/17 12:27:58 AM: dec_val_scale = 250
09/17 12:27:58 AM: training_data_fraction = 1
09/17 12:27:58 AM: type = adam
09/17 12:27:58 AM: parameter_groups = None
09/17 12:27:58 AM: Number of trainable parameters: 657154
09/17 12:27:58 AM: infer_type_and_cast = True
09/17 12:27:58 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 12:27:58 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 12:27:58 AM: lr = 0.0001
09/17 12:27:58 AM: amsgrad = True
09/17 12:27:58 AM: type = reduce_on_plateau
09/17 12:27:58 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 12:27:58 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 12:27:58 AM: mode = max
09/17 12:27:58 AM: factor = 0.5
09/17 12:27:58 AM: patience = 3
09/17 12:27:58 AM: threshold = 0.0001
09/17 12:27:58 AM: threshold_mode = abs
09/17 12:27:58 AM: verbose = True
09/17 12:27:58 AM: type = adam
09/17 12:27:58 AM: parameter_groups = None
09/17 12:27:58 AM: Number of trainable parameters: 657154
09/17 12:27:58 AM: infer_type_and_cast = True
09/17 12:27:58 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 12:27:58 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 12:27:58 AM: lr = 0.0001
09/17 12:27:58 AM: amsgrad = True
09/17 12:27:58 AM: type = reduce_on_plateau
09/17 12:27:58 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 12:27:58 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 12:27:58 AM: mode = max
09/17 12:27:58 AM: factor = 0.5
09/17 12:27:58 AM: patience = 3
09/17 12:27:58 AM: threshold = 0.0001
09/17 12:27:58 AM: threshold_mode = abs
09/17 12:27:58 AM: verbose = True
09/17 12:27:58 AM: Starting training without restoring from a checkpoint.
09/17 12:27:58 AM: Training examples per task, before any subsampling: {'edges-coref-ontonotes': 41777}
09/17 12:27:58 AM: Beginning training with stopping criteria based on metric: edges-coref-ontonotes_f1
09/17 12:28:08 AM: Update 277: task edges-coref-ontonotes, batch 277 (277): mcc: 0.5898, acc: 0.7584, precision: 0.7933, recall: 0.7977, f1: 0.7955, edges-coref-ontonotes_loss: 0.4275
09/17 12:28:18 AM: Update 509: task edges-coref-ontonotes, batch 509 (509): mcc: 0.6040, acc: 0.7692, precision: 0.8015, recall: 0.8028, f1: 0.8022, edges-coref-ontonotes_loss: 0.4207
09/17 12:28:28 AM: Update 751: task edges-coref-ontonotes, batch 751 (751): mcc: 0.6215, acc: 0.7818, precision: 0.8108, recall: 0.8107, f1: 0.8107, edges-coref-ontonotes_loss: 0.4113
09/17 12:28:38 AM: Update 994: task edges-coref-ontonotes, batch 994 (994): mcc: 0.6424, acc: 0.7954, precision: 0.8215, recall: 0.8208, f1: 0.8211, edges-coref-ontonotes_loss: 0.3969
09/17 12:28:38 AM: ***** Step 1000 / Validation 1 *****
09/17 12:28:38 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:28:38 AM: Validating...
09/17 12:28:44 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:28:44 AM: Best result seen so far for micro.
09/17 12:28:44 AM: Best result seen so far for macro.
09/17 12:28:44 AM: Updating LR scheduler:
09/17 12:28:44 AM: 	Best result seen so far for macro_avg: 0.855
09/17 12:28:44 AM: 	# validation passes without improvement: 0
09/17 12:28:44 AM: edges-coref-ontonotes_loss: training: 0.396376 validation: 0.376320
09/17 12:28:44 AM: macro_avg: validation: 0.855059
09/17 12:28:44 AM: micro_avg: validation: 0.000000
09/17 12:28:44 AM: edges-coref-ontonotes_mcc: training: 0.642931 validation: 0.710916
09/17 12:28:44 AM: edges-coref-ontonotes_acc: training: 0.795711 validation: 0.851087
09/17 12:28:44 AM: edges-coref-ontonotes_precision: training: 0.821752 validation: 0.857396
09/17 12:28:44 AM: edges-coref-ontonotes_recall: training: 0.821020 validation: 0.852734
09/17 12:28:44 AM: edges-coref-ontonotes_f1: training: 0.821386 validation: 0.855059
09/17 12:28:44 AM: Global learning rate: 0.0001
09/17 12:28:44 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:28:48 AM: Update 1137: task edges-coref-ontonotes, batch 137 (1137): mcc: 0.7641, acc: 0.8694, precision: 0.8836, recall: 0.8800, f1: 0.8818, edges-coref-ontonotes_loss: 0.2933
09/17 12:28:58 AM: Update 1372: task edges-coref-ontonotes, batch 372 (1372): mcc: 0.7413, acc: 0.8579, precision: 0.8719, recall: 0.8689, f1: 0.8704, edges-coref-ontonotes_loss: 0.3087
09/17 12:29:23 AM: Update 1642: task edges-coref-ontonotes, batch 642 (1642): mcc: 0.7468, acc: 0.8619, precision: 0.8743, recall: 0.8722, f1: 0.8732, edges-coref-ontonotes_loss: 0.3048
09/17 12:29:35 AM: Update 1935: task edges-coref-ontonotes, batch 935 (1935): mcc: 0.7312, acc: 0.8525, precision: 0.8662, recall: 0.8649, f1: 0.8655, edges-coref-ontonotes_loss: 0.3194
09/17 12:29:37 AM: ***** Step 2000 / Validation 2 *****
09/17 12:29:37 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:29:37 AM: Validating...
09/17 12:29:42 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:29:42 AM: Best result seen so far for macro.
09/17 12:29:42 AM: Updating LR scheduler:
09/17 12:29:42 AM: 	Best result seen so far for macro_avg: 0.868
09/17 12:29:42 AM: 	# validation passes without improvement: 0
09/17 12:29:42 AM: edges-coref-ontonotes_loss: training: 0.318515 validation: 0.346143
09/17 12:29:42 AM: macro_avg: validation: 0.868341
09/17 12:29:42 AM: micro_avg: validation: 0.000000
09/17 12:29:42 AM: edges-coref-ontonotes_mcc: training: 0.731424 validation: 0.736713
09/17 12:29:42 AM: edges-coref-ontonotes_acc: training: 0.852770 validation: 0.866519
09/17 12:29:42 AM: edges-coref-ontonotes_precision: training: 0.866244 validation: 0.868441
09/17 12:29:42 AM: edges-coref-ontonotes_recall: training: 0.864985 validation: 0.868242
09/17 12:29:42 AM: edges-coref-ontonotes_f1: training: 0.865614 validation: 0.868341
09/17 12:29:42 AM: Global learning rate: 0.0001
09/17 12:29:42 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:29:45 AM: Update 2074: task edges-coref-ontonotes, batch 74 (2074): mcc: 0.7496, acc: 0.8665, precision: 0.8751, recall: 0.8745, f1: 0.8748, edges-coref-ontonotes_loss: 0.3025
09/17 12:29:55 AM: Update 2369: task edges-coref-ontonotes, batch 369 (2369): mcc: 0.7676, acc: 0.8753, precision: 0.8842, recall: 0.8834, f1: 0.8838, edges-coref-ontonotes_loss: 0.2721
09/17 12:30:05 AM: Update 2620: task edges-coref-ontonotes, batch 620 (2620): mcc: 0.7703, acc: 0.8764, precision: 0.8854, recall: 0.8849, f1: 0.8851, edges-coref-ontonotes_loss: 0.2646
09/17 12:30:16 AM: Update 2930: task edges-coref-ontonotes, batch 930 (2930): mcc: 0.7719, acc: 0.8774, precision: 0.8862, recall: 0.8857, f1: 0.8859, edges-coref-ontonotes_loss: 0.2646
09/17 12:30:19 AM: ***** Step 3000 / Validation 3 *****
09/17 12:30:19 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:30:19 AM: Validating...
09/17 12:30:24 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:30:24 AM: Best result seen so far for macro.
09/17 12:30:24 AM: Updating LR scheduler:
09/17 12:30:24 AM: 	Best result seen so far for macro_avg: 0.874
09/17 12:30:24 AM: 	# validation passes without improvement: 0
09/17 12:30:24 AM: edges-coref-ontonotes_loss: training: 0.269108 validation: 0.318473
09/17 12:30:24 AM: macro_avg: validation: 0.873817
09/17 12:30:24 AM: micro_avg: validation: 0.000000
09/17 12:30:24 AM: edges-coref-ontonotes_mcc: training: 0.768240 validation: 0.747896
09/17 12:30:24 AM: edges-coref-ontonotes_acc: training: 0.875310 validation: 0.871305
09/17 12:30:24 AM: edges-coref-ontonotes_precision: training: 0.884269 validation: 0.874722
09/17 12:30:24 AM: edges-coref-ontonotes_recall: training: 0.883927 validation: 0.872913
09/17 12:30:24 AM: edges-coref-ontonotes_f1: training: 0.884098 validation: 0.873817
09/17 12:30:24 AM: Global learning rate: 0.0001
09/17 12:30:24 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:30:26 AM: Update 3063: task edges-coref-ontonotes, batch 63 (3063): mcc: 0.7014, acc: 0.8383, precision: 0.8504, recall: 0.8511, f1: 0.8507, edges-coref-ontonotes_loss: 0.3439
09/17 12:30:36 AM: Update 3308: task edges-coref-ontonotes, batch 308 (3308): mcc: 0.7265, acc: 0.8516, precision: 0.8631, recall: 0.8634, f1: 0.8633, edges-coref-ontonotes_loss: 0.3165
09/17 12:30:47 AM: Update 3557: task edges-coref-ontonotes, batch 557 (3557): mcc: 0.7429, acc: 0.8614, precision: 0.8715, recall: 0.8713, f1: 0.8714, edges-coref-ontonotes_loss: 0.2968
09/17 12:30:57 AM: Update 3896: task edges-coref-ontonotes, batch 896 (3896): mcc: 0.7640, acc: 0.8731, precision: 0.8820, recall: 0.8819, f1: 0.8820, edges-coref-ontonotes_loss: 0.2665
09/17 12:31:02 AM: ***** Step 4000 / Validation 4 *****
09/17 12:31:02 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:31:02 AM: Validating...
09/17 12:31:07 AM: Evaluate: task edges-coref-ontonotes, batch 137 (157): mcc: 0.7563, acc: 0.8770, precision: 0.8784, recall: 0.8778, f1: 0.8781, edges-coref-ontonotes_loss: 0.3145
09/17 12:31:07 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:31:07 AM: Best result seen so far for macro.
09/17 12:31:07 AM: Updating LR scheduler:
09/17 12:31:07 AM: 	Best result seen so far for macro_avg: 0.876
09/17 12:31:07 AM: 	# validation passes without improvement: 0
09/17 12:31:07 AM: edges-coref-ontonotes_loss: training: 0.267364 validation: 0.314306
09/17 12:31:07 AM: macro_avg: validation: 0.876151
09/17 12:31:07 AM: micro_avg: validation: 0.000000
09/17 12:31:07 AM: edges-coref-ontonotes_mcc: training: 0.763490 validation: 0.752374
09/17 12:31:07 AM: edges-coref-ontonotes_acc: training: 0.872837 validation: 0.874981
09/17 12:31:07 AM: edges-coref-ontonotes_precision: training: 0.881812 validation: 0.876403
09/17 12:31:07 AM: edges-coref-ontonotes_recall: training: 0.881658 validation: 0.875900
09/17 12:31:07 AM: edges-coref-ontonotes_f1: training: 0.881735 validation: 0.876151
09/17 12:31:07 AM: Global learning rate: 0.0001
09/17 12:31:07 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:31:17 AM: Update 4238: task edges-coref-ontonotes, batch 238 (4238): mcc: 0.7860, acc: 0.8859, precision: 0.8925, recall: 0.8936, f1: 0.8930, edges-coref-ontonotes_loss: 0.2468
09/17 12:31:27 AM: Update 4542: task edges-coref-ontonotes, batch 542 (4542): mcc: 0.7523, acc: 0.8668, precision: 0.8758, recall: 0.8766, f1: 0.8762, edges-coref-ontonotes_loss: 0.2847
09/17 12:31:37 AM: Update 4787: task edges-coref-ontonotes, batch 787 (4787): mcc: 0.7564, acc: 0.8697, precision: 0.8780, recall: 0.8785, f1: 0.8783, edges-coref-ontonotes_loss: 0.2792
09/17 12:31:45 AM: ***** Step 5000 / Validation 5 *****
09/17 12:31:45 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:31:45 AM: Validating...
09/17 12:31:47 AM: Evaluate: task edges-coref-ontonotes, batch 44 (157): mcc: 0.7709, acc: 0.8835, precision: 0.8853, recall: 0.8856, f1: 0.8855, edges-coref-ontonotes_loss: 0.2802
09/17 12:31:51 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:31:51 AM: Best result seen so far for macro.
09/17 12:31:51 AM: Updating LR scheduler:
09/17 12:31:51 AM: 	Best result seen so far for macro_avg: 0.877
09/17 12:31:51 AM: 	# validation passes without improvement: 0
09/17 12:31:51 AM: edges-coref-ontonotes_loss: training: 0.266083 validation: 0.302720
09/17 12:31:51 AM: macro_avg: validation: 0.876720
09/17 12:31:51 AM: micro_avg: validation: 0.000000
09/17 12:31:51 AM: edges-coref-ontonotes_mcc: training: 0.764888 validation: 0.753294
09/17 12:31:51 AM: edges-coref-ontonotes_acc: training: 0.874439 validation: 0.874751
09/17 12:31:51 AM: edges-coref-ontonotes_precision: training: 0.882355 validation: 0.876200
09/17 12:31:51 AM: edges-coref-ontonotes_recall: training: 0.882561 validation: 0.877240
09/17 12:31:51 AM: edges-coref-ontonotes_f1: training: 0.882458 validation: 0.876720
09/17 12:31:51 AM: Global learning rate: 0.0001
09/17 12:31:51 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:31:57 AM: Update 5186: task edges-coref-ontonotes, batch 186 (5186): mcc: 0.8132, acc: 0.9013, precision: 0.9074, recall: 0.9057, f1: 0.9065, edges-coref-ontonotes_loss: 0.2003
09/17 12:32:07 AM: Update 5463: task edges-coref-ontonotes, batch 463 (5463): mcc: 0.7894, acc: 0.8884, precision: 0.8951, recall: 0.8942, f1: 0.8947, edges-coref-ontonotes_loss: 0.2314
09/17 12:32:17 AM: Update 5706: task edges-coref-ontonotes, batch 706 (5706): mcc: 0.7737, acc: 0.8799, precision: 0.8870, recall: 0.8866, f1: 0.8868, edges-coref-ontonotes_loss: 0.2512
09/17 12:32:27 AM: Update 5923: task edges-coref-ontonotes, batch 923 (5923): mcc: 0.7675, acc: 0.8766, precision: 0.8840, recall: 0.8834, f1: 0.8837, edges-coref-ontonotes_loss: 0.2602
09/17 12:32:29 AM: ***** Step 6000 / Validation 6 *****
09/17 12:32:29 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:32:29 AM: Validating...
09/17 12:32:35 AM: Updating LR scheduler:
09/17 12:32:35 AM: 	Best result seen so far for macro_avg: 0.877
09/17 12:32:35 AM: 	# validation passes without improvement: 1
09/17 12:32:35 AM: edges-coref-ontonotes_loss: training: 0.259978 validation: 0.306096
09/17 12:32:35 AM: macro_avg: validation: 0.876718
09/17 12:32:35 AM: micro_avg: validation: 0.000000
09/17 12:32:35 AM: edges-coref-ontonotes_mcc: training: 0.767962 validation: 0.753755
09/17 12:32:35 AM: edges-coref-ontonotes_acc: training: 0.876875 validation: 0.874981
09/17 12:32:35 AM: edges-coref-ontonotes_precision: training: 0.884220 validation: 0.877846
09/17 12:32:35 AM: edges-coref-ontonotes_recall: training: 0.883671 validation: 0.875593
09/17 12:32:35 AM: edges-coref-ontonotes_f1: training: 0.883945 validation: 0.876718
09/17 12:32:35 AM: Global learning rate: 0.0001
09/17 12:32:35 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:32:37 AM: Update 6055: task edges-coref-ontonotes, batch 55 (6055): mcc: 0.7790, acc: 0.8824, precision: 0.8896, recall: 0.8893, f1: 0.8895, edges-coref-ontonotes_loss: 0.2541
09/17 12:32:47 AM: Update 6298: task edges-coref-ontonotes, batch 298 (6298): mcc: 0.7893, acc: 0.8890, precision: 0.8948, recall: 0.8945, f1: 0.8946, edges-coref-ontonotes_loss: 0.2285
09/17 12:32:57 AM: Update 6573: task edges-coref-ontonotes, batch 573 (6573): mcc: 0.7935, acc: 0.8910, precision: 0.8968, recall: 0.8967, f1: 0.8967, edges-coref-ontonotes_loss: 0.2213
09/17 12:33:07 AM: Update 6854: task edges-coref-ontonotes, batch 854 (6854): mcc: 0.7939, acc: 0.8914, precision: 0.8971, recall: 0.8968, f1: 0.8969, edges-coref-ontonotes_loss: 0.2248
09/17 12:33:12 AM: ***** Step 7000 / Validation 7 *****
09/17 12:33:12 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:33:12 AM: Validating...
09/17 12:33:17 AM: Evaluate: task edges-coref-ontonotes, batch 152 (157): mcc: 0.7467, acc: 0.8715, precision: 0.8730, recall: 0.8739, f1: 0.8734, edges-coref-ontonotes_loss: 0.3023
09/17 12:33:17 AM: Updating LR scheduler:
09/17 12:33:17 AM: 	Best result seen so far for macro_avg: 0.877
09/17 12:33:17 AM: 	# validation passes without improvement: 2
09/17 12:33:17 AM: edges-coref-ontonotes_loss: training: 0.235499 validation: 0.305269
09/17 12:33:17 AM: macro_avg: validation: 0.873015
09/17 12:33:17 AM: micro_avg: validation: 0.000000
09/17 12:33:17 AM: edges-coref-ontonotes_mcc: training: 0.785487 validation: 0.745903
09/17 12:33:17 AM: edges-coref-ontonotes_acc: training: 0.886955 validation: 0.871037
09/17 12:33:17 AM: edges-coref-ontonotes_precision: training: 0.892829 validation: 0.872581
09/17 12:33:17 AM: edges-coref-ontonotes_recall: training: 0.892634 validation: 0.873449
09/17 12:33:17 AM: edges-coref-ontonotes_f1: training: 0.892732 validation: 0.873015
09/17 12:33:17 AM: Global learning rate: 0.0001
09/17 12:33:17 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:33:27 AM: Update 7243: task edges-coref-ontonotes, batch 243 (7243): mcc: 0.7475, acc: 0.8662, precision: 0.8732, recall: 0.8744, f1: 0.8738, edges-coref-ontonotes_loss: 0.2877
09/17 12:33:37 AM: Update 7522: task edges-coref-ontonotes, batch 522 (7522): mcc: 0.7635, acc: 0.8750, precision: 0.8815, recall: 0.8821, f1: 0.8818, edges-coref-ontonotes_loss: 0.2614
09/17 12:33:47 AM: Update 7833: task edges-coref-ontonotes, batch 833 (7833): mcc: 0.7803, acc: 0.8841, precision: 0.8901, recall: 0.8902, f1: 0.8902, edges-coref-ontonotes_loss: 0.2386
09/17 12:33:54 AM: ***** Step 8000 / Validation 8 *****
09/17 12:33:54 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:33:54 AM: Validating...
09/17 12:33:57 AM: Evaluate: task edges-coref-ontonotes, batch 81 (157): mcc: 0.7656, acc: 0.8812, precision: 0.8828, recall: 0.8827, f1: 0.8828, edges-coref-ontonotes_loss: 0.2973
09/17 12:34:00 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:34:00 AM: Best result seen so far for macro.
09/17 12:34:00 AM: Updating LR scheduler:
09/17 12:34:00 AM: 	Best result seen so far for macro_avg: 0.882
09/17 12:34:00 AM: 	# validation passes without improvement: 0
09/17 12:34:00 AM: edges-coref-ontonotes_loss: training: 0.239005 validation: 0.290982
09/17 12:34:00 AM: macro_avg: validation: 0.882106
09/17 12:34:00 AM: micro_avg: validation: 0.000000
09/17 12:34:00 AM: edges-coref-ontonotes_mcc: training: 0.780116 validation: 0.764244
09/17 12:34:00 AM: edges-coref-ontonotes_acc: training: 0.884083 validation: 0.880571
09/17 12:34:00 AM: edges-coref-ontonotes_precision: training: 0.890087 validation: 0.882225
09/17 12:34:00 AM: edges-coref-ontonotes_recall: training: 0.890020 validation: 0.881988
09/17 12:34:00 AM: edges-coref-ontonotes_f1: training: 0.890054 validation: 0.882106
09/17 12:34:00 AM: Global learning rate: 0.0001
09/17 12:34:00 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:34:07 AM: Update 8171: task edges-coref-ontonotes, batch 171 (8171): mcc: 0.7997, acc: 0.8952, precision: 0.8999, recall: 0.8998, f1: 0.8999, edges-coref-ontonotes_loss: 0.2271
09/17 12:34:19 AM: Update 8475: task edges-coref-ontonotes, batch 475 (8475): mcc: 0.7660, acc: 0.8767, precision: 0.8829, recall: 0.8831, f1: 0.8830, edges-coref-ontonotes_loss: 0.2675
09/17 12:34:30 AM: Update 8788: task edges-coref-ontonotes, batch 788 (8788): mcc: 0.7706, acc: 0.8793, precision: 0.8851, recall: 0.8855, f1: 0.8853, edges-coref-ontonotes_loss: 0.2595
09/17 12:34:36 AM: ***** Step 9000 / Validation 9 *****
09/17 12:34:36 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:34:36 AM: Validating...
09/17 12:34:40 AM: Evaluate: task edges-coref-ontonotes, batch 97 (157): mcc: 0.7570, acc: 0.8775, precision: 0.8784, recall: 0.8787, f1: 0.8785, edges-coref-ontonotes_loss: 0.3122
09/17 12:34:42 AM: Updating LR scheduler:
09/17 12:34:42 AM: 	Best result seen so far for macro_avg: 0.882
09/17 12:34:42 AM: 	# validation passes without improvement: 1
09/17 12:34:42 AM: edges-coref-ontonotes_loss: training: 0.243232 validation: 0.293417
09/17 12:34:42 AM: macro_avg: validation: 0.881827
09/17 12:34:42 AM: micro_avg: validation: 0.000000
09/17 12:34:42 AM: edges-coref-ontonotes_mcc: training: 0.783708 validation: 0.763632
09/17 12:34:42 AM: edges-coref-ontonotes_acc: training: 0.886268 validation: 0.880992
09/17 12:34:42 AM: edges-coref-ontonotes_precision: training: 0.891731 validation: 0.881743
09/17 12:34:42 AM: edges-coref-ontonotes_recall: training: 0.892011 validation: 0.881911
09/17 12:34:42 AM: edges-coref-ontonotes_f1: training: 0.891871 validation: 0.881827
09/17 12:34:42 AM: Global learning rate: 0.0001
09/17 12:34:42 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:34:50 AM: Update 9195: task edges-coref-ontonotes, batch 195 (9195): mcc: 0.7787, acc: 0.8845, precision: 0.8898, recall: 0.8889, f1: 0.8893, edges-coref-ontonotes_loss: 0.2233
09/17 12:35:00 AM: Update 9471: task edges-coref-ontonotes, batch 471 (9471): mcc: 0.7910, acc: 0.8907, precision: 0.8956, recall: 0.8953, f1: 0.8955, edges-coref-ontonotes_loss: 0.2237
09/17 12:35:12 AM: Update 9783: task edges-coref-ontonotes, batch 783 (9783): mcc: 0.7739, acc: 0.8815, precision: 0.8871, recall: 0.8868, f1: 0.8870, edges-coref-ontonotes_loss: 0.2482
09/17 12:35:19 AM: ***** Step 10000 / Validation 10 *****
09/17 12:35:19 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:35:19 AM: Validating...
09/17 12:35:22 AM: Evaluate: task edges-coref-ontonotes, batch 91 (157): mcc: 0.7533, acc: 0.8745, precision: 0.8758, recall: 0.8777, f1: 0.8768, edges-coref-ontonotes_loss: 0.3133
09/17 12:35:25 AM: Updating LR scheduler:
09/17 12:35:25 AM: 	Best result seen so far for macro_avg: 0.882
09/17 12:35:25 AM: 	# validation passes without improvement: 2
09/17 12:35:25 AM: edges-coref-ontonotes_loss: training: 0.246197 validation: 0.294770
09/17 12:35:25 AM: macro_avg: validation: 0.880528
09/17 12:35:25 AM: micro_avg: validation: 0.000000
09/17 12:35:25 AM: edges-coref-ontonotes_mcc: training: 0.776514 validation: 0.760724
09/17 12:35:25 AM: edges-coref-ontonotes_acc: training: 0.882958 validation: 0.877929
09/17 12:35:25 AM: edges-coref-ontonotes_precision: training: 0.888328 validation: 0.879300
09/17 12:35:25 AM: edges-coref-ontonotes_recall: training: 0.888166 validation: 0.881758
09/17 12:35:25 AM: edges-coref-ontonotes_f1: training: 0.888247 validation: 0.880528
09/17 12:35:25 AM: Global learning rate: 0.0001
09/17 12:35:25 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:35:32 AM: Update 10183: task edges-coref-ontonotes, batch 183 (10183): mcc: 0.7909, acc: 0.8907, precision: 0.8956, recall: 0.8952, f1: 0.8954, edges-coref-ontonotes_loss: 0.2218
09/17 12:35:42 AM: Update 10465: task edges-coref-ontonotes, batch 465 (10465): mcc: 0.7998, acc: 0.8955, precision: 0.9001, recall: 0.8997, f1: 0.8999, edges-coref-ontonotes_loss: 0.2091
09/17 12:35:54 AM: Update 10778: task edges-coref-ontonotes, batch 778 (10778): mcc: 0.7997, acc: 0.8957, precision: 0.9000, recall: 0.8997, f1: 0.8999, edges-coref-ontonotes_loss: 0.2136
09/17 12:36:02 AM: ***** Step 11000 / Validation 11 *****
09/17 12:36:02 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:36:02 AM: Validating...
09/17 12:36:04 AM: Evaluate: task edges-coref-ontonotes, batch 57 (157): mcc: 0.7718, acc: 0.8840, precision: 0.8860, recall: 0.8859, f1: 0.8859, edges-coref-ontonotes_loss: 0.2822
09/17 12:36:08 AM: Updating LR scheduler:
09/17 12:36:08 AM: 	Best result seen so far for macro_avg: 0.882
09/17 12:36:08 AM: 	# validation passes without improvement: 3
09/17 12:36:08 AM: edges-coref-ontonotes_loss: training: 0.228741 validation: 0.292005
09/17 12:36:08 AM: macro_avg: validation: 0.880289
09/17 12:36:08 AM: micro_avg: validation: 0.000000
09/17 12:36:08 AM: edges-coref-ontonotes_mcc: training: 0.789410 validation: 0.760607
09/17 12:36:08 AM: edges-coref-ontonotes_acc: training: 0.890169 validation: 0.878695
09/17 12:36:08 AM: edges-coref-ontonotes_precision: training: 0.894750 validation: 0.880391
09/17 12:36:08 AM: edges-coref-ontonotes_recall: training: 0.894649 validation: 0.880188
09/17 12:36:08 AM: edges-coref-ontonotes_f1: training: 0.894699 validation: 0.880289
09/17 12:36:08 AM: Global learning rate: 0.0001
09/17 12:36:08 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:36:14 AM: Update 11092: task edges-coref-ontonotes, batch 92 (11092): mcc: 0.7513, acc: 0.8701, precision: 0.8753, recall: 0.8762, f1: 0.8757, edges-coref-ontonotes_loss: 0.2775
09/17 12:36:24 AM: Update 11396: task edges-coref-ontonotes, batch 396 (11396): mcc: 0.7797, acc: 0.8850, precision: 0.8898, recall: 0.8899, f1: 0.8898, edges-coref-ontonotes_loss: 0.2474
09/17 12:36:34 AM: Update 11667: task edges-coref-ontonotes, batch 667 (11667): mcc: 0.7959, acc: 0.8937, precision: 0.8978, recall: 0.8981, f1: 0.8980, edges-coref-ontonotes_loss: 0.2204
09/17 12:36:44 AM: Update 11912: task edges-coref-ontonotes, batch 912 (11912): mcc: 0.7946, acc: 0.8931, precision: 0.8972, recall: 0.8973, f1: 0.8973, edges-coref-ontonotes_loss: 0.2211
09/17 12:36:48 AM: ***** Step 12000 / Validation 12 *****
09/17 12:36:48 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:36:48 AM: Validating...
09/17 12:36:53 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:36:53 AM: Best result seen so far for macro.
09/17 12:36:53 AM: Updating LR scheduler:
09/17 12:36:53 AM: 	Best result seen so far for macro_avg: 0.883
09/17 12:36:53 AM: 	# validation passes without improvement: 0
09/17 12:36:53 AM: edges-coref-ontonotes_loss: training: 0.219000 validation: 0.294049
09/17 12:36:53 AM: macro_avg: validation: 0.883380
09/17 12:36:53 AM: micro_avg: validation: 0.000000
09/17 12:36:53 AM: edges-coref-ontonotes_mcc: training: 0.795836 validation: 0.766733
09/17 12:36:53 AM: edges-coref-ontonotes_acc: training: 0.893771 validation: 0.882371
09/17 12:36:53 AM: edges-coref-ontonotes_precision: training: 0.897913 validation: 0.883278
09/17 12:36:53 AM: edges-coref-ontonotes_recall: training: 0.897925 validation: 0.883481
09/17 12:36:53 AM: edges-coref-ontonotes_f1: training: 0.897919 validation: 0.883380
09/17 12:36:53 AM: Global learning rate: 0.0001
09/17 12:36:53 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:36:54 AM: Update 12026: task edges-coref-ontonotes, batch 26 (12026): mcc: 0.8001, acc: 0.8955, precision: 0.9008, recall: 0.8991, f1: 0.8999, edges-coref-ontonotes_loss: 0.2444
09/17 12:37:04 AM: Update 12265: task edges-coref-ontonotes, batch 265 (12265): mcc: 0.7691, acc: 0.8800, precision: 0.8846, recall: 0.8845, f1: 0.8846, edges-coref-ontonotes_loss: 0.2668
09/17 12:37:15 AM: Update 12523: task edges-coref-ontonotes, batch 523 (12523): mcc: 0.7677, acc: 0.8792, precision: 0.8838, recall: 0.8838, f1: 0.8838, edges-coref-ontonotes_loss: 0.2618
09/17 12:37:25 AM: Update 12767: task edges-coref-ontonotes, batch 767 (12767): mcc: 0.7743, acc: 0.8827, precision: 0.8871, recall: 0.8872, f1: 0.8872, edges-coref-ontonotes_loss: 0.2507
09/17 12:37:31 AM: ***** Step 13000 / Validation 13 *****
09/17 12:37:31 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:37:31 AM: Validating...
09/17 12:37:35 AM: Evaluate: task edges-coref-ontonotes, batch 102 (157): mcc: 0.7647, acc: 0.8816, precision: 0.8827, recall: 0.8819, f1: 0.8823, edges-coref-ontonotes_loss: 0.3044
09/17 12:37:37 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:37:37 AM: Best result seen so far for macro.
09/17 12:37:37 AM: Updating LR scheduler:
09/17 12:37:37 AM: 	Best result seen so far for macro_avg: 0.885
09/17 12:37:37 AM: 	# validation passes without improvement: 0
09/17 12:37:37 AM: edges-coref-ontonotes_loss: training: 0.232505 validation: 0.288022
09/17 12:37:37 AM: macro_avg: validation: 0.884951
09/17 12:37:37 AM: micro_avg: validation: 0.000000
09/17 12:37:37 AM: edges-coref-ontonotes_mcc: training: 0.787480 validation: 0.770026
09/17 12:37:37 AM: edges-coref-ontonotes_acc: training: 0.889549 validation: 0.884171
09/17 12:37:37 AM: edges-coref-ontonotes_precision: training: 0.893789 validation: 0.885426
09/17 12:37:37 AM: edges-coref-ontonotes_recall: training: 0.893678 validation: 0.884477
09/17 12:37:37 AM: edges-coref-ontonotes_f1: training: 0.893733 validation: 0.884951
09/17 12:37:37 AM: Global learning rate: 0.0001
09/17 12:37:37 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:37:45 AM: Update 13179: task edges-coref-ontonotes, batch 179 (13179): mcc: 0.7842, acc: 0.8886, precision: 0.8917, recall: 0.8926, f1: 0.8922, edges-coref-ontonotes_loss: 0.2326
09/17 12:37:55 AM: Update 13458: task edges-coref-ontonotes, batch 458 (13458): mcc: 0.7893, acc: 0.8911, precision: 0.8945, recall: 0.8948, f1: 0.8946, edges-coref-ontonotes_loss: 0.2302
09/17 12:38:05 AM: Update 13707: task edges-coref-ontonotes, batch 707 (13707): mcc: 0.7795, acc: 0.8857, precision: 0.8898, recall: 0.8897, f1: 0.8897, edges-coref-ontonotes_loss: 0.2448
09/17 12:38:14 AM: ***** Step 14000 / Validation 14 *****
09/17 12:38:14 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:38:14 AM: Validating...
09/17 12:38:15 AM: Evaluate: task edges-coref-ontonotes, batch 25 (157): mcc: 0.7945, acc: 0.8958, precision: 0.8982, recall: 0.8960, f1: 0.8971, edges-coref-ontonotes_loss: 0.2696
09/17 12:38:20 AM: Updating LR scheduler:
09/17 12:38:20 AM: 	Best result seen so far for macro_avg: 0.885
09/17 12:38:20 AM: 	# validation passes without improvement: 1
09/17 12:38:20 AM: edges-coref-ontonotes_loss: training: 0.242653 validation: 0.292144
09/17 12:38:20 AM: macro_avg: validation: 0.879701
09/17 12:38:20 AM: micro_avg: validation: 0.000000
09/17 12:38:20 AM: edges-coref-ontonotes_mcc: training: 0.780739 validation: 0.759458
09/17 12:38:20 AM: edges-coref-ontonotes_acc: training: 0.886376 validation: 0.878580
09/17 12:38:20 AM: edges-coref-ontonotes_precision: training: 0.890348 validation: 0.879903
09/17 12:38:20 AM: edges-coref-ontonotes_recall: training: 0.890397 validation: 0.879499
09/17 12:38:20 AM: edges-coref-ontonotes_f1: training: 0.890372 validation: 0.879701
09/17 12:38:20 AM: Global learning rate: 0.0001
09/17 12:38:20 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:38:25 AM: Update 14103: task edges-coref-ontonotes, batch 103 (14103): mcc: 0.7978, acc: 0.8954, precision: 0.8990, recall: 0.8987, f1: 0.8989, edges-coref-ontonotes_loss: 0.1874
09/17 12:38:36 AM: Update 14389: task edges-coref-ontonotes, batch 389 (14389): mcc: 0.8118, acc: 0.9025, precision: 0.9060, recall: 0.9058, f1: 0.9059, edges-coref-ontonotes_loss: 0.1916
09/17 12:38:48 AM: Update 14702: task edges-coref-ontonotes, batch 702 (14702): mcc: 0.8092, acc: 0.9014, precision: 0.9045, recall: 0.9047, f1: 0.9046, edges-coref-ontonotes_loss: 0.2004
09/17 12:38:58 AM: Update 14999: task edges-coref-ontonotes, batch 999 (14999): mcc: 0.7945, acc: 0.8937, precision: 0.8972, recall: 0.8973, f1: 0.8973, edges-coref-ontonotes_loss: 0.2213
09/17 12:38:58 AM: ***** Step 15000 / Validation 15 *****
09/17 12:38:58 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:38:58 AM: Validating...
09/17 12:39:04 AM: Updating LR scheduler:
09/17 12:39:04 AM: 	Best result seen so far for macro_avg: 0.885
09/17 12:39:04 AM: 	# validation passes without improvement: 2
09/17 12:39:04 AM: edges-coref-ontonotes_loss: training: 0.221368 validation: 0.290339
09/17 12:39:04 AM: macro_avg: validation: 0.881039
09/17 12:39:04 AM: micro_avg: validation: 0.000000
09/17 12:39:04 AM: edges-coref-ontonotes_mcc: training: 0.794408 validation: 0.762100
09/17 12:39:04 AM: edges-coref-ontonotes_acc: training: 0.893639 validation: 0.880265
09/17 12:39:04 AM: edges-coref-ontonotes_precision: training: 0.897166 validation: 0.881123
09/17 12:39:04 AM: edges-coref-ontonotes_recall: training: 0.897252 validation: 0.880954
09/17 12:39:04 AM: edges-coref-ontonotes_f1: training: 0.897209 validation: 0.881039
09/17 12:39:04 AM: Global learning rate: 0.0001
09/17 12:39:04 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:39:08 AM: Update 15053: task edges-coref-ontonotes, batch 53 (15053): mcc: 0.7646, acc: 0.8788, precision: 0.8824, recall: 0.8823, f1: 0.8823, edges-coref-ontonotes_loss: 0.2459
09/17 12:39:19 AM: Update 15328: task edges-coref-ontonotes, batch 328 (15328): mcc: 0.7851, acc: 0.8892, precision: 0.8925, recall: 0.8927, f1: 0.8926, edges-coref-ontonotes_loss: 0.2368
09/17 12:39:29 AM: Update 15641: task edges-coref-ontonotes, batch 641 (15641): mcc: 0.8064, acc: 0.9000, precision: 0.9033, recall: 0.9032, f1: 0.9032, edges-coref-ontonotes_loss: 0.2045
09/17 12:39:39 AM: Update 15893: task edges-coref-ontonotes, batch 893 (15893): mcc: 0.8020, acc: 0.8978, precision: 0.9011, recall: 0.9010, f1: 0.9010, edges-coref-ontonotes_loss: 0.2120
09/17 12:39:42 AM: ***** Step 16000 / Validation 16 *****
09/17 12:39:42 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:39:42 AM: Validating...
09/17 12:39:48 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:39:48 AM: Best result seen so far for macro.
09/17 12:39:48 AM: Updating LR scheduler:
09/17 12:39:48 AM: 	Best result seen so far for macro_avg: 0.885
09/17 12:39:48 AM: 	# validation passes without improvement: 0
09/17 12:39:48 AM: edges-coref-ontonotes_loss: training: 0.209651 validation: 0.293325
09/17 12:39:48 AM: macro_avg: validation: 0.885311
09/17 12:39:48 AM: micro_avg: validation: 0.000000
09/17 12:39:48 AM: edges-coref-ontonotes_mcc: training: 0.803806 validation: 0.770639
09/17 12:39:48 AM: edges-coref-ontonotes_acc: training: 0.898816 validation: 0.884745
09/17 12:39:48 AM: edges-coref-ontonotes_precision: training: 0.901936 validation: 0.885378
09/17 12:39:48 AM: edges-coref-ontonotes_recall: training: 0.901862 validation: 0.885243
09/17 12:39:48 AM: edges-coref-ontonotes_f1: training: 0.901899 validation: 0.885311
09/17 12:39:48 AM: Global learning rate: 0.0001
09/17 12:39:48 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:39:51 AM: Update 16010: task edges-coref-ontonotes, batch 10 (16010): mcc: 0.7796, acc: 0.8866, precision: 0.8904, recall: 0.8890, f1: 0.8897, edges-coref-ontonotes_loss: 0.1918
09/17 12:40:01 AM: Update 16305: task edges-coref-ontonotes, batch 305 (16305): mcc: 0.7657, acc: 0.8790, precision: 0.8830, recall: 0.8827, f1: 0.8829, edges-coref-ontonotes_loss: 0.2661
09/17 12:40:11 AM: Update 16537: task edges-coref-ontonotes, batch 537 (16537): mcc: 0.7748, acc: 0.8836, precision: 0.8874, recall: 0.8874, f1: 0.8874, edges-coref-ontonotes_loss: 0.2530
09/17 12:40:21 AM: Update 16795: task edges-coref-ontonotes, batch 795 (16795): mcc: 0.7873, acc: 0.8902, precision: 0.8937, recall: 0.8936, f1: 0.8937, edges-coref-ontonotes_loss: 0.2334
09/17 12:40:28 AM: ***** Step 17000 / Validation 17 *****
09/17 12:40:28 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:40:28 AM: Validating...
09/17 12:40:31 AM: Evaluate: task edges-coref-ontonotes, batch 84 (157): mcc: 0.7727, acc: 0.8855, precision: 0.8862, recall: 0.8865, f1: 0.8863, edges-coref-ontonotes_loss: 0.2907
09/17 12:40:34 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:40:34 AM: Best result seen so far for macro.
09/17 12:40:34 AM: Updating LR scheduler:
09/17 12:40:34 AM: 	Best result seen so far for macro_avg: 0.887
09/17 12:40:34 AM: 	# validation passes without improvement: 0
09/17 12:40:34 AM: edges-coref-ontonotes_loss: training: 0.224922 validation: 0.278533
09/17 12:40:34 AM: macro_avg: validation: 0.886898
09/17 12:40:34 AM: micro_avg: validation: 0.000000
09/17 12:40:34 AM: edges-coref-ontonotes_mcc: training: 0.791983 validation: 0.773779
09/17 12:40:34 AM: edges-coref-ontonotes_acc: training: 0.892656 validation: 0.886200
09/17 12:40:34 AM: edges-coref-ontonotes_precision: training: 0.896029 validation: 0.886830
09/17 12:40:34 AM: edges-coref-ontonotes_recall: training: 0.895944 validation: 0.886966
09/17 12:40:34 AM: edges-coref-ontonotes_f1: training: 0.895986 validation: 0.886898
09/17 12:40:34 AM: Global learning rate: 0.0001
09/17 12:40:34 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:40:41 AM: Update 17172: task edges-coref-ontonotes, batch 172 (17172): mcc: 0.8074, acc: 0.9009, precision: 0.9041, recall: 0.9032, f1: 0.9037, edges-coref-ontonotes_loss: 0.2018
09/17 12:40:51 AM: Update 17408: task edges-coref-ontonotes, batch 408 (17408): mcc: 0.7979, acc: 0.8958, precision: 0.8993, recall: 0.8985, f1: 0.8989, edges-coref-ontonotes_loss: 0.2196
09/17 12:41:01 AM: Update 17651: task edges-coref-ontonotes, batch 651 (17651): mcc: 0.7849, acc: 0.8891, precision: 0.8927, recall: 0.8921, f1: 0.8924, edges-coref-ontonotes_loss: 0.2384
09/17 12:41:12 AM: Update 17944: task edges-coref-ontonotes, batch 944 (17944): mcc: 0.7872, acc: 0.8903, precision: 0.8937, recall: 0.8935, f1: 0.8936, edges-coref-ontonotes_loss: 0.2353
09/17 12:41:14 AM: ***** Step 18000 / Validation 18 *****
09/17 12:41:14 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:41:14 AM: Validating...
09/17 12:41:20 AM: Updating LR scheduler:
09/17 12:41:20 AM: 	Best result seen so far for macro_avg: 0.887
09/17 12:41:20 AM: 	# validation passes without improvement: 1
09/17 12:41:20 AM: edges-coref-ontonotes_loss: training: 0.232677 validation: 0.284280
09/17 12:41:20 AM: macro_avg: validation: 0.886813
09/17 12:41:20 AM: micro_avg: validation: 0.000000
09/17 12:41:20 AM: edges-coref-ontonotes_mcc: training: 0.787826 validation: 0.773625
09/17 12:41:20 AM: edges-coref-ontonotes_acc: training: 0.890678 validation: 0.886009
09/17 12:41:20 AM: edges-coref-ontonotes_precision: training: 0.893978 validation: 0.886813
09/17 12:41:20 AM: edges-coref-ontonotes_recall: training: 0.893831 validation: 0.886813
09/17 12:41:20 AM: edges-coref-ontonotes_f1: training: 0.893904 validation: 0.886813
09/17 12:41:20 AM: Global learning rate: 0.0001
09/17 12:41:20 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:41:22 AM: Update 18073: task edges-coref-ontonotes, batch 73 (18073): mcc: 0.8499, acc: 0.9230, precision: 0.9248, recall: 0.9251, f1: 0.9249, edges-coref-ontonotes_loss: 0.1606
09/17 12:41:32 AM: Update 18326: task edges-coref-ontonotes, batch 326 (18326): mcc: 0.8223, acc: 0.9088, precision: 0.9112, recall: 0.9112, f1: 0.9112, edges-coref-ontonotes_loss: 0.1838
09/17 12:41:44 AM: Update 18626: task edges-coref-ontonotes, batch 626 (18626): mcc: 0.8184, acc: 0.9068, precision: 0.9093, recall: 0.9091, f1: 0.9092, edges-coref-ontonotes_loss: 0.1924
09/17 12:41:54 AM: Update 18922: task edges-coref-ontonotes, batch 922 (18922): mcc: 0.8021, acc: 0.8982, precision: 0.9011, recall: 0.9010, f1: 0.9010, edges-coref-ontonotes_loss: 0.2156
09/17 12:41:59 AM: ***** Step 19000 / Validation 19 *****
09/17 12:41:59 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:41:59 AM: Validating...
09/17 12:42:04 AM: Evaluate: task edges-coref-ontonotes, batch 141 (157): mcc: 0.7690, acc: 0.8838, precision: 0.8846, recall: 0.8844, f1: 0.8845, edges-coref-ontonotes_loss: 0.2900
09/17 12:42:05 AM: Updating LR scheduler:
09/17 12:42:05 AM: 	Best result seen so far for macro_avg: 0.887
09/17 12:42:05 AM: 	# validation passes without improvement: 2
09/17 12:42:05 AM: edges-coref-ontonotes_loss: training: 0.215735 validation: 0.295000
09/17 12:42:05 AM: macro_avg: validation: 0.882387
09/17 12:42:05 AM: micro_avg: validation: 0.000000
09/17 12:42:05 AM: edges-coref-ontonotes_mcc: training: 0.801813 validation: 0.764819
09/17 12:42:05 AM: edges-coref-ontonotes_acc: training: 0.898066 validation: 0.881720
09/17 12:42:05 AM: edges-coref-ontonotes_precision: training: 0.900930 validation: 0.882556
09/17 12:42:05 AM: edges-coref-ontonotes_recall: training: 0.900878 validation: 0.882218
09/17 12:42:05 AM: edges-coref-ontonotes_f1: training: 0.900904 validation: 0.882387
09/17 12:42:05 AM: Global learning rate: 0.0001
09/17 12:42:05 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:42:15 AM: Update 19252: task edges-coref-ontonotes, batch 252 (19252): mcc: 0.7917, acc: 0.8927, precision: 0.8960, recall: 0.8956, f1: 0.8958, edges-coref-ontonotes_loss: 0.2328
09/17 12:42:25 AM: Update 19596: task edges-coref-ontonotes, batch 596 (19596): mcc: 0.8124, acc: 0.9034, precision: 0.9064, recall: 0.9059, f1: 0.9062, edges-coref-ontonotes_loss: 0.1997
09/17 12:42:35 AM: Update 19876: task edges-coref-ontonotes, batch 876 (19876): mcc: 0.8099, acc: 0.9023, precision: 0.9050, recall: 0.9049, f1: 0.9049, edges-coref-ontonotes_loss: 0.2020
09/17 12:42:41 AM: ***** Step 20000 / Validation 20 *****
09/17 12:42:41 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:42:41 AM: Validating...
09/17 12:42:45 AM: Evaluate: task edges-coref-ontonotes, batch 111 (157): mcc: 0.7698, acc: 0.8845, precision: 0.8850, recall: 0.8847, f1: 0.8849, edges-coref-ontonotes_loss: 0.2976
09/17 12:42:46 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:42:46 AM: Best result seen so far for macro.
09/17 12:42:46 AM: Updating LR scheduler:
09/17 12:42:46 AM: 	Best result seen so far for macro_avg: 0.888
09/17 12:42:46 AM: 	# validation passes without improvement: 0
09/17 12:42:46 AM: edges-coref-ontonotes_loss: training: 0.207041 validation: 0.281655
09/17 12:42:46 AM: macro_avg: validation: 0.888268
09/17 12:42:46 AM: micro_avg: validation: 0.000000
09/17 12:42:46 AM: edges-coref-ontonotes_mcc: training: 0.807749 validation: 0.776574
09/17 12:42:46 AM: edges-coref-ontonotes_acc: training: 0.901246 validation: 0.887885
09/17 12:42:46 AM: edges-coref-ontonotes_precision: training: 0.903946 validation: 0.888421
09/17 12:42:46 AM: edges-coref-ontonotes_recall: training: 0.903786 validation: 0.888115
09/17 12:42:46 AM: edges-coref-ontonotes_f1: training: 0.903866 validation: 0.888268
09/17 12:42:46 AM: Global learning rate: 0.0001
09/17 12:42:46 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:42:56 AM: Update 20247: task edges-coref-ontonotes, batch 247 (20247): mcc: 0.7711, acc: 0.8822, precision: 0.8859, recall: 0.8852, f1: 0.8855, edges-coref-ontonotes_loss: 0.2600
09/17 12:43:08 AM: Update 20560: task edges-coref-ontonotes, batch 560 (20560): mcc: 0.7826, acc: 0.8882, precision: 0.8915, recall: 0.8911, f1: 0.8913, edges-coref-ontonotes_loss: 0.2423
09/17 12:43:18 AM: Update 20889: task edges-coref-ontonotes, batch 889 (20889): mcc: 0.8001, acc: 0.8974, precision: 0.9001, recall: 0.9000, f1: 0.9001, edges-coref-ontonotes_loss: 0.2152
09/17 12:43:23 AM: ***** Step 21000 / Validation 21 *****
09/17 12:43:23 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:43:23 AM: Validating...
09/17 12:43:28 AM: Evaluate: task edges-coref-ontonotes, batch 141 (157): mcc: 0.7830, acc: 0.8908, precision: 0.8915, recall: 0.8914, f1: 0.8915, edges-coref-ontonotes_loss: 0.2790
09/17 12:43:29 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:43:29 AM: Best result seen so far for macro.
09/17 12:43:29 AM: Updating LR scheduler:
09/17 12:43:29 AM: 	Best result seen so far for macro_avg: 0.889
09/17 12:43:29 AM: 	# validation passes without improvement: 0
09/17 12:43:29 AM: edges-coref-ontonotes_loss: training: 0.217218 validation: 0.281599
09/17 12:43:29 AM: macro_avg: validation: 0.888944
09/17 12:43:29 AM: micro_avg: validation: 0.000000
09/17 12:43:29 AM: edges-coref-ontonotes_mcc: training: 0.797907 validation: 0.777914
09/17 12:43:29 AM: edges-coref-ontonotes_acc: training: 0.896203 validation: 0.888306
09/17 12:43:29 AM: edges-coref-ontonotes_precision: training: 0.898976 validation: 0.889046
09/17 12:43:29 AM: edges-coref-ontonotes_recall: training: 0.898925 validation: 0.888842
09/17 12:43:29 AM: edges-coref-ontonotes_f1: training: 0.898951 validation: 0.888944
09/17 12:43:29 AM: Global learning rate: 0.0001
09/17 12:43:29 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:43:38 AM: Update 21242: task edges-coref-ontonotes, batch 242 (21242): mcc: 0.8167, acc: 0.9064, precision: 0.9085, recall: 0.9082, f1: 0.9084, edges-coref-ontonotes_loss: 0.1980
09/17 12:43:50 AM: Update 21555: task edges-coref-ontonotes, batch 555 (21555): mcc: 0.7921, acc: 0.8934, precision: 0.8961, recall: 0.8960, f1: 0.8960, edges-coref-ontonotes_loss: 0.2318
09/17 12:44:01 AM: Update 21868: task edges-coref-ontonotes, batch 868 (21868): mcc: 0.7932, acc: 0.8939, precision: 0.8966, recall: 0.8966, f1: 0.8966, edges-coref-ontonotes_loss: 0.2297
09/17 12:44:05 AM: ***** Step 22000 / Validation 22 *****
09/17 12:44:05 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:44:05 AM: Validating...
09/17 12:44:11 AM: Updating LR scheduler:
09/17 12:44:11 AM: 	Best result seen so far for macro_avg: 0.889
09/17 12:44:11 AM: 	# validation passes without improvement: 1
09/17 12:44:11 AM: edges-coref-ontonotes_loss: training: 0.220414 validation: 0.279807
09/17 12:44:11 AM: macro_avg: validation: 0.888102
09/17 12:44:11 AM: micro_avg: validation: 0.000000
09/17 12:44:11 AM: edges-coref-ontonotes_mcc: training: 0.799080 validation: 0.776115
09/17 12:44:11 AM: edges-coref-ontonotes_acc: training: 0.896971 validation: 0.887042
09/17 12:44:11 AM: edges-coref-ontonotes_precision: training: 0.899560 validation: 0.887745
09/17 12:44:11 AM: edges-coref-ontonotes_recall: training: 0.899515 validation: 0.888459
09/17 12:44:11 AM: edges-coref-ontonotes_f1: training: 0.899537 validation: 0.888102
09/17 12:44:11 AM: Global learning rate: 0.0001
09/17 12:44:11 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:44:11 AM: Update 22028: task edges-coref-ontonotes, batch 28 (22028): mcc: 0.8818, acc: 0.9398, precision: 0.9412, recall: 0.9406, f1: 0.9409, edges-coref-ontonotes_loss: 0.1336
09/17 12:44:21 AM: Update 22269: task edges-coref-ontonotes, batch 269 (22269): mcc: 0.8115, acc: 0.9034, precision: 0.9057, recall: 0.9058, f1: 0.9057, edges-coref-ontonotes_loss: 0.1941
09/17 12:44:33 AM: Update 22550: task edges-coref-ontonotes, batch 550 (22550): mcc: 0.8140, acc: 0.9047, precision: 0.9069, recall: 0.9071, f1: 0.9070, edges-coref-ontonotes_loss: 0.1949
09/17 12:44:43 AM: Update 22861: task edges-coref-ontonotes, batch 861 (22861): mcc: 0.8000, acc: 0.8974, precision: 0.9000, recall: 0.9001, f1: 0.9000, edges-coref-ontonotes_loss: 0.2166
09/17 12:44:49 AM: ***** Step 23000 / Validation 23 *****
09/17 12:44:49 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:44:49 AM: Validating...
09/17 12:44:53 AM: Evaluate: task edges-coref-ontonotes, batch 92 (157): mcc: 0.7674, acc: 0.8833, precision: 0.8838, recall: 0.8836, f1: 0.8837, edges-coref-ontonotes_loss: 0.3008
09/17 12:44:55 AM: Updating LR scheduler:
09/17 12:44:55 AM: 	Best result seen so far for macro_avg: 0.889
09/17 12:44:55 AM: 	# validation passes without improvement: 2
09/17 12:44:55 AM: edges-coref-ontonotes_loss: training: 0.218548 validation: 0.283364
09/17 12:44:55 AM: macro_avg: validation: 0.885949
09/17 12:44:55 AM: micro_avg: validation: 0.000000
09/17 12:44:55 AM: edges-coref-ontonotes_mcc: training: 0.799074 validation: 0.771902
09/17 12:44:55 AM: edges-coref-ontonotes_acc: training: 0.896940 validation: 0.885549
09/17 12:44:55 AM: edges-coref-ontonotes_precision: training: 0.899462 validation: 0.885966
09/17 12:44:55 AM: edges-coref-ontonotes_recall: training: 0.899631 validation: 0.885932
09/17 12:44:55 AM: edges-coref-ontonotes_f1: training: 0.899546 validation: 0.885949
09/17 12:44:55 AM: Global learning rate: 0.0001
09/17 12:44:55 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:45:03 AM: Update 23176: task edges-coref-ontonotes, batch 176 (23176): mcc: 0.7935, acc: 0.8944, precision: 0.8966, recall: 0.8970, f1: 0.8968, edges-coref-ontonotes_loss: 0.2254
09/17 12:45:13 AM: Update 23499: task edges-coref-ontonotes, batch 499 (23499): mcc: 0.8191, acc: 0.9073, precision: 0.9095, recall: 0.9096, f1: 0.9095, edges-coref-ontonotes_loss: 0.1870
09/17 12:45:23 AM: Update 23765: task edges-coref-ontonotes, batch 765 (23765): mcc: 0.8152, acc: 0.9053, precision: 0.9077, recall: 0.9075, f1: 0.9076, edges-coref-ontonotes_loss: 0.1940
09/17 12:45:32 AM: ***** Step 24000 / Validation 24 *****
09/17 12:45:32 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:45:32 AM: Validating...
09/17 12:45:33 AM: Evaluate: task edges-coref-ontonotes, batch 19 (157): mcc: 0.8186, acc: 0.9086, precision: 0.9093, recall: 0.9093, f1: 0.9093, edges-coref-ontonotes_loss: 0.2531
09/17 12:45:38 AM: Updating LR scheduler:
09/17 12:45:38 AM: 	Best result seen so far for macro_avg: 0.889
09/17 12:45:38 AM: 	# validation passes without improvement: 3
09/17 12:45:38 AM: edges-coref-ontonotes_loss: training: 0.203958 validation: 0.282485
09/17 12:45:38 AM: macro_avg: validation: 0.886106
09/17 12:45:38 AM: micro_avg: validation: 0.000000
09/17 12:45:38 AM: edges-coref-ontonotes_mcc: training: 0.809469 validation: 0.772247
09/17 12:45:38 AM: edges-coref-ontonotes_acc: training: 0.902404 validation: 0.885511
09/17 12:45:38 AM: edges-coref-ontonotes_precision: training: 0.904891 validation: 0.886242
09/17 12:45:38 AM: edges-coref-ontonotes_recall: training: 0.904541 validation: 0.885970
09/17 12:45:38 AM: edges-coref-ontonotes_f1: training: 0.904716 validation: 0.886106
09/17 12:45:38 AM: Global learning rate: 0.0001
09/17 12:45:38 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:45:43 AM: Update 24160: task edges-coref-ontonotes, batch 160 (24160): mcc: 0.7793, acc: 0.8868, precision: 0.8898, recall: 0.8894, f1: 0.8896, edges-coref-ontonotes_loss: 0.2489
09/17 12:45:53 AM: Update 24417: task edges-coref-ontonotes, batch 417 (24417): mcc: 0.7906, acc: 0.8927, precision: 0.8955, recall: 0.8951, f1: 0.8953, edges-coref-ontonotes_loss: 0.2348
09/17 12:46:03 AM: Update 24666: task edges-coref-ontonotes, batch 666 (24666): mcc: 0.8010, acc: 0.8983, precision: 0.9006, recall: 0.9004, f1: 0.9005, edges-coref-ontonotes_loss: 0.2143
09/17 12:46:13 AM: Update 24928: task edges-coref-ontonotes, batch 928 (24928): mcc: 0.8038, acc: 0.8997, precision: 0.9020, recall: 0.9018, f1: 0.9019, edges-coref-ontonotes_loss: 0.2083
09/17 12:46:15 AM: ***** Step 25000 / Validation 25 *****
09/17 12:46:15 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:46:15 AM: Validating...
09/17 12:46:21 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:46:21 AM: Best result seen so far for macro.
09/17 12:46:21 AM: Updating LR scheduler:
09/17 12:46:21 AM: 	Best result seen so far for macro_avg: 0.889
09/17 12:46:21 AM: 	# validation passes without improvement: 0
09/17 12:46:21 AM: edges-coref-ontonotes_loss: training: 0.207106 validation: 0.280978
09/17 12:46:21 AM: macro_avg: validation: 0.889408
09/17 12:46:21 AM: micro_avg: validation: 0.000000
09/17 12:46:21 AM: edges-coref-ontonotes_mcc: training: 0.804644 validation: 0.778871
09/17 12:46:21 AM: edges-coref-ontonotes_acc: training: 0.900155 validation: 0.888957
09/17 12:46:21 AM: edges-coref-ontonotes_precision: training: 0.902383 validation: 0.889630
09/17 12:46:21 AM: edges-coref-ontonotes_recall: training: 0.902246 validation: 0.889187
09/17 12:46:21 AM: edges-coref-ontonotes_f1: training: 0.902315 validation: 0.889408
09/17 12:46:21 AM: Global learning rate: 0.0001
09/17 12:46:21 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:46:23 AM: Update 25059: task edges-coref-ontonotes, batch 59 (25059): mcc: 0.8185, acc: 0.9072, precision: 0.9091, recall: 0.9093, f1: 0.9092, edges-coref-ontonotes_loss: 0.1935
09/17 12:46:33 AM: Update 25290: task edges-coref-ontonotes, batch 290 (25290): mcc: 0.8022, acc: 0.8988, precision: 0.9012, recall: 0.9010, f1: 0.9011, edges-coref-ontonotes_loss: 0.2218
09/17 12:46:43 AM: Update 25521: task edges-coref-ontonotes, batch 521 (25521): mcc: 0.7919, acc: 0.8935, precision: 0.8960, recall: 0.8959, f1: 0.8959, edges-coref-ontonotes_loss: 0.2329
09/17 12:46:53 AM: Update 25794: task edges-coref-ontonotes, batch 794 (25794): mcc: 0.7922, acc: 0.8937, precision: 0.8961, recall: 0.8961, f1: 0.8961, edges-coref-ontonotes_loss: 0.2291
09/17 12:46:59 AM: ***** Step 26000 / Validation 26 *****
09/17 12:46:59 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:46:59 AM: Validating...
09/17 12:47:03 AM: Evaluate: task edges-coref-ontonotes, batch 93 (157): mcc: 0.7749, acc: 0.8870, precision: 0.8874, recall: 0.8876, f1: 0.8875, edges-coref-ontonotes_loss: 0.2986
09/17 12:47:05 AM: Best result seen so far for edges-coref-ontonotes.
09/17 12:47:05 AM: Best result seen so far for macro.
09/17 12:47:05 AM: Updating LR scheduler:
09/17 12:47:05 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:47:05 AM: 	# validation passes without improvement: 0
09/17 12:47:05 AM: edges-coref-ontonotes_loss: training: 0.214131 validation: 0.281936
09/17 12:47:05 AM: macro_avg: validation: 0.889595
09/17 12:47:05 AM: micro_avg: validation: 0.000000
09/17 12:47:05 AM: edges-coref-ontonotes_mcc: training: 0.801618 validation: 0.779178
09/17 12:47:05 AM: edges-coref-ontonotes_acc: training: 0.898608 validation: 0.889225
09/17 12:47:05 AM: edges-coref-ontonotes_precision: training: 0.900829 validation: 0.889544
09/17 12:47:05 AM: edges-coref-ontonotes_recall: training: 0.900784 validation: 0.889646
09/17 12:47:05 AM: edges-coref-ontonotes_f1: training: 0.900807 validation: 0.889595
09/17 12:47:05 AM: Global learning rate: 0.0001
09/17 12:47:05 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:47:13 AM: Update 26197: task edges-coref-ontonotes, batch 197 (26197): mcc: 0.8165, acc: 0.9068, precision: 0.9083, recall: 0.9082, f1: 0.9083, edges-coref-ontonotes_loss: 0.1976
09/17 12:47:24 AM: Update 26474: task edges-coref-ontonotes, batch 474 (26474): mcc: 0.8181, acc: 0.9074, precision: 0.9090, recall: 0.9091, f1: 0.9091, edges-coref-ontonotes_loss: 0.1960
09/17 12:47:34 AM: Update 26767: task edges-coref-ontonotes, batch 767 (26767): mcc: 0.8019, acc: 0.8991, precision: 0.9009, recall: 0.9010, f1: 0.9009, edges-coref-ontonotes_loss: 0.2161
09/17 12:47:43 AM: ***** Step 27000 / Validation 27 *****
09/17 12:47:43 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:47:43 AM: Validating...
09/17 12:47:44 AM: Evaluate: task edges-coref-ontonotes, batch 12 (157): mcc: 0.8064, acc: 0.9026, precision: 0.9032, recall: 0.9032, f1: 0.9032, edges-coref-ontonotes_loss: 0.2617
09/17 12:47:49 AM: Updating LR scheduler:
09/17 12:47:49 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:47:49 AM: 	# validation passes without improvement: 1
09/17 12:47:49 AM: edges-coref-ontonotes_loss: training: 0.217880 validation: 0.285888
09/17 12:47:49 AM: macro_avg: validation: 0.884158
09/17 12:47:49 AM: micro_avg: validation: 0.000000
09/17 12:47:49 AM: edges-coref-ontonotes_mcc: training: 0.800198 validation: 0.768303
09/17 12:47:49 AM: edges-coref-ontonotes_acc: training: 0.898311 validation: 0.883749
09/17 12:47:49 AM: edges-coref-ontonotes_precision: training: 0.900022 validation: 0.884107
09/17 12:47:49 AM: edges-coref-ontonotes_recall: training: 0.900195 validation: 0.884209
09/17 12:47:49 AM: edges-coref-ontonotes_f1: training: 0.900109 validation: 0.884158
09/17 12:47:49 AM: Global learning rate: 0.0001
09/17 12:47:49 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:47:54 AM: Update 27100: task edges-coref-ontonotes, batch 100 (27100): mcc: 0.8027, acc: 0.8992, precision: 0.9011, recall: 0.9017, f1: 0.9014, edges-coref-ontonotes_loss: 0.2195
09/17 12:48:04 AM: Update 27422: task edges-coref-ontonotes, batch 422 (27422): mcc: 0.8284, acc: 0.9124, precision: 0.9141, recall: 0.9142, f1: 0.9142, edges-coref-ontonotes_loss: 0.1753
09/17 12:48:14 AM: Update 27695: task edges-coref-ontonotes, batch 695 (27695): mcc: 0.8203, acc: 0.9083, precision: 0.9102, recall: 0.9101, f1: 0.9101, edges-coref-ontonotes_loss: 0.1882
09/17 12:48:24 AM: Update 27948: task edges-coref-ontonotes, batch 948 (27948): mcc: 0.8123, acc: 0.9043, precision: 0.9063, recall: 0.9060, f1: 0.9062, edges-coref-ontonotes_loss: 0.1988
09/17 12:48:26 AM: ***** Step 28000 / Validation 28 *****
09/17 12:48:26 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:48:26 AM: Validating...
09/17 12:48:32 AM: Updating LR scheduler:
09/17 12:48:32 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:48:32 AM: 	# validation passes without improvement: 2
09/17 12:48:32 AM: edges-coref-ontonotes_loss: training: 0.201037 validation: 0.282871
09/17 12:48:32 AM: macro_avg: validation: 0.886706
09/17 12:48:32 AM: micro_avg: validation: 0.000000
09/17 12:48:32 AM: edges-coref-ontonotes_mcc: training: 0.811527 validation: 0.773434
09/17 12:48:32 AM: edges-coref-ontonotes_acc: training: 0.903885 validation: 0.886277
09/17 12:48:32 AM: edges-coref-ontonotes_precision: training: 0.905892 validation: 0.886791
09/17 12:48:32 AM: edges-coref-ontonotes_recall: training: 0.905605 validation: 0.886621
09/17 12:48:32 AM: edges-coref-ontonotes_f1: training: 0.905749 validation: 0.886706
09/17 12:48:32 AM: Global learning rate: 0.0001
09/17 12:48:32 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:48:34 AM: Update 28081: task edges-coref-ontonotes, batch 81 (28081): mcc: 0.7868, acc: 0.8914, precision: 0.8934, recall: 0.8934, f1: 0.8934, edges-coref-ontonotes_loss: 0.2418
09/17 12:48:44 AM: Update 28322: task edges-coref-ontonotes, batch 322 (28322): mcc: 0.7944, acc: 0.8953, precision: 0.8972, recall: 0.8972, f1: 0.8972, edges-coref-ontonotes_loss: 0.2272
09/17 12:48:54 AM: Update 28587: task edges-coref-ontonotes, batch 587 (28587): mcc: 0.8087, acc: 0.9026, precision: 0.9045, recall: 0.9043, f1: 0.9044, edges-coref-ontonotes_loss: 0.2047
09/17 12:49:04 AM: Update 28841: task edges-coref-ontonotes, batch 841 (28841): mcc: 0.8113, acc: 0.9039, precision: 0.9057, recall: 0.9056, f1: 0.9057, edges-coref-ontonotes_loss: 0.1979
09/17 12:49:09 AM: ***** Step 29000 / Validation 29 *****
09/17 12:49:09 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:49:09 AM: Validating...
09/17 12:49:14 AM: Evaluate: task edges-coref-ontonotes, batch 146 (157): mcc: 0.7805, acc: 0.8897, precision: 0.8903, recall: 0.8902, f1: 0.8902, edges-coref-ontonotes_loss: 0.2794
09/17 12:49:15 AM: Updating LR scheduler:
09/17 12:49:15 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:49:15 AM: 	# validation passes without improvement: 3
09/17 12:49:15 AM: edges-coref-ontonotes_loss: training: 0.197151 validation: 0.281676
09/17 12:49:15 AM: macro_avg: validation: 0.888931
09/17 12:49:15 AM: micro_avg: validation: 0.000000
09/17 12:49:15 AM: edges-coref-ontonotes_mcc: training: 0.813481 validation: 0.777876
09/17 12:49:15 AM: edges-coref-ontonotes_acc: training: 0.905094 validation: 0.888459
09/17 12:49:15 AM: edges-coref-ontonotes_precision: training: 0.906746 validation: 0.888982
09/17 12:49:15 AM: edges-coref-ontonotes_recall: training: 0.906734 validation: 0.888880
09/17 12:49:15 AM: edges-coref-ontonotes_f1: training: 0.906740 validation: 0.888931
09/17 12:49:15 AM: Global learning rate: 0.0001
09/17 12:49:15 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:49:24 AM: Update 29235: task edges-coref-ontonotes, batch 235 (29235): mcc: 0.7982, acc: 0.8971, precision: 0.8994, recall: 0.8988, f1: 0.8991, edges-coref-ontonotes_loss: 0.2213
09/17 12:49:34 AM: Update 29481: task edges-coref-ontonotes, batch 481 (29481): mcc: 0.7897, acc: 0.8928, precision: 0.8950, recall: 0.8946, f1: 0.8948, edges-coref-ontonotes_loss: 0.2323
09/17 12:49:44 AM: Update 29745: task edges-coref-ontonotes, batch 745 (29745): mcc: 0.7933, acc: 0.8947, precision: 0.8967, recall: 0.8965, f1: 0.8966, edges-coref-ontonotes_loss: 0.2257
09/17 12:49:52 AM: ***** Step 30000 / Validation 30 *****
09/17 12:49:52 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:49:52 AM: Validating...
09/17 12:49:54 AM: Evaluate: task edges-coref-ontonotes, batch 78 (157): mcc: 0.7784, acc: 0.8889, precision: 0.8892, recall: 0.8892, f1: 0.8892, edges-coref-ontonotes_loss: 0.2972
09/17 12:49:57 AM: Updating LR scheduler:
09/17 12:49:57 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:49:57 AM: 	# validation passes without improvement: 0
09/17 12:49:57 AM: edges-coref-ontonotes_loss: training: 0.207799 validation: 0.284257
09/17 12:49:57 AM: macro_avg: validation: 0.889365
09/17 12:49:57 AM: micro_avg: validation: 0.000000
09/17 12:49:57 AM: edges-coref-ontonotes_mcc: training: 0.805326 validation: 0.778718
09/17 12:49:57 AM: edges-coref-ontonotes_acc: training: 0.900837 validation: 0.888919
09/17 12:49:57 AM: edges-coref-ontonotes_precision: training: 0.902766 validation: 0.889314
09/17 12:49:57 AM: edges-coref-ontonotes_recall: training: 0.902535 validation: 0.889416
09/17 12:49:57 AM: edges-coref-ontonotes_f1: training: 0.902651 validation: 0.889365
09/17 12:49:57 AM: Global learning rate: 5e-05
09/17 12:49:57 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:50:04 AM: Update 30171: task edges-coref-ontonotes, batch 171 (30171): mcc: 0.7977, acc: 0.8971, precision: 0.8987, recall: 0.8990, f1: 0.8989, edges-coref-ontonotes_loss: 0.2114
09/17 12:50:14 AM: Update 30432: task edges-coref-ontonotes, batch 432 (30432): mcc: 0.8099, acc: 0.9034, precision: 0.9048, recall: 0.9051, f1: 0.9050, edges-coref-ontonotes_loss: 0.2000
09/17 12:50:26 AM: Update 30711: task edges-coref-ontonotes, batch 711 (30711): mcc: 0.8002, acc: 0.8984, precision: 0.9001, recall: 0.9001, f1: 0.9001, edges-coref-ontonotes_loss: 0.2173
09/17 12:50:35 AM: ***** Step 31000 / Validation 31 *****
09/17 12:50:35 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:50:35 AM: Validating...
09/17 12:50:36 AM: Evaluate: task edges-coref-ontonotes, batch 17 (157): mcc: 0.8153, acc: 0.9076, precision: 0.9076, recall: 0.9076, f1: 0.9076, edges-coref-ontonotes_loss: 0.2598
09/17 12:50:41 AM: Updating LR scheduler:
09/17 12:50:41 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:50:41 AM: 	# validation passes without improvement: 1
09/17 12:50:41 AM: edges-coref-ontonotes_loss: training: 0.217930 validation: 0.282985
09/17 12:50:41 AM: macro_avg: validation: 0.886184
09/17 12:50:41 AM: micro_avg: validation: 0.000000
09/17 12:50:41 AM: edges-coref-ontonotes_mcc: training: 0.800096 validation: 0.772324
09/17 12:50:41 AM: edges-coref-ontonotes_acc: training: 0.898313 validation: 0.885741
09/17 12:50:41 AM: edges-coref-ontonotes_precision: training: 0.900056 validation: 0.886014
09/17 12:50:41 AM: edges-coref-ontonotes_recall: training: 0.900039 validation: 0.886353
09/17 12:50:41 AM: edges-coref-ontonotes_f1: training: 0.900047 validation: 0.886184
09/17 12:50:41 AM: Global learning rate: 5e-05
09/17 12:50:41 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:50:46 AM: Update 31091: task edges-coref-ontonotes, batch 91 (31091): mcc: 0.8240, acc: 0.9103, precision: 0.9121, recall: 0.9118, f1: 0.9120, edges-coref-ontonotes_loss: 0.1664
09/17 12:50:57 AM: Update 31393: task edges-coref-ontonotes, batch 393 (31393): mcc: 0.8288, acc: 0.9131, precision: 0.9144, recall: 0.9144, f1: 0.9144, edges-coref-ontonotes_loss: 0.1723
09/17 12:51:08 AM: Update 31706: task edges-coref-ontonotes, batch 706 (31706): mcc: 0.8253, acc: 0.9112, precision: 0.9126, recall: 0.9126, f1: 0.9126, edges-coref-ontonotes_loss: 0.1795
09/17 12:51:17 AM: ***** Step 32000 / Validation 32 *****
09/17 12:51:17 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:51:17 AM: Validating...
09/17 12:51:18 AM: Evaluate: task edges-coref-ontonotes, batch 25 (157): mcc: 0.8097, acc: 0.9047, precision: 0.9050, recall: 0.9047, f1: 0.9048, edges-coref-ontonotes_loss: 0.2515
09/17 12:51:23 AM: Updating LR scheduler:
09/17 12:51:23 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:51:23 AM: 	# validation passes without improvement: 2
09/17 12:51:23 AM: edges-coref-ontonotes_loss: training: 0.197209 validation: 0.279912
09/17 12:51:23 AM: macro_avg: validation: 0.887183
09/17 12:51:23 AM: micro_avg: validation: 0.000000
09/17 12:51:23 AM: edges-coref-ontonotes_mcc: training: 0.813253 validation: 0.774391
09/17 12:51:23 AM: edges-coref-ontonotes_acc: training: 0.905011 validation: 0.886966
09/17 12:51:23 AM: edges-coref-ontonotes_precision: training: 0.906678 validation: 0.887285
09/17 12:51:23 AM: edges-coref-ontonotes_recall: training: 0.906563 validation: 0.887081
09/17 12:51:23 AM: edges-coref-ontonotes_f1: training: 0.906620 validation: 0.887183
09/17 12:51:23 AM: Global learning rate: 5e-05
09/17 12:51:23 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:51:28 AM: Update 32083: task edges-coref-ontonotes, batch 83 (32083): mcc: 0.7998, acc: 0.8982, precision: 0.8999, recall: 0.8998, f1: 0.8999, edges-coref-ontonotes_loss: 0.2265
09/17 12:51:39 AM: Update 32332: task edges-coref-ontonotes, batch 332 (32332): mcc: 0.7992, acc: 0.8977, precision: 0.8996, recall: 0.8996, f1: 0.8996, edges-coref-ontonotes_loss: 0.2176
09/17 12:51:49 AM: Update 32650: task edges-coref-ontonotes, batch 650 (32650): mcc: 0.8191, acc: 0.9079, precision: 0.9096, recall: 0.9096, f1: 0.9096, edges-coref-ontonotes_loss: 0.1878
09/17 12:51:59 AM: Update 32901: task edges-coref-ontonotes, batch 901 (32901): mcc: 0.8163, acc: 0.9066, precision: 0.9081, recall: 0.9082, f1: 0.9082, edges-coref-ontonotes_loss: 0.1906
09/17 12:52:02 AM: ***** Step 33000 / Validation 33 *****
09/17 12:52:02 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:52:02 AM: Validating...
09/17 12:52:08 AM: Updating LR scheduler:
09/17 12:52:08 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:52:08 AM: 	# validation passes without improvement: 3
09/17 12:52:08 AM: edges-coref-ontonotes_loss: training: 0.190189 validation: 0.281368
09/17 12:52:08 AM: macro_avg: validation: 0.889506
09/17 12:52:08 AM: micro_avg: validation: 0.000000
09/17 12:52:08 AM: edges-coref-ontonotes_mcc: training: 0.817556 validation: 0.778986
09/17 12:52:08 AM: edges-coref-ontonotes_acc: training: 0.907258 validation: 0.889187
09/17 12:52:08 AM: edges-coref-ontonotes_precision: training: 0.908719 validation: 0.889404
09/17 12:52:08 AM: edges-coref-ontonotes_recall: training: 0.908850 validation: 0.889608
09/17 12:52:08 AM: edges-coref-ontonotes_f1: training: 0.908784 validation: 0.889506
09/17 12:52:08 AM: Global learning rate: 5e-05
09/17 12:52:08 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:52:10 AM: Update 33014: task edges-coref-ontonotes, batch 14 (33014): mcc: 0.8228, acc: 0.9107, precision: 0.9108, recall: 0.9121, f1: 0.9114, edges-coref-ontonotes_loss: 0.1778
09/17 12:52:20 AM: Update 33314: task edges-coref-ontonotes, batch 314 (33314): mcc: 0.7892, acc: 0.8928, precision: 0.8945, recall: 0.8947, f1: 0.8946, edges-coref-ontonotes_loss: 0.2369
09/17 12:52:30 AM: Update 33547: task edges-coref-ontonotes, batch 547 (33547): mcc: 0.7957, acc: 0.8961, precision: 0.8978, recall: 0.8979, f1: 0.8979, edges-coref-ontonotes_loss: 0.2279
09/17 12:52:40 AM: Update 33786: task edges-coref-ontonotes, batch 786 (33786): mcc: 0.8021, acc: 0.8994, precision: 0.9011, recall: 0.9011, f1: 0.9011, edges-coref-ontonotes_loss: 0.2148
09/17 12:52:47 AM: ***** Step 34000 / Validation 34 *****
09/17 12:52:47 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:52:47 AM: Validating...
09/17 12:52:50 AM: Evaluate: task edges-coref-ontonotes, batch 91 (157): mcc: 0.7721, acc: 0.8856, precision: 0.8861, recall: 0.8859, f1: 0.8860, edges-coref-ontonotes_loss: 0.2942
09/17 12:52:53 AM: Updating LR scheduler:
09/17 12:52:53 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:52:53 AM: 	# validation passes without improvement: 0
09/17 12:52:53 AM: edges-coref-ontonotes_loss: training: 0.205615 validation: 0.275936
09/17 12:52:53 AM: macro_avg: validation: 0.888838
09/17 12:52:53 AM: micro_avg: validation: 0.000000
09/17 12:52:53 AM: edges-coref-ontonotes_mcc: training: 0.807507 validation: 0.777684
09/17 12:52:53 AM: edges-coref-ontonotes_acc: training: 0.902141 validation: 0.888383
09/17 12:52:53 AM: edges-coref-ontonotes_precision: training: 0.903787 validation: 0.888872
09/17 12:52:53 AM: edges-coref-ontonotes_recall: training: 0.903712 validation: 0.888804
09/17 12:52:53 AM: edges-coref-ontonotes_f1: training: 0.903749 validation: 0.888838
09/17 12:52:53 AM: Global learning rate: 2.5e-05
09/17 12:52:53 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:53:00 AM: Update 34182: task edges-coref-ontonotes, batch 182 (34182): mcc: 0.8184, acc: 0.9078, precision: 0.9090, recall: 0.9094, f1: 0.9092, edges-coref-ontonotes_loss: 0.1911
09/17 12:53:10 AM: Update 34429: task edges-coref-ontonotes, batch 429 (34429): mcc: 0.8129, acc: 0.9051, precision: 0.9064, recall: 0.9066, f1: 0.9065, edges-coref-ontonotes_loss: 0.2002
09/17 12:53:20 AM: Update 34654: task edges-coref-ontonotes, batch 654 (34654): mcc: 0.8030, acc: 0.9000, precision: 0.9014, recall: 0.9016, f1: 0.9015, edges-coref-ontonotes_loss: 0.2136
09/17 12:53:32 AM: Update 34948: task edges-coref-ontonotes, batch 948 (34948): mcc: 0.8030, acc: 0.8999, precision: 0.9015, recall: 0.9016, f1: 0.9015, edges-coref-ontonotes_loss: 0.2135
09/17 12:53:33 AM: ***** Step 35000 / Validation 35 *****
09/17 12:53:33 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:53:33 AM: Validating...
09/17 12:53:39 AM: Updating LR scheduler:
09/17 12:53:39 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:53:39 AM: 	# validation passes without improvement: 1
09/17 12:53:39 AM: edges-coref-ontonotes_loss: training: 0.210863 validation: 0.281037
09/17 12:53:39 AM: macro_avg: validation: 0.889531
09/17 12:53:39 AM: micro_avg: validation: 0.000000
09/17 12:53:39 AM: edges-coref-ontonotes_mcc: training: 0.804042 validation: 0.779101
09/17 12:53:39 AM: edges-coref-ontonotes_acc: training: 0.900465 validation: 0.889148
09/17 12:53:39 AM: edges-coref-ontonotes_precision: training: 0.901981 validation: 0.889685
09/17 12:53:39 AM: edges-coref-ontonotes_recall: training: 0.902071 validation: 0.889378
09/17 12:53:39 AM: edges-coref-ontonotes_f1: training: 0.902026 validation: 0.889531
09/17 12:53:39 AM: Global learning rate: 2.5e-05
09/17 12:53:39 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:53:42 AM: Update 35086: task edges-coref-ontonotes, batch 86 (35086): mcc: 0.8396, acc: 0.9188, precision: 0.9199, recall: 0.9197, f1: 0.9198, edges-coref-ontonotes_loss: 0.1559
09/17 12:53:52 AM: Update 35349: task edges-coref-ontonotes, batch 349 (35349): mcc: 0.8287, acc: 0.9132, precision: 0.9144, recall: 0.9143, f1: 0.9144, edges-coref-ontonotes_loss: 0.1707
09/17 12:54:02 AM: Update 35630: task edges-coref-ontonotes, batch 630 (35630): mcc: 0.8277, acc: 0.9126, precision: 0.9139, recall: 0.9138, f1: 0.9139, edges-coref-ontonotes_loss: 0.1766
09/17 12:54:14 AM: Update 35943: task edges-coref-ontonotes, batch 943 (35943): mcc: 0.8150, acc: 0.9061, precision: 0.9075, recall: 0.9075, f1: 0.9075, edges-coref-ontonotes_loss: 0.1968
09/17 12:54:16 AM: ***** Step 36000 / Validation 36 *****
09/17 12:54:16 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 12:54:16 AM: Validating...
09/17 12:54:21 AM: Updating LR scheduler:
09/17 12:54:21 AM: 	Best result seen so far for macro_avg: 0.890
09/17 12:54:21 AM: 	# validation passes without improvement: 2
09/17 12:54:21 AM: Ran out of early stopping patience. Stopping training.
09/17 12:54:21 AM: edges-coref-ontonotes_loss: training: 0.197802 validation: 0.278702
09/17 12:54:21 AM: macro_avg: validation: 0.889208
09/17 12:54:21 AM: micro_avg: validation: 0.000000
09/17 12:54:21 AM: edges-coref-ontonotes_mcc: training: 0.814282 validation: 0.778450
09/17 12:54:21 AM: edges-coref-ontonotes_acc: training: 0.905749 validation: 0.888919
09/17 12:54:21 AM: edges-coref-ontonotes_precision: training: 0.907136 validation: 0.889344
09/17 12:54:21 AM: edges-coref-ontonotes_recall: training: 0.907147 validation: 0.889072
09/17 12:54:21 AM: edges-coref-ontonotes_f1: training: 0.907142 validation: 0.889208
09/17 12:54:21 AM: Global learning rate: 2.5e-05
09/17 12:54:21 AM: Saving checkpoints to: ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:54:21 AM: Stopped training after 36 validation checks
09/17 12:54:21 AM: Trained edges-coref-ontonotes for 36000 batches or 27.565 epochs
09/17 12:54:21 AM: ***** VALIDATION RESULTS *****
09/17 12:54:21 AM: edges-coref-ontonotes_f1 (for best val pass 26): edges-coref-ontonotes_loss: 0.28194, macro_avg: 0.88960, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.77918, edges-coref-ontonotes_acc: 0.88922, edges-coref-ontonotes_precision: 0.88954, edges-coref-ontonotes_recall: 0.88965, edges-coref-ontonotes_f1: 0.88960
09/17 12:54:21 AM: micro_avg (for best val pass 1): edges-coref-ontonotes_loss: 0.37632, macro_avg: 0.85506, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.71092, edges-coref-ontonotes_acc: 0.85109, edges-coref-ontonotes_precision: 0.85740, edges-coref-ontonotes_recall: 0.85273, edges-coref-ontonotes_f1: 0.85506
09/17 12:54:21 AM: macro_avg (for best val pass 26): edges-coref-ontonotes_loss: 0.28194, macro_avg: 0.88960, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.77918, edges-coref-ontonotes_acc: 0.88922, edges-coref-ontonotes_precision: 0.88954, edges-coref-ontonotes_recall: 0.88965, edges-coref-ontonotes_f1: 0.88960
09/17 12:54:21 AM: Evaluating...
09/17 12:54:21 AM: Loaded model state from ./experiments/coref-ontonotes-hotpot-only/run/edges-coref-ontonotes/model_state_target_train_val_26.best.th
09/17 12:54:21 AM: Evaluating on: edges-coref-ontonotes, split: val
09/17 12:54:28 AM: Task 'edges-coref-ontonotes': sorting predictions by 'idx'
09/17 12:54:28 AM: Finished evaluating on: edges-coref-ontonotes
09/17 12:54:28 AM: Task 'edges-coref-ontonotes': joining predictions with input split 'val'
09/17 12:54:28 AM: Task 'edges-coref-ontonotes': Wrote predictions to ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:54:28 AM: Wrote all preds for split 'val' to ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:54:28 AM: Evaluating on: edges-coref-ontonotes, split: test
09/17 12:54:35 AM: Task 'edges-coref-ontonotes': sorting predictions by 'idx'
09/17 12:54:35 AM: Finished evaluating on: edges-coref-ontonotes
09/17 12:54:35 AM: Task 'edges-coref-ontonotes': joining predictions with input split 'test'
09/17 12:54:35 AM: Task 'edges-coref-ontonotes': Wrote predictions to ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:54:35 AM: Wrote all preds for split 'test' to ./experiments/coref-ontonotes-hotpot-only/run
09/17 12:54:35 AM: Writing results for split 'val' to ./experiments/coref-ontonotes-hotpot-only/results.tsv
09/17 12:54:35 AM: micro_avg: 0.000, macro_avg: 0.890, edges-coref-ontonotes_mcc: 0.780, edges-coref-ontonotes_acc: 0.889, edges-coref-ontonotes_precision: 0.890, edges-coref-ontonotes_recall: 0.890, edges-coref-ontonotes_f1: 0.890
09/17 12:54:35 AM: Done!
09/17 12:54:35 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
