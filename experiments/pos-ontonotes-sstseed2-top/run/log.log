10/01 03:08:54 AM: Git branch: master
10/01 03:08:54 AM: Git SHA: 62183b2d03f2fae12b41eef8779808b6d354875e
10/01 03:08:54 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/pos-ontonotes-sstseed2-top/",
  "exp_name": "experiments/pos-ontonotes-sstseed2-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/pos-ontonotes-sstseed2-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/sstseed2",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/pos-ontonotes-sstseed2-top__run",
  "run_dir": "./experiments/pos-ontonotes-sstseed2-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-pos-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 03:08:54 AM: Saved config to ./experiments/pos-ontonotes-sstseed2-top/run/params.conf
10/01 03:08:54 AM: Using random seed 1234
10/01 03:09:18 AM: Using GPU 0
10/01 03:09:18 AM: Loading tasks...
10/01 03:09:18 AM: Writing pre-preprocessed tasks to ./experiments/pos-ontonotes-sstseed2-top/
10/01 03:09:18 AM: 	Creating task edges-pos-ontonotes from scratch.
10/01 03:09:37 AM: Read=110514, Skip=5298, Total=115812 from ./probing_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased
10/01 03:09:38 AM: Read=15060, Skip=620, Total=15680 from ./probing_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased
10/01 03:09:42 AM: Read=11462, Skip=755, Total=12217 from ./probing_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased
10/01 03:09:52 AM: 	Task 'edges-pos-ontonotes': |train|=110514 |val|=15060 |test|=11462
10/01 03:09:52 AM: 	Finished loading tasks: edges-pos-ontonotes.
10/01 03:09:52 AM: 	Building vocab from scratch.
10/01 03:09:52 AM: 	Counting units for task edges-pos-ontonotes.
10/01 03:09:54 AM: 	Task 'edges-pos-ontonotes': adding vocab namespace 'edges-pos-ontonotes_labels'
10/01 03:09:55 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:55 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 03:09:56 AM: 	Saved vocab to ./experiments/pos-ontonotes-sstseed2-top/vocab
10/01 03:09:56 AM: Loading token dictionary from ./experiments/pos-ontonotes-sstseed2-top/vocab.
10/01 03:09:56 AM: 	Loaded vocab from ./experiments/pos-ontonotes-sstseed2-top/vocab
10/01 03:09:56 AM: 	Vocab namespace tokens: size 24015
10/01 03:09:56 AM: 	Vocab namespace bert_uncased: size 30524
10/01 03:09:56 AM: 	Vocab namespace edges-pos-ontonotes_labels: size 48
10/01 03:09:56 AM: 	Vocab namespace chars: size 81
10/01 03:09:56 AM: 	Finished building vocab.
10/01 03:09:56 AM: 	Task edges-pos-ontonotes (train): Indexing from scratch.
10/01 03:10:36 AM: 	Task edges-pos-ontonotes (train): Saved 110514 instances to ./experiments/pos-ontonotes-sstseed2-top/preproc/edges-pos-ontonotes__train_data
10/01 03:10:36 AM: 	Task edges-pos-ontonotes (val): Indexing from scratch.
10/01 03:10:41 AM: 	Task edges-pos-ontonotes (val): Saved 15060 instances to ./experiments/pos-ontonotes-sstseed2-top/preproc/edges-pos-ontonotes__val_data
10/01 03:10:41 AM: 	Task edges-pos-ontonotes (test): Indexing from scratch.
10/01 03:10:46 AM: 	Task edges-pos-ontonotes (test): Saved 11462 instances to ./experiments/pos-ontonotes-sstseed2-top/preproc/edges-pos-ontonotes__test_data
10/01 03:10:46 AM: 	Finished indexing tasks
10/01 03:10:46 AM: 	Creating trimmed target-only version of edges-pos-ontonotes train.
10/01 03:10:46 AM: 	  Training on 
10/01 03:10:46 AM: 	  Evaluating on edges-pos-ontonotes
10/01 03:10:46 AM: 	Finished loading tasks in 87.323s
10/01 03:10:46 AM: 	 Tasks: ['edges-pos-ontonotes']
10/01 03:10:46 AM: Building model...
10/01 03:10:46 AM: Using BERT model (bert-base-uncased).
10/01 03:10:46 AM: LOADING A FUNETUNED MODEL from: 
10/01 03:10:46 AM: models/sstseed2
10/01 03:10:46 AM: loading configuration file models/sstseed2/config.json
10/01 03:10:46 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst-2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 03:10:46 AM: loading weights file models/sstseed2/pytorch_model.bin
10/01 03:10:49 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpp_8d_0wa
10/01 03:10:51 AM: copying /tmp/tmpp_8d_0wa to cache at ./experiments/pos-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:10:51 AM: creating metadata file for ./experiments/pos-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:10:51 AM: removing temp file /tmp/tmpp_8d_0wa
10/01 03:10:51 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/pos-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:10:51 AM: Initializing parameters
10/01 03:10:51 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 03:10:51 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 03:10:51 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 03:10:51 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 03:10:51 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 03:10:51 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 03:10:51 AM: 	Task 'edges-pos-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-pos-ontonotes"
}
10/01 03:10:56 AM: Model specification:
10/01 03:10:56 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-pos-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=48, bias=True)
    )
  )
)
10/01 03:10:56 AM: Model parameters:
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:56 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:56 AM: 	edges-pos-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 03:10:56 AM: 	edges-pos-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:10:56 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 24576 with torch.Size([48, 512])
10/01 03:10:56 AM: 	edges-pos-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 48 with torch.Size([48])
10/01 03:10:56 AM: Total number of parameters: 109703728 (1.09704e+08)
10/01 03:10:56 AM: Number of trainable parameters: 221488 (221488)
10/01 03:10:56 AM: Finished building model in 10.080s
10/01 03:10:56 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-pos-ontonotes 

10/01 03:12:12 AM: patience = 9
10/01 03:12:12 AM: val_interval = 1000
10/01 03:12:12 AM: max_vals = 250
10/01 03:12:12 AM: cuda_device = 0
10/01 03:12:12 AM: grad_norm = 5.0
10/01 03:12:12 AM: grad_clipping = None
10/01 03:12:12 AM: lr_decay = 0.99
10/01 03:12:12 AM: min_lr = 1e-06
10/01 03:12:12 AM: keep_all_checkpoints = 0
10/01 03:12:12 AM: val_data_limit = 5000
10/01 03:12:12 AM: max_epochs = -1
10/01 03:12:12 AM: dec_val_scale = 250
10/01 03:12:12 AM: training_data_fraction = 1
10/01 03:12:12 AM: type = adam
10/01 03:12:12 AM: parameter_groups = None
10/01 03:12:12 AM: Number of trainable parameters: 221488
10/01 03:12:12 AM: infer_type_and_cast = True
10/01 03:12:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:12:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:12:12 AM: lr = 0.0001
10/01 03:12:12 AM: amsgrad = True
10/01 03:12:12 AM: type = reduce_on_plateau
10/01 03:12:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:12:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:12:12 AM: mode = max
10/01 03:12:12 AM: factor = 0.5
10/01 03:12:12 AM: patience = 3
10/01 03:12:12 AM: threshold = 0.0001
10/01 03:12:12 AM: threshold_mode = abs
10/01 03:12:12 AM: verbose = True
10/01 03:12:12 AM: type = adam
10/01 03:12:12 AM: parameter_groups = None
10/01 03:12:12 AM: Number of trainable parameters: 221488
10/01 03:12:12 AM: infer_type_and_cast = True
10/01 03:12:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:12:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:12:12 AM: lr = 0.0001
10/01 03:12:12 AM: amsgrad = True
10/01 03:12:12 AM: type = reduce_on_plateau
10/01 03:12:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:12:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:12:12 AM: mode = max
10/01 03:12:12 AM: factor = 0.5
10/01 03:12:12 AM: patience = 3
10/01 03:12:12 AM: threshold = 0.0001
10/01 03:12:12 AM: threshold_mode = abs
10/01 03:12:12 AM: verbose = True
10/01 03:12:12 AM: Starting training without restoring from a checkpoint.
10/01 03:12:12 AM: Training examples per task, before any subsampling: {'edges-pos-ontonotes': 110514}
10/01 03:12:12 AM: Beginning training with stopping criteria based on metric: edges-pos-ontonotes_f1
10/01 03:12:22 AM: Update 50: task edges-pos-ontonotes, batch 50 (50): mcc: 0.0102, acc: 0.0028, precision: 0.0244, recall: 0.1674, f1: 0.0426, edges-pos-ontonotes_loss: 0.4406
10/01 03:12:32 AM: Update 117: task edges-pos-ontonotes, batch 117 (117): mcc: 0.0068, acc: 0.0013, precision: 0.0245, recall: 0.0770, f1: 0.0371, edges-pos-ontonotes_loss: 0.2527
10/01 03:12:42 AM: Update 184: task edges-pos-ontonotes, batch 184 (184): mcc: 0.0094, acc: 0.0065, precision: 0.0272, recall: 0.0558, f1: 0.0365, edges-pos-ontonotes_loss: 0.1907
10/01 03:12:52 AM: Update 250: task edges-pos-ontonotes, batch 250 (250): mcc: 0.0142, acc: 0.0124, precision: 0.0320, recall: 0.0487, f1: 0.0386, edges-pos-ontonotes_loss: 0.1608
10/01 03:13:04 AM: Update 314: task edges-pos-ontonotes, batch 314 (314): mcc: 0.0225, acc: 0.0208, precision: 0.0407, recall: 0.0497, f1: 0.0447, edges-pos-ontonotes_loss: 0.1431
10/01 03:13:15 AM: Update 359: task edges-pos-ontonotes, batch 359 (359): mcc: 0.0231, acc: 0.0201, precision: 0.0433, recall: 0.0441, f1: 0.0437, edges-pos-ontonotes_loss: 0.1349
10/01 03:13:25 AM: Update 409: task edges-pos-ontonotes, batch 409 (409): mcc: 0.0276, acc: 0.0232, precision: 0.0496, recall: 0.0439, f1: 0.0466, edges-pos-ontonotes_loss: 0.1272
10/01 03:13:35 AM: Update 454: task edges-pos-ontonotes, batch 454 (454): mcc: 0.0333, acc: 0.0271, precision: 0.0578, recall: 0.0454, f1: 0.0508, edges-pos-ontonotes_loss: 0.1214
10/01 03:13:45 AM: Update 499: task edges-pos-ontonotes, batch 499 (499): mcc: 0.0411, acc: 0.0324, precision: 0.0688, recall: 0.0488, f1: 0.0571, edges-pos-ontonotes_loss: 0.1164
10/01 03:13:55 AM: Update 543: task edges-pos-ontonotes, batch 543 (543): mcc: 0.0508, acc: 0.0388, precision: 0.0827, recall: 0.0538, f1: 0.0652, edges-pos-ontonotes_loss: 0.1123
10/01 03:14:05 AM: Update 589: task edges-pos-ontonotes, batch 589 (589): mcc: 0.0624, acc: 0.0464, precision: 0.0997, recall: 0.0603, f1: 0.0752, edges-pos-ontonotes_loss: 0.1083
10/01 03:14:16 AM: Update 627: task edges-pos-ontonotes, batch 627 (627): mcc: 0.0735, acc: 0.0534, precision: 0.1162, recall: 0.0667, f1: 0.0848, edges-pos-ontonotes_loss: 0.1053
10/01 03:14:26 AM: Update 661: task edges-pos-ontonotes, batch 661 (661): mcc: 0.0870, acc: 0.0615, precision: 0.1369, recall: 0.0744, f1: 0.0964, edges-pos-ontonotes_loss: 0.1031
10/01 03:14:37 AM: Update 693: task edges-pos-ontonotes, batch 693 (693): mcc: 0.1011, acc: 0.0700, precision: 0.1591, recall: 0.0824, f1: 0.1086, edges-pos-ontonotes_loss: 0.1012
10/01 03:14:47 AM: Update 729: task edges-pos-ontonotes, batch 729 (729): mcc: 0.1151, acc: 0.0783, precision: 0.1815, recall: 0.0903, f1: 0.1206, edges-pos-ontonotes_loss: 0.0991
10/01 03:14:57 AM: Update 770: task edges-pos-ontonotes, batch 770 (770): mcc: 0.1292, acc: 0.0867, precision: 0.2042, recall: 0.0983, f1: 0.1327, edges-pos-ontonotes_loss: 0.0969
10/01 03:15:07 AM: Update 812: task edges-pos-ontonotes, batch 812 (812): mcc: 0.1448, acc: 0.0959, precision: 0.2294, recall: 0.1072, f1: 0.1461, edges-pos-ontonotes_loss: 0.0949
10/01 03:15:17 AM: Update 861: task edges-pos-ontonotes, batch 861 (861): mcc: 0.1608, acc: 0.1053, precision: 0.2554, recall: 0.1164, f1: 0.1599, edges-pos-ontonotes_loss: 0.0926
10/01 03:15:27 AM: Update 902: task edges-pos-ontonotes, batch 902 (902): mcc: 0.1786, acc: 0.1161, precision: 0.2838, recall: 0.1269, f1: 0.1754, edges-pos-ontonotes_loss: 0.0907
10/01 03:15:39 AM: Update 940: task edges-pos-ontonotes, batch 940 (940): mcc: 0.1928, acc: 0.1245, precision: 0.3068, recall: 0.1352, f1: 0.1877, edges-pos-ontonotes_loss: 0.0892
10/01 03:15:49 AM: Update 981: task edges-pos-ontonotes, batch 981 (981): mcc: 0.2089, acc: 0.1340, precision: 0.3327, recall: 0.1448, f1: 0.2018, edges-pos-ontonotes_loss: 0.0876
10/01 03:15:53 AM: ***** Step 1000 / Validation 1 *****
10/01 03:15:53 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:15:53 AM: Validating...
10/01 03:15:59 AM: Evaluate: task edges-pos-ontonotes, batch 30 (157): mcc: 0.4875, acc: 0.2547, precision: 0.9419, recall: 0.2569, f1: 0.4037, edges-pos-ontonotes_loss: 0.0498
10/01 03:16:09 AM: Evaluate: task edges-pos-ontonotes, batch 82 (157): mcc: 0.5192, acc: 0.2880, precision: 0.9429, recall: 0.2909, f1: 0.4446, edges-pos-ontonotes_loss: 0.0487
10/01 03:16:19 AM: Evaluate: task edges-pos-ontonotes, batch 119 (157): mcc: 0.5417, acc: 0.3167, precision: 0.9305, recall: 0.3208, f1: 0.4771, edges-pos-ontonotes_loss: 0.0475
10/01 03:16:29 AM: Evaluate: task edges-pos-ontonotes, batch 154 (157): mcc: 0.5481, acc: 0.3242, precision: 0.9282, recall: 0.3291, f1: 0.4860, edges-pos-ontonotes_loss: 0.0472
10/01 03:16:30 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:16:30 AM: Best result seen so far for micro.
10/01 03:16:30 AM: Best result seen so far for macro.
10/01 03:16:30 AM: Updating LR scheduler:
10/01 03:16:30 AM: 	Best result seen so far for macro_avg: 0.487
10/01 03:16:30 AM: 	# validation passes without improvement: 0
10/01 03:16:30 AM: edges-pos-ontonotes_loss: training: 0.086858 validation: 0.047187
10/01 03:16:30 AM: macro_avg: validation: 0.486869
10/01 03:16:30 AM: micro_avg: validation: 0.000000
10/01 03:16:30 AM: edges-pos-ontonotes_mcc: training: 0.216127 validation: 0.548747
10/01 03:16:30 AM: edges-pos-ontonotes_acc: training: 0.138452 validation: 0.325037
10/01 03:16:30 AM: edges-pos-ontonotes_precision: training: 0.344183 validation: 0.928093
10/01 03:16:30 AM: edges-pos-ontonotes_recall: training: 0.149203 validation: 0.329989
10/01 03:16:30 AM: edges-pos-ontonotes_f1: training: 0.208166 validation: 0.486869
10/01 03:16:30 AM: Global learning rate: 0.0001
10/01 03:16:30 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:16:39 AM: Update 1042: task edges-pos-ontonotes, batch 42 (1042): mcc: 0.5357, acc: 0.3147, precision: 0.8989, recall: 0.3251, f1: 0.4775, edges-pos-ontonotes_loss: 0.0492
10/01 03:16:49 AM: Update 1082: task edges-pos-ontonotes, batch 82 (1082): mcc: 0.5489, acc: 0.3303, precision: 0.8977, recall: 0.3417, f1: 0.4950, edges-pos-ontonotes_loss: 0.0481
10/01 03:17:00 AM: Update 1129: task edges-pos-ontonotes, batch 129 (1129): mcc: 0.5607, acc: 0.3438, precision: 0.8995, recall: 0.3557, f1: 0.5098, edges-pos-ontonotes_loss: 0.0473
10/01 03:17:10 AM: Update 1168: task edges-pos-ontonotes, batch 168 (1168): mcc: 0.5682, acc: 0.3531, precision: 0.8995, recall: 0.3652, f1: 0.5194, edges-pos-ontonotes_loss: 0.0466
10/01 03:17:20 AM: Update 1212: task edges-pos-ontonotes, batch 212 (1212): mcc: 0.5759, acc: 0.3624, precision: 0.8998, recall: 0.3749, f1: 0.5293, edges-pos-ontonotes_loss: 0.0460
10/01 03:17:30 AM: Update 1252: task edges-pos-ontonotes, batch 252 (1252): mcc: 0.5821, acc: 0.3705, precision: 0.8998, recall: 0.3830, f1: 0.5373, edges-pos-ontonotes_loss: 0.0454
10/01 03:17:43 AM: Update 1253: task edges-pos-ontonotes, batch 253 (1253): mcc: 0.5820, acc: 0.3705, precision: 0.8997, recall: 0.3830, f1: 0.5372, edges-pos-ontonotes_loss: 0.0454
10/01 03:17:53 AM: Update 1297: task edges-pos-ontonotes, batch 297 (1297): mcc: 0.5873, acc: 0.3770, precision: 0.9004, recall: 0.3896, f1: 0.5438, edges-pos-ontonotes_loss: 0.0448
10/01 03:18:03 AM: Update 1344: task edges-pos-ontonotes, batch 344 (1344): mcc: 0.5933, acc: 0.3845, precision: 0.9010, recall: 0.3972, f1: 0.5513, edges-pos-ontonotes_loss: 0.0442
10/01 03:18:14 AM: Update 1388: task edges-pos-ontonotes, batch 388 (1388): mcc: 0.5992, acc: 0.3922, precision: 0.9011, recall: 0.4050, f1: 0.5589, edges-pos-ontonotes_loss: 0.0437
10/01 03:18:24 AM: Update 1432: task edges-pos-ontonotes, batch 432 (1432): mcc: 0.6052, acc: 0.3998, precision: 0.9017, recall: 0.4129, f1: 0.5664, edges-pos-ontonotes_loss: 0.0431
10/01 03:18:34 AM: Update 1473: task edges-pos-ontonotes, batch 473 (1473): mcc: 0.6109, acc: 0.4072, precision: 0.9017, recall: 0.4205, f1: 0.5736, edges-pos-ontonotes_loss: 0.0426
10/01 03:18:44 AM: Update 1515: task edges-pos-ontonotes, batch 515 (1515): mcc: 0.6165, acc: 0.4144, precision: 0.9023, recall: 0.4280, f1: 0.5806, edges-pos-ontonotes_loss: 0.0421
10/01 03:18:54 AM: Update 1561: task edges-pos-ontonotes, batch 561 (1561): mcc: 0.6213, acc: 0.4206, precision: 0.9024, recall: 0.4344, f1: 0.5865, edges-pos-ontonotes_loss: 0.0416
10/01 03:19:04 AM: Update 1595: task edges-pos-ontonotes, batch 595 (1595): mcc: 0.6250, acc: 0.4254, precision: 0.9028, recall: 0.4394, f1: 0.5911, edges-pos-ontonotes_loss: 0.0413
10/01 03:19:15 AM: Update 1638: task edges-pos-ontonotes, batch 638 (1638): mcc: 0.6303, acc: 0.4325, precision: 0.9026, recall: 0.4469, f1: 0.5979, edges-pos-ontonotes_loss: 0.0408
10/01 03:19:25 AM: Update 1681: task edges-pos-ontonotes, batch 681 (1681): mcc: 0.6352, acc: 0.4390, precision: 0.9027, recall: 0.4538, f1: 0.6039, edges-pos-ontonotes_loss: 0.0404
10/01 03:19:35 AM: Update 1723: task edges-pos-ontonotes, batch 723 (1723): mcc: 0.6398, acc: 0.4451, precision: 0.9028, recall: 0.4603, f1: 0.6097, edges-pos-ontonotes_loss: 0.0400
10/01 03:19:45 AM: Update 1766: task edges-pos-ontonotes, batch 766 (1766): mcc: 0.6439, acc: 0.4506, precision: 0.9028, recall: 0.4661, f1: 0.6148, edges-pos-ontonotes_loss: 0.0396
10/01 03:19:55 AM: Update 1814: task edges-pos-ontonotes, batch 814 (1814): mcc: 0.6481, acc: 0.4564, precision: 0.9029, recall: 0.4721, f1: 0.6200, edges-pos-ontonotes_loss: 0.0392
10/01 03:20:06 AM: Update 1856: task edges-pos-ontonotes, batch 856 (1856): mcc: 0.6524, acc: 0.4622, precision: 0.9028, recall: 0.4783, f1: 0.6253, edges-pos-ontonotes_loss: 0.0388
10/01 03:20:16 AM: Update 1888: task edges-pos-ontonotes, batch 888 (1888): mcc: 0.6550, acc: 0.4658, precision: 0.9025, recall: 0.4822, f1: 0.6286, edges-pos-ontonotes_loss: 0.0386
10/01 03:20:26 AM: Update 1933: task edges-pos-ontonotes, batch 933 (1933): mcc: 0.6578, acc: 0.4698, precision: 0.9020, recall: 0.4867, f1: 0.6323, edges-pos-ontonotes_loss: 0.0383
10/01 03:20:36 AM: Update 1985: task edges-pos-ontonotes, batch 985 (1985): mcc: 0.6606, acc: 0.4738, precision: 0.9015, recall: 0.4911, f1: 0.6358, edges-pos-ontonotes_loss: 0.0380
10/01 03:20:38 AM: ***** Step 2000 / Validation 2 *****
10/01 03:20:38 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:20:38 AM: Validating...
10/01 03:20:46 AM: Evaluate: task edges-pos-ontonotes, batch 43 (157): mcc: 0.7057, acc: 0.5323, precision: 0.9315, recall: 0.5411, f1: 0.6846, edges-pos-ontonotes_loss: 0.0337
10/01 03:20:56 AM: Evaluate: task edges-pos-ontonotes, batch 92 (157): mcc: 0.7360, acc: 0.5775, precision: 0.9296, recall: 0.5891, f1: 0.7212, edges-pos-ontonotes_loss: 0.0315
10/01 03:21:06 AM: Evaluate: task edges-pos-ontonotes, batch 127 (157): mcc: 0.7420, acc: 0.5885, precision: 0.9229, recall: 0.6030, f1: 0.7295, edges-pos-ontonotes_loss: 0.0306
10/01 03:21:15 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:21:15 AM: Best result seen so far for macro.
10/01 03:21:15 AM: Updating LR scheduler:
10/01 03:21:15 AM: 	Best result seen so far for macro_avg: 0.740
10/01 03:21:15 AM: 	# validation passes without improvement: 0
10/01 03:21:15 AM: edges-pos-ontonotes_loss: training: 0.037865 validation: 0.029687
10/01 03:21:15 AM: macro_avg: validation: 0.740128
10/01 03:21:15 AM: micro_avg: validation: 0.000000
10/01 03:21:15 AM: edges-pos-ontonotes_mcc: training: 0.661274 validation: 0.751058
10/01 03:21:15 AM: edges-pos-ontonotes_acc: training: 0.474686 validation: 0.601755
10/01 03:21:15 AM: edges-pos-ontonotes_precision: training: 0.901583 validation: 0.922581
10/01 03:21:15 AM: edges-pos-ontonotes_recall: training: 0.491989 validation: 0.617924
10/01 03:21:15 AM: edges-pos-ontonotes_f1: training: 0.636592 validation: 0.740128
10/01 03:21:15 AM: Global learning rate: 0.0001
10/01 03:21:15 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:21:16 AM: Update 2007: task edges-pos-ontonotes, batch 7 (2007): mcc: 0.7197, acc: 0.5671, precision: 0.8825, recall: 0.5943, f1: 0.7103, edges-pos-ontonotes_loss: 0.0317
10/01 03:21:27 AM: Update 2065: task edges-pos-ontonotes, batch 65 (2065): mcc: 0.7311, acc: 0.5755, precision: 0.9008, recall: 0.6003, f1: 0.7205, edges-pos-ontonotes_loss: 0.0312
10/01 03:21:37 AM: Update 2114: task edges-pos-ontonotes, batch 114 (2114): mcc: 0.7339, acc: 0.5809, precision: 0.8977, recall: 0.6071, f1: 0.7243, edges-pos-ontonotes_loss: 0.0309
10/01 03:21:47 AM: Update 2169: task edges-pos-ontonotes, batch 169 (2169): mcc: 0.7374, acc: 0.5858, precision: 0.8974, recall: 0.6130, f1: 0.7284, edges-pos-ontonotes_loss: 0.0304
10/01 03:21:57 AM: Update 2217: task edges-pos-ontonotes, batch 217 (2217): mcc: 0.7362, acc: 0.5843, precision: 0.8969, recall: 0.6114, f1: 0.7271, edges-pos-ontonotes_loss: 0.0304
10/01 03:22:07 AM: Update 2287: task edges-pos-ontonotes, batch 287 (2287): mcc: 0.7355, acc: 0.5829, precision: 0.8972, recall: 0.6101, f1: 0.7263, edges-pos-ontonotes_loss: 0.0303
10/01 03:22:17 AM: Update 2360: task edges-pos-ontonotes, batch 360 (2360): mcc: 0.7372, acc: 0.5847, precision: 0.8982, recall: 0.6121, f1: 0.7280, edges-pos-ontonotes_loss: 0.0300
10/01 03:22:27 AM: Update 2430: task edges-pos-ontonotes, batch 430 (2430): mcc: 0.7398, acc: 0.5882, precision: 0.8989, recall: 0.6158, f1: 0.7309, edges-pos-ontonotes_loss: 0.0297
10/01 03:22:37 AM: Update 2497: task edges-pos-ontonotes, batch 497 (2497): mcc: 0.7426, acc: 0.5919, precision: 0.8998, recall: 0.6198, f1: 0.7340, edges-pos-ontonotes_loss: 0.0294
10/01 03:22:47 AM: Update 2565: task edges-pos-ontonotes, batch 565 (2565): mcc: 0.7458, acc: 0.5965, precision: 0.9008, recall: 0.6244, f1: 0.7375, edges-pos-ontonotes_loss: 0.0292
10/01 03:22:57 AM: Update 2644: task edges-pos-ontonotes, batch 644 (2644): mcc: 0.7495, acc: 0.6017, precision: 0.9025, recall: 0.6294, f1: 0.7416, edges-pos-ontonotes_loss: 0.0288
10/01 03:23:08 AM: Update 2733: task edges-pos-ontonotes, batch 733 (2733): mcc: 0.7532, acc: 0.6069, precision: 0.9036, recall: 0.6346, f1: 0.7456, edges-pos-ontonotes_loss: 0.0284
10/01 03:23:19 AM: Update 2818: task edges-pos-ontonotes, batch 818 (2818): mcc: 0.7574, acc: 0.6129, precision: 0.9048, recall: 0.6408, f1: 0.7503, edges-pos-ontonotes_loss: 0.0279
10/01 03:23:29 AM: Update 2915: task edges-pos-ontonotes, batch 915 (2915): mcc: 0.7559, acc: 0.6109, precision: 0.9036, recall: 0.6392, f1: 0.7487, edges-pos-ontonotes_loss: 0.0280
10/01 03:23:37 AM: ***** Step 3000 / Validation 3 *****
10/01 03:23:37 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:23:37 AM: Validating...
10/01 03:23:39 AM: Evaluate: task edges-pos-ontonotes, batch 13 (157): mcc: 0.7695, acc: 0.6306, precision: 0.9123, recall: 0.6556, f1: 0.7629, edges-pos-ontonotes_loss: 0.0255
10/01 03:23:50 AM: Evaluate: task edges-pos-ontonotes, batch 67 (157): mcc: 0.7865, acc: 0.6530, precision: 0.9264, recall: 0.6738, f1: 0.7802, edges-pos-ontonotes_loss: 0.0243
10/01 03:24:00 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.7953, acc: 0.6698, precision: 0.9168, recall: 0.6961, f1: 0.7914, edges-pos-ontonotes_loss: 0.0238
10/01 03:24:10 AM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.7954, acc: 0.6703, precision: 0.9130, recall: 0.6992, f1: 0.7919, edges-pos-ontonotes_loss: 0.0240
10/01 03:24:14 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:24:14 AM: Best result seen so far for macro.
10/01 03:24:14 AM: Updating LR scheduler:
10/01 03:24:14 AM: 	Best result seen so far for macro_avg: 0.793
10/01 03:24:14 AM: 	# validation passes without improvement: 0
10/01 03:24:14 AM: edges-pos-ontonotes_loss: training: 0.027844 validation: 0.023944
10/01 03:24:14 AM: macro_avg: validation: 0.792667
10/01 03:24:14 AM: micro_avg: validation: 0.000000
10/01 03:24:14 AM: edges-pos-ontonotes_mcc: training: 0.755471 validation: 0.796058
10/01 03:24:14 AM: edges-pos-ontonotes_acc: training: 0.610269 validation: 0.671566
10/01 03:24:14 AM: edges-pos-ontonotes_precision: training: 0.903006 validation: 0.913053
10/01 03:24:14 AM: edges-pos-ontonotes_recall: training: 0.638891 validation: 0.700329
10/01 03:24:14 AM: edges-pos-ontonotes_f1: training: 0.748328 validation: 0.792667
10/01 03:24:14 AM: Global learning rate: 0.0001
10/01 03:24:14 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:24:20 AM: Update 3063: task edges-pos-ontonotes, batch 63 (3063): mcc: 0.7557, acc: 0.6123, precision: 0.8914, recall: 0.6478, f1: 0.7503, edges-pos-ontonotes_loss: 0.0267
10/01 03:24:30 AM: Update 3132: task edges-pos-ontonotes, batch 132 (3132): mcc: 0.7618, acc: 0.6219, precision: 0.8868, recall: 0.6616, f1: 0.7578, edges-pos-ontonotes_loss: 0.0255
10/01 03:24:40 AM: Update 3177: task edges-pos-ontonotes, batch 177 (3177): mcc: 0.7502, acc: 0.6062, precision: 0.8801, recall: 0.6468, f1: 0.7456, edges-pos-ontonotes_loss: 0.0268
10/01 03:24:51 AM: Update 3218: task edges-pos-ontonotes, batch 218 (3218): mcc: 0.7465, acc: 0.6021, precision: 0.8759, recall: 0.6436, f1: 0.7420, edges-pos-ontonotes_loss: 0.0275
10/01 03:25:01 AM: Update 3263: task edges-pos-ontonotes, batch 263 (3263): mcc: 0.7448, acc: 0.6001, precision: 0.8749, recall: 0.6415, f1: 0.7403, edges-pos-ontonotes_loss: 0.0281
10/01 03:25:11 AM: Update 3310: task edges-pos-ontonotes, batch 310 (3310): mcc: 0.7450, acc: 0.6004, precision: 0.8756, recall: 0.6414, f1: 0.7404, edges-pos-ontonotes_loss: 0.0284
10/01 03:25:21 AM: Update 3361: task edges-pos-ontonotes, batch 361 (3361): mcc: 0.7457, acc: 0.6014, precision: 0.8760, recall: 0.6422, f1: 0.7411, edges-pos-ontonotes_loss: 0.0286
10/01 03:25:31 AM: Update 3412: task edges-pos-ontonotes, batch 412 (3412): mcc: 0.7467, acc: 0.6030, precision: 0.8762, recall: 0.6438, f1: 0.7422, edges-pos-ontonotes_loss: 0.0286
10/01 03:25:42 AM: Update 3459: task edges-pos-ontonotes, batch 459 (3459): mcc: 0.7464, acc: 0.6029, precision: 0.8761, recall: 0.6434, f1: 0.7419, edges-pos-ontonotes_loss: 0.0287
10/01 03:25:53 AM: Update 3461: task edges-pos-ontonotes, batch 461 (3461): mcc: 0.7466, acc: 0.6031, precision: 0.8762, recall: 0.6436, f1: 0.7421, edges-pos-ontonotes_loss: 0.0287
10/01 03:26:03 AM: Update 3533: task edges-pos-ontonotes, batch 533 (3533): mcc: 0.7487, acc: 0.6060, precision: 0.8776, recall: 0.6461, f1: 0.7443, edges-pos-ontonotes_loss: 0.0283
10/01 03:26:14 AM: Update 3606: task edges-pos-ontonotes, batch 606 (3606): mcc: 0.7516, acc: 0.6100, precision: 0.8791, recall: 0.6499, f1: 0.7473, edges-pos-ontonotes_loss: 0.0278
10/01 03:26:24 AM: Update 3678: task edges-pos-ontonotes, batch 678 (3678): mcc: 0.7538, acc: 0.6132, precision: 0.8801, recall: 0.6530, f1: 0.7497, edges-pos-ontonotes_loss: 0.0274
10/01 03:26:34 AM: Update 3744: task edges-pos-ontonotes, batch 744 (3744): mcc: 0.7559, acc: 0.6161, precision: 0.8809, recall: 0.6559, f1: 0.7519, edges-pos-ontonotes_loss: 0.0271
10/01 03:26:44 AM: Update 3794: task edges-pos-ontonotes, batch 794 (3794): mcc: 0.7579, acc: 0.6190, precision: 0.8814, recall: 0.6590, f1: 0.7542, edges-pos-ontonotes_loss: 0.0268
10/01 03:26:54 AM: Update 3847: task edges-pos-ontonotes, batch 847 (3847): mcc: 0.7606, acc: 0.6230, precision: 0.8822, recall: 0.6631, f1: 0.7571, edges-pos-ontonotes_loss: 0.0266
10/01 03:27:04 AM: Update 3895: task edges-pos-ontonotes, batch 895 (3895): mcc: 0.7629, acc: 0.6263, precision: 0.8828, recall: 0.6665, f1: 0.7595, edges-pos-ontonotes_loss: 0.0264
10/01 03:27:15 AM: Update 3958: task edges-pos-ontonotes, batch 958 (3958): mcc: 0.7648, acc: 0.6291, precision: 0.8835, recall: 0.6693, f1: 0.7616, edges-pos-ontonotes_loss: 0.0261
10/01 03:27:21 AM: ***** Step 4000 / Validation 4 *****
10/01 03:27:21 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:27:21 AM: Validating...
10/01 03:27:25 AM: Evaluate: task edges-pos-ontonotes, batch 17 (157): mcc: 0.8038, acc: 0.6827, precision: 0.9287, recall: 0.7017, f1: 0.7994, edges-pos-ontonotes_loss: 0.0213
10/01 03:27:35 AM: Evaluate: task edges-pos-ontonotes, batch 70 (157): mcc: 0.8253, acc: 0.7138, precision: 0.9394, recall: 0.7304, f1: 0.8219, edges-pos-ontonotes_loss: 0.0197
10/01 03:27:45 AM: Evaluate: task edges-pos-ontonotes, batch 111 (157): mcc: 0.8306, acc: 0.7245, precision: 0.9321, recall: 0.7456, f1: 0.8285, edges-pos-ontonotes_loss: 0.0194
10/01 03:27:55 AM: Evaluate: task edges-pos-ontonotes, batch 145 (157): mcc: 0.8281, acc: 0.7214, precision: 0.9277, recall: 0.7447, f1: 0.8262, edges-pos-ontonotes_loss: 0.0198
10/01 03:27:58 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:27:58 AM: Best result seen so far for macro.
10/01 03:27:58 AM: Updating LR scheduler:
10/01 03:27:58 AM: 	Best result seen so far for macro_avg: 0.827
10/01 03:27:58 AM: 	# validation passes without improvement: 0
10/01 03:27:58 AM: edges-pos-ontonotes_loss: training: 0.026014 validation: 0.019718
10/01 03:27:58 AM: macro_avg: validation: 0.827337
10/01 03:27:58 AM: micro_avg: validation: 0.000000
10/01 03:27:58 AM: edges-pos-ontonotes_mcc: training: 0.766020 validation: 0.829098
10/01 03:27:58 AM: edges-pos-ontonotes_acc: training: 0.630732 validation: 0.723049
10/01 03:27:58 AM: edges-pos-ontonotes_precision: training: 0.883890 validation: 0.927449
10/01 03:27:58 AM: edges-pos-ontonotes_recall: training: 0.671016 validation: 0.746733
10/01 03:27:58 AM: edges-pos-ontonotes_f1: training: 0.762881 validation: 0.827337
10/01 03:27:58 AM: Global learning rate: 0.0001
10/01 03:27:58 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:28:05 AM: Update 4040: task edges-pos-ontonotes, batch 40 (4040): mcc: 0.7957, acc: 0.6723, precision: 0.8927, recall: 0.7158, f1: 0.7945, edges-pos-ontonotes_loss: 0.0235
10/01 03:28:17 AM: Update 4087: task edges-pos-ontonotes, batch 87 (4087): mcc: 0.7951, acc: 0.6725, precision: 0.8915, recall: 0.7158, f1: 0.7940, edges-pos-ontonotes_loss: 0.0233
10/01 03:28:28 AM: Update 4131: task edges-pos-ontonotes, batch 131 (4131): mcc: 0.7878, acc: 0.6640, precision: 0.8857, recall: 0.7075, f1: 0.7867, edges-pos-ontonotes_loss: 0.0244
10/01 03:28:38 AM: Update 4175: task edges-pos-ontonotes, batch 175 (4175): mcc: 0.7862, acc: 0.6621, precision: 0.8849, recall: 0.7054, f1: 0.7850, edges-pos-ontonotes_loss: 0.0247
10/01 03:28:48 AM: Update 4221: task edges-pos-ontonotes, batch 221 (4221): mcc: 0.7857, acc: 0.6614, precision: 0.8850, recall: 0.7044, f1: 0.7844, edges-pos-ontonotes_loss: 0.0248
10/01 03:28:58 AM: Update 4269: task edges-pos-ontonotes, batch 269 (4269): mcc: 0.7855, acc: 0.6612, precision: 0.8854, recall: 0.7038, f1: 0.7843, edges-pos-ontonotes_loss: 0.0249
10/01 03:29:08 AM: Update 4311: task edges-pos-ontonotes, batch 311 (4311): mcc: 0.7858, acc: 0.6620, precision: 0.8852, recall: 0.7046, f1: 0.7846, edges-pos-ontonotes_loss: 0.0249
10/01 03:29:18 AM: Update 4356: task edges-pos-ontonotes, batch 356 (4356): mcc: 0.7867, acc: 0.6630, precision: 0.8860, recall: 0.7053, f1: 0.7854, edges-pos-ontonotes_loss: 0.0248
10/01 03:29:28 AM: Update 4399: task edges-pos-ontonotes, batch 399 (4399): mcc: 0.7866, acc: 0.6627, precision: 0.8862, recall: 0.7051, f1: 0.7853, edges-pos-ontonotes_loss: 0.0247
10/01 03:29:39 AM: Update 4430: task edges-pos-ontonotes, batch 430 (4430): mcc: 0.7871, acc: 0.6634, precision: 0.8857, recall: 0.7063, f1: 0.7859, edges-pos-ontonotes_loss: 0.0247
10/01 03:29:49 AM: Update 4472: task edges-pos-ontonotes, batch 472 (4472): mcc: 0.7881, acc: 0.6651, precision: 0.8855, recall: 0.7083, f1: 0.7870, edges-pos-ontonotes_loss: 0.0246
10/01 03:29:59 AM: Update 4515: task edges-pos-ontonotes, batch 515 (4515): mcc: 0.7896, acc: 0.6672, precision: 0.8862, recall: 0.7103, f1: 0.7886, edges-pos-ontonotes_loss: 0.0244
10/01 03:30:09 AM: Update 4553: task edges-pos-ontonotes, batch 553 (4553): mcc: 0.7905, acc: 0.6687, precision: 0.8865, recall: 0.7116, f1: 0.7895, edges-pos-ontonotes_loss: 0.0243
10/01 03:30:19 AM: Update 4595: task edges-pos-ontonotes, batch 595 (4595): mcc: 0.7918, acc: 0.6707, precision: 0.8872, recall: 0.7134, f1: 0.7909, edges-pos-ontonotes_loss: 0.0242
10/01 03:30:29 AM: Update 4638: task edges-pos-ontonotes, batch 638 (4638): mcc: 0.7929, acc: 0.6723, precision: 0.8878, recall: 0.7148, f1: 0.7920, edges-pos-ontonotes_loss: 0.0241
10/01 03:30:39 AM: Update 4677: task edges-pos-ontonotes, batch 677 (4677): mcc: 0.7937, acc: 0.6736, precision: 0.8883, recall: 0.7159, f1: 0.7929, edges-pos-ontonotes_loss: 0.0240
10/01 03:30:50 AM: Update 4713: task edges-pos-ontonotes, batch 713 (4713): mcc: 0.7943, acc: 0.6744, precision: 0.8886, recall: 0.7167, f1: 0.7934, edges-pos-ontonotes_loss: 0.0239
10/01 03:31:00 AM: Update 4756: task edges-pos-ontonotes, batch 756 (4756): mcc: 0.7950, acc: 0.6757, precision: 0.8890, recall: 0.7177, f1: 0.7942, edges-pos-ontonotes_loss: 0.0238
10/01 03:31:10 AM: Update 4803: task edges-pos-ontonotes, batch 803 (4803): mcc: 0.7958, acc: 0.6769, precision: 0.8893, recall: 0.7189, f1: 0.7951, edges-pos-ontonotes_loss: 0.0237
10/01 03:31:20 AM: Update 4844: task edges-pos-ontonotes, batch 844 (4844): mcc: 0.7967, acc: 0.6781, precision: 0.8899, recall: 0.7198, f1: 0.7959, edges-pos-ontonotes_loss: 0.0236
10/01 03:31:30 AM: Update 4888: task edges-pos-ontonotes, batch 888 (4888): mcc: 0.7974, acc: 0.6793, precision: 0.8903, recall: 0.7209, f1: 0.7967, edges-pos-ontonotes_loss: 0.0235
10/01 03:31:40 AM: Update 4934: task edges-pos-ontonotes, batch 934 (4934): mcc: 0.7981, acc: 0.6803, precision: 0.8906, recall: 0.7218, f1: 0.7974, edges-pos-ontonotes_loss: 0.0235
10/01 03:31:50 AM: Update 4976: task edges-pos-ontonotes, batch 976 (4976): mcc: 0.7988, acc: 0.6814, precision: 0.8910, recall: 0.7228, f1: 0.7981, edges-pos-ontonotes_loss: 0.0234
10/01 03:31:57 AM: ***** Step 5000 / Validation 5 *****
10/01 03:31:57 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:31:57 AM: Validating...
10/01 03:32:00 AM: Evaluate: task edges-pos-ontonotes, batch 17 (157): mcc: 0.8205, acc: 0.7117, precision: 0.9267, recall: 0.7321, f1: 0.8180, edges-pos-ontonotes_loss: 0.0198
10/01 03:32:11 AM: Evaluate: task edges-pos-ontonotes, batch 70 (157): mcc: 0.8367, acc: 0.7349, precision: 0.9360, recall: 0.7532, f1: 0.8347, edges-pos-ontonotes_loss: 0.0190
10/01 03:32:21 AM: Evaluate: task edges-pos-ontonotes, batch 111 (157): mcc: 0.8453, acc: 0.7502, precision: 0.9338, recall: 0.7704, f1: 0.8443, edges-pos-ontonotes_loss: 0.0182
10/01 03:32:31 AM: Evaluate: task edges-pos-ontonotes, batch 145 (157): mcc: 0.8463, acc: 0.7522, precision: 0.9315, recall: 0.7741, f1: 0.8456, edges-pos-ontonotes_loss: 0.0181
10/01 03:32:34 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:32:34 AM: Best result seen so far for macro.
10/01 03:32:34 AM: Updating LR scheduler:
10/01 03:32:34 AM: 	Best result seen so far for macro_avg: 0.848
10/01 03:32:34 AM: 	# validation passes without improvement: 0
10/01 03:32:34 AM: edges-pos-ontonotes_loss: training: 0.023366 validation: 0.017890
10/01 03:32:34 AM: macro_avg: validation: 0.847861
10/01 03:32:34 AM: micro_avg: validation: 0.000000
10/01 03:32:34 AM: edges-pos-ontonotes_mcc: training: 0.799289 validation: 0.848541
10/01 03:32:34 AM: edges-pos-ontonotes_acc: training: 0.682116 validation: 0.755759
10/01 03:32:34 AM: edges-pos-ontonotes_precision: training: 0.891306 validation: 0.932231
10/01 03:32:34 AM: edges-pos-ontonotes_recall: training: 0.723394 validation: 0.777496
10/01 03:32:34 AM: edges-pos-ontonotes_f1: training: 0.798619 validation: 0.847861
10/01 03:32:34 AM: Global learning rate: 0.0001
10/01 03:32:34 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:32:51 AM: Update 5026: task edges-pos-ontonotes, batch 26 (5026): mcc: 0.8208, acc: 0.7118, precision: 0.9016, recall: 0.7532, f1: 0.8208, edges-pos-ontonotes_loss: 0.0213
10/01 03:33:01 AM: Update 5073: task edges-pos-ontonotes, batch 73 (5073): mcc: 0.8176, acc: 0.7091, precision: 0.9002, recall: 0.7488, f1: 0.8176, edges-pos-ontonotes_loss: 0.0220
10/01 03:33:11 AM: Update 5116: task edges-pos-ontonotes, batch 116 (5116): mcc: 0.8166, acc: 0.7087, precision: 0.8984, recall: 0.7484, f1: 0.8166, edges-pos-ontonotes_loss: 0.0219
10/01 03:33:21 AM: Update 5162: task edges-pos-ontonotes, batch 162 (5162): mcc: 0.8168, acc: 0.7090, precision: 0.8985, recall: 0.7488, f1: 0.8169, edges-pos-ontonotes_loss: 0.0217
10/01 03:33:31 AM: Update 5206: task edges-pos-ontonotes, batch 206 (5206): mcc: 0.8167, acc: 0.7089, precision: 0.8981, recall: 0.7489, f1: 0.8167, edges-pos-ontonotes_loss: 0.0217
10/01 03:33:41 AM: Update 5249: task edges-pos-ontonotes, batch 249 (5249): mcc: 0.8174, acc: 0.7100, precision: 0.8986, recall: 0.7497, f1: 0.8174, edges-pos-ontonotes_loss: 0.0216
10/01 03:33:52 AM: Update 5290: task edges-pos-ontonotes, batch 290 (5290): mcc: 0.8175, acc: 0.7102, precision: 0.8986, recall: 0.7499, f1: 0.8175, edges-pos-ontonotes_loss: 0.0215
10/01 03:34:02 AM: Update 5330: task edges-pos-ontonotes, batch 330 (5330): mcc: 0.8180, acc: 0.7111, precision: 0.8988, recall: 0.7507, f1: 0.8181, edges-pos-ontonotes_loss: 0.0215
10/01 03:34:12 AM: Update 5361: task edges-pos-ontonotes, batch 361 (5361): mcc: 0.8176, acc: 0.7105, precision: 0.8984, recall: 0.7502, f1: 0.8177, edges-pos-ontonotes_loss: 0.0214
10/01 03:34:22 AM: Update 5413: task edges-pos-ontonotes, batch 413 (5413): mcc: 0.8173, acc: 0.7098, precision: 0.8987, recall: 0.7494, f1: 0.8173, edges-pos-ontonotes_loss: 0.0213
10/01 03:34:32 AM: Update 5465: task edges-pos-ontonotes, batch 465 (5465): mcc: 0.8171, acc: 0.7093, precision: 0.8987, recall: 0.7490, f1: 0.8171, edges-pos-ontonotes_loss: 0.0212
10/01 03:34:42 AM: Update 5523: task edges-pos-ontonotes, batch 523 (5523): mcc: 0.8173, acc: 0.7094, precision: 0.8990, recall: 0.7492, f1: 0.8173, edges-pos-ontonotes_loss: 0.0211
10/01 03:34:53 AM: Update 5579: task edges-pos-ontonotes, batch 579 (5579): mcc: 0.8175, acc: 0.7095, precision: 0.8990, recall: 0.7495, f1: 0.8175, edges-pos-ontonotes_loss: 0.0211
10/01 03:35:03 AM: Update 5626: task edges-pos-ontonotes, batch 626 (5626): mcc: 0.8175, acc: 0.7097, precision: 0.8989, recall: 0.7498, f1: 0.8176, edges-pos-ontonotes_loss: 0.0210
10/01 03:35:13 AM: Update 5668: task edges-pos-ontonotes, batch 668 (5668): mcc: 0.8174, acc: 0.7093, precision: 0.8989, recall: 0.7495, f1: 0.8174, edges-pos-ontonotes_loss: 0.0209
10/01 03:35:23 AM: Update 5739: task edges-pos-ontonotes, batch 739 (5739): mcc: 0.8180, acc: 0.7099, precision: 0.8995, recall: 0.7501, f1: 0.8180, edges-pos-ontonotes_loss: 0.0209
10/01 03:35:33 AM: Update 5814: task edges-pos-ontonotes, batch 814 (5814): mcc: 0.8187, acc: 0.7105, precision: 0.9001, recall: 0.7508, f1: 0.8187, edges-pos-ontonotes_loss: 0.0208
10/01 03:35:43 AM: Update 5884: task edges-pos-ontonotes, batch 884 (5884): mcc: 0.8194, acc: 0.7114, precision: 0.9006, recall: 0.7518, f1: 0.8195, edges-pos-ontonotes_loss: 0.0207
10/01 03:35:53 AM: Update 5955: task edges-pos-ontonotes, batch 955 (5955): mcc: 0.8203, acc: 0.7123, precision: 0.9011, recall: 0.7528, f1: 0.8203, edges-pos-ontonotes_loss: 0.0205
10/01 03:36:01 AM: ***** Step 6000 / Validation 6 *****
10/01 03:36:01 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:36:01 AM: Validating...
10/01 03:36:03 AM: Evaluate: task edges-pos-ontonotes, batch 13 (157): mcc: 0.8186, acc: 0.7048, precision: 0.9344, recall: 0.7228, f1: 0.8151, edges-pos-ontonotes_loss: 0.0195
10/01 03:36:13 AM: Evaluate: task edges-pos-ontonotes, batch 67 (157): mcc: 0.8347, acc: 0.7273, precision: 0.9441, recall: 0.7431, f1: 0.8316, edges-pos-ontonotes_loss: 0.0184
10/01 03:36:24 AM: Evaluate: task edges-pos-ontonotes, batch 109 (157): mcc: 0.8423, acc: 0.7413, precision: 0.9406, recall: 0.7593, f1: 0.8403, edges-pos-ontonotes_loss: 0.0178
10/01 03:36:34 AM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.8419, acc: 0.7414, precision: 0.9382, recall: 0.7607, f1: 0.8402, edges-pos-ontonotes_loss: 0.0177
10/01 03:36:38 AM: Updating LR scheduler:
10/01 03:36:38 AM: 	Best result seen so far for macro_avg: 0.848
10/01 03:36:38 AM: 	# validation passes without improvement: 1
10/01 03:36:38 AM: edges-pos-ontonotes_loss: training: 0.020517 validation: 0.017591
10/01 03:36:38 AM: macro_avg: validation: 0.841663
10/01 03:36:38 AM: micro_avg: validation: 0.000000
10/01 03:36:38 AM: edges-pos-ontonotes_mcc: training: 0.820826 validation: 0.843346
10/01 03:36:38 AM: edges-pos-ontonotes_acc: training: 0.713102 validation: 0.743643
10/01 03:36:38 AM: edges-pos-ontonotes_precision: training: 0.901491 validation: 0.938749
10/01 03:36:38 AM: edges-pos-ontonotes_recall: training: 0.753490 validation: 0.762776
10/01 03:36:38 AM: edges-pos-ontonotes_f1: training: 0.820873 validation: 0.841663
10/01 03:36:38 AM: Global learning rate: 0.0001
10/01 03:36:38 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:36:44 AM: Update 6056: task edges-pos-ontonotes, batch 56 (6056): mcc: 0.8442, acc: 0.7446, precision: 0.9182, recall: 0.7816, f1: 0.8444, edges-pos-ontonotes_loss: 0.0188
10/01 03:36:54 AM: Update 6139: task edges-pos-ontonotes, batch 139 (6139): mcc: 0.8445, acc: 0.7451, precision: 0.9185, recall: 0.7819, f1: 0.8447, edges-pos-ontonotes_loss: 0.0184
10/01 03:37:04 AM: Update 6218: task edges-pos-ontonotes, batch 218 (6218): mcc: 0.8464, acc: 0.7485, precision: 0.9178, recall: 0.7859, f1: 0.8468, edges-pos-ontonotes_loss: 0.0182
10/01 03:37:14 AM: Update 6292: task edges-pos-ontonotes, batch 292 (6292): mcc: 0.8448, acc: 0.7462, precision: 0.9167, recall: 0.7840, f1: 0.8451, edges-pos-ontonotes_loss: 0.0182
10/01 03:37:24 AM: Update 6396: task edges-pos-ontonotes, batch 396 (6396): mcc: 0.8361, acc: 0.7339, precision: 0.9120, recall: 0.7721, f1: 0.8363, edges-pos-ontonotes_loss: 0.0190
10/01 03:37:34 AM: Update 6499: task edges-pos-ontonotes, batch 499 (6499): mcc: 0.8305, acc: 0.7259, precision: 0.9088, recall: 0.7647, f1: 0.8306, edges-pos-ontonotes_loss: 0.0194
10/01 03:37:45 AM: Update 6591: task edges-pos-ontonotes, batch 591 (6591): mcc: 0.8275, acc: 0.7217, precision: 0.9065, recall: 0.7613, f1: 0.8276, edges-pos-ontonotes_loss: 0.0195
10/01 03:37:55 AM: Update 6641: task edges-pos-ontonotes, batch 641 (6641): mcc: 0.8210, acc: 0.7124, precision: 0.9015, recall: 0.7539, f1: 0.8211, edges-pos-ontonotes_loss: 0.0200
10/01 03:38:06 AM: Update 6692: task edges-pos-ontonotes, batch 692 (6692): mcc: 0.8165, acc: 0.7060, precision: 0.8982, recall: 0.7484, f1: 0.8165, edges-pos-ontonotes_loss: 0.0204
10/01 03:38:16 AM: Update 6733: task edges-pos-ontonotes, batch 733 (6733): mcc: 0.8134, acc: 0.7017, precision: 0.8956, recall: 0.7451, f1: 0.8134, edges-pos-ontonotes_loss: 0.0206
10/01 03:38:26 AM: Update 6776: task edges-pos-ontonotes, batch 776 (6776): mcc: 0.8114, acc: 0.6990, precision: 0.8939, recall: 0.7429, f1: 0.8114, edges-pos-ontonotes_loss: 0.0208
10/01 03:38:37 AM: Update 6821: task edges-pos-ontonotes, batch 821 (6821): mcc: 0.8095, acc: 0.6962, precision: 0.8925, recall: 0.7407, f1: 0.8096, edges-pos-ontonotes_loss: 0.0210
10/01 03:38:47 AM: Update 6871: task edges-pos-ontonotes, batch 871 (6871): mcc: 0.8080, acc: 0.6943, precision: 0.8912, recall: 0.7392, f1: 0.8081, edges-pos-ontonotes_loss: 0.0211
10/01 03:38:57 AM: Update 6919: task edges-pos-ontonotes, batch 919 (6919): mcc: 0.8061, acc: 0.6917, precision: 0.8899, recall: 0.7367, f1: 0.8061, edges-pos-ontonotes_loss: 0.0213
10/01 03:39:07 AM: Update 6978: task edges-pos-ontonotes, batch 978 (6978): mcc: 0.8062, acc: 0.6918, precision: 0.8900, recall: 0.7368, f1: 0.8062, edges-pos-ontonotes_loss: 0.0213
10/01 03:39:11 AM: ***** Step 7000 / Validation 7 *****
10/01 03:39:11 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:39:11 AM: Validating...
10/01 03:39:17 AM: Evaluate: task edges-pos-ontonotes, batch 33 (157): mcc: 0.8446, acc: 0.7475, precision: 0.9321, recall: 0.7705, f1: 0.8436, edges-pos-ontonotes_loss: 0.0172
10/01 03:39:27 AM: Evaluate: task edges-pos-ontonotes, batch 83 (157): mcc: 0.8606, acc: 0.7732, precision: 0.9351, recall: 0.7968, f1: 0.8604, edges-pos-ontonotes_loss: 0.0162
10/01 03:39:37 AM: Evaluate: task edges-pos-ontonotes, batch 119 (157): mcc: 0.8608, acc: 0.7754, precision: 0.9296, recall: 0.8020, f1: 0.8611, edges-pos-ontonotes_loss: 0.0162
10/01 03:39:48 AM: Evaluate: task edges-pos-ontonotes, batch 154 (157): mcc: 0.8606, acc: 0.7753, precision: 0.9272, recall: 0.8037, f1: 0.8611, edges-pos-ontonotes_loss: 0.0162
10/01 03:39:48 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:39:48 AM: Best result seen so far for macro.
10/01 03:39:48 AM: Updating LR scheduler:
10/01 03:39:48 AM: 	Best result seen so far for macro_avg: 0.861
10/01 03:39:48 AM: 	# validation passes without improvement: 0
10/01 03:39:48 AM: edges-pos-ontonotes_loss: training: 0.021330 validation: 0.016153
10/01 03:39:48 AM: macro_avg: validation: 0.861406
10/01 03:39:48 AM: micro_avg: validation: 0.000000
10/01 03:39:48 AM: edges-pos-ontonotes_mcc: training: 0.806435 validation: 0.860904
10/01 03:39:48 AM: edges-pos-ontonotes_acc: training: 0.692208 validation: 0.775887
10/01 03:39:48 AM: edges-pos-ontonotes_precision: training: 0.889978 validation: 0.926847
10/01 03:39:48 AM: edges-pos-ontonotes_recall: training: 0.737275 validation: 0.804597
10/01 03:39:48 AM: edges-pos-ontonotes_f1: training: 0.806462 validation: 0.861406
10/01 03:39:48 AM: Global learning rate: 0.0001
10/01 03:39:48 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:39:58 AM: Update 7063: task edges-pos-ontonotes, batch 63 (7063): mcc: 0.8143, acc: 0.7024, precision: 0.8918, recall: 0.7499, f1: 0.8147, edges-pos-ontonotes_loss: 0.0197
10/01 03:40:08 AM: Update 7136: task edges-pos-ontonotes, batch 136 (7136): mcc: 0.8116, acc: 0.6994, precision: 0.8914, recall: 0.7454, f1: 0.8119, edges-pos-ontonotes_loss: 0.0200
10/01 03:40:18 AM: Update 7209: task edges-pos-ontonotes, batch 209 (7209): mcc: 0.8119, acc: 0.6994, precision: 0.8917, recall: 0.7457, f1: 0.8122, edges-pos-ontonotes_loss: 0.0199
10/01 03:40:28 AM: Update 7251: task edges-pos-ontonotes, batch 251 (7251): mcc: 0.8142, acc: 0.7030, precision: 0.8906, recall: 0.7507, f1: 0.8147, edges-pos-ontonotes_loss: 0.0200
10/01 03:40:38 AM: Update 7309: task edges-pos-ontonotes, batch 309 (7309): mcc: 0.8171, acc: 0.7071, precision: 0.8917, recall: 0.7550, f1: 0.8177, edges-pos-ontonotes_loss: 0.0200
10/01 03:40:48 AM: Update 7358: task edges-pos-ontonotes, batch 358 (7358): mcc: 0.8182, acc: 0.7088, precision: 0.8924, recall: 0.7566, f1: 0.8189, edges-pos-ontonotes_loss: 0.0199
10/01 03:40:59 AM: Update 7415: task edges-pos-ontonotes, batch 415 (7415): mcc: 0.8187, acc: 0.7098, precision: 0.8923, recall: 0.7575, f1: 0.8194, edges-pos-ontonotes_loss: 0.0200
10/01 03:41:09 AM: Update 7469: task edges-pos-ontonotes, batch 469 (7469): mcc: 0.8192, acc: 0.7106, precision: 0.8921, recall: 0.7585, f1: 0.8199, edges-pos-ontonotes_loss: 0.0200
10/01 03:41:19 AM: Update 7530: task edges-pos-ontonotes, batch 530 (7530): mcc: 0.8199, acc: 0.7117, precision: 0.8924, recall: 0.7594, f1: 0.8206, edges-pos-ontonotes_loss: 0.0199
10/01 03:41:33 AM: Update 7547: task edges-pos-ontonotes, batch 547 (7547): mcc: 0.8199, acc: 0.7119, precision: 0.8925, recall: 0.7595, f1: 0.8206, edges-pos-ontonotes_loss: 0.0199
10/01 03:41:43 AM: Update 7586: task edges-pos-ontonotes, batch 586 (7586): mcc: 0.8183, acc: 0.7097, precision: 0.8916, recall: 0.7574, f1: 0.8190, edges-pos-ontonotes_loss: 0.0201
10/01 03:41:53 AM: Update 7626: task edges-pos-ontonotes, batch 626 (7626): mcc: 0.8174, acc: 0.7085, precision: 0.8912, recall: 0.7561, f1: 0.8181, edges-pos-ontonotes_loss: 0.0202
10/01 03:42:03 AM: Update 7669: task edges-pos-ontonotes, batch 669 (7669): mcc: 0.8168, acc: 0.7079, precision: 0.8905, recall: 0.7556, f1: 0.8175, edges-pos-ontonotes_loss: 0.0204
10/01 03:42:13 AM: Update 7718: task edges-pos-ontonotes, batch 718 (7718): mcc: 0.8162, acc: 0.7071, precision: 0.8902, recall: 0.7547, f1: 0.8169, edges-pos-ontonotes_loss: 0.0205
10/01 03:42:23 AM: Update 7769: task edges-pos-ontonotes, batch 769 (7769): mcc: 0.8161, acc: 0.7071, precision: 0.8902, recall: 0.7545, f1: 0.8167, edges-pos-ontonotes_loss: 0.0205
10/01 03:42:33 AM: Update 7814: task edges-pos-ontonotes, batch 814 (7814): mcc: 0.8159, acc: 0.7069, precision: 0.8900, recall: 0.7543, f1: 0.8165, edges-pos-ontonotes_loss: 0.0206
10/01 03:42:43 AM: Update 7858: task edges-pos-ontonotes, batch 858 (7858): mcc: 0.8156, acc: 0.7066, precision: 0.8898, recall: 0.7541, f1: 0.8163, edges-pos-ontonotes_loss: 0.0206
10/01 03:42:54 AM: Update 7885: task edges-pos-ontonotes, batch 885 (7885): mcc: 0.8158, acc: 0.7069, precision: 0.8896, recall: 0.7544, f1: 0.8165, edges-pos-ontonotes_loss: 0.0206
10/01 03:43:04 AM: Update 7930: task edges-pos-ontonotes, batch 930 (7930): mcc: 0.8162, acc: 0.7076, precision: 0.8897, recall: 0.7551, f1: 0.8169, edges-pos-ontonotes_loss: 0.0206
10/01 03:43:14 AM: Update 7972: task edges-pos-ontonotes, batch 972 (7972): mcc: 0.8167, acc: 0.7084, precision: 0.8900, recall: 0.7559, f1: 0.8175, edges-pos-ontonotes_loss: 0.0206
10/01 03:43:21 AM: ***** Step 8000 / Validation 8 *****
10/01 03:43:21 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:43:21 AM: Validating...
10/01 03:43:24 AM: Evaluate: task edges-pos-ontonotes, batch 16 (157): mcc: 0.8445, acc: 0.7488, precision: 0.9331, recall: 0.7695, f1: 0.8434, edges-pos-ontonotes_loss: 0.0170
10/01 03:43:34 AM: Evaluate: task edges-pos-ontonotes, batch 69 (157): mcc: 0.8612, acc: 0.7721, precision: 0.9440, recall: 0.7904, f1: 0.8604, edges-pos-ontonotes_loss: 0.0159
10/01 03:43:44 AM: Evaluate: task edges-pos-ontonotes, batch 110 (157): mcc: 0.8669, acc: 0.7828, precision: 0.9410, recall: 0.8033, f1: 0.8667, edges-pos-ontonotes_loss: 0.0154
10/01 03:43:55 AM: Evaluate: task edges-pos-ontonotes, batch 144 (157): mcc: 0.8661, acc: 0.7823, precision: 0.9376, recall: 0.8047, f1: 0.8661, edges-pos-ontonotes_loss: 0.0154
10/01 03:43:58 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:43:58 AM: Best result seen so far for macro.
10/01 03:43:58 AM: Updating LR scheduler:
10/01 03:43:58 AM: 	Best result seen so far for macro_avg: 0.868
10/01 03:43:58 AM: 	# validation passes without improvement: 0
10/01 03:43:58 AM: edges-pos-ontonotes_loss: training: 0.020553 validation: 0.015275
10/01 03:43:58 AM: macro_avg: validation: 0.867537
10/01 03:43:58 AM: micro_avg: validation: 0.000000
10/01 03:43:58 AM: edges-pos-ontonotes_mcc: training: 0.817111 validation: 0.867488
10/01 03:43:58 AM: edges-pos-ontonotes_acc: training: 0.709022 validation: 0.784744
10/01 03:43:58 AM: edges-pos-ontonotes_precision: training: 0.890269 validation: 0.938067
10/01 03:43:58 AM: edges-pos-ontonotes_recall: training: 0.756327 validation: 0.806872
10/01 03:43:58 AM: edges-pos-ontonotes_f1: training: 0.817850 validation: 0.867537
10/01 03:43:58 AM: Global learning rate: 0.0001
10/01 03:43:58 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:44:05 AM: Update 8032: task edges-pos-ontonotes, batch 32 (8032): mcc: 0.8252, acc: 0.7214, precision: 0.8947, recall: 0.7672, f1: 0.8261, edges-pos-ontonotes_loss: 0.0204
10/01 03:44:15 AM: Update 8073: task edges-pos-ontonotes, batch 73 (8073): mcc: 0.8274, acc: 0.7257, precision: 0.8950, recall: 0.7709, f1: 0.8284, edges-pos-ontonotes_loss: 0.0199
10/01 03:44:26 AM: Update 8115: task edges-pos-ontonotes, batch 115 (8115): mcc: 0.8282, acc: 0.7273, precision: 0.8950, recall: 0.7725, f1: 0.8293, edges-pos-ontonotes_loss: 0.0198
10/01 03:44:36 AM: Update 8161: task edges-pos-ontonotes, batch 161 (8161): mcc: 0.8290, acc: 0.7283, precision: 0.8961, recall: 0.7730, f1: 0.8300, edges-pos-ontonotes_loss: 0.0197
10/01 03:44:46 AM: Update 8190: task edges-pos-ontonotes, batch 190 (8190): mcc: 0.8277, acc: 0.7267, precision: 0.8952, recall: 0.7714, f1: 0.8287, edges-pos-ontonotes_loss: 0.0198
10/01 03:44:56 AM: Update 8231: task edges-pos-ontonotes, batch 231 (8231): mcc: 0.8272, acc: 0.7263, precision: 0.8946, recall: 0.7710, f1: 0.8282, edges-pos-ontonotes_loss: 0.0200
10/01 03:45:07 AM: Update 8274: task edges-pos-ontonotes, batch 274 (8274): mcc: 0.8280, acc: 0.7276, precision: 0.8951, recall: 0.7720, f1: 0.8290, edges-pos-ontonotes_loss: 0.0198
10/01 03:45:17 AM: Update 8318: task edges-pos-ontonotes, batch 318 (8318): mcc: 0.8285, acc: 0.7283, precision: 0.8958, recall: 0.7723, f1: 0.8294, edges-pos-ontonotes_loss: 0.0198
10/01 03:45:27 AM: Update 8364: task edges-pos-ontonotes, batch 364 (8364): mcc: 0.8287, acc: 0.7288, precision: 0.8960, recall: 0.7726, f1: 0.8297, edges-pos-ontonotes_loss: 0.0198
10/01 03:45:37 AM: Update 8407: task edges-pos-ontonotes, batch 407 (8407): mcc: 0.8292, acc: 0.7297, precision: 0.8963, recall: 0.7732, f1: 0.8302, edges-pos-ontonotes_loss: 0.0197
10/01 03:45:47 AM: Update 8453: task edges-pos-ontonotes, batch 453 (8453): mcc: 0.8296, acc: 0.7301, precision: 0.8966, recall: 0.7736, f1: 0.8306, edges-pos-ontonotes_loss: 0.0197
10/01 03:45:57 AM: Update 8486: task edges-pos-ontonotes, batch 486 (8486): mcc: 0.8298, acc: 0.7304, precision: 0.8968, recall: 0.7738, f1: 0.8308, edges-pos-ontonotes_loss: 0.0196
10/01 03:46:07 AM: Update 8524: task edges-pos-ontonotes, batch 524 (8524): mcc: 0.8300, acc: 0.7307, precision: 0.8971, recall: 0.7740, f1: 0.8310, edges-pos-ontonotes_loss: 0.0196
10/01 03:46:17 AM: Update 8568: task edges-pos-ontonotes, batch 568 (8568): mcc: 0.8301, acc: 0.7308, precision: 0.8970, recall: 0.7742, f1: 0.8311, edges-pos-ontonotes_loss: 0.0196
10/01 03:46:28 AM: Update 8613: task edges-pos-ontonotes, batch 613 (8613): mcc: 0.8307, acc: 0.7316, precision: 0.8974, recall: 0.7749, f1: 0.8317, edges-pos-ontonotes_loss: 0.0196
10/01 03:46:38 AM: Update 8656: task edges-pos-ontonotes, batch 656 (8656): mcc: 0.8312, acc: 0.7325, precision: 0.8976, recall: 0.7757, f1: 0.8322, edges-pos-ontonotes_loss: 0.0195
10/01 03:46:48 AM: Update 8701: task edges-pos-ontonotes, batch 701 (8701): mcc: 0.8316, acc: 0.7332, precision: 0.8977, recall: 0.7764, f1: 0.8326, edges-pos-ontonotes_loss: 0.0195
10/01 03:46:58 AM: Update 8749: task edges-pos-ontonotes, batch 749 (8749): mcc: 0.8318, acc: 0.7335, precision: 0.8978, recall: 0.7767, f1: 0.8329, edges-pos-ontonotes_loss: 0.0195
10/01 03:47:08 AM: Update 8789: task edges-pos-ontonotes, batch 789 (8789): mcc: 0.8322, acc: 0.7340, precision: 0.8980, recall: 0.7771, f1: 0.8332, edges-pos-ontonotes_loss: 0.0195
10/01 03:47:18 AM: Update 8822: task edges-pos-ontonotes, batch 822 (8822): mcc: 0.8320, acc: 0.7338, precision: 0.8980, recall: 0.7768, f1: 0.8330, edges-pos-ontonotes_loss: 0.0195
10/01 03:47:28 AM: Update 8871: task edges-pos-ontonotes, batch 871 (8871): mcc: 0.8321, acc: 0.7338, precision: 0.8982, recall: 0.7769, f1: 0.8331, edges-pos-ontonotes_loss: 0.0194
10/01 03:47:39 AM: Update 8927: task edges-pos-ontonotes, batch 927 (8927): mcc: 0.8319, acc: 0.7334, precision: 0.8981, recall: 0.7766, f1: 0.8329, edges-pos-ontonotes_loss: 0.0194
10/01 03:47:49 AM: Update 8975: task edges-pos-ontonotes, batch 975 (8975): mcc: 0.8319, acc: 0.7333, precision: 0.8982, recall: 0.7765, f1: 0.8329, edges-pos-ontonotes_loss: 0.0193
10/01 03:47:54 AM: ***** Step 9000 / Validation 9 *****
10/01 03:47:54 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:47:54 AM: Validating...
10/01 03:47:59 AM: Evaluate: task edges-pos-ontonotes, batch 27 (157): mcc: 0.8451, acc: 0.7473, precision: 0.9376, recall: 0.7669, f1: 0.8437, edges-pos-ontonotes_loss: 0.0170
10/01 03:48:09 AM: Evaluate: task edges-pos-ontonotes, batch 78 (157): mcc: 0.8633, acc: 0.7760, precision: 0.9424, recall: 0.7954, f1: 0.8627, edges-pos-ontonotes_loss: 0.0157
10/01 03:48:19 AM: Evaluate: task edges-pos-ontonotes, batch 116 (157): mcc: 0.8669, acc: 0.7833, precision: 0.9397, recall: 0.8044, f1: 0.8668, edges-pos-ontonotes_loss: 0.0154
10/01 03:48:29 AM: Evaluate: task edges-pos-ontonotes, batch 151 (157): mcc: 0.8685, acc: 0.7863, precision: 0.9383, recall: 0.8084, f1: 0.8686, edges-pos-ontonotes_loss: 0.0151
10/01 03:48:31 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:48:31 AM: Best result seen so far for macro.
10/01 03:48:31 AM: Updating LR scheduler:
10/01 03:48:31 AM: 	Best result seen so far for macro_avg: 0.870
10/01 03:48:31 AM: 	# validation passes without improvement: 0
10/01 03:48:31 AM: edges-pos-ontonotes_loss: training: 0.019323 validation: 0.015066
10/01 03:48:31 AM: macro_avg: validation: 0.869702
10/01 03:48:31 AM: micro_avg: validation: 0.000000
10/01 03:48:31 AM: edges-pos-ontonotes_mcc: training: 0.831962 validation: 0.869552
10/01 03:48:31 AM: edges-pos-ontonotes_acc: training: 0.733342 validation: 0.788332
10/01 03:48:31 AM: edges-pos-ontonotes_precision: training: 0.898167 validation: 0.938246
10/01 03:48:31 AM: edges-pos-ontonotes_recall: training: 0.776599 validation: 0.810491
10/01 03:48:31 AM: edges-pos-ontonotes_f1: training: 0.832970 validation: 0.869702
10/01 03:48:31 AM: Global learning rate: 0.0001
10/01 03:48:31 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:48:40 AM: Update 9047: task edges-pos-ontonotes, batch 47 (9047): mcc: 0.8354, acc: 0.7363, precision: 0.9013, recall: 0.7802, f1: 0.8364, edges-pos-ontonotes_loss: 0.0184
10/01 03:48:50 AM: Update 9103: task edges-pos-ontonotes, batch 103 (9103): mcc: 0.8339, acc: 0.7345, precision: 0.9011, recall: 0.7777, f1: 0.8349, edges-pos-ontonotes_loss: 0.0183
10/01 03:49:01 AM: Update 9112: task edges-pos-ontonotes, batch 112 (9112): mcc: 0.8339, acc: 0.7344, precision: 0.9004, recall: 0.7783, f1: 0.8349, edges-pos-ontonotes_loss: 0.0183
10/01 03:49:11 AM: Update 9184: task edges-pos-ontonotes, batch 184 (9184): mcc: 0.8365, acc: 0.7362, precision: 0.9029, recall: 0.7808, f1: 0.8374, edges-pos-ontonotes_loss: 0.0182
10/01 03:49:21 AM: Update 9256: task edges-pos-ontonotes, batch 256 (9256): mcc: 0.8390, acc: 0.7387, precision: 0.9049, recall: 0.7835, f1: 0.8399, edges-pos-ontonotes_loss: 0.0181
10/01 03:49:32 AM: Update 9328: task edges-pos-ontonotes, batch 328 (9328): mcc: 0.8403, acc: 0.7405, precision: 0.9062, recall: 0.7849, f1: 0.8412, edges-pos-ontonotes_loss: 0.0180
10/01 03:49:42 AM: Update 9394: task edges-pos-ontonotes, batch 394 (9394): mcc: 0.8417, acc: 0.7420, precision: 0.9072, recall: 0.7865, f1: 0.8426, edges-pos-ontonotes_loss: 0.0179
10/01 03:49:52 AM: Update 9456: task edges-pos-ontonotes, batch 456 (9456): mcc: 0.8426, acc: 0.7434, precision: 0.9074, recall: 0.7880, f1: 0.8435, edges-pos-ontonotes_loss: 0.0179
10/01 03:50:02 AM: Update 9536: task edges-pos-ontonotes, batch 536 (9536): mcc: 0.8441, acc: 0.7459, precision: 0.9085, recall: 0.7898, f1: 0.8450, edges-pos-ontonotes_loss: 0.0178
10/01 03:50:12 AM: Update 9622: task edges-pos-ontonotes, batch 622 (9622): mcc: 0.8457, acc: 0.7484, precision: 0.9096, recall: 0.7918, f1: 0.8466, edges-pos-ontonotes_loss: 0.0176
10/01 03:50:22 AM: Update 9711: task edges-pos-ontonotes, batch 711 (9711): mcc: 0.8468, acc: 0.7501, precision: 0.9104, recall: 0.7932, f1: 0.8478, edges-pos-ontonotes_loss: 0.0174
10/01 03:50:32 AM: Update 9788: task edges-pos-ontonotes, batch 788 (9788): mcc: 0.8456, acc: 0.7482, precision: 0.9097, recall: 0.7915, f1: 0.8465, edges-pos-ontonotes_loss: 0.0176
10/01 03:50:42 AM: Update 9899: task edges-pos-ontonotes, batch 899 (9899): mcc: 0.8433, acc: 0.7448, precision: 0.9082, recall: 0.7885, f1: 0.8442, edges-pos-ontonotes_loss: 0.0179
10/01 03:50:51 AM: ***** Step 10000 / Validation 10 *****
10/01 03:50:51 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:50:51 AM: Validating...
10/01 03:50:52 AM: Evaluate: task edges-pos-ontonotes, batch 7 (157): mcc: 0.8395, acc: 0.7416, precision: 0.9263, recall: 0.7663, f1: 0.8387, edges-pos-ontonotes_loss: 0.0173
10/01 03:51:02 AM: Evaluate: task edges-pos-ontonotes, batch 61 (157): mcc: 0.8596, acc: 0.7712, precision: 0.9371, recall: 0.7934, f1: 0.8593, edges-pos-ontonotes_loss: 0.0161
10/01 03:51:12 AM: Evaluate: task edges-pos-ontonotes, batch 104 (157): mcc: 0.8646, acc: 0.7807, precision: 0.9322, recall: 0.8068, f1: 0.8650, edges-pos-ontonotes_loss: 0.0156
10/01 03:51:22 AM: Evaluate: task edges-pos-ontonotes, batch 138 (157): mcc: 0.8621, acc: 0.7771, precision: 0.9282, recall: 0.8056, f1: 0.8625, edges-pos-ontonotes_loss: 0.0157
10/01 03:51:28 AM: Updating LR scheduler:
10/01 03:51:28 AM: 	Best result seen so far for macro_avg: 0.870
10/01 03:51:28 AM: 	# validation passes without improvement: 1
10/01 03:51:28 AM: edges-pos-ontonotes_loss: training: 0.017920 validation: 0.015576
10/01 03:51:28 AM: macro_avg: validation: 0.863764
10/01 03:51:28 AM: micro_avg: validation: 0.000000
10/01 03:51:28 AM: edges-pos-ontonotes_mcc: training: 0.841703 validation: 0.863223
10/01 03:51:28 AM: edges-pos-ontonotes_acc: training: 0.742609 validation: 0.779157
10/01 03:51:28 AM: edges-pos-ontonotes_precision: training: 0.907220 validation: 0.927998
10/01 03:51:28 AM: edges-pos-ontonotes_recall: training: 0.786546 validation: 0.807846
10/01 03:51:28 AM: edges-pos-ontonotes_f1: training: 0.842584 validation: 0.863764
10/01 03:51:28 AM: Global learning rate: 0.0001
10/01 03:51:28 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:51:33 AM: Update 10038: task edges-pos-ontonotes, batch 38 (10038): mcc: 0.8173, acc: 0.7121, precision: 0.8880, recall: 0.7585, f1: 0.8182, edges-pos-ontonotes_loss: 0.0200
10/01 03:51:43 AM: Update 10072: task edges-pos-ontonotes, batch 72 (10072): mcc: 0.8087, acc: 0.6989, precision: 0.8776, recall: 0.7519, f1: 0.8099, edges-pos-ontonotes_loss: 0.0210
10/01 03:51:53 AM: Update 10121: task edges-pos-ontonotes, batch 121 (10121): mcc: 0.8059, acc: 0.6937, precision: 0.8781, recall: 0.7463, f1: 0.8069, edges-pos-ontonotes_loss: 0.0216
10/01 03:52:03 AM: Update 10168: task edges-pos-ontonotes, batch 168 (10168): mcc: 0.8043, acc: 0.6915, precision: 0.8767, recall: 0.7447, f1: 0.8054, edges-pos-ontonotes_loss: 0.0220
10/01 03:52:14 AM: Update 10217: task edges-pos-ontonotes, batch 217 (10217): mcc: 0.8049, acc: 0.6919, precision: 0.8774, recall: 0.7451, f1: 0.8059, edges-pos-ontonotes_loss: 0.0223
10/01 03:52:24 AM: Update 10267: task edges-pos-ontonotes, batch 267 (10267): mcc: 0.8057, acc: 0.6927, precision: 0.8783, recall: 0.7459, f1: 0.8067, edges-pos-ontonotes_loss: 0.0222
10/01 03:52:34 AM: Update 10315: task edges-pos-ontonotes, batch 315 (10315): mcc: 0.8059, acc: 0.6927, precision: 0.8788, recall: 0.7458, f1: 0.8068, edges-pos-ontonotes_loss: 0.0224
10/01 03:52:44 AM: Update 10360: task edges-pos-ontonotes, batch 360 (10360): mcc: 0.8056, acc: 0.6923, precision: 0.8791, recall: 0.7450, f1: 0.8065, edges-pos-ontonotes_loss: 0.0223
10/01 03:52:54 AM: Update 10404: task edges-pos-ontonotes, batch 404 (10404): mcc: 0.8055, acc: 0.6921, precision: 0.8793, recall: 0.7447, f1: 0.8064, edges-pos-ontonotes_loss: 0.0223
10/01 03:53:04 AM: Update 10476: task edges-pos-ontonotes, batch 476 (10476): mcc: 0.8074, acc: 0.6948, precision: 0.8809, recall: 0.7468, f1: 0.8083, edges-pos-ontonotes_loss: 0.0219
10/01 03:53:14 AM: Update 10548: task edges-pos-ontonotes, batch 548 (10548): mcc: 0.8086, acc: 0.6965, precision: 0.8819, recall: 0.7481, f1: 0.8095, edges-pos-ontonotes_loss: 0.0216
10/01 03:53:24 AM: Update 10616: task edges-pos-ontonotes, batch 616 (10616): mcc: 0.8100, acc: 0.6984, precision: 0.8828, recall: 0.7499, f1: 0.8109, edges-pos-ontonotes_loss: 0.0213
10/01 03:53:34 AM: Update 10688: task edges-pos-ontonotes, batch 688 (10688): mcc: 0.8113, acc: 0.7003, precision: 0.8834, recall: 0.7516, f1: 0.8122, edges-pos-ontonotes_loss: 0.0210
10/01 03:53:44 AM: Update 10733: task edges-pos-ontonotes, batch 733 (10733): mcc: 0.8125, acc: 0.7022, precision: 0.8839, recall: 0.7534, f1: 0.8135, edges-pos-ontonotes_loss: 0.0209
10/01 03:53:54 AM: Update 10780: task edges-pos-ontonotes, batch 780 (10780): mcc: 0.8142, acc: 0.7045, precision: 0.8847, recall: 0.7557, f1: 0.8152, edges-pos-ontonotes_loss: 0.0207
10/01 03:54:04 AM: Update 10838: task edges-pos-ontonotes, batch 838 (10838): mcc: 0.8152, acc: 0.7061, precision: 0.8852, recall: 0.7573, f1: 0.8163, edges-pos-ontonotes_loss: 0.0206
10/01 03:54:15 AM: Update 10894: task edges-pos-ontonotes, batch 894 (10894): mcc: 0.8167, acc: 0.7081, precision: 0.8861, recall: 0.7592, f1: 0.8177, edges-pos-ontonotes_loss: 0.0205
10/01 03:54:25 AM: Update 10949: task edges-pos-ontonotes, batch 949 (10949): mcc: 0.8177, acc: 0.7096, precision: 0.8866, recall: 0.7605, f1: 0.8187, edges-pos-ontonotes_loss: 0.0204
10/01 03:54:33 AM: ***** Step 11000 / Validation 11 *****
10/01 03:54:33 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:54:33 AM: Validating...
10/01 03:54:35 AM: Evaluate: task edges-pos-ontonotes, batch 10 (157): mcc: 0.8525, acc: 0.7609, precision: 0.9344, recall: 0.7828, f1: 0.8520, edges-pos-ontonotes_loss: 0.0160
10/01 03:54:45 AM: Evaluate: task edges-pos-ontonotes, batch 64 (157): mcc: 0.8694, acc: 0.7851, precision: 0.9446, recall: 0.8046, f1: 0.8690, edges-pos-ontonotes_loss: 0.0148
10/01 03:54:55 AM: Evaluate: task edges-pos-ontonotes, batch 107 (157): mcc: 0.8749, acc: 0.7955, precision: 0.9408, recall: 0.8180, f1: 0.8751, edges-pos-ontonotes_loss: 0.0143
10/01 03:55:05 AM: Evaluate: task edges-pos-ontonotes, batch 141 (157): mcc: 0.8715, acc: 0.7906, precision: 0.9377, recall: 0.8146, f1: 0.8718, edges-pos-ontonotes_loss: 0.0145
10/01 03:55:10 AM: Best result seen so far for edges-pos-ontonotes.
10/01 03:55:10 AM: Best result seen so far for macro.
10/01 03:55:10 AM: Updating LR scheduler:
10/01 03:55:10 AM: 	Best result seen so far for macro_avg: 0.873
10/01 03:55:10 AM: 	# validation passes without improvement: 0
10/01 03:55:10 AM: edges-pos-ontonotes_loss: training: 0.020308 validation: 0.014433
10/01 03:55:10 AM: macro_avg: validation: 0.872621
10/01 03:55:10 AM: micro_avg: validation: 0.000000
10/01 03:55:10 AM: edges-pos-ontonotes_mcc: training: 0.818275 validation: 0.872267
10/01 03:55:10 AM: edges-pos-ontonotes_acc: training: 0.710449 validation: 0.792237
10/01 03:55:10 AM: edges-pos-ontonotes_precision: training: 0.886944 validation: 0.937344
10/01 03:55:10 AM: edges-pos-ontonotes_recall: training: 0.761321 validation: 0.816259
10/01 03:55:10 AM: edges-pos-ontonotes_f1: training: 0.819345 validation: 0.872621
10/01 03:55:10 AM: Global learning rate: 0.0001
10/01 03:55:10 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 03:55:16 AM: Update 11014: task edges-pos-ontonotes, batch 14 (11014): mcc: 0.8073, acc: 0.6980, precision: 0.8750, recall: 0.7516, f1: 0.8086, edges-pos-ontonotes_loss: 0.0207
10/01 03:55:26 AM: Update 11062: task edges-pos-ontonotes, batch 62 (11062): mcc: 0.8154, acc: 0.7070, precision: 0.8835, recall: 0.7592, f1: 0.8166, edges-pos-ontonotes_loss: 0.0211
10/01 03:55:36 AM: Update 11105: task edges-pos-ontonotes, batch 105 (11105): mcc: 0.8185, acc: 0.7119, precision: 0.8847, recall: 0.7637, f1: 0.8198, edges-pos-ontonotes_loss: 0.0209
10/01 03:55:46 AM: Update 11145: task edges-pos-ontonotes, batch 145 (11145): mcc: 0.8190, acc: 0.7126, precision: 0.8853, recall: 0.7641, f1: 0.8202, edges-pos-ontonotes_loss: 0.0208
10/01 03:55:56 AM: Update 11189: task edges-pos-ontonotes, batch 189 (11189): mcc: 0.8196, acc: 0.7133, precision: 0.8859, recall: 0.7647, f1: 0.8208, edges-pos-ontonotes_loss: 0.0207
10/01 03:56:06 AM: Update 11233: task edges-pos-ontonotes, batch 233 (11233): mcc: 0.8196, acc: 0.7134, precision: 0.8855, recall: 0.7649, f1: 0.8208, edges-pos-ontonotes_loss: 0.0207
10/01 03:56:16 AM: Update 11280: task edges-pos-ontonotes, batch 280 (11280): mcc: 0.8200, acc: 0.7143, precision: 0.8860, recall: 0.7653, f1: 0.8212, edges-pos-ontonotes_loss: 0.0206
10/01 03:56:36 AM: Update 11320: task edges-pos-ontonotes, batch 320 (11320): mcc: 0.8202, acc: 0.7146, precision: 0.8864, recall: 0.7653, f1: 0.8214, edges-pos-ontonotes_loss: 0.0205
10/01 03:56:46 AM: Update 11364: task edges-pos-ontonotes, batch 364 (11364): mcc: 0.8216, acc: 0.7166, precision: 0.8868, recall: 0.7676, f1: 0.8229, edges-pos-ontonotes_loss: 0.0204
10/01 03:56:56 AM: Update 11407: task edges-pos-ontonotes, batch 407 (11407): mcc: 0.8232, acc: 0.7191, precision: 0.8881, recall: 0.7694, f1: 0.8245, edges-pos-ontonotes_loss: 0.0203
10/01 03:57:06 AM: Update 11445: task edges-pos-ontonotes, batch 445 (11445): mcc: 0.8248, acc: 0.7215, precision: 0.8890, recall: 0.7714, f1: 0.8261, edges-pos-ontonotes_loss: 0.0202
10/01 03:57:16 AM: Update 11486: task edges-pos-ontonotes, batch 486 (11486): mcc: 0.8256, acc: 0.7228, precision: 0.8895, recall: 0.7725, f1: 0.8269, edges-pos-ontonotes_loss: 0.0201
10/01 03:57:26 AM: Update 11529: task edges-pos-ontonotes, batch 529 (11529): mcc: 0.8263, acc: 0.7240, precision: 0.8899, recall: 0.7734, f1: 0.8276, edges-pos-ontonotes_loss: 0.0200
10/01 03:57:36 AM: Update 11572: task edges-pos-ontonotes, batch 572 (11572): mcc: 0.8269, acc: 0.7250, precision: 0.8905, recall: 0.7741, f1: 0.8282, edges-pos-ontonotes_loss: 0.0199
10/01 03:57:46 AM: Update 11613: task edges-pos-ontonotes, batch 613 (11613): mcc: 0.8280, acc: 0.7265, precision: 0.8912, recall: 0.7755, f1: 0.8293, edges-pos-ontonotes_loss: 0.0198
10/01 03:57:57 AM: Update 11644: task edges-pos-ontonotes, batch 644 (11644): mcc: 0.8285, acc: 0.7272, precision: 0.8916, recall: 0.7761, f1: 0.8298, edges-pos-ontonotes_loss: 0.0197
10/01 03:58:07 AM: Update 11693: task edges-pos-ontonotes, batch 693 (11693): mcc: 0.8290, acc: 0.7280, precision: 0.8920, recall: 0.7766, f1: 0.8303, edges-pos-ontonotes_loss: 0.0197
10/01 03:58:17 AM: Update 11736: task edges-pos-ontonotes, batch 736 (11736): mcc: 0.8297, acc: 0.7291, precision: 0.8925, recall: 0.7774, f1: 0.8309, edges-pos-ontonotes_loss: 0.0196
10/01 03:58:27 AM: Update 11780: task edges-pos-ontonotes, batch 780 (11780): mcc: 0.8302, acc: 0.7300, precision: 0.8930, recall: 0.7779, f1: 0.8315, edges-pos-ontonotes_loss: 0.0196
10/01 03:58:37 AM: Update 11825: task edges-pos-ontonotes, batch 825 (11825): mcc: 0.8307, acc: 0.7307, precision: 0.8933, recall: 0.7785, f1: 0.8320, edges-pos-ontonotes_loss: 0.0195
10/01 03:58:47 AM: Update 11864: task edges-pos-ontonotes, batch 864 (11864): mcc: 0.8312, acc: 0.7316, precision: 0.8937, recall: 0.7791, f1: 0.8325, edges-pos-ontonotes_loss: 0.0195
10/01 03:58:57 AM: Update 11906: task edges-pos-ontonotes, batch 906 (11906): mcc: 0.8313, acc: 0.7317, precision: 0.8938, recall: 0.7792, f1: 0.8326, edges-pos-ontonotes_loss: 0.0195
10/01 03:59:09 AM: Update 11946: task edges-pos-ontonotes, batch 946 (11946): mcc: 0.8318, acc: 0.7325, precision: 0.8942, recall: 0.7798, f1: 0.8331, edges-pos-ontonotes_loss: 0.0194
10/01 03:59:19 AM: Update 11986: task edges-pos-ontonotes, batch 986 (11986): mcc: 0.8322, acc: 0.7332, precision: 0.8944, recall: 0.7803, f1: 0.8335, edges-pos-ontonotes_loss: 0.0194
10/01 03:59:23 AM: ***** Step 12000 / Validation 12 *****
10/01 03:59:23 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 03:59:23 AM: Validating...
10/01 03:59:30 AM: Evaluate: task edges-pos-ontonotes, batch 35 (157): mcc: 0.8613, acc: 0.7716, precision: 0.9421, recall: 0.7921, f1: 0.8606, edges-pos-ontonotes_loss: 0.0155
10/01 03:59:40 AM: Evaluate: task edges-pos-ontonotes, batch 86 (157): mcc: 0.8729, acc: 0.7918, precision: 0.9426, recall: 0.8127, f1: 0.8729, edges-pos-ontonotes_loss: 0.0146
10/01 03:59:50 AM: Evaluate: task edges-pos-ontonotes, batch 122 (157): mcc: 0.8749, acc: 0.7960, precision: 0.9403, recall: 0.8185, f1: 0.8752, edges-pos-ontonotes_loss: 0.0144
10/01 04:00:00 AM: Evaluate: task edges-pos-ontonotes, batch 157 (157): mcc: 0.8775, acc: 0.8016, precision: 0.9390, recall: 0.8244, f1: 0.8780, edges-pos-ontonotes_loss: 0.0141
10/01 04:00:00 AM: Best result seen so far for edges-pos-ontonotes.
10/01 04:00:00 AM: Best result seen so far for macro.
10/01 04:00:00 AM: Updating LR scheduler:
10/01 04:00:00 AM: 	Best result seen so far for macro_avg: 0.878
10/01 04:00:00 AM: 	# validation passes without improvement: 0
10/01 04:00:00 AM: edges-pos-ontonotes_loss: training: 0.019356 validation: 0.014132
10/01 04:00:00 AM: macro_avg: validation: 0.877994
10/01 04:00:00 AM: micro_avg: validation: 0.000000
10/01 04:00:00 AM: edges-pos-ontonotes_mcc: training: 0.832358 validation: 0.877501
10/01 04:00:00 AM: edges-pos-ontonotes_acc: training: 0.733397 validation: 0.801581
10/01 04:00:00 AM: edges-pos-ontonotes_precision: training: 0.894484 validation: 0.939032
10/01 04:00:00 AM: edges-pos-ontonotes_recall: training: 0.780559 validation: 0.824407
10/01 04:00:00 AM: edges-pos-ontonotes_f1: training: 0.833647 validation: 0.877994
10/01 04:00:00 AM: Global learning rate: 0.0001
10/01 04:00:00 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 04:00:10 AM: Update 12043: task edges-pos-ontonotes, batch 43 (12043): mcc: 0.8446, acc: 0.7523, precision: 0.9022, recall: 0.7963, f1: 0.8459, edges-pos-ontonotes_loss: 0.0183
10/01 04:00:20 AM: Update 12089: task edges-pos-ontonotes, batch 89 (12089): mcc: 0.8430, acc: 0.7506, precision: 0.9002, recall: 0.7952, f1: 0.8444, edges-pos-ontonotes_loss: 0.0190
10/01 04:00:30 AM: Update 12134: task edges-pos-ontonotes, batch 134 (12134): mcc: 0.8421, acc: 0.7490, precision: 0.9006, recall: 0.7931, f1: 0.8434, edges-pos-ontonotes_loss: 0.0189
10/01 04:00:40 AM: Update 12178: task edges-pos-ontonotes, batch 178 (12178): mcc: 0.8422, acc: 0.7493, precision: 0.9005, recall: 0.7934, f1: 0.8436, edges-pos-ontonotes_loss: 0.0187
10/01 04:00:50 AM: Update 12222: task edges-pos-ontonotes, batch 222 (12222): mcc: 0.8425, acc: 0.7498, precision: 0.9008, recall: 0.7938, f1: 0.8439, edges-pos-ontonotes_loss: 0.0186
10/01 04:01:02 AM: Update 12259: task edges-pos-ontonotes, batch 259 (12259): mcc: 0.8423, acc: 0.7496, precision: 0.9010, recall: 0.7932, f1: 0.8437, edges-pos-ontonotes_loss: 0.0185
10/01 04:01:12 AM: Update 12305: task edges-pos-ontonotes, batch 305 (12305): mcc: 0.8408, acc: 0.7470, precision: 0.9001, recall: 0.7910, f1: 0.8421, edges-pos-ontonotes_loss: 0.0185
10/01 04:01:22 AM: Update 12361: task edges-pos-ontonotes, batch 361 (12361): mcc: 0.8409, acc: 0.7467, precision: 0.9004, recall: 0.7911, f1: 0.8422, edges-pos-ontonotes_loss: 0.0184
10/01 04:01:32 AM: Update 12416: task edges-pos-ontonotes, batch 416 (12416): mcc: 0.8408, acc: 0.7461, precision: 0.9004, recall: 0.7909, f1: 0.8421, edges-pos-ontonotes_loss: 0.0183
10/01 04:01:42 AM: Update 12468: task edges-pos-ontonotes, batch 468 (12468): mcc: 0.8406, acc: 0.7457, precision: 0.9006, recall: 0.7904, f1: 0.8419, edges-pos-ontonotes_loss: 0.0182
10/01 04:01:53 AM: Update 12518: task edges-pos-ontonotes, batch 518 (12518): mcc: 0.8404, acc: 0.7452, precision: 0.9001, recall: 0.7903, f1: 0.8417, edges-pos-ontonotes_loss: 0.0181
10/01 04:02:03 AM: Update 12569: task edges-pos-ontonotes, batch 569 (12569): mcc: 0.8406, acc: 0.7456, precision: 0.9003, recall: 0.7907, f1: 0.8419, edges-pos-ontonotes_loss: 0.0181
10/01 04:02:13 AM: Update 12623: task edges-pos-ontonotes, batch 623 (12623): mcc: 0.8412, acc: 0.7460, precision: 0.9009, recall: 0.7911, f1: 0.8424, edges-pos-ontonotes_loss: 0.0180
10/01 04:02:23 AM: Update 12698: task edges-pos-ontonotes, batch 698 (12698): mcc: 0.8420, acc: 0.7467, precision: 0.9017, recall: 0.7919, f1: 0.8432, edges-pos-ontonotes_loss: 0.0179
10/01 04:02:33 AM: Update 12767: task edges-pos-ontonotes, batch 767 (12767): mcc: 0.8427, acc: 0.7473, precision: 0.9022, recall: 0.7927, f1: 0.8439, edges-pos-ontonotes_loss: 0.0178
10/01 04:02:43 AM: Update 12836: task edges-pos-ontonotes, batch 836 (12836): mcc: 0.8432, acc: 0.7479, precision: 0.9027, recall: 0.7933, f1: 0.8445, edges-pos-ontonotes_loss: 0.0177
10/01 04:02:54 AM: Update 12898: task edges-pos-ontonotes, batch 898 (12898): mcc: 0.8438, acc: 0.7486, precision: 0.9031, recall: 0.7939, f1: 0.8450, edges-pos-ontonotes_loss: 0.0177
10/01 04:03:04 AM: Update 12984: task edges-pos-ontonotes, batch 984 (12984): mcc: 0.8447, acc: 0.7498, precision: 0.9040, recall: 0.7949, f1: 0.8459, edges-pos-ontonotes_loss: 0.0175
10/01 04:03:05 AM: ***** Step 13000 / Validation 13 *****
10/01 04:03:05 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 04:03:05 AM: Validating...
10/01 04:03:14 AM: Evaluate: task edges-pos-ontonotes, batch 44 (157): mcc: 0.8538, acc: 0.7585, precision: 0.9463, recall: 0.7751, f1: 0.8522, edges-pos-ontonotes_loss: 0.0159
10/01 04:03:24 AM: Evaluate: task edges-pos-ontonotes, batch 92 (157): mcc: 0.8684, acc: 0.7839, precision: 0.9445, recall: 0.8030, f1: 0.8680, edges-pos-ontonotes_loss: 0.0149
10/01 04:03:34 AM: Evaluate: task edges-pos-ontonotes, batch 128 (157): mcc: 0.8664, acc: 0.7821, precision: 0.9405, recall: 0.8028, f1: 0.8662, edges-pos-ontonotes_loss: 0.0148
10/01 04:03:42 AM: Updating LR scheduler:
10/01 04:03:42 AM: 	Best result seen so far for macro_avg: 0.878
10/01 04:03:42 AM: 	# validation passes without improvement: 1
10/01 04:03:42 AM: edges-pos-ontonotes_loss: training: 0.017485 validation: 0.014595
10/01 04:03:42 AM: macro_avg: validation: 0.869382
10/01 04:03:42 AM: micro_avg: validation: 0.000000
10/01 04:03:42 AM: edges-pos-ontonotes_mcc: training: 0.844935 validation: 0.869360
10/01 04:03:42 AM: edges-pos-ontonotes_acc: training: 0.750178 validation: 0.787623
10/01 04:03:42 AM: edges-pos-ontonotes_precision: training: 0.904214 validation: 0.939862
10/01 04:03:42 AM: edges-pos-ontonotes_recall: training: 0.795149 validation: 0.808735
10/01 04:03:42 AM: edges-pos-ontonotes_f1: training: 0.846182 validation: 0.869382
10/01 04:03:42 AM: Global learning rate: 0.0001
10/01 04:03:42 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 04:03:44 AM: Update 13014: task edges-pos-ontonotes, batch 14 (13014): mcc: 0.8612, acc: 0.7754, precision: 0.9210, recall: 0.8103, f1: 0.8622, edges-pos-ontonotes_loss: 0.0167
10/01 04:03:55 AM: Update 13097: task edges-pos-ontonotes, batch 97 (13097): mcc: 0.8628, acc: 0.7757, precision: 0.9161, recall: 0.8177, f1: 0.8641, edges-pos-ontonotes_loss: 0.0162
10/01 04:04:05 AM: Update 13188: task edges-pos-ontonotes, batch 188 (13188): mcc: 0.8647, acc: 0.7784, precision: 0.9175, recall: 0.8199, f1: 0.8660, edges-pos-ontonotes_loss: 0.0158
10/01 04:04:15 AM: Update 13272: task edges-pos-ontonotes, batch 272 (13272): mcc: 0.8560, acc: 0.7654, precision: 0.9135, recall: 0.8074, f1: 0.8572, edges-pos-ontonotes_loss: 0.0167
10/01 04:04:25 AM: Update 13371: task edges-pos-ontonotes, batch 371 (13371): mcc: 0.8484, acc: 0.7542, precision: 0.9087, recall: 0.7975, f1: 0.8495, edges-pos-ontonotes_loss: 0.0173
10/01 04:04:35 AM: Update 13475: task edges-pos-ontonotes, batch 475 (13475): mcc: 0.8438, acc: 0.7478, precision: 0.9054, recall: 0.7921, f1: 0.8449, edges-pos-ontonotes_loss: 0.0176
10/01 04:04:49 AM: Update 13511: task edges-pos-ontonotes, batch 511 (13511): mcc: 0.8426, acc: 0.7461, precision: 0.9043, recall: 0.7909, f1: 0.8438, edges-pos-ontonotes_loss: 0.0176
10/01 04:04:59 AM: Update 13556: task edges-pos-ontonotes, batch 556 (13556): mcc: 0.8367, acc: 0.7374, precision: 0.8993, recall: 0.7844, f1: 0.8379, edges-pos-ontonotes_loss: 0.0180
10/01 04:05:09 AM: Update 13600: task edges-pos-ontonotes, batch 600 (13600): mcc: 0.8327, acc: 0.7316, precision: 0.8962, recall: 0.7797, f1: 0.8339, edges-pos-ontonotes_loss: 0.0183
10/01 04:05:19 AM: Update 13654: task edges-pos-ontonotes, batch 654 (13654): mcc: 0.8299, acc: 0.7273, precision: 0.8941, recall: 0.7764, f1: 0.8311, edges-pos-ontonotes_loss: 0.0186
10/01 04:05:29 AM: Update 13697: task edges-pos-ontonotes, batch 697 (13697): mcc: 0.8283, acc: 0.7250, precision: 0.8927, recall: 0.7747, f1: 0.8295, edges-pos-ontonotes_loss: 0.0187
10/01 04:05:39 AM: Update 13745: task edges-pos-ontonotes, batch 745 (13745): mcc: 0.8266, acc: 0.7226, precision: 0.8913, recall: 0.7728, f1: 0.8279, edges-pos-ontonotes_loss: 0.0189
10/01 04:05:49 AM: Update 13789: task edges-pos-ontonotes, batch 789 (13789): mcc: 0.8255, acc: 0.7211, precision: 0.8902, recall: 0.7717, f1: 0.8267, edges-pos-ontonotes_loss: 0.0191
10/01 04:05:59 AM: Update 13833: task edges-pos-ontonotes, batch 833 (13833): mcc: 0.8242, acc: 0.7193, precision: 0.8892, recall: 0.7702, f1: 0.8254, edges-pos-ontonotes_loss: 0.0193
10/01 04:06:09 AM: Update 13884: task edges-pos-ontonotes, batch 884 (13884): mcc: 0.8240, acc: 0.7190, precision: 0.8892, recall: 0.7699, f1: 0.8252, edges-pos-ontonotes_loss: 0.0193
10/01 04:06:19 AM: Update 13956: task edges-pos-ontonotes, batch 956 (13956): mcc: 0.8243, acc: 0.7196, precision: 0.8894, recall: 0.7703, f1: 0.8256, edges-pos-ontonotes_loss: 0.0193
10/01 04:06:26 AM: ***** Step 14000 / Validation 14 *****
10/01 04:06:26 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 04:06:26 AM: Validating...
10/01 04:06:29 AM: Evaluate: task edges-pos-ontonotes, batch 15 (157): mcc: 0.8575, acc: 0.7700, precision: 0.9328, recall: 0.7933, f1: 0.8574, edges-pos-ontonotes_loss: 0.0153
10/01 04:06:39 AM: Evaluate: task edges-pos-ontonotes, batch 68 (157): mcc: 0.8748, acc: 0.7954, precision: 0.9417, recall: 0.8171, f1: 0.8750, edges-pos-ontonotes_loss: 0.0141
10/01 04:06:50 AM: Evaluate: task edges-pos-ontonotes, batch 110 (157): mcc: 0.8779, acc: 0.8024, precision: 0.9360, recall: 0.8278, f1: 0.8786, edges-pos-ontonotes_loss: 0.0139
10/01 04:07:00 AM: Evaluate: task edges-pos-ontonotes, batch 144 (157): mcc: 0.8764, acc: 0.8003, precision: 0.9323, recall: 0.8284, f1: 0.8773, edges-pos-ontonotes_loss: 0.0140
10/01 04:07:03 AM: Best result seen so far for edges-pos-ontonotes.
10/01 04:07:03 AM: Best result seen so far for macro.
10/01 04:07:03 AM: Updating LR scheduler:
10/01 04:07:03 AM: 	Best result seen so far for macro_avg: 0.879
10/01 04:07:03 AM: 	# validation passes without improvement: 0
10/01 04:07:03 AM: edges-pos-ontonotes_loss: training: 0.019242 validation: 0.013911
10/01 04:07:03 AM: macro_avg: validation: 0.878568
10/01 04:07:03 AM: micro_avg: validation: 0.000000
10/01 04:07:03 AM: edges-pos-ontonotes_mcc: training: 0.824371 validation: 0.877666
10/01 04:07:03 AM: edges-pos-ontonotes_acc: training: 0.719658 validation: 0.802650
10/01 04:07:03 AM: edges-pos-ontonotes_precision: training: 0.889327 validation: 0.932511
10/01 04:07:03 AM: edges-pos-ontonotes_recall: training: 0.770409 validation: 0.830524
10/01 04:07:03 AM: edges-pos-ontonotes_f1: training: 0.825607 validation: 0.878568
10/01 04:07:03 AM: Global learning rate: 0.0001
10/01 04:07:03 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 04:07:10 AM: Update 14057: task edges-pos-ontonotes, batch 57 (14057): mcc: 0.8314, acc: 0.7297, precision: 0.8936, recall: 0.7796, f1: 0.8327, edges-pos-ontonotes_loss: 0.0182
10/01 04:07:20 AM: Update 14127: task edges-pos-ontonotes, batch 127 (14127): mcc: 0.8306, acc: 0.7288, precision: 0.8938, recall: 0.7778, f1: 0.8318, edges-pos-ontonotes_loss: 0.0182
10/01 04:07:30 AM: Update 14175: task edges-pos-ontonotes, batch 175 (14175): mcc: 0.8321, acc: 0.7308, precision: 0.8935, recall: 0.7808, f1: 0.8334, edges-pos-ontonotes_loss: 0.0182
10/01 04:07:40 AM: Update 14237: task edges-pos-ontonotes, batch 237 (14237): mcc: 0.8340, acc: 0.7337, precision: 0.8942, recall: 0.7837, f1: 0.8353, edges-pos-ontonotes_loss: 0.0182
10/01 04:07:51 AM: Update 14292: task edges-pos-ontonotes, batch 292 (14292): mcc: 0.8344, acc: 0.7342, precision: 0.8940, recall: 0.7848, f1: 0.8358, edges-pos-ontonotes_loss: 0.0182
10/01 04:08:01 AM: Update 14347: task edges-pos-ontonotes, batch 347 (14347): mcc: 0.8348, acc: 0.7348, precision: 0.8942, recall: 0.7853, f1: 0.8362, edges-pos-ontonotes_loss: 0.0183
10/01 04:08:11 AM: Update 14403: task edges-pos-ontonotes, batch 403 (14403): mcc: 0.8349, acc: 0.7351, precision: 0.8942, recall: 0.7855, f1: 0.8363, edges-pos-ontonotes_loss: 0.0183
10/01 04:08:21 AM: Update 14455: task edges-pos-ontonotes, batch 455 (14455): mcc: 0.8352, acc: 0.7355, precision: 0.8943, recall: 0.7860, f1: 0.8366, edges-pos-ontonotes_loss: 0.0183
10/01 04:08:31 AM: Update 14488: task edges-pos-ontonotes, batch 488 (14488): mcc: 0.8340, acc: 0.7339, precision: 0.8934, recall: 0.7846, f1: 0.8355, edges-pos-ontonotes_loss: 0.0184
10/01 04:08:41 AM: Update 14534: task edges-pos-ontonotes, batch 534 (14534): mcc: 0.8326, acc: 0.7320, precision: 0.8925, recall: 0.7827, f1: 0.8340, edges-pos-ontonotes_loss: 0.0186
10/01 04:08:51 AM: Update 14582: task edges-pos-ontonotes, batch 582 (14582): mcc: 0.8319, acc: 0.7311, precision: 0.8920, recall: 0.7819, f1: 0.8334, edges-pos-ontonotes_loss: 0.0188
10/01 04:09:01 AM: Update 14629: task edges-pos-ontonotes, batch 629 (14629): mcc: 0.8312, acc: 0.7302, precision: 0.8916, recall: 0.7810, f1: 0.8327, edges-pos-ontonotes_loss: 0.0188
10/01 04:09:11 AM: Update 14668: task edges-pos-ontonotes, batch 668 (14668): mcc: 0.8308, acc: 0.7298, precision: 0.8914, recall: 0.7805, f1: 0.8323, edges-pos-ontonotes_loss: 0.0189
10/01 04:09:22 AM: Update 14714: task edges-pos-ontonotes, batch 714 (14714): mcc: 0.8306, acc: 0.7294, precision: 0.8911, recall: 0.7803, f1: 0.8320, edges-pos-ontonotes_loss: 0.0189
10/01 04:09:32 AM: Update 14757: task edges-pos-ontonotes, batch 757 (14757): mcc: 0.8305, acc: 0.7294, precision: 0.8912, recall: 0.7800, f1: 0.8319, edges-pos-ontonotes_loss: 0.0190
10/01 04:09:42 AM: Update 14786: task edges-pos-ontonotes, batch 786 (14786): mcc: 0.8306, acc: 0.7296, precision: 0.8912, recall: 0.7802, f1: 0.8320, edges-pos-ontonotes_loss: 0.0189
10/01 04:09:52 AM: Update 14828: task edges-pos-ontonotes, batch 828 (14828): mcc: 0.8310, acc: 0.7304, precision: 0.8913, recall: 0.7809, f1: 0.8325, edges-pos-ontonotes_loss: 0.0189
10/01 04:10:02 AM: Update 14867: task edges-pos-ontonotes, batch 867 (14867): mcc: 0.8314, acc: 0.7311, precision: 0.8915, recall: 0.7815, f1: 0.8329, edges-pos-ontonotes_loss: 0.0189
10/01 04:10:12 AM: Update 14909: task edges-pos-ontonotes, batch 909 (14909): mcc: 0.8319, acc: 0.7317, precision: 0.8917, recall: 0.7821, f1: 0.8333, edges-pos-ontonotes_loss: 0.0189
10/01 04:10:22 AM: Update 14947: task edges-pos-ontonotes, batch 947 (14947): mcc: 0.8324, acc: 0.7325, precision: 0.8920, recall: 0.7827, f1: 0.8338, edges-pos-ontonotes_loss: 0.0188
10/01 04:10:32 AM: Update 14992: task edges-pos-ontonotes, batch 992 (14992): mcc: 0.8327, acc: 0.7330, precision: 0.8922, recall: 0.7832, f1: 0.8341, edges-pos-ontonotes_loss: 0.0188
10/01 04:10:35 AM: ***** Step 15000 / Validation 15 *****
10/01 04:10:35 AM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
10/01 04:10:35 AM: Validating...
10/01 04:10:43 AM: Evaluate: task edges-pos-ontonotes, batch 42 (157): mcc: 0.8670, acc: 0.7796, precision: 0.9460, recall: 0.7992, f1: 0.8664, edges-pos-ontonotes_loss: 0.0147
10/01 04:10:53 AM: Evaluate: task edges-pos-ontonotes, batch 91 (157): mcc: 0.8796, acc: 0.8018, precision: 0.9456, recall: 0.8225, f1: 0.8798, edges-pos-ontonotes_loss: 0.0138
10/01 04:11:03 AM: Evaluate: task edges-pos-ontonotes, batch 126 (157): mcc: 0.8785, acc: 0.8015, precision: 0.9416, recall: 0.8240, f1: 0.8789, edges-pos-ontonotes_loss: 0.0138
10/01 04:11:12 AM: Best result seen so far for edges-pos-ontonotes.
10/01 04:11:12 AM: Best result seen so far for macro.
10/01 04:11:12 AM: Updating LR scheduler:
10/01 04:11:12 AM: 	Best result seen so far for macro_avg: 0.882
10/01 04:11:12 AM: 	# validation passes without improvement: 0
10/01 04:11:12 AM: edges-pos-ontonotes_loss: training: 0.018812 validation: 0.013562
10/01 04:11:12 AM: macro_avg: validation: 0.882013
10/01 04:11:12 AM: micro_avg: validation: 0.000000
10/01 04:11:12 AM: edges-pos-ontonotes_mcc: training: 0.832831 validation: 0.881477
10/01 04:11:12 AM: edges-pos-ontonotes_acc: training: 0.733227 validation: 0.807444
10/01 04:11:12 AM: edges-pos-ontonotes_precision: training: 0.892374 validation: 0.941178
10/01 04:11:12 AM: edges-pos-ontonotes_recall: training: 0.783296 validation: 0.829846
10/01 04:11:12 AM: edges-pos-ontonotes_f1: training: 0.834285 validation: 0.882013
10/01 04:11:12 AM: Global learning rate: 0.0001
10/01 04:11:12 AM: Saving checkpoints to: ./experiments/pos-ontonotes-sstseed2-top/run
10/01 04:11:13 AM: Update 15004: task edges-pos-ontonotes, batch 4 (15004): mcc: 0.8557, acc: 0.7662, precision: 0.9025, recall: 0.8167, f1: 0.8575, edges-pos-ontonotes_loss: 0.0164
10/01 04:11:23 AM: Update 15043: task edges-pos-ontonotes, batch 43 (15043): mcc: 0.8446, acc: 0.7537, precision: 0.8985, recall: 0.7998, f1: 0.8462, edges-pos-ontonotes_loss: 0.0181
10/01 04:11:33 AM: Update 15087: task edges-pos-ontonotes, batch 87 (15087): mcc: 0.8419, acc: 0.7490, precision: 0.8967, recall: 0.7962, f1: 0.8435, edges-pos-ontonotes_loss: 0.0186
10/01 04:11:46 AM: Update 15093: task edges-pos-ontonotes, batch 93 (15093): mcc: 0.8420, acc: 0.7491, precision: 0.8968, recall: 0.7964, f1: 0.8436, edges-pos-ontonotes_loss: 0.0186
10/01 04:11:56 AM: Update 15136: task edges-pos-ontonotes, batch 136 (15136): mcc: 0.8418, acc: 0.7484, precision: 0.8975, recall: 0.7953, f1: 0.8433, edges-pos-ontonotes_loss: 0.0186
10/01 04:12:06 AM: Update 15178: task edges-pos-ontonotes, batch 178 (15178): mcc: 0.8424, acc: 0.7492, precision: 0.8985, recall: 0.7957, f1: 0.8439, edges-pos-ontonotes_loss: 0.0185
10/01 04:12:16 AM: Update 15219: task edges-pos-ontonotes, batch 219 (15219): mcc: 0.8421, acc: 0.7492, precision: 0.8983, recall: 0.7952, f1: 0.8436, edges-pos-ontonotes_loss: 0.0185
10/01 04:12:26 AM: Update 15260: task edges-pos-ontonotes, batch 260 (15260): mcc: 0.8423, acc: 0.7494, precision: 0.8984, recall: 0.7955, f1: 0.8438, edges-pos-ontonotes_loss: 0.0184
10/01 04:12:36 AM: Update 15302: task edges-pos-ontonotes, batch 302 (15302): mcc: 0.8429, acc: 0.7500, precision: 0.8991, recall: 0.7959, f1: 0.8443, edges-pos-ontonotes_loss: 0.0183
10/01 04:12:46 AM: Update 15348: task edges-pos-ontonotes, batch 348 (15348): mcc: 0.8432, acc: 0.7507, precision: 0.8994, recall: 0.7963, f1: 0.8447, edges-pos-ontonotes_loss: 0.0183
10/01 04:12:56 AM: Update 15390: task edges-pos-ontonotes, batch 390 (15390): mcc: 0.8432, acc: 0.7508, precision: 0.8993, recall: 0.7962, f1: 0.8446, edges-pos-ontonotes_loss: 0.0183
10/01 04:13:06 AM: Update 15423: task edges-pos-ontonotes, batch 423 (15423): mcc: 0.8434, acc: 0.7510, precision: 0.8997, recall: 0.7964, f1: 0.8449, edges-pos-ontonotes_loss: 0.0183
10/01 04:13:17 AM: Update 15464: task edges-pos-ontonotes, batch 464 (15464): mcc: 0.8434, acc: 0.7510, precision: 0.8998, recall: 0.7962, f1: 0.8448, edges-pos-ontonotes_loss: 0.0183
10/01 04:13:27 AM: Update 15509: task edges-pos-ontonotes, batch 509 (15509): mcc: 0.8436, acc: 0.7514, precision: 0.8999, recall: 0.7965, f1: 0.8451, edges-pos-ontonotes_loss: 0.0183
10/01 04:13:37 AM: Update 15550: task edges-pos-ontonotes, batch 550 (15550): mcc: 0.8439, acc: 0.7517, precision: 0.9000, recall: 0.7969, f1: 0.8453, edges-pos-ontonotes_loss: 0.0182
