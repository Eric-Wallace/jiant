09/16 09:13:23 AM: Git branch: master
09/16 09:13:23 AM: Git SHA: 3ca0f74688379229ab3eec908a215358ad18b3f4
09/16 09:13:23 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-squad-top/",
  "exp_name": "experiments/ner-ontonotes-squad-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-squad-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/squad",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-squad-top__run",
  "run_dir": "./experiments/ner-ontonotes-squad-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:13:23 AM: Saved config to ./experiments/ner-ontonotes-squad-top/run/params.conf
09/16 09:13:23 AM: Using random seed 1234
09/16 09:13:58 AM: Using GPU 0
09/16 09:13:58 AM: Loading tasks...
09/16 09:13:58 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-squad-top/
09/16 09:13:58 AM: 	Creating task edges-ner-ontonotes from scratch.
09/16 09:13:59 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 09:13:59 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 09:13:59 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 09:14:00 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 09:14:00 AM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 09:14:00 AM: 	Building vocab from scratch.
09/16 09:14:00 AM: 	Counting units for task edges-ner-ontonotes.
09/16 09:14:01 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 09:14:02 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:02 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:14:03 AM: 	Saved vocab to ./experiments/ner-ontonotes-squad-top/vocab
09/16 09:14:03 AM: Loading token dictionary from ./experiments/ner-ontonotes-squad-top/vocab.
09/16 09:14:04 AM: 	Loaded vocab from ./experiments/ner-ontonotes-squad-top/vocab
09/16 09:14:04 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 09:14:04 AM: 	Vocab namespace tokens: size 22840
09/16 09:14:04 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:14:04 AM: 	Vocab namespace chars: size 77
09/16 09:14:04 AM: 	Finished building vocab.
09/16 09:14:04 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 09:14:14 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-squad-top/preproc/edges-ner-ontonotes__train_data
09/16 09:14:14 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 09:14:16 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-squad-top/preproc/edges-ner-ontonotes__val_data
09/16 09:14:16 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 09:14:17 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-squad-top/preproc/edges-ner-ontonotes__test_data
09/16 09:14:17 AM: 	Finished indexing tasks
09/16 09:14:17 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 09:14:17 AM: 	  Training on 
09/16 09:14:17 AM: 	  Evaluating on edges-ner-ontonotes
09/16 09:14:17 AM: 	Finished loading tasks in 18.941s
09/16 09:14:17 AM: 	 Tasks: ['edges-ner-ontonotes']
09/16 09:14:17 AM: Building model...
09/16 09:14:17 AM: Using BERT model (bert-base-uncased).
09/16 09:14:17 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:14:17 AM: models/squad
09/16 09:14:17 AM: loading configuration file models/squad/config.json
09/16 09:14:17 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:14:17 AM: loading weights file models/squad/pytorch_model.bin
09/16 09:14:22 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpx79r_wcv
09/16 09:14:29 AM: copying /tmp/tmpx79r_wcv to cache at ./experiments/ner-ontonotes-squad-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: creating metadata file for ./experiments/ner-ontonotes-squad-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: removing temp file /tmp/tmpx79r_wcv
09/16 09:14:29 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-squad-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:30 AM: Initializing parameters
09/16 09:14:30 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:14:30 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 09:14:53 AM: Model specification:
09/16 09:14:53 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 09:14:53 AM: Model parameters:
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 09:14:53 AM: Total number of parameters: 109688338 (1.09688e+08)
09/16 09:14:53 AM: Number of trainable parameters: 206098 (206098)
09/16 09:14:53 AM: Finished building model in 36.562s
09/16 09:14:53 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 09:14:56 AM: patience = 9
09/16 09:14:56 AM: val_interval = 1000
09/16 09:14:56 AM: max_vals = 250
09/16 09:14:56 AM: cuda_device = 0
09/16 09:14:56 AM: grad_norm = 5.0
09/16 09:14:56 AM: grad_clipping = None
09/16 09:14:56 AM: lr_decay = 0.99
09/16 09:14:56 AM: min_lr = 1e-06
09/16 09:14:56 AM: keep_all_checkpoints = 0
09/16 09:14:56 AM: val_data_limit = 5000
09/16 09:14:56 AM: max_epochs = -1
09/16 09:14:56 AM: dec_val_scale = 250
09/16 09:14:56 AM: training_data_fraction = 1
09/16 09:14:56 AM: type = adam
09/16 09:14:56 AM: parameter_groups = None
09/16 09:14:56 AM: Number of trainable parameters: 206098
09/16 09:14:56 AM: infer_type_and_cast = True
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: lr = 0.0001
09/16 09:14:56 AM: amsgrad = True
09/16 09:14:56 AM: type = reduce_on_plateau
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: mode = max
09/16 09:14:56 AM: factor = 0.5
09/16 09:14:56 AM: patience = 3
09/16 09:14:56 AM: threshold = 0.0001
09/16 09:14:56 AM: threshold_mode = abs
09/16 09:14:56 AM: verbose = True
09/16 09:14:56 AM: type = adam
09/16 09:14:56 AM: parameter_groups = None
09/16 09:14:56 AM: Number of trainable parameters: 206098
09/16 09:14:56 AM: infer_type_and_cast = True
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: lr = 0.0001
09/16 09:14:56 AM: amsgrad = True
09/16 09:14:56 AM: type = reduce_on_plateau
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: mode = max
09/16 09:14:56 AM: factor = 0.5
09/16 09:14:56 AM: patience = 3
09/16 09:14:56 AM: threshold = 0.0001
09/16 09:14:56 AM: threshold_mode = abs
09/16 09:14:56 AM: verbose = True
09/16 09:14:56 AM: Starting training without restoring from a checkpoint.
09/16 09:14:56 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 09:14:56 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 09:15:06 AM: Update 42: task edges-ner-ontonotes, batch 42 (42): mcc: -0.0055, acc: 0.0050, precision: 0.0511, recall: 0.0682, f1: 0.0584, edges-ner-ontonotes_loss: 0.3571
09/16 09:15:16 AM: Update 209: task edges-ner-ontonotes, batch 209 (209): mcc: 0.0257, acc: 0.0169, precision: 0.1006, recall: 0.0304, f1: 0.0467, edges-ner-ontonotes_loss: 0.2066
09/16 09:15:26 AM: Update 323: task edges-ner-ontonotes, batch 323 (323): mcc: 0.0753, acc: 0.0382, precision: 0.2094, recall: 0.0468, f1: 0.0765, edges-ner-ontonotes_loss: 0.1883
09/16 09:15:37 AM: Update 466: task edges-ner-ontonotes, batch 466 (466): mcc: 0.1750, acc: 0.0839, precision: 0.4228, recall: 0.0896, f1: 0.1479, edges-ner-ontonotes_loss: 0.1725
09/16 09:15:47 AM: Update 603: task edges-ner-ontonotes, batch 603 (603): mcc: 0.2524, acc: 0.1246, precision: 0.5660, recall: 0.1291, f1: 0.2102, edges-ner-ontonotes_loss: 0.1619
09/16 09:15:57 AM: Update 717: task edges-ner-ontonotes, batch 717 (717): mcc: 0.3062, acc: 0.1574, precision: 0.6461, recall: 0.1618, f1: 0.2588, edges-ner-ontonotes_loss: 0.1552
09/16 09:16:07 AM: Update 860: task edges-ner-ontonotes, batch 860 (860): mcc: 0.3587, acc: 0.1941, precision: 0.7063, recall: 0.1995, f1: 0.3111, edges-ner-ontonotes_loss: 0.1474
09/16 09:16:17 AM: Update 970: task edges-ner-ontonotes, batch 970 (970): mcc: 0.3921, acc: 0.2208, precision: 0.7350, recall: 0.2272, f1: 0.3471, edges-ner-ontonotes_loss: 0.1425
09/16 09:16:19 AM: ***** Step 1000 / Validation 1 *****
09/16 09:16:19 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:16:19 AM: Validating...
09/16 09:16:33 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.5691, acc: 0.3945, precision: 0.8493, recall: 0.4015, f1: 0.5452, edges-ner-ontonotes_loss: 0.1152
09/16 09:16:45 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.5798, acc: 0.4087, precision: 0.8453, recall: 0.4185, f1: 0.5598, edges-ner-ontonotes_loss: 0.1094
09/16 09:16:50 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:16:51 AM: Best result seen so far for micro.
09/16 09:16:51 AM: Best result seen so far for macro.
09/16 09:16:51 AM: Updating LR scheduler:
09/16 09:16:51 AM: 	Best result seen so far for macro_avg: 0.570
09/16 09:16:51 AM: 	# validation passes without improvement: 0
09/16 09:16:51 AM: edges-ner-ontonotes_loss: training: 0.141351 validation: 0.106526
09/16 09:16:51 AM: macro_avg: validation: 0.570206
09/16 09:16:51 AM: micro_avg: validation: 0.000000
09/16 09:16:51 AM: edges-ner-ontonotes_mcc: training: 0.400726 validation: 0.588740
09/16 09:16:51 AM: edges-ner-ontonotes_acc: training: 0.228013 validation: 0.418941
09/16 09:16:51 AM: edges-ner-ontonotes_precision: training: 0.742077 validation: 0.848390
09/16 09:16:51 AM: edges-ner-ontonotes_recall: training: 0.234595 validation: 0.429406
09/16 09:16:51 AM: edges-ner-ontonotes_f1: training: 0.356491 validation: 0.570206
09/16 09:16:51 AM: Global learning rate: 0.0001
09/16 09:16:51 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:16:55 AM: Update 1053: task edges-ner-ontonotes, batch 53 (1053): mcc: 0.6001, acc: 0.4301, precision: 0.8461, recall: 0.4470, f1: 0.5850, edges-ner-ontonotes_loss: 0.0999
09/16 09:17:05 AM: Update 1188: task edges-ner-ontonotes, batch 188 (1188): mcc: 0.6057, acc: 0.4390, precision: 0.8432, recall: 0.4569, f1: 0.5926, edges-ner-ontonotes_loss: 0.0998
09/16 09:17:15 AM: Update 1296: task edges-ner-ontonotes, batch 296 (1296): mcc: 0.5994, acc: 0.4352, precision: 0.8344, recall: 0.4527, f1: 0.5870, edges-ner-ontonotes_loss: 0.1007
09/16 09:17:25 AM: Update 1440: task edges-ner-ontonotes, batch 440 (1440): mcc: 0.5931, acc: 0.4290, precision: 0.8301, recall: 0.4460, f1: 0.5802, edges-ner-ontonotes_loss: 0.1026
09/16 09:17:35 AM: Update 1569: task edges-ner-ontonotes, batch 569 (1569): mcc: 0.5903, acc: 0.4274, precision: 0.8264, recall: 0.4440, f1: 0.5777, edges-ner-ontonotes_loss: 0.1030
09/16 09:17:45 AM: Update 1741: task edges-ner-ontonotes, batch 741 (1741): mcc: 0.5927, acc: 0.4322, precision: 0.8252, recall: 0.4482, f1: 0.5809, edges-ner-ontonotes_loss: 0.1026
09/16 09:17:55 AM: Update 1875: task edges-ner-ontonotes, batch 875 (1875): mcc: 0.5933, acc: 0.4344, precision: 0.8229, recall: 0.4505, f1: 0.5823, edges-ner-ontonotes_loss: 0.1021
09/16 09:18:04 AM: ***** Step 2000 / Validation 2 *****
09/16 09:18:04 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:18:04 AM: Validating...
09/16 09:18:05 AM: Evaluate: task edges-ner-ontonotes, batch 14 (157): mcc: 0.6020, acc: 0.4642, precision: 0.8158, recall: 0.4677, f1: 0.5945, edges-ner-ontonotes_loss: 0.1036
09/16 09:18:15 AM: Evaluate: task edges-ner-ontonotes, batch 106 (157): mcc: 0.6871, acc: 0.5467, precision: 0.8755, recall: 0.5600, f1: 0.6831, edges-ner-ontonotes_loss: 0.0854
09/16 09:18:22 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:18:22 AM: Best result seen so far for macro.
09/16 09:18:22 AM: Updating LR scheduler:
09/16 09:18:22 AM: 	Best result seen so far for macro_avg: 0.678
09/16 09:18:22 AM: 	# validation passes without improvement: 0
09/16 09:18:22 AM: edges-ner-ontonotes_loss: training: 0.100652 validation: 0.085224
09/16 09:18:22 AM: macro_avg: validation: 0.677741
09/16 09:18:22 AM: micro_avg: validation: 0.000000
09/16 09:18:22 AM: edges-ner-ontonotes_mcc: training: 0.600871 validation: 0.682155
09/16 09:18:22 AM: edges-ner-ontonotes_acc: training: 0.444290 validation: 0.537989
09/16 09:18:22 AM: edges-ner-ontonotes_precision: training: 0.823570 validation: 0.873251
09/16 09:18:22 AM: edges-ner-ontonotes_recall: training: 0.461287 validation: 0.553761
09/16 09:18:22 AM: edges-ner-ontonotes_f1: training: 0.591354 validation: 0.677741
09/16 09:18:22 AM: Global learning rate: 0.0001
09/16 09:18:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:18:25 AM: Update 2050: task edges-ner-ontonotes, batch 50 (2050): mcc: 0.6507, acc: 0.5130, precision: 0.8270, recall: 0.5356, f1: 0.6502, edges-ner-ontonotes_loss: 0.0898
09/16 09:18:36 AM: Update 2183: task edges-ner-ontonotes, batch 183 (2183): mcc: 0.6555, acc: 0.5169, precision: 0.8325, recall: 0.5394, f1: 0.6547, edges-ner-ontonotes_loss: 0.0883
09/16 09:18:47 AM: Update 2318: task edges-ner-ontonotes, batch 318 (2318): mcc: 0.6622, acc: 0.5240, precision: 0.8353, recall: 0.5483, f1: 0.6620, edges-ner-ontonotes_loss: 0.0870
09/16 09:18:57 AM: Update 2452: task edges-ner-ontonotes, batch 452 (2452): mcc: 0.6703, acc: 0.5339, precision: 0.8385, recall: 0.5589, f1: 0.6708, edges-ner-ontonotes_loss: 0.0855
09/16 09:19:07 AM: Update 2562: task edges-ner-ontonotes, batch 562 (2562): mcc: 0.6709, acc: 0.5341, precision: 0.8397, recall: 0.5591, f1: 0.6713, edges-ner-ontonotes_loss: 0.0853
09/16 09:19:17 AM: Update 2699: task edges-ner-ontonotes, batch 699 (2699): mcc: 0.6731, acc: 0.5378, precision: 0.8388, recall: 0.5632, f1: 0.6739, edges-ner-ontonotes_loss: 0.0849
09/16 09:19:27 AM: Update 2811: task edges-ner-ontonotes, batch 811 (2811): mcc: 0.6738, acc: 0.5394, precision: 0.8383, recall: 0.5647, f1: 0.6748, edges-ner-ontonotes_loss: 0.0846
09/16 09:19:37 AM: Update 2945: task edges-ner-ontonotes, batch 945 (2945): mcc: 0.6679, acc: 0.5325, precision: 0.8345, recall: 0.5579, f1: 0.6687, edges-ner-ontonotes_loss: 0.0860
09/16 09:19:40 AM: ***** Step 3000 / Validation 3 *****
09/16 09:19:40 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:19:40 AM: Validating...
09/16 09:19:47 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.7127, acc: 0.5909, precision: 0.8652, recall: 0.6084, f1: 0.7144, edges-ner-ontonotes_loss: 0.0808
09/16 09:19:57 AM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.7250, acc: 0.5990, precision: 0.8792, recall: 0.6182, f1: 0.7260, edges-ner-ontonotes_loss: 0.0769
09/16 09:19:58 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:19:58 AM: Best result seen so far for macro.
09/16 09:19:58 AM: Updating LR scheduler:
09/16 09:19:58 AM: 	Best result seen so far for macro_avg: 0.721
09/16 09:19:58 AM: 	# validation passes without improvement: 0
09/16 09:19:58 AM: edges-ner-ontonotes_loss: training: 0.086405 validation: 0.077634
09/16 09:19:58 AM: macro_avg: validation: 0.720984
09/16 09:19:58 AM: micro_avg: validation: 0.000000
09/16 09:19:58 AM: edges-ner-ontonotes_mcc: training: 0.666220 validation: 0.719889
09/16 09:19:58 AM: edges-ner-ontonotes_acc: training: 0.530761 validation: 0.593722
09/16 09:19:58 AM: edges-ner-ontonotes_precision: training: 0.833299 validation: 0.874567
09/16 09:19:58 AM: edges-ner-ontonotes_recall: training: 0.556075 validation: 0.613285
09/16 09:19:58 AM: edges-ner-ontonotes_f1: training: 0.667030 validation: 0.720984
09/16 09:19:58 AM: Global learning rate: 0.0001
09/16 09:19:58 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:20:07 AM: Update 3113: task edges-ner-ontonotes, batch 113 (3113): mcc: 0.6426, acc: 0.5051, precision: 0.8162, recall: 0.5303, f1: 0.6429, edges-ner-ontonotes_loss: 0.0925
09/16 09:20:17 AM: Update 3265: task edges-ner-ontonotes, batch 265 (3265): mcc: 0.6412, acc: 0.5050, precision: 0.8157, recall: 0.5285, f1: 0.6414, edges-ner-ontonotes_loss: 0.0920
09/16 09:20:27 AM: Update 3420: task edges-ner-ontonotes, batch 420 (3420): mcc: 0.6440, acc: 0.5078, precision: 0.8176, recall: 0.5316, f1: 0.6443, edges-ner-ontonotes_loss: 0.0910
09/16 09:20:37 AM: Update 3533: task edges-ner-ontonotes, batch 533 (3533): mcc: 0.6524, acc: 0.5184, precision: 0.8194, recall: 0.5437, f1: 0.6536, edges-ner-ontonotes_loss: 0.0891
09/16 09:20:47 AM: Update 3670: task edges-ner-ontonotes, batch 670 (3670): mcc: 0.6608, acc: 0.5284, precision: 0.8232, recall: 0.5545, f1: 0.6627, edges-ner-ontonotes_loss: 0.0874
09/16 09:20:57 AM: Update 3782: task edges-ner-ontonotes, batch 782 (3782): mcc: 0.6646, acc: 0.5329, precision: 0.8248, recall: 0.5595, f1: 0.6668, edges-ner-ontonotes_loss: 0.0866
09/16 09:21:08 AM: Update 3921: task edges-ner-ontonotes, batch 921 (3921): mcc: 0.6724, acc: 0.5426, precision: 0.8279, recall: 0.5699, f1: 0.6751, edges-ner-ontonotes_loss: 0.0852
09/16 09:21:13 AM: ***** Step 4000 / Validation 4 *****
09/16 09:21:13 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:21:13 AM: Validating...
09/16 09:21:18 AM: Evaluate: task edges-ner-ontonotes, batch 46 (157): mcc: 0.6928, acc: 0.5763, precision: 0.8490, recall: 0.5878, f1: 0.6946, edges-ner-ontonotes_loss: 0.0847
09/16 09:21:28 AM: Evaluate: task edges-ner-ontonotes, batch 125 (157): mcc: 0.7295, acc: 0.6116, precision: 0.8779, recall: 0.6266, f1: 0.7313, edges-ner-ontonotes_loss: 0.0750
09/16 09:21:31 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:21:31 AM: Best result seen so far for macro.
09/16 09:21:31 AM: Updating LR scheduler:
09/16 09:21:31 AM: 	Best result seen so far for macro_avg: 0.733
09/16 09:21:31 AM: 	# validation passes without improvement: 0
09/16 09:21:31 AM: edges-ner-ontonotes_loss: training: 0.084346 validation: 0.074195
09/16 09:21:31 AM: macro_avg: validation: 0.732717
09/16 09:21:31 AM: micro_avg: validation: 0.000000
09/16 09:21:31 AM: edges-ner-ontonotes_mcc: training: 0.676096 validation: 0.730199
09/16 09:21:31 AM: edges-ner-ontonotes_acc: training: 0.546964 validation: 0.613285
09/16 09:21:31 AM: edges-ner-ontonotes_precision: training: 0.829663 validation: 0.873766
09/16 09:21:31 AM: edges-ner-ontonotes_recall: training: 0.574710 validation: 0.630877
09/16 09:21:31 AM: edges-ner-ontonotes_f1: training: 0.679044 validation: 0.732717
09/16 09:21:31 AM: Global learning rate: 0.0001
09/16 09:21:31 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:21:38 AM: Update 4060: task edges-ner-ontonotes, batch 60 (4060): mcc: 0.6926, acc: 0.5664, precision: 0.8311, recall: 0.6008, f1: 0.6974, edges-ner-ontonotes_loss: 0.0790
09/16 09:21:48 AM: Update 4199: task edges-ner-ontonotes, batch 199 (4199): mcc: 0.7028, acc: 0.5801, precision: 0.8403, recall: 0.6109, f1: 0.7075, edges-ner-ontonotes_loss: 0.0775
09/16 09:21:58 AM: Update 4335: task edges-ner-ontonotes, batch 335 (4335): mcc: 0.7034, acc: 0.5809, precision: 0.8412, recall: 0.6112, f1: 0.7080, edges-ner-ontonotes_loss: 0.0776
09/16 09:22:08 AM: Update 4458: task edges-ner-ontonotes, batch 458 (4458): mcc: 0.6951, acc: 0.5706, precision: 0.8366, recall: 0.6008, f1: 0.6993, edges-ner-ontonotes_loss: 0.0795
09/16 09:22:18 AM: Update 4599: task edges-ner-ontonotes, batch 599 (4599): mcc: 0.6853, acc: 0.5592, precision: 0.8298, recall: 0.5897, f1: 0.6894, edges-ner-ontonotes_loss: 0.0821
09/16 09:22:28 AM: Update 4728: task edges-ner-ontonotes, batch 728 (4728): mcc: 0.6803, acc: 0.5535, precision: 0.8265, recall: 0.5839, f1: 0.6844, edges-ner-ontonotes_loss: 0.0830
09/16 09:22:38 AM: Update 4892: task edges-ner-ontonotes, batch 892 (4892): mcc: 0.6770, acc: 0.5496, precision: 0.8246, recall: 0.5800, f1: 0.6810, edges-ner-ontonotes_loss: 0.0836
09/16 09:22:47 AM: ***** Step 5000 / Validation 5 *****
09/16 09:22:47 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:22:47 AM: Validating...
09/16 09:22:48 AM: Evaluate: task edges-ner-ontonotes, batch 12 (157): mcc: 0.6617, acc: 0.5468, precision: 0.8071, recall: 0.5679, f1: 0.6667, edges-ner-ontonotes_loss: 0.0860
09/16 09:22:58 AM: Evaluate: task edges-ner-ontonotes, batch 104 (157): mcc: 0.7556, acc: 0.6483, precision: 0.8856, recall: 0.6642, f1: 0.7591, edges-ner-ontonotes_loss: 0.0691
09/16 09:23:05 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:23:05 AM: Best result seen so far for macro.
09/16 09:23:05 AM: Updating LR scheduler:
09/16 09:23:05 AM: 	Best result seen so far for macro_avg: 0.754
09/16 09:23:05 AM: 	# validation passes without improvement: 0
09/16 09:23:05 AM: edges-ner-ontonotes_loss: training: 0.083796 validation: 0.069156
09/16 09:23:05 AM: macro_avg: validation: 0.753603
09/16 09:23:05 AM: micro_avg: validation: 0.000000
09/16 09:23:05 AM: edges-ner-ontonotes_mcc: training: 0.675430 validation: 0.750532
09/16 09:23:05 AM: edges-ner-ontonotes_acc: training: 0.547654 validation: 0.637625
09/16 09:23:05 AM: edges-ner-ontonotes_precision: training: 0.823552 validation: 0.884958
09/16 09:23:05 AM: edges-ner-ontonotes_recall: training: 0.578140 validation: 0.656203
09/16 09:23:05 AM: edges-ner-ontonotes_f1: training: 0.679362 validation: 0.753603
09/16 09:23:05 AM: Global learning rate: 0.0001
09/16 09:23:05 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:23:08 AM: Update 5044: task edges-ner-ontonotes, batch 44 (5044): mcc: 0.6912, acc: 0.5730, precision: 0.8237, recall: 0.6042, f1: 0.6971, edges-ner-ontonotes_loss: 0.0809
09/16 09:23:18 AM: Update 5182: task edges-ner-ontonotes, batch 182 (5182): mcc: 0.6902, acc: 0.5702, precision: 0.8251, recall: 0.6014, f1: 0.6957, edges-ner-ontonotes_loss: 0.0796
09/16 09:23:28 AM: Update 5309: task edges-ner-ontonotes, batch 309 (5309): mcc: 0.6969, acc: 0.5765, precision: 0.8310, recall: 0.6080, f1: 0.7022, edges-ner-ontonotes_loss: 0.0778
09/16 09:23:38 AM: Update 5448: task edges-ner-ontonotes, batch 448 (5448): mcc: 0.7047, acc: 0.5859, precision: 0.8343, recall: 0.6186, f1: 0.7105, edges-ner-ontonotes_loss: 0.0761
09/16 09:23:48 AM: Update 5583: task edges-ner-ontonotes, batch 583 (5583): mcc: 0.7099, acc: 0.5914, precision: 0.8382, recall: 0.6243, f1: 0.7156, edges-ner-ontonotes_loss: 0.0754
09/16 09:23:58 AM: Update 5683: task edges-ner-ontonotes, batch 683 (5683): mcc: 0.7079, acc: 0.5893, precision: 0.8369, recall: 0.6220, f1: 0.7137, edges-ner-ontonotes_loss: 0.0756
09/16 09:24:08 AM: Update 5823: task edges-ner-ontonotes, batch 823 (5823): mcc: 0.7109, acc: 0.5925, precision: 0.8386, recall: 0.6258, f1: 0.7167, edges-ner-ontonotes_loss: 0.0752
09/16 09:24:18 AM: Update 5936: task edges-ner-ontonotes, batch 936 (5936): mcc: 0.7093, acc: 0.5910, precision: 0.8372, recall: 0.6242, f1: 0.7152, edges-ner-ontonotes_loss: 0.0756
09/16 09:24:23 AM: ***** Step 6000 / Validation 6 *****
09/16 09:24:23 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:24:23 AM: Validating...
09/16 09:24:29 AM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.7372, acc: 0.6285, precision: 0.8625, recall: 0.6513, f1: 0.7422, edges-ner-ontonotes_loss: 0.0727
09/16 09:24:39 AM: Evaluate: task edges-ner-ontonotes, batch 134 (157): mcc: 0.7628, acc: 0.6508, precision: 0.8909, recall: 0.6722, f1: 0.7662, edges-ner-ontonotes_loss: 0.0670
09/16 09:24:41 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:24:41 AM: Best result seen so far for macro.
09/16 09:24:41 AM: Updating LR scheduler:
09/16 09:24:41 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:24:41 AM: 	# validation passes without improvement: 0
09/16 09:24:41 AM: edges-ner-ontonotes_loss: training: 0.076327 validation: 0.067560
09/16 09:24:41 AM: macro_avg: validation: 0.762197
09/16 09:24:41 AM: micro_avg: validation: 0.000000
09/16 09:24:41 AM: edges-ner-ontonotes_mcc: training: 0.706433 validation: 0.758481
09/16 09:24:41 AM: edges-ner-ontonotes_acc: training: 0.587256 validation: 0.646269
09/16 09:24:41 AM: edges-ner-ontonotes_precision: training: 0.835943 validation: 0.886065
09/16 09:24:41 AM: edges-ner-ontonotes_recall: training: 0.620286 validation: 0.668714
09/16 09:24:41 AM: edges-ner-ontonotes_f1: training: 0.712146 validation: 0.762197
09/16 09:24:41 AM: Global learning rate: 0.0001
09/16 09:24:41 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:24:49 AM: Update 6100: task edges-ner-ontonotes, batch 100 (6100): mcc: 0.6728, acc: 0.5454, precision: 0.8158, recall: 0.5796, f1: 0.6777, edges-ner-ontonotes_loss: 0.0852
09/16 09:24:59 AM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.6735, acc: 0.5466, precision: 0.8162, recall: 0.5805, f1: 0.6785, edges-ner-ontonotes_loss: 0.0854
09/16 09:25:09 AM: Update 6390: task edges-ner-ontonotes, batch 390 (6390): mcc: 0.6730, acc: 0.5465, precision: 0.8170, recall: 0.5790, f1: 0.6777, edges-ner-ontonotes_loss: 0.0849
09/16 09:25:20 AM: Update 6538: task edges-ner-ontonotes, batch 538 (6538): mcc: 0.6709, acc: 0.5441, precision: 0.8148, recall: 0.5773, f1: 0.6758, edges-ner-ontonotes_loss: 0.0848
09/16 09:25:30 AM: Update 6677: task edges-ner-ontonotes, batch 677 (6677): mcc: 0.6781, acc: 0.5530, precision: 0.8181, recall: 0.5867, f1: 0.6833, edges-ner-ontonotes_loss: 0.0832
09/16 09:25:40 AM: Update 6817: task edges-ner-ontonotes, batch 817 (6817): mcc: 0.6833, acc: 0.5597, precision: 0.8204, recall: 0.5936, f1: 0.6888, edges-ner-ontonotes_loss: 0.0818
09/16 09:25:50 AM: Update 6932: task edges-ner-ontonotes, batch 932 (6932): mcc: 0.6873, acc: 0.5645, precision: 0.8227, recall: 0.5986, f1: 0.6930, edges-ner-ontonotes_loss: 0.0808
09/16 09:25:55 AM: ***** Step 7000 / Validation 7 *****
09/16 09:25:55 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:25:55 AM: Validating...
09/16 09:26:00 AM: Evaluate: task edges-ner-ontonotes, batch 50 (157): mcc: 0.7238, acc: 0.6128, precision: 0.8637, recall: 0.6279, f1: 0.7272, edges-ner-ontonotes_loss: 0.0751
09/16 09:26:10 AM: Evaluate: task edges-ner-ontonotes, batch 129 (157): mcc: 0.7553, acc: 0.6438, precision: 0.8917, recall: 0.6590, f1: 0.7579, edges-ner-ontonotes_loss: 0.0674
09/16 09:26:13 AM: Updating LR scheduler:
09/16 09:26:13 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:26:13 AM: 	# validation passes without improvement: 1
09/16 09:26:13 AM: edges-ner-ontonotes_loss: training: 0.080232 validation: 0.067043
09/16 09:26:13 AM: macro_avg: validation: 0.757472
09/16 09:26:13 AM: micro_avg: validation: 0.000000
09/16 09:26:13 AM: edges-ner-ontonotes_mcc: training: 0.690146 validation: 0.754503
09/16 09:26:13 AM: edges-ner-ontonotes_acc: training: 0.567995 validation: 0.643464
09/16 09:26:13 AM: edges-ner-ontonotes_precision: training: 0.823792 validation: 0.888458
09/16 09:26:13 AM: edges-ner-ontonotes_recall: training: 0.602386 validation: 0.660146
09/16 09:26:13 AM: edges-ner-ontonotes_f1: training: 0.695903 validation: 0.757472
09/16 09:26:13 AM: Global learning rate: 0.0001
09/16 09:26:13 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:26:20 AM: Update 7095: task edges-ner-ontonotes, batch 95 (7095): mcc: 0.7355, acc: 0.6229, precision: 0.8469, recall: 0.6610, f1: 0.7425, edges-ner-ontonotes_loss: 0.0696
09/16 09:26:30 AM: Update 7211: task edges-ner-ontonotes, batch 211 (7211): mcc: 0.7259, acc: 0.6110, precision: 0.8453, recall: 0.6459, f1: 0.7323, edges-ner-ontonotes_loss: 0.0716
09/16 09:26:40 AM: Update 7345: task edges-ner-ontonotes, batch 345 (7345): mcc: 0.7239, acc: 0.6093, precision: 0.8424, recall: 0.6449, f1: 0.7305, edges-ner-ontonotes_loss: 0.0721
09/16 09:26:51 AM: Update 7477: task edges-ner-ontonotes, batch 477 (7477): mcc: 0.7226, acc: 0.6080, precision: 0.8409, recall: 0.6437, f1: 0.7292, edges-ner-ontonotes_loss: 0.0726
09/16 09:27:01 AM: Update 7614: task edges-ner-ontonotes, batch 614 (7614): mcc: 0.7108, acc: 0.5932, precision: 0.8342, recall: 0.6290, f1: 0.7172, edges-ner-ontonotes_loss: 0.0753
09/16 09:27:12 AM: Update 7758: task edges-ner-ontonotes, batch 758 (7758): mcc: 0.7051, acc: 0.5859, precision: 0.8310, recall: 0.6219, f1: 0.7114, edges-ner-ontonotes_loss: 0.0771
09/16 09:27:22 AM: Update 7895: task edges-ner-ontonotes, batch 895 (7895): mcc: 0.7029, acc: 0.5828, precision: 0.8302, recall: 0.6187, f1: 0.7091, edges-ner-ontonotes_loss: 0.0778
09/16 09:27:28 AM: ***** Step 8000 / Validation 8 *****
09/16 09:27:28 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:27:28 AM: Validating...
09/16 09:27:32 AM: Evaluate: task edges-ner-ontonotes, batch 41 (157): mcc: 0.7433, acc: 0.6293, precision: 0.8637, recall: 0.6608, f1: 0.7487, edges-ner-ontonotes_loss: 0.0684
09/16 09:27:42 AM: Evaluate: task edges-ner-ontonotes, batch 120 (157): mcc: 0.7633, acc: 0.6484, precision: 0.8946, recall: 0.6701, f1: 0.7663, edges-ner-ontonotes_loss: 0.0641
09/16 09:27:46 AM: Updating LR scheduler:
09/16 09:27:46 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:27:48 AM: 	# validation passes without improvement: 2
09/16 09:27:48 AM: edges-ner-ontonotes_loss: training: 0.078006 validation: 0.064813
09/16 09:27:48 AM: macro_avg: validation: 0.762120
09/16 09:27:48 AM: micro_avg: validation: 0.000000
09/16 09:27:48 AM: edges-ner-ontonotes_mcc: training: 0.701819 validation: 0.759359
09/16 09:27:48 AM: edges-ner-ontonotes_acc: training: 0.581258 validation: 0.642554
09/16 09:27:48 AM: edges-ner-ontonotes_precision: training: 0.829838 validation: 0.893283
09/16 09:27:48 AM: edges-ner-ontonotes_recall: training: 0.617293 validation: 0.664544
09/16 09:27:48 AM: edges-ner-ontonotes_f1: training: 0.707957 validation: 0.762120
09/16 09:27:48 AM: Global learning rate: 0.0001
09/16 09:27:48 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:27:52 AM: Update 8060: task edges-ner-ontonotes, batch 60 (8060): mcc: 0.6727, acc: 0.5470, precision: 0.8145, recall: 0.5805, f1: 0.6779, edges-ner-ontonotes_loss: 0.0808
09/16 09:28:03 AM: Update 8177: task edges-ner-ontonotes, batch 177 (8177): mcc: 0.6875, acc: 0.5654, precision: 0.8210, recall: 0.6001, f1: 0.6934, edges-ner-ontonotes_loss: 0.0795
09/16 09:28:13 AM: Update 8316: task edges-ner-ontonotes, batch 316 (8316): mcc: 0.7005, acc: 0.5821, precision: 0.8289, recall: 0.6158, f1: 0.7067, edges-ner-ontonotes_loss: 0.0771
09/16 09:28:23 AM: Update 8430: task edges-ner-ontonotes, batch 430 (8430): mcc: 0.7010, acc: 0.5827, precision: 0.8286, recall: 0.6169, f1: 0.7073, edges-ner-ontonotes_loss: 0.0767
09/16 09:28:33 AM: Update 8561: task edges-ner-ontonotes, batch 561 (8561): mcc: 0.7089, acc: 0.5912, precision: 0.8336, recall: 0.6262, f1: 0.7152, edges-ner-ontonotes_loss: 0.0754
09/16 09:28:43 AM: Update 8693: task edges-ner-ontonotes, batch 693 (8693): mcc: 0.7141, acc: 0.5977, precision: 0.8361, recall: 0.6331, f1: 0.7206, edges-ner-ontonotes_loss: 0.0744
09/16 09:28:53 AM: Update 8811: task edges-ner-ontonotes, batch 811 (8811): mcc: 0.7151, acc: 0.5987, precision: 0.8365, recall: 0.6345, f1: 0.7216, edges-ner-ontonotes_loss: 0.0739
09/16 09:29:03 AM: Update 8953: task edges-ner-ontonotes, batch 953 (8953): mcc: 0.7159, acc: 0.6001, precision: 0.8359, recall: 0.6363, f1: 0.7226, edges-ner-ontonotes_loss: 0.0736
09/16 09:29:07 AM: ***** Step 9000 / Validation 9 *****
09/16 09:29:07 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:29:07 AM: Validating...
09/16 09:29:13 AM: Evaluate: task edges-ner-ontonotes, batch 59 (157): mcc: 0.7434, acc: 0.6322, precision: 0.8748, recall: 0.6522, f1: 0.7473, edges-ner-ontonotes_loss: 0.0695
09/16 09:29:23 AM: Evaluate: task edges-ner-ontonotes, batch 139 (157): mcc: 0.7678, acc: 0.6570, precision: 0.8978, recall: 0.6752, f1: 0.7708, edges-ner-ontonotes_loss: 0.0635
09/16 09:29:25 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:29:25 AM: Best result seen so far for macro.
09/16 09:29:25 AM: Updating LR scheduler:
09/16 09:29:25 AM: 	Best result seen so far for macro_avg: 0.768
09/16 09:29:25 AM: 	# validation passes without improvement: 0
09/16 09:29:25 AM: edges-ner-ontonotes_loss: training: 0.073542 validation: 0.063975
09/16 09:29:25 AM: macro_avg: validation: 0.768107
09/16 09:29:25 AM: micro_avg: validation: 0.000000
09/16 09:29:25 AM: edges-ner-ontonotes_mcc: training: 0.716813 validation: 0.764847
09/16 09:29:25 AM: edges-ner-ontonotes_acc: training: 0.601062 validation: 0.654534
09/16 09:29:25 AM: edges-ner-ontonotes_precision: training: 0.836579 validation: 0.893651
09/16 09:29:25 AM: edges-ner-ontonotes_recall: training: 0.637361 validation: 0.673491
09/16 09:29:25 AM: edges-ner-ontonotes_f1: training: 0.723507 validation: 0.768107
09/16 09:29:25 AM: Global learning rate: 0.0001
09/16 09:29:25 AM: Saving checkpoints to: ./experiments/ner-ontonotes-squad-top/run
09/16 09:29:33 AM: Update 9082: task edges-ner-ontonotes, batch 82 (9082): mcc: 0.6750, acc: 0.5529, precision: 0.8055, recall: 0.5912, f1: 0.6819, edges-ner-ontonotes_loss: 0.0830
09/16 09:29:43 AM: Update 9221: task edges-ner-ontonotes, batch 221 (9221): mcc: 0.6770, acc: 0.5523, precision: 0.8116, recall: 0.5899, f1: 0.6832, edges-ner-ontonotes_loss: 0.0839
09/16 09:29:53 AM: Update 9349: task edges-ner-ontonotes, batch 349 (9349): mcc: 0.6798, acc: 0.5555, precision: 0.8145, recall: 0.5923, f1: 0.6859, edges-ner-ontonotes_loss: 0.0834
09/16 09:30:03 AM: Update 9511: task edges-ner-ontonotes, batch 511 (9511): mcc: 0.6796, acc: 0.5550, precision: 0.8151, recall: 0.5915, f1: 0.6855, edges-ner-ontonotes_loss: 0.0826
09/16 09:30:13 AM: Update 9650: task edges-ner-ontonotes, batch 650 (9650): mcc: 0.6799, acc: 0.5546, precision: 0.8156, recall: 0.5915, f1: 0.6857, edges-ner-ontonotes_loss: 0.0823
