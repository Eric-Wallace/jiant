09/16 12:22:32 PM: Git branch: master
09/16 12:22:32 PM: Git SHA: ce97551376ebcff91ec7c178ddad0ca53f8fcb03
09/16 12:22:33 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/pos-ontonotes-sst-only/",
  "exp_name": "experiments/pos-ontonotes-sst-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/pos-ontonotes-sst-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/sst",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/pos-ontonotes-sst-only__run",
  "run_dir": "./experiments/pos-ontonotes-sst-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-pos-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 12:22:33 PM: Saved config to ./experiments/pos-ontonotes-sst-only/run/params.conf
09/16 12:22:33 PM: Using random seed 1234
09/16 12:22:34 PM: Using GPU 0
09/16 12:22:34 PM: Loading tasks...
09/16 12:22:34 PM: Writing pre-preprocessed tasks to ./experiments/pos-ontonotes-sst-only/
09/16 12:22:34 PM: 	Creating task edges-pos-ontonotes from scratch.
09/16 12:22:58 PM: Read=110514, Skip=5298, Total=115812 from ./probing_data/edges/ontonotes/const/pos/train.json.retokenized.bert-base-uncased
09/16 12:22:59 PM: Read=15060, Skip=620, Total=15680 from ./probing_data/edges/ontonotes/const/pos/development.json.retokenized.bert-base-uncased
09/16 12:23:04 PM: Read=11462, Skip=755, Total=12217 from ./probing_data/edges/ontonotes/const/pos/test.json.retokenized.bert-base-uncased
09/16 12:23:18 PM: 	Task 'edges-pos-ontonotes': |train|=110514 |val|=15060 |test|=11462
09/16 12:23:18 PM: 	Finished loading tasks: edges-pos-ontonotes.
09/16 12:23:18 PM: 	Building vocab from scratch.
09/16 12:23:18 PM: 	Counting units for task edges-pos-ontonotes.
09/16 12:23:21 PM: 	Task 'edges-pos-ontonotes': adding vocab namespace 'edges-pos-ontonotes_labels'
09/16 12:23:22 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:23:22 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 12:23:22 PM: 	Saved vocab to ./experiments/pos-ontonotes-sst-only/vocab
09/16 12:23:22 PM: Loading token dictionary from ./experiments/pos-ontonotes-sst-only/vocab.
09/16 12:23:22 PM: 	Loaded vocab from ./experiments/pos-ontonotes-sst-only/vocab
09/16 12:23:22 PM: 	Vocab namespace bert_uncased: size 30524
09/16 12:23:22 PM: 	Vocab namespace tokens: size 24015
09/16 12:23:22 PM: 	Vocab namespace edges-pos-ontonotes_labels: size 48
09/16 12:23:22 PM: 	Vocab namespace chars: size 81
09/16 12:23:22 PM: 	Finished building vocab.
09/16 12:23:22 PM: 	Task edges-pos-ontonotes (train): Indexing from scratch.
09/16 12:23:58 PM: 	Task edges-pos-ontonotes (train): Saved 110514 instances to ./experiments/pos-ontonotes-sst-only/preproc/edges-pos-ontonotes__train_data
09/16 12:23:58 PM: 	Task edges-pos-ontonotes (val): Indexing from scratch.
09/16 12:24:03 PM: 	Task edges-pos-ontonotes (val): Saved 15060 instances to ./experiments/pos-ontonotes-sst-only/preproc/edges-pos-ontonotes__val_data
09/16 12:24:03 PM: 	Task edges-pos-ontonotes (test): Indexing from scratch.
09/16 12:24:06 PM: 	Task edges-pos-ontonotes (test): Saved 11462 instances to ./experiments/pos-ontonotes-sst-only/preproc/edges-pos-ontonotes__test_data
09/16 12:24:06 PM: 	Finished indexing tasks
09/16 12:24:06 PM: 	Creating trimmed target-only version of edges-pos-ontonotes train.
09/16 12:24:06 PM: 	  Training on 
09/16 12:24:06 PM: 	  Evaluating on edges-pos-ontonotes
09/16 12:24:06 PM: 	Finished loading tasks in 92.182s
09/16 12:24:06 PM: 	 Tasks: ['edges-pos-ontonotes']
09/16 12:24:06 PM: Building model...
09/16 12:24:06 PM: Using BERT model (bert-base-uncased).
09/16 12:24:06 PM: LOADING A FUNETUNED MODEL from: 
09/16 12:24:06 PM: models/sst
09/16 12:24:06 PM: loading configuration file models/sst/config.json
09/16 12:24:06 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst-2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 12:24:06 PM: loading weights file models/sst/pytorch_model.bin
09/16 12:24:09 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp7oup90vv
09/16 12:24:13 PM: copying /tmp/tmp7oup90vv to cache at ./experiments/pos-ontonotes-sst-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:24:13 PM: creating metadata file for ./experiments/pos-ontonotes-sst-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:24:13 PM: removing temp file /tmp/tmp7oup90vv
09/16 12:24:13 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/pos-ontonotes-sst-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:24:13 PM: Initializing parameters
09/16 12:24:13 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 12:24:13 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 12:24:13 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 12:24:13 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 12:24:13 PM:    _text_field_embedder.model.pooler.dense.bias
09/16 12:24:13 PM:    _text_field_embedder.model.pooler.dense.weight
09/16 12:24:13 PM: 	Task 'edges-pos-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-pos-ontonotes"
}
09/16 12:24:18 PM: Model specification:
09/16 12:24:18 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-pos-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=48, bias=True)
    )
  )
)
09/16 12:24:18 PM: Model parameters:
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:24:18 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:24:18 PM: 	edges-pos-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 12:24:18 PM: 	edges-pos-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 12:24:18 PM: 	edges-pos-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 24576 with torch.Size([48, 512])
09/16 12:24:18 PM: 	edges-pos-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 48 with torch.Size([48])
09/16 12:24:18 PM: Total number of parameters: 109703728 (1.09704e+08)
09/16 12:24:18 PM: Number of trainable parameters: 221488 (221488)
09/16 12:24:18 PM: Finished building model in 12.269s
09/16 12:24:18 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-pos-ontonotes 

09/16 12:25:33 PM: patience = 9
09/16 12:25:33 PM: val_interval = 1000
09/16 12:25:33 PM: max_vals = 250
09/16 12:25:33 PM: cuda_device = 0
09/16 12:25:33 PM: grad_norm = 5.0
09/16 12:25:33 PM: grad_clipping = None
09/16 12:25:33 PM: lr_decay = 0.99
09/16 12:25:33 PM: min_lr = 1e-06
09/16 12:25:33 PM: keep_all_checkpoints = 0
09/16 12:25:33 PM: val_data_limit = 5000
09/16 12:25:33 PM: max_epochs = -1
09/16 12:25:33 PM: dec_val_scale = 250
09/16 12:25:33 PM: training_data_fraction = 1
09/16 12:25:33 PM: type = adam
09/16 12:25:33 PM: parameter_groups = None
09/16 12:25:33 PM: Number of trainable parameters: 221488
09/16 12:25:33 PM: infer_type_and_cast = True
09/16 12:25:33 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:33 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:33 PM: lr = 0.0001
09/16 12:25:33 PM: amsgrad = True
09/16 12:25:33 PM: type = reduce_on_plateau
09/16 12:25:33 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:33 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:33 PM: mode = max
09/16 12:25:33 PM: factor = 0.5
09/16 12:25:33 PM: patience = 3
09/16 12:25:33 PM: threshold = 0.0001
09/16 12:25:33 PM: threshold_mode = abs
09/16 12:25:33 PM: verbose = True
09/16 12:25:33 PM: type = adam
09/16 12:25:33 PM: parameter_groups = None
09/16 12:25:33 PM: Number of trainable parameters: 221488
09/16 12:25:33 PM: infer_type_and_cast = True
09/16 12:25:33 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:33 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:33 PM: lr = 0.0001
09/16 12:25:33 PM: amsgrad = True
09/16 12:25:33 PM: type = reduce_on_plateau
09/16 12:25:33 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:25:33 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:25:33 PM: mode = max
09/16 12:25:33 PM: factor = 0.5
09/16 12:25:33 PM: patience = 3
09/16 12:25:33 PM: threshold = 0.0001
09/16 12:25:33 PM: threshold_mode = abs
09/16 12:25:33 PM: verbose = True
09/16 12:25:33 PM: Starting training without restoring from a checkpoint.
09/16 12:25:33 PM: Training examples per task, before any subsampling: {'edges-pos-ontonotes': 110514}
09/16 12:25:33 PM: Beginning training with stopping criteria based on metric: edges-pos-ontonotes_f1
09/16 12:25:43 PM: Update 95: task edges-pos-ontonotes, batch 95 (95): mcc: 0.0654, acc: 0.0807, precision: 0.0447, recall: 0.2850, f1: 0.0773, edges-pos-ontonotes_loss: 0.3245
09/16 12:25:53 PM: Update 174: task edges-pos-ontonotes, batch 174 (174): mcc: 0.0982, acc: 0.1389, precision: 0.0708, recall: 0.2483, f1: 0.1102, edges-pos-ontonotes_loss: 0.2108
09/16 12:26:03 PM: Update 239: task edges-pos-ontonotes, batch 239 (239): mcc: 0.1356, acc: 0.1873, precision: 0.1010, recall: 0.2674, f1: 0.1466, edges-pos-ontonotes_loss: 0.1681
09/16 12:26:15 PM: Update 314: task edges-pos-ontonotes, batch 314 (314): mcc: 0.1826, acc: 0.2403, precision: 0.1419, recall: 0.3022, f1: 0.1931, edges-pos-ontonotes_loss: 0.1380
09/16 12:26:25 PM: Update 369: task edges-pos-ontonotes, batch 369 (369): mcc: 0.2268, acc: 0.2830, precision: 0.1841, recall: 0.3347, f1: 0.2376, edges-pos-ontonotes_loss: 0.1238
09/16 12:26:35 PM: Update 427: task edges-pos-ontonotes, batch 427 (427): mcc: 0.2680, acc: 0.3193, precision: 0.2259, recall: 0.3650, f1: 0.2791, edges-pos-ontonotes_loss: 0.1119
09/16 12:26:45 PM: Update 496: task edges-pos-ontonotes, batch 496 (496): mcc: 0.3124, acc: 0.3572, precision: 0.2727, recall: 0.3983, f1: 0.3238, edges-pos-ontonotes_loss: 0.1009
09/16 12:26:56 PM: Update 569: task edges-pos-ontonotes, batch 569 (569): mcc: 0.3593, acc: 0.3960, precision: 0.3239, recall: 0.4335, f1: 0.3707, edges-pos-ontonotes_loss: 0.0916
09/16 12:27:07 PM: Update 627: task edges-pos-ontonotes, batch 627 (627): mcc: 0.3895, acc: 0.4204, precision: 0.3576, recall: 0.4560, f1: 0.4009, edges-pos-ontonotes_loss: 0.0856
09/16 12:27:17 PM: Update 670: task edges-pos-ontonotes, batch 670 (670): mcc: 0.4159, acc: 0.4393, precision: 0.3888, recall: 0.4740, f1: 0.4271, edges-pos-ontonotes_loss: 0.0820
09/16 12:27:28 PM: Update 712: task edges-pos-ontonotes, batch 712 (712): mcc: 0.4412, acc: 0.4580, precision: 0.4186, recall: 0.4918, f1: 0.4523, edges-pos-ontonotes_loss: 0.0788
09/16 12:27:38 PM: Update 766: task edges-pos-ontonotes, batch 766 (766): mcc: 0.4637, acc: 0.4753, precision: 0.4450, recall: 0.5084, f1: 0.4746, edges-pos-ontonotes_loss: 0.0751
09/16 12:27:48 PM: Update 813: task edges-pos-ontonotes, batch 813 (813): mcc: 0.4829, acc: 0.4899, precision: 0.4675, recall: 0.5225, f1: 0.4935, edges-pos-ontonotes_loss: 0.0722
09/16 12:27:58 PM: Update 860: task edges-pos-ontonotes, batch 860 (860): mcc: 0.4992, acc: 0.5025, precision: 0.4865, recall: 0.5349, f1: 0.5096, edges-pos-ontonotes_loss: 0.0696
09/16 12:28:08 PM: Update 924: task edges-pos-ontonotes, batch 924 (924): mcc: 0.5239, acc: 0.5213, precision: 0.5151, recall: 0.5539, f1: 0.5338, edges-pos-ontonotes_loss: 0.0663
09/16 12:28:18 PM: Update 972: task edges-pos-ontonotes, batch 972 (972): mcc: 0.5387, acc: 0.5320, precision: 0.5329, recall: 0.5646, f1: 0.5483, edges-pos-ontonotes_loss: 0.0642
09/16 12:28:23 PM: ***** Step 1000 / Validation 1 *****
09/16 12:28:23 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:28:23 PM: Validating...
09/16 12:28:29 PM: Evaluate: task edges-pos-ontonotes, batch 52 (157): mcc: 0.8213, acc: 0.7492, precision: 0.8786, recall: 0.7742, f1: 0.8231, edges-pos-ontonotes_loss: 0.0197
09/16 12:28:39 PM: Evaluate: task edges-pos-ontonotes, batch 126 (157): mcc: 0.8261, acc: 0.7534, precision: 0.8842, recall: 0.7781, f1: 0.8278, edges-pos-ontonotes_loss: 0.0194
09/16 12:28:44 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:28:44 PM: Best result seen so far for micro.
09/16 12:28:44 PM: Best result seen so far for macro.
09/16 12:28:44 PM: Updating LR scheduler:
09/16 12:28:44 PM: 	Best result seen so far for macro_avg: 0.824
09/16 12:28:44 PM: 	# validation passes without improvement: 0
09/16 12:28:44 PM: edges-pos-ontonotes_loss: training: 0.063017 validation: 0.019727
09/16 12:28:44 PM: macro_avg: validation: 0.824244
09/16 12:28:44 PM: micro_avg: validation: 0.000000
09/16 12:28:44 PM: edges-pos-ontonotes_mcc: training: 0.547331 validation: 0.822693
09/16 12:28:44 PM: edges-pos-ontonotes_acc: training: 0.538459 validation: 0.748870
09/16 12:28:44 PM: edges-pos-ontonotes_precision: training: 0.543424 validation: 0.883597
09/16 12:28:44 PM: edges-pos-ontonotes_recall: training: 0.570902 validation: 0.772363
09/16 12:28:44 PM: edges-pos-ontonotes_f1: training: 0.556824 validation: 0.824244
09/16 12:28:44 PM: Global learning rate: 0.0001
09/16 12:28:44 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:28:49 PM: Update 1022: task edges-pos-ontonotes, batch 22 (1022): mcc: 0.8012, acc: 0.7074, precision: 0.8755, recall: 0.7400, f1: 0.8021, edges-pos-ontonotes_loss: 0.0230
09/16 12:28:59 PM: Update 1084: task edges-pos-ontonotes, batch 84 (1084): mcc: 0.8105, acc: 0.7209, precision: 0.8804, recall: 0.7529, f1: 0.8117, edges-pos-ontonotes_loss: 0.0221
09/16 12:29:09 PM: Update 1127: task edges-pos-ontonotes, batch 127 (1127): mcc: 0.8144, acc: 0.7261, precision: 0.8820, recall: 0.7586, f1: 0.8157, edges-pos-ontonotes_loss: 0.0217
09/16 12:29:19 PM: Update 1179: task edges-pos-ontonotes, batch 179 (1179): mcc: 0.8150, acc: 0.7284, precision: 0.8799, recall: 0.7616, f1: 0.8165, edges-pos-ontonotes_loss: 0.0214
09/16 12:29:30 PM: Update 1247: task edges-pos-ontonotes, batch 247 (1247): mcc: 0.8165, acc: 0.7311, precision: 0.8789, recall: 0.7652, f1: 0.8181, edges-pos-ontonotes_loss: 0.0211
09/16 12:29:43 PM: Update 1253: task edges-pos-ontonotes, batch 253 (1253): mcc: 0.8164, acc: 0.7313, precision: 0.8783, recall: 0.7654, f1: 0.8180, edges-pos-ontonotes_loss: 0.0211
09/16 12:29:53 PM: Update 1317: task edges-pos-ontonotes, batch 317 (1317): mcc: 0.8184, acc: 0.7346, precision: 0.8787, recall: 0.7687, f1: 0.8200, edges-pos-ontonotes_loss: 0.0208
09/16 12:30:03 PM: Update 1382: task edges-pos-ontonotes, batch 382 (1382): mcc: 0.8204, acc: 0.7378, precision: 0.8793, recall: 0.7719, f1: 0.8221, edges-pos-ontonotes_loss: 0.0206
09/16 12:30:13 PM: Update 1451: task edges-pos-ontonotes, batch 451 (1451): mcc: 0.8230, acc: 0.7418, precision: 0.8801, recall: 0.7760, f1: 0.8248, edges-pos-ontonotes_loss: 0.0203
09/16 12:30:23 PM: Update 1512: task edges-pos-ontonotes, batch 512 (1512): mcc: 0.8249, acc: 0.7449, precision: 0.8807, recall: 0.7790, f1: 0.8268, edges-pos-ontonotes_loss: 0.0201
09/16 12:30:33 PM: Update 1566: task edges-pos-ontonotes, batch 566 (1566): mcc: 0.8262, acc: 0.7471, precision: 0.8809, recall: 0.7813, f1: 0.8281, edges-pos-ontonotes_loss: 0.0199
09/16 12:30:43 PM: Update 1635: task edges-pos-ontonotes, batch 635 (1635): mcc: 0.8280, acc: 0.7500, precision: 0.8813, recall: 0.7843, f1: 0.8300, edges-pos-ontonotes_loss: 0.0196
09/16 12:30:53 PM: Update 1704: task edges-pos-ontonotes, batch 704 (1704): mcc: 0.8294, acc: 0.7521, precision: 0.8815, recall: 0.7866, f1: 0.8314, edges-pos-ontonotes_loss: 0.0195
09/16 12:31:03 PM: Update 1767: task edges-pos-ontonotes, batch 767 (1767): mcc: 0.8306, acc: 0.7541, precision: 0.8816, recall: 0.7888, f1: 0.8326, edges-pos-ontonotes_loss: 0.0193
09/16 12:31:13 PM: Update 1833: task edges-pos-ontonotes, batch 833 (1833): mcc: 0.8316, acc: 0.7559, precision: 0.8818, recall: 0.7905, f1: 0.8337, edges-pos-ontonotes_loss: 0.0192
09/16 12:31:23 PM: Update 1881: task edges-pos-ontonotes, batch 881 (1881): mcc: 0.8322, acc: 0.7570, precision: 0.8817, recall: 0.7917, f1: 0.8343, edges-pos-ontonotes_loss: 0.0190
09/16 12:31:33 PM: Update 1934: task edges-pos-ontonotes, batch 934 (1934): mcc: 0.8332, acc: 0.7585, precision: 0.8817, recall: 0.7935, f1: 0.8353, edges-pos-ontonotes_loss: 0.0188
09/16 12:31:43 PM: ***** Step 2000 / Validation 2 *****
09/16 12:31:43 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:31:43 PM: Validating...
09/16 12:31:44 PM: Evaluate: task edges-pos-ontonotes, batch 9 (157): mcc: 0.8501, acc: 0.8022, precision: 0.8851, recall: 0.8223, f1: 0.8526, edges-pos-ontonotes_loss: 0.0166
09/16 12:31:54 PM: Evaluate: task edges-pos-ontonotes, batch 103 (157): mcc: 0.8405, acc: 0.7921, precision: 0.8827, recall: 0.8064, f1: 0.8428, edges-pos-ontonotes_loss: 0.0170
09/16 12:32:02 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:32:02 PM: Best result seen so far for macro.
09/16 12:32:02 PM: Updating LR scheduler:
09/16 12:32:02 PM: 	Best result seen so far for macro_avg: 0.846
09/16 12:32:02 PM: 	# validation passes without improvement: 0
09/16 12:32:02 PM: edges-pos-ontonotes_loss: training: 0.018539 validation: 0.016554
09/16 12:32:02 PM: macro_avg: validation: 0.845609
09/16 12:32:02 PM: micro_avg: validation: 0.000000
09/16 12:32:02 PM: edges-pos-ontonotes_mcc: training: 0.834587 validation: 0.843444
09/16 12:32:02 PM: edges-pos-ontonotes_acc: training: 0.760369 validation: 0.793422
09/16 12:32:02 PM: edges-pos-ontonotes_precision: training: 0.882284 validation: 0.887140
09/16 12:32:02 PM: edges-pos-ontonotes_recall: training: 0.795630 validation: 0.807793
09/16 12:32:02 PM: edges-pos-ontonotes_f1: training: 0.836719 validation: 0.845609
09/16 12:32:02 PM: Global learning rate: 0.0001
09/16 12:32:02 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:32:04 PM: Update 2015: task edges-pos-ontonotes, batch 15 (2015): mcc: 0.8520, acc: 0.7819, precision: 0.8900, recall: 0.8213, f1: 0.8543, edges-pos-ontonotes_loss: 0.0153
09/16 12:32:14 PM: Update 2104: task edges-pos-ontonotes, batch 104 (2104): mcc: 0.8615, acc: 0.7963, precision: 0.8943, recall: 0.8352, f1: 0.8637, edges-pos-ontonotes_loss: 0.0141
09/16 12:32:24 PM: Update 2182: task edges-pos-ontonotes, batch 182 (2182): mcc: 0.8604, acc: 0.7955, precision: 0.8933, recall: 0.8342, f1: 0.8627, edges-pos-ontonotes_loss: 0.0139
09/16 12:32:34 PM: Update 2273: task edges-pos-ontonotes, batch 273 (2273): mcc: 0.8650, acc: 0.8019, precision: 0.8961, recall: 0.8402, f1: 0.8673, edges-pos-ontonotes_loss: 0.0134
09/16 12:32:44 PM: Update 2388: task edges-pos-ontonotes, batch 388 (2388): mcc: 0.8703, acc: 0.8093, precision: 0.8993, recall: 0.8473, f1: 0.8725, edges-pos-ontonotes_loss: 0.0128
09/16 12:32:54 PM: Update 2501: task edges-pos-ontonotes, batch 501 (2501): mcc: 0.8745, acc: 0.8150, precision: 0.9018, recall: 0.8529, f1: 0.8767, edges-pos-ontonotes_loss: 0.0123
09/16 12:33:04 PM: Update 2608: task edges-pos-ontonotes, batch 608 (2608): mcc: 0.8748, acc: 0.8156, precision: 0.9024, recall: 0.8529, f1: 0.8770, edges-pos-ontonotes_loss: 0.0125
09/16 12:33:14 PM: Update 2740: task edges-pos-ontonotes, batch 740 (2740): mcc: 0.8762, acc: 0.8180, precision: 0.9032, recall: 0.8548, f1: 0.8783, edges-pos-ontonotes_loss: 0.0125
09/16 12:33:24 PM: Update 2864: task edges-pos-ontonotes, batch 864 (2864): mcc: 0.8757, acc: 0.8182, precision: 0.9023, recall: 0.8548, f1: 0.8779, edges-pos-ontonotes_loss: 0.0125
09/16 12:33:32 PM: ***** Step 3000 / Validation 3 *****
09/16 12:33:32 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:33:32 PM: Validating...
09/16 12:33:35 PM: Evaluate: task edges-pos-ontonotes, batch 30 (157): mcc: 0.8456, acc: 0.8079, precision: 0.8803, recall: 0.8183, f1: 0.8482, edges-pos-ontonotes_loss: 0.0168
09/16 12:33:45 PM: Evaluate: task edges-pos-ontonotes, batch 116 (157): mcc: 0.8450, acc: 0.8030, precision: 0.8797, recall: 0.8176, f1: 0.8475, edges-pos-ontonotes_loss: 0.0163
09/16 12:33:51 PM: Updating LR scheduler:
09/16 12:33:51 PM: 	Best result seen so far for macro_avg: 0.846
09/16 12:33:51 PM: 	# validation passes without improvement: 1
09/16 12:33:51 PM: edges-pos-ontonotes_loss: training: 0.012529 validation: 0.016830
09/16 12:33:51 PM: macro_avg: validation: 0.841207
09/16 12:33:51 PM: micro_avg: validation: 0.000000
09/16 12:33:51 PM: edges-pos-ontonotes_mcc: training: 0.874896 validation: 0.838688
09/16 12:33:51 PM: edges-pos-ontonotes_acc: training: 0.818019 validation: 0.793020
09/16 12:33:51 PM: edges-pos-ontonotes_precision: training: 0.901438 validation: 0.876806
09/16 12:33:51 PM: edges-pos-ontonotes_recall: training: 0.854054 validation: 0.808385
09/16 12:33:51 PM: edges-pos-ontonotes_f1: training: 0.877106 validation: 0.841207
09/16 12:33:51 PM: Global learning rate: 0.0001
09/16 12:33:51 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:33:55 PM: Update 3062: task edges-pos-ontonotes, batch 62 (3062): mcc: 0.8593, acc: 0.8077, precision: 0.8842, recall: 0.8407, f1: 0.8619, edges-pos-ontonotes_loss: 0.0131
09/16 12:34:05 PM: Update 3158: task edges-pos-ontonotes, batch 158 (3158): mcc: 0.8448, acc: 0.7861, precision: 0.8734, recall: 0.8232, f1: 0.8476, edges-pos-ontonotes_loss: 0.0140
09/16 12:34:15 PM: Update 3224: task edges-pos-ontonotes, batch 224 (3224): mcc: 0.8369, acc: 0.7737, precision: 0.8697, recall: 0.8117, f1: 0.8397, edges-pos-ontonotes_loss: 0.0153
09/16 12:34:25 PM: Update 3292: task edges-pos-ontonotes, batch 292 (3292): mcc: 0.8352, acc: 0.7701, precision: 0.8705, recall: 0.8077, f1: 0.8379, edges-pos-ontonotes_loss: 0.0162
09/16 12:34:35 PM: Update 3350: task edges-pos-ontonotes, batch 350 (3350): mcc: 0.8353, acc: 0.7697, precision: 0.8710, recall: 0.8073, f1: 0.8379, edges-pos-ontonotes_loss: 0.0165
09/16 12:34:45 PM: Update 3433: task edges-pos-ontonotes, batch 433 (3433): mcc: 0.8354, acc: 0.7696, precision: 0.8714, recall: 0.8072, f1: 0.8381, edges-pos-ontonotes_loss: 0.0167
09/16 12:35:00 PM: Update 3461: task edges-pos-ontonotes, batch 461 (3461): mcc: 0.8347, acc: 0.7685, precision: 0.8711, recall: 0.8063, f1: 0.8374, edges-pos-ontonotes_loss: 0.0168
09/16 12:35:10 PM: Update 3570: task edges-pos-ontonotes, batch 570 (3570): mcc: 0.8380, acc: 0.7732, precision: 0.8732, recall: 0.8104, f1: 0.8406, edges-pos-ontonotes_loss: 0.0163
09/16 12:35:20 PM: Update 3642: task edges-pos-ontonotes, batch 642 (3642): mcc: 0.8396, acc: 0.7755, precision: 0.8740, recall: 0.8126, f1: 0.8422, edges-pos-ontonotes_loss: 0.0160
09/16 12:35:30 PM: Update 3704: task edges-pos-ontonotes, batch 704 (3704): mcc: 0.8406, acc: 0.7772, precision: 0.8744, recall: 0.8142, f1: 0.8432, edges-pos-ontonotes_loss: 0.0158
09/16 12:35:40 PM: Update 3780: task edges-pos-ontonotes, batch 780 (3780): mcc: 0.8421, acc: 0.7795, precision: 0.8751, recall: 0.8163, f1: 0.8447, edges-pos-ontonotes_loss: 0.0155
09/16 12:35:50 PM: Update 3865: task edges-pos-ontonotes, batch 865 (3865): mcc: 0.8432, acc: 0.7808, precision: 0.8756, recall: 0.8181, f1: 0.8459, edges-pos-ontonotes_loss: 0.0155
09/16 12:36:00 PM: Update 3954: task edges-pos-ontonotes, batch 954 (3954): mcc: 0.8446, acc: 0.7821, precision: 0.8765, recall: 0.8199, f1: 0.8472, edges-pos-ontonotes_loss: 0.0153
09/16 12:36:08 PM: ***** Step 4000 / Validation 4 *****
09/16 12:36:08 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:36:08 PM: Validating...
09/16 12:36:10 PM: Evaluate: task edges-pos-ontonotes, batch 27 (157): mcc: 0.8600, acc: 0.8284, precision: 0.8829, recall: 0.8433, f1: 0.8626, edges-pos-ontonotes_loss: 0.0144
09/16 12:36:21 PM: Evaluate: task edges-pos-ontonotes, batch 105 (157): mcc: 0.8696, acc: 0.8397, precision: 0.8909, recall: 0.8540, f1: 0.8721, edges-pos-ontonotes_loss: 0.0134
09/16 12:36:29 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:36:29 PM: Best result seen so far for macro.
09/16 12:36:29 PM: Updating LR scheduler:
09/16 12:36:29 PM: 	Best result seen so far for macro_avg: 0.859
09/16 12:36:29 PM: 	# validation passes without improvement: 0
09/16 12:36:29 PM: edges-pos-ontonotes_loss: training: 0.015241 validation: 0.014191
09/16 12:36:29 PM: macro_avg: validation: 0.859364
09/16 12:36:29 PM: micro_avg: validation: 0.000000
09/16 12:36:29 PM: edges-pos-ontonotes_mcc: training: 0.845272 validation: 0.856714
09/16 12:36:29 PM: edges-pos-ontonotes_acc: training: 0.782820 validation: 0.822344
09/16 12:36:29 PM: edges-pos-ontonotes_precision: training: 0.876974 validation: 0.880939
09/16 12:36:29 PM: edges-pos-ontonotes_recall: training: 0.820719 validation: 0.838820
09/16 12:36:29 PM: edges-pos-ontonotes_f1: training: 0.847914 validation: 0.859364
09/16 12:36:29 PM: Global learning rate: 0.0001
09/16 12:36:29 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:36:31 PM: Update 4016: task edges-pos-ontonotes, batch 16 (4016): mcc: 0.8615, acc: 0.8042, precision: 0.8844, recall: 0.8447, f1: 0.8641, edges-pos-ontonotes_loss: 0.0140
09/16 12:36:41 PM: Update 4081: task edges-pos-ontonotes, batch 81 (4081): mcc: 0.8614, acc: 0.8038, precision: 0.8875, recall: 0.8415, f1: 0.8639, edges-pos-ontonotes_loss: 0.0140
09/16 12:36:51 PM: Update 4130: task edges-pos-ontonotes, batch 130 (4130): mcc: 0.8514, acc: 0.7907, precision: 0.8788, recall: 0.8307, f1: 0.8541, edges-pos-ontonotes_loss: 0.0150
09/16 12:37:01 PM: Update 4192: task edges-pos-ontonotes, batch 192 (4192): mcc: 0.8498, acc: 0.7890, precision: 0.8769, recall: 0.8295, f1: 0.8525, edges-pos-ontonotes_loss: 0.0154
09/16 12:37:11 PM: Update 4268: task edges-pos-ontonotes, batch 268 (4268): mcc: 0.8507, acc: 0.7894, precision: 0.8776, recall: 0.8305, f1: 0.8534, edges-pos-ontonotes_loss: 0.0155
09/16 12:37:21 PM: Update 4338: task edges-pos-ontonotes, batch 338 (4338): mcc: 0.8508, acc: 0.7898, precision: 0.8776, recall: 0.8307, f1: 0.8535, edges-pos-ontonotes_loss: 0.0155
09/16 12:37:32 PM: Update 4400: task edges-pos-ontonotes, batch 400 (4400): mcc: 0.8505, acc: 0.7894, precision: 0.8774, recall: 0.8304, f1: 0.8532, edges-pos-ontonotes_loss: 0.0154
09/16 12:37:43 PM: Update 4462: task edges-pos-ontonotes, batch 462 (4462): mcc: 0.8500, acc: 0.7892, precision: 0.8771, recall: 0.8296, f1: 0.8527, edges-pos-ontonotes_loss: 0.0155
09/16 12:37:53 PM: Update 4519: task edges-pos-ontonotes, batch 519 (4519): mcc: 0.8506, acc: 0.7903, precision: 0.8776, recall: 0.8302, f1: 0.8533, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:03 PM: Update 4577: task edges-pos-ontonotes, batch 577 (4577): mcc: 0.8503, acc: 0.7903, precision: 0.8772, recall: 0.8300, f1: 0.8530, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:13 PM: Update 4617: task edges-pos-ontonotes, batch 617 (4617): mcc: 0.8506, acc: 0.7910, precision: 0.8776, recall: 0.8303, f1: 0.8533, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:23 PM: Update 4654: task edges-pos-ontonotes, batch 654 (4654): mcc: 0.8510, acc: 0.7915, precision: 0.8780, recall: 0.8307, f1: 0.8536, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:33 PM: Update 4691: task edges-pos-ontonotes, batch 691 (4691): mcc: 0.8511, acc: 0.7918, precision: 0.8781, recall: 0.8307, f1: 0.8538, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:43 PM: Update 4721: task edges-pos-ontonotes, batch 721 (4721): mcc: 0.8512, acc: 0.7920, precision: 0.8783, recall: 0.8307, f1: 0.8538, edges-pos-ontonotes_loss: 0.0156
09/16 12:38:53 PM: Update 4793: task edges-pos-ontonotes, batch 793 (4793): mcc: 0.8514, acc: 0.7927, precision: 0.8786, recall: 0.8310, f1: 0.8541, edges-pos-ontonotes_loss: 0.0155
09/16 12:39:03 PM: Update 4862: task edges-pos-ontonotes, batch 862 (4862): mcc: 0.8518, acc: 0.7933, precision: 0.8789, recall: 0.8313, f1: 0.8545, edges-pos-ontonotes_loss: 0.0155
09/16 12:39:13 PM: Update 4935: task edges-pos-ontonotes, batch 935 (4935): mcc: 0.8520, acc: 0.7936, precision: 0.8792, recall: 0.8314, f1: 0.8546, edges-pos-ontonotes_loss: 0.0155
09/16 12:39:24 PM: Update 4989: task edges-pos-ontonotes, batch 989 (4989): mcc: 0.8523, acc: 0.7941, precision: 0.8795, recall: 0.8318, f1: 0.8550, edges-pos-ontonotes_loss: 0.0155
09/16 12:39:25 PM: ***** Step 5000 / Validation 5 *****
09/16 12:39:25 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:39:25 PM: Validating...
09/16 12:39:34 PM: Evaluate: task edges-pos-ontonotes, batch 58 (157): mcc: 0.8599, acc: 0.8280, precision: 0.8810, recall: 0.8448, f1: 0.8625, edges-pos-ontonotes_loss: 0.0149
09/16 12:39:44 PM: Evaluate: task edges-pos-ontonotes, batch 127 (157): mcc: 0.8625, acc: 0.8311, precision: 0.8834, recall: 0.8475, f1: 0.8651, edges-pos-ontonotes_loss: 0.0144
09/16 12:39:48 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:39:48 PM: Best result seen so far for macro.
09/16 12:39:48 PM: Updating LR scheduler:
09/16 12:39:48 PM: 	Best result seen so far for macro_avg: 0.866
09/16 12:39:48 PM: 	# validation passes without improvement: 0
09/16 12:39:48 PM: edges-pos-ontonotes_loss: training: 0.015534 validation: 0.014220
09/16 12:39:48 PM: macro_avg: validation: 0.866481
09/16 12:39:48 PM: micro_avg: validation: 0.000000
09/16 12:39:48 PM: edges-pos-ontonotes_mcc: training: 0.852351 validation: 0.863908
09/16 12:39:48 PM: edges-pos-ontonotes_acc: training: 0.794165 validation: 0.832058
09/16 12:39:48 PM: edges-pos-ontonotes_precision: training: 0.879494 validation: 0.885411
09/16 12:39:48 PM: edges-pos-ontonotes_recall: training: 0.831841 validation: 0.848344
09/16 12:39:48 PM: edges-pos-ontonotes_f1: training: 0.855004 validation: 0.866481
09/16 12:39:48 PM: Global learning rate: 0.0001
09/16 12:39:48 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:40:03 PM: Update 5026: task edges-pos-ontonotes, batch 26 (5026): mcc: 0.8626, acc: 0.8062, precision: 0.8906, recall: 0.8410, f1: 0.8651, edges-pos-ontonotes_loss: 0.0144
09/16 12:40:13 PM: Update 5090: task edges-pos-ontonotes, batch 90 (5090): mcc: 0.8581, acc: 0.8016, precision: 0.8848, recall: 0.8378, f1: 0.8607, edges-pos-ontonotes_loss: 0.0151
09/16 12:40:23 PM: Update 5153: task edges-pos-ontonotes, batch 153 (5153): mcc: 0.8594, acc: 0.8038, precision: 0.8860, recall: 0.8392, f1: 0.8620, edges-pos-ontonotes_loss: 0.0150
09/16 12:40:33 PM: Update 5195: task edges-pos-ontonotes, batch 195 (5195): mcc: 0.8595, acc: 0.8043, precision: 0.8860, recall: 0.8394, f1: 0.8621, edges-pos-ontonotes_loss: 0.0150
09/16 12:40:43 PM: Update 5259: task edges-pos-ontonotes, batch 259 (5259): mcc: 0.8592, acc: 0.8037, precision: 0.8854, recall: 0.8393, f1: 0.8617, edges-pos-ontonotes_loss: 0.0150
09/16 12:40:53 PM: Update 5322: task edges-pos-ontonotes, batch 322 (5322): mcc: 0.8590, acc: 0.8036, precision: 0.8850, recall: 0.8393, f1: 0.8615, edges-pos-ontonotes_loss: 0.0150
09/16 12:41:03 PM: Update 5377: task edges-pos-ontonotes, batch 377 (5377): mcc: 0.8582, acc: 0.8029, precision: 0.8839, recall: 0.8389, f1: 0.8608, edges-pos-ontonotes_loss: 0.0149
09/16 12:41:13 PM: Update 5457: task edges-pos-ontonotes, batch 457 (5457): mcc: 0.8596, acc: 0.8040, precision: 0.8853, recall: 0.8402, f1: 0.8622, edges-pos-ontonotes_loss: 0.0145
09/16 12:41:24 PM: Update 5544: task edges-pos-ontonotes, batch 544 (5544): mcc: 0.8614, acc: 0.8056, precision: 0.8867, recall: 0.8422, f1: 0.8639, edges-pos-ontonotes_loss: 0.0141
09/16 12:41:34 PM: Update 5622: task edges-pos-ontonotes, batch 622 (5622): mcc: 0.8622, acc: 0.8064, precision: 0.8873, recall: 0.8433, f1: 0.8647, edges-pos-ontonotes_loss: 0.0139
09/16 12:41:44 PM: Update 5693: task edges-pos-ontonotes, batch 693 (5693): mcc: 0.8637, acc: 0.8078, precision: 0.8884, recall: 0.8450, f1: 0.8662, edges-pos-ontonotes_loss: 0.0136
09/16 12:41:54 PM: Update 5795: task edges-pos-ontonotes, batch 795 (5795): mcc: 0.8662, acc: 0.8109, precision: 0.8904, recall: 0.8479, f1: 0.8687, edges-pos-ontonotes_loss: 0.0132
09/16 12:42:04 PM: Update 5908: task edges-pos-ontonotes, batch 908 (5908): mcc: 0.8689, acc: 0.8141, precision: 0.8924, recall: 0.8511, f1: 0.8713, edges-pos-ontonotes_loss: 0.0128
09/16 12:42:13 PM: ***** Step 6000 / Validation 6 *****
09/16 12:42:13 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:42:13 PM: Validating...
09/16 12:42:14 PM: Evaluate: task edges-pos-ontonotes, batch 10 (157): mcc: 0.8518, acc: 0.8089, precision: 0.8872, recall: 0.8236, f1: 0.8542, edges-pos-ontonotes_loss: 0.0155
09/16 12:42:24 PM: Evaluate: task edges-pos-ontonotes, batch 104 (157): mcc: 0.8541, acc: 0.8097, precision: 0.8900, recall: 0.8253, f1: 0.8564, edges-pos-ontonotes_loss: 0.0147
09/16 12:42:32 PM: Updating LR scheduler:
09/16 12:42:32 PM: 	Best result seen so far for macro_avg: 0.866
09/16 12:42:32 PM: 	# validation passes without improvement: 1
09/16 12:42:32 PM: edges-pos-ontonotes_loss: training: 0.012639 validation: 0.014963
09/16 12:42:32 PM: macro_avg: validation: 0.853484
09/16 12:42:32 PM: micro_avg: validation: 0.000000
09/16 12:42:32 PM: edges-pos-ontonotes_mcc: training: 0.870229 validation: 0.851237
09/16 12:42:32 PM: edges-pos-ontonotes_acc: training: 0.815721 validation: 0.804671
09/16 12:42:32 PM: edges-pos-ontonotes_precision: training: 0.893536 validation: 0.889827
09/16 12:42:32 PM: edges-pos-ontonotes_recall: training: 0.852673 validation: 0.819994
09/16 12:42:32 PM: edges-pos-ontonotes_f1: training: 0.872626 validation: 0.853484
09/16 12:42:32 PM: Global learning rate: 0.0001
09/16 12:42:32 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:42:34 PM: Update 6025: task edges-pos-ontonotes, batch 25 (6025): mcc: 0.8869, acc: 0.8344, precision: 0.9071, recall: 0.8716, f1: 0.8890, edges-pos-ontonotes_loss: 0.0123
09/16 12:42:44 PM: Update 6163: task edges-pos-ontonotes, batch 163 (6163): mcc: 0.8905, acc: 0.8436, precision: 0.9080, recall: 0.8778, f1: 0.8926, edges-pos-ontonotes_loss: 0.0109
09/16 12:42:54 PM: Update 6285: task edges-pos-ontonotes, batch 285 (6285): mcc: 0.8894, acc: 0.8430, precision: 0.9061, recall: 0.8775, f1: 0.8916, edges-pos-ontonotes_loss: 0.0108
09/16 12:43:04 PM: Update 6444: task edges-pos-ontonotes, batch 444 (6444): mcc: 0.8819, acc: 0.8357, precision: 0.9000, recall: 0.8689, f1: 0.8842, edges-pos-ontonotes_loss: 0.0114
09/16 12:43:14 PM: Update 6591: task edges-pos-ontonotes, batch 591 (6591): mcc: 0.8784, acc: 0.8322, precision: 0.8970, recall: 0.8651, f1: 0.8808, edges-pos-ontonotes_loss: 0.0115
09/16 12:43:24 PM: Update 6673: task edges-pos-ontonotes, batch 673 (6673): mcc: 0.8691, acc: 0.8192, precision: 0.8903, recall: 0.8537, f1: 0.8716, edges-pos-ontonotes_loss: 0.0123
09/16 12:43:34 PM: Update 6736: task edges-pos-ontonotes, batch 736 (6736): mcc: 0.8636, acc: 0.8109, precision: 0.8869, recall: 0.8463, f1: 0.8661, edges-pos-ontonotes_loss: 0.0127
09/16 12:43:44 PM: Update 6808: task edges-pos-ontonotes, batch 808 (6808): mcc: 0.8601, acc: 0.8059, precision: 0.8847, recall: 0.8417, f1: 0.8627, edges-pos-ontonotes_loss: 0.0131
09/16 12:43:54 PM: Update 6880: task edges-pos-ontonotes, batch 880 (6880): mcc: 0.8579, acc: 0.8030, precision: 0.8834, recall: 0.8387, f1: 0.8605, edges-pos-ontonotes_loss: 0.0134
09/16 12:44:04 PM: Update 6956: task edges-pos-ontonotes, batch 956 (6956): mcc: 0.8566, acc: 0.8012, precision: 0.8826, recall: 0.8370, f1: 0.8592, edges-pos-ontonotes_loss: 0.0135
09/16 12:44:10 PM: ***** Step 7000 / Validation 7 *****
09/16 12:44:10 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:44:10 PM: Validating...
09/16 12:44:14 PM: Evaluate: task edges-pos-ontonotes, batch 45 (157): mcc: 0.8664, acc: 0.8388, precision: 0.8908, recall: 0.8479, f1: 0.8688, edges-pos-ontonotes_loss: 0.0135
09/16 12:44:25 PM: Evaluate: task edges-pos-ontonotes, batch 125 (157): mcc: 0.8615, acc: 0.8303, precision: 0.8851, recall: 0.8441, f1: 0.8641, edges-pos-ontonotes_loss: 0.0136
09/16 12:44:30 PM: Updating LR scheduler:
09/16 12:44:30 PM: 	Best result seen so far for macro_avg: 0.866
09/16 12:44:30 PM: 	# validation passes without improvement: 2
09/16 12:44:30 PM: edges-pos-ontonotes_loss: training: 0.013528 validation: 0.013925
09/16 12:44:30 PM: macro_avg: validation: 0.860131
09/16 12:44:30 PM: micro_avg: validation: 0.000000
09/16 12:44:30 PM: edges-pos-ontonotes_mcc: training: 0.856709 validation: 0.857505
09/16 12:44:30 PM: edges-pos-ontonotes_acc: training: 0.801389 validation: 0.824195
09/16 12:44:30 PM: edges-pos-ontonotes_precision: training: 0.882494 validation: 0.881992
09/16 12:44:30 PM: edges-pos-ontonotes_recall: training: 0.837322 validation: 0.839328
09/16 12:44:30 PM: edges-pos-ontonotes_f1: training: 0.859315 validation: 0.860131
09/16 12:44:30 PM: Global learning rate: 0.0001
09/16 12:44:30 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:44:35 PM: Update 7060: task edges-pos-ontonotes, batch 60 (7060): mcc: 0.8638, acc: 0.8143, precision: 0.8865, recall: 0.8472, f1: 0.8664, edges-pos-ontonotes_loss: 0.0124
09/16 12:44:45 PM: Update 7161: task edges-pos-ontonotes, batch 161 (7161): mcc: 0.8620, acc: 0.8105, precision: 0.8842, recall: 0.8457, f1: 0.8646, edges-pos-ontonotes_loss: 0.0127
09/16 12:44:55 PM: Update 7245: task edges-pos-ontonotes, batch 245 (7245): mcc: 0.8622, acc: 0.8109, precision: 0.8847, recall: 0.8457, f1: 0.8647, edges-pos-ontonotes_loss: 0.0127
09/16 12:45:05 PM: Update 7328: task edges-pos-ontonotes, batch 328 (7328): mcc: 0.8623, acc: 0.8095, precision: 0.8853, recall: 0.8453, f1: 0.8648, edges-pos-ontonotes_loss: 0.0129
09/16 12:45:15 PM: Update 7415: task edges-pos-ontonotes, batch 415 (7415): mcc: 0.8624, acc: 0.8084, precision: 0.8855, recall: 0.8452, f1: 0.8649, edges-pos-ontonotes_loss: 0.0130
09/16 12:45:25 PM: Update 7468: task edges-pos-ontonotes, batch 468 (7468): mcc: 0.8624, acc: 0.8079, precision: 0.8855, recall: 0.8453, f1: 0.8650, edges-pos-ontonotes_loss: 0.0130
09/16 12:45:46 PM: Update 7547: task edges-pos-ontonotes, batch 547 (7547): mcc: 0.8624, acc: 0.8078, precision: 0.8854, recall: 0.8454, f1: 0.8649, edges-pos-ontonotes_loss: 0.0130
09/16 12:45:56 PM: Update 7610: task edges-pos-ontonotes, batch 610 (7610): mcc: 0.8595, acc: 0.8037, precision: 0.8829, recall: 0.8423, f1: 0.8621, edges-pos-ontonotes_loss: 0.0133
09/16 12:46:06 PM: Update 7677: task edges-pos-ontonotes, batch 677 (7677): mcc: 0.8584, acc: 0.8024, precision: 0.8819, recall: 0.8412, f1: 0.8611, edges-pos-ontonotes_loss: 0.0136
09/16 12:46:16 PM: Update 7729: task edges-pos-ontonotes, batch 729 (7729): mcc: 0.8583, acc: 0.8022, precision: 0.8818, recall: 0.8411, f1: 0.8610, edges-pos-ontonotes_loss: 0.0136
09/16 12:46:26 PM: Update 7782: task edges-pos-ontonotes, batch 782 (7782): mcc: 0.8584, acc: 0.8022, precision: 0.8818, recall: 0.8412, f1: 0.8610, edges-pos-ontonotes_loss: 0.0137
09/16 12:46:37 PM: Update 7845: task edges-pos-ontonotes, batch 845 (7845): mcc: 0.8584, acc: 0.8023, precision: 0.8817, recall: 0.8413, f1: 0.8611, edges-pos-ontonotes_loss: 0.0137
09/16 12:46:47 PM: Update 7897: task edges-pos-ontonotes, batch 897 (7897): mcc: 0.8578, acc: 0.8018, precision: 0.8813, recall: 0.8406, f1: 0.8605, edges-pos-ontonotes_loss: 0.0139
09/16 12:46:57 PM: Update 7961: task edges-pos-ontonotes, batch 961 (7961): mcc: 0.8577, acc: 0.8019, precision: 0.8813, recall: 0.8404, f1: 0.8604, edges-pos-ontonotes_loss: 0.0139
09/16 12:47:03 PM: ***** Step 8000 / Validation 8 *****
09/16 12:47:03 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:47:03 PM: Validating...
09/16 12:47:07 PM: Evaluate: task edges-pos-ontonotes, batch 43 (157): mcc: 0.8581, acc: 0.8280, precision: 0.8797, recall: 0.8427, f1: 0.8608, edges-pos-ontonotes_loss: 0.0141
09/16 12:47:17 PM: Evaluate: task edges-pos-ontonotes, batch 124 (157): mcc: 0.8686, acc: 0.8391, precision: 0.8883, recall: 0.8545, f1: 0.8711, edges-pos-ontonotes_loss: 0.0133
09/16 12:47:22 PM: Best result seen so far for edges-pos-ontonotes.
09/16 12:47:22 PM: Best result seen so far for macro.
09/16 12:47:22 PM: Updating LR scheduler:
09/16 12:47:22 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:47:22 PM: 	# validation passes without improvement: 0
09/16 12:47:22 PM: edges-pos-ontonotes_loss: training: 0.013972 validation: 0.013383
09/16 12:47:22 PM: macro_avg: validation: 0.870913
09/16 12:47:22 PM: micro_avg: validation: 0.000000
09/16 12:47:22 PM: edges-pos-ontonotes_mcc: training: 0.857546 validation: 0.868396
09/16 12:47:22 PM: edges-pos-ontonotes_acc: training: 0.801750 validation: 0.838365
09/16 12:47:22 PM: edges-pos-ontonotes_precision: training: 0.881170 validation: 0.888353
09/16 12:47:22 PM: edges-pos-ontonotes_recall: training: 0.840196 validation: 0.854144
09/16 12:47:22 PM: edges-pos-ontonotes_f1: training: 0.860195 validation: 0.870913
09/16 12:47:22 PM: Global learning rate: 0.0001
09/16 12:47:22 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:47:27 PM: Update 8036: task edges-pos-ontonotes, batch 36 (8036): mcc: 0.8567, acc: 0.8039, precision: 0.8815, recall: 0.8382, f1: 0.8593, edges-pos-ontonotes_loss: 0.0151
09/16 12:47:37 PM: Update 8100: task edges-pos-ontonotes, batch 100 (8100): mcc: 0.8575, acc: 0.8044, precision: 0.8821, recall: 0.8392, f1: 0.8601, edges-pos-ontonotes_loss: 0.0149
09/16 12:47:47 PM: Update 8167: task edges-pos-ontonotes, batch 167 (8167): mcc: 0.8581, acc: 0.8050, precision: 0.8829, recall: 0.8395, f1: 0.8607, edges-pos-ontonotes_loss: 0.0148
09/16 12:47:57 PM: Update 8217: task edges-pos-ontonotes, batch 217 (8217): mcc: 0.8568, acc: 0.8037, precision: 0.8819, recall: 0.8380, f1: 0.8594, edges-pos-ontonotes_loss: 0.0151
09/16 12:48:07 PM: Update 8268: task edges-pos-ontonotes, batch 268 (8268): mcc: 0.8575, acc: 0.8045, precision: 0.8821, recall: 0.8393, f1: 0.8601, edges-pos-ontonotes_loss: 0.0149
09/16 12:48:17 PM: Update 8334: task edges-pos-ontonotes, batch 334 (8334): mcc: 0.8579, acc: 0.8049, precision: 0.8827, recall: 0.8393, f1: 0.8605, edges-pos-ontonotes_loss: 0.0149
09/16 12:48:28 PM: Update 8405: task edges-pos-ontonotes, batch 405 (8405): mcc: 0.8587, acc: 0.8059, precision: 0.8835, recall: 0.8401, f1: 0.8613, edges-pos-ontonotes_loss: 0.0149
09/16 12:48:38 PM: Update 8475: task edges-pos-ontonotes, batch 475 (8475): mcc: 0.8590, acc: 0.8064, precision: 0.8837, recall: 0.8405, f1: 0.8616, edges-pos-ontonotes_loss: 0.0148
09/16 12:48:48 PM: Update 8526: task edges-pos-ontonotes, batch 526 (8526): mcc: 0.8590, acc: 0.8064, precision: 0.8837, recall: 0.8405, f1: 0.8616, edges-pos-ontonotes_loss: 0.0148
09/16 12:48:58 PM: Update 8594: task edges-pos-ontonotes, batch 594 (8594): mcc: 0.8592, acc: 0.8067, precision: 0.8838, recall: 0.8408, f1: 0.8618, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:08 PM: Update 8650: task edges-pos-ontonotes, batch 650 (8650): mcc: 0.8595, acc: 0.8070, precision: 0.8840, recall: 0.8412, f1: 0.8621, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:18 PM: Update 8699: task edges-pos-ontonotes, batch 699 (8699): mcc: 0.8596, acc: 0.8069, precision: 0.8841, recall: 0.8414, f1: 0.8622, edges-pos-ontonotes_loss: 0.0148
09/16 12:49:28 PM: Update 8758: task edges-pos-ontonotes, batch 758 (8758): mcc: 0.8598, acc: 0.8070, precision: 0.8842, recall: 0.8415, f1: 0.8623, edges-pos-ontonotes_loss: 0.0147
09/16 12:49:38 PM: Update 8801: task edges-pos-ontonotes, batch 801 (8801): mcc: 0.8597, acc: 0.8069, precision: 0.8840, recall: 0.8416, f1: 0.8623, edges-pos-ontonotes_loss: 0.0147
09/16 12:49:48 PM: Update 8878: task edges-pos-ontonotes, batch 878 (8878): mcc: 0.8603, acc: 0.8076, precision: 0.8843, recall: 0.8425, f1: 0.8629, edges-pos-ontonotes_loss: 0.0145
09/16 12:49:58 PM: Update 8955: task edges-pos-ontonotes, batch 955 (8955): mcc: 0.8609, acc: 0.8080, precision: 0.8848, recall: 0.8432, f1: 0.8635, edges-pos-ontonotes_loss: 0.0144
09/16 12:50:04 PM: ***** Step 9000 / Validation 9 *****
09/16 12:50:04 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:50:04 PM: Validating...
09/16 12:50:09 PM: Evaluate: task edges-pos-ontonotes, batch 48 (157): mcc: 0.8532, acc: 0.8232, precision: 0.8749, recall: 0.8379, f1: 0.8560, edges-pos-ontonotes_loss: 0.0148
09/16 12:50:19 PM: Evaluate: task edges-pos-ontonotes, batch 127 (157): mcc: 0.8609, acc: 0.8296, precision: 0.8841, recall: 0.8438, f1: 0.8635, edges-pos-ontonotes_loss: 0.0141
09/16 12:50:23 PM: Updating LR scheduler:
09/16 12:50:23 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:50:23 PM: 	# validation passes without improvement: 1
09/16 12:50:23 PM: edges-pos-ontonotes_loss: training: 0.014240 validation: 0.013992
09/16 12:50:23 PM: macro_avg: validation: 0.864794
09/16 12:50:23 PM: micro_avg: validation: 0.000000
09/16 12:50:23 PM: edges-pos-ontonotes_mcc: training: 0.861335 validation: 0.862270
09/16 12:50:23 PM: edges-pos-ontonotes_acc: training: 0.808341 validation: 0.829921
09/16 12:50:23 PM: edges-pos-ontonotes_precision: training: 0.885200 validation: 0.886925
09/16 12:50:23 PM: edges-pos-ontonotes_recall: training: 0.843601 validation: 0.843741
09/16 12:50:23 PM: edges-pos-ontonotes_f1: training: 0.863900 validation: 0.864794
09/16 12:50:23 PM: Global learning rate: 0.0001
09/16 12:50:23 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:50:29 PM: Update 9047: task edges-pos-ontonotes, batch 47 (9047): mcc: 0.8758, acc: 0.8213, precision: 0.8984, recall: 0.8587, f1: 0.8781, edges-pos-ontonotes_loss: 0.0119
09/16 12:50:49 PM: Update 9112: task edges-pos-ontonotes, batch 112 (9112): mcc: 0.8744, acc: 0.8198, precision: 0.8963, recall: 0.8580, f1: 0.8767, edges-pos-ontonotes_loss: 0.0118
09/16 12:50:59 PM: Update 9230: task edges-pos-ontonotes, batch 230 (9230): mcc: 0.8828, acc: 0.8311, precision: 0.9021, recall: 0.8685, f1: 0.8850, edges-pos-ontonotes_loss: 0.0109
09/16 12:51:09 PM: Update 9335: task edges-pos-ontonotes, batch 335 (9335): mcc: 0.8869, acc: 0.8367, precision: 0.9053, recall: 0.8734, f1: 0.8891, edges-pos-ontonotes_loss: 0.0106
09/16 12:51:21 PM: Update 9425: task edges-pos-ontonotes, batch 425 (9425): mcc: 0.8888, acc: 0.8388, precision: 0.9065, recall: 0.8759, f1: 0.8909, edges-pos-ontonotes_loss: 0.0104
09/16 12:51:31 PM: Update 9532: task edges-pos-ontonotes, batch 532 (9532): mcc: 0.8885, acc: 0.8388, precision: 0.9063, recall: 0.8755, f1: 0.8906, edges-pos-ontonotes_loss: 0.0106
09/16 12:51:41 PM: Update 9618: task edges-pos-ontonotes, batch 618 (9618): mcc: 0.8893, acc: 0.8400, precision: 0.9067, recall: 0.8766, f1: 0.8914, edges-pos-ontonotes_loss: 0.0105
09/16 12:51:51 PM: Update 9736: task edges-pos-ontonotes, batch 736 (9736): mcc: 0.8901, acc: 0.8417, precision: 0.9072, recall: 0.8778, f1: 0.8923, edges-pos-ontonotes_loss: 0.0104
09/16 12:52:01 PM: Update 9889: task edges-pos-ontonotes, batch 889 (9889): mcc: 0.8872, acc: 0.8392, precision: 0.9046, recall: 0.8747, f1: 0.8894, edges-pos-ontonotes_loss: 0.0108
09/16 12:52:07 PM: ***** Step 10000 / Validation 10 *****
09/16 12:52:07 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:52:07 PM: Validating...
09/16 12:52:11 PM: Evaluate: task edges-pos-ontonotes, batch 46 (157): mcc: 0.8534, acc: 0.8243, precision: 0.8818, recall: 0.8317, f1: 0.8560, edges-pos-ontonotes_loss: 0.0150
09/16 12:52:21 PM: Evaluate: task edges-pos-ontonotes, batch 126 (157): mcc: 0.8528, acc: 0.8207, precision: 0.8820, recall: 0.8303, f1: 0.8554, edges-pos-ontonotes_loss: 0.0151
09/16 12:52:26 PM: Updating LR scheduler:
09/16 12:52:26 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:52:26 PM: 	# validation passes without improvement: 2
09/16 12:52:26 PM: edges-pos-ontonotes_loss: training: 0.010765 validation: 0.015486
09/16 12:52:26 PM: macro_avg: validation: 0.851292
09/16 12:52:26 PM: micro_avg: validation: 0.000000
09/16 12:52:26 PM: edges-pos-ontonotes_mcc: training: 0.886394 validation: 0.848705
09/16 12:52:26 PM: edges-pos-ontonotes_acc: training: 0.838760 validation: 0.814364
09/16 12:52:26 PM: edges-pos-ontonotes_precision: training: 0.903811 validation: 0.880037
09/16 12:52:26 PM: edges-pos-ontonotes_recall: training: 0.873878 validation: 0.824365
09/16 12:52:26 PM: edges-pos-ontonotes_f1: training: 0.888592 validation: 0.851292
09/16 12:52:26 PM: Global learning rate: 0.0001
09/16 12:52:26 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:52:32 PM: Update 10051: task edges-pos-ontonotes, batch 51 (10051): mcc: 0.8538, acc: 0.8060, precision: 0.8752, recall: 0.8387, f1: 0.8566, edges-pos-ontonotes_loss: 0.0131
09/16 12:52:42 PM: Update 10117: task edges-pos-ontonotes, batch 117 (10117): mcc: 0.8413, acc: 0.7848, precision: 0.8677, recall: 0.8220, f1: 0.8442, edges-pos-ontonotes_loss: 0.0154
09/16 12:52:52 PM: Update 10192: task edges-pos-ontonotes, batch 192 (10192): mcc: 0.8410, acc: 0.7823, precision: 0.8685, recall: 0.8205, f1: 0.8439, edges-pos-ontonotes_loss: 0.0159
09/16 12:53:02 PM: Update 10273: task edges-pos-ontonotes, batch 273 (10273): mcc: 0.8426, acc: 0.7842, precision: 0.8705, recall: 0.8217, f1: 0.8454, edges-pos-ontonotes_loss: 0.0161
09/16 12:53:12 PM: Update 10327: task edges-pos-ontonotes, batch 327 (10327): mcc: 0.8426, acc: 0.7836, precision: 0.8712, recall: 0.8211, f1: 0.8454, edges-pos-ontonotes_loss: 0.0162
09/16 12:53:22 PM: Update 10368: task edges-pos-ontonotes, batch 368 (10368): mcc: 0.8430, acc: 0.7839, precision: 0.8716, recall: 0.8214, f1: 0.8458, edges-pos-ontonotes_loss: 0.0162
09/16 12:53:32 PM: Update 10429: task edges-pos-ontonotes, batch 429 (10429): mcc: 0.8437, acc: 0.7847, precision: 0.8721, recall: 0.8222, f1: 0.8464, edges-pos-ontonotes_loss: 0.0160
09/16 12:53:42 PM: Update 10497: task edges-pos-ontonotes, batch 497 (10497): mcc: 0.8460, acc: 0.7880, precision: 0.8737, recall: 0.8251, f1: 0.8487, edges-pos-ontonotes_loss: 0.0155
09/16 12:53:52 PM: Update 10579: task edges-pos-ontonotes, batch 579 (10579): mcc: 0.8478, acc: 0.7908, precision: 0.8749, recall: 0.8274, f1: 0.8505, edges-pos-ontonotes_loss: 0.0151
09/16 12:54:02 PM: Update 10687: task edges-pos-ontonotes, batch 687 (10687): mcc: 0.8497, acc: 0.7936, precision: 0.8762, recall: 0.8300, f1: 0.8524, edges-pos-ontonotes_loss: 0.0146
09/16 12:54:12 PM: Update 10754: task edges-pos-ontonotes, batch 754 (10754): mcc: 0.8509, acc: 0.7951, precision: 0.8770, recall: 0.8316, f1: 0.8537, edges-pos-ontonotes_loss: 0.0145
09/16 12:54:22 PM: Update 10836: task edges-pos-ontonotes, batch 836 (10836): mcc: 0.8524, acc: 0.7967, precision: 0.8780, recall: 0.8333, f1: 0.8551, edges-pos-ontonotes_loss: 0.0144
09/16 12:54:32 PM: Update 10914: task edges-pos-ontonotes, batch 914 (10914): mcc: 0.8537, acc: 0.7982, precision: 0.8791, recall: 0.8349, f1: 0.8564, edges-pos-ontonotes_loss: 0.0143
09/16 12:54:42 PM: Update 10965: task edges-pos-ontonotes, batch 965 (10965): mcc: 0.8544, acc: 0.7990, precision: 0.8795, recall: 0.8359, f1: 0.8571, edges-pos-ontonotes_loss: 0.0142
09/16 12:54:47 PM: ***** Step 11000 / Validation 11 *****
09/16 12:54:47 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:54:47 PM: Validating...
09/16 12:54:52 PM: Evaluate: task edges-pos-ontonotes, batch 52 (157): mcc: 0.8746, acc: 0.8409, precision: 0.9054, recall: 0.8497, f1: 0.8767, edges-pos-ontonotes_loss: 0.0127
09/16 12:55:03 PM: Evaluate: task edges-pos-ontonotes, batch 130 (157): mcc: 0.8700, acc: 0.8351, precision: 0.8999, recall: 0.8463, f1: 0.8723, edges-pos-ontonotes_loss: 0.0129
09/16 12:55:07 PM: Updating LR scheduler:
09/16 12:55:07 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:55:07 PM: 	# validation passes without improvement: 3
09/16 12:55:07 PM: edges-pos-ontonotes_loss: training: 0.014130 validation: 0.013142
09/16 12:55:07 PM: macro_avg: validation: 0.870050
09/16 12:55:07 PM: micro_avg: validation: 0.000000
09/16 12:55:07 PM: edges-pos-ontonotes_mcc: training: 0.854755 validation: 0.867785
09/16 12:55:07 PM: edges-pos-ontonotes_acc: training: 0.799319 validation: 0.832524
09/16 12:55:07 PM: edges-pos-ontonotes_precision: training: 0.879756 validation: 0.897215
09/16 12:55:07 PM: edges-pos-ontonotes_recall: training: 0.836196 validation: 0.844482
09/16 12:55:07 PM: edges-pos-ontonotes_f1: training: 0.857423 validation: 0.870050
09/16 12:55:07 PM: Global learning rate: 0.0001
09/16 12:55:07 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:55:13 PM: Update 11031: task edges-pos-ontonotes, batch 31 (11031): mcc: 0.8454, acc: 0.7852, precision: 0.8688, recall: 0.8288, f1: 0.8483, edges-pos-ontonotes_loss: 0.0152
09/16 12:55:23 PM: Update 11103: task edges-pos-ontonotes, batch 103 (11103): mcc: 0.8534, acc: 0.7961, precision: 0.8759, recall: 0.8373, f1: 0.8562, edges-pos-ontonotes_loss: 0.0151
09/16 12:55:33 PM: Update 11171: task edges-pos-ontonotes, batch 171 (11171): mcc: 0.8548, acc: 0.7982, precision: 0.8771, recall: 0.8389, f1: 0.8575, edges-pos-ontonotes_loss: 0.0149
09/16 12:55:43 PM: Update 11243: task edges-pos-ontonotes, batch 243 (11243): mcc: 0.8550, acc: 0.7989, precision: 0.8771, recall: 0.8392, f1: 0.8577, edges-pos-ontonotes_loss: 0.0149
09/16 12:55:53 PM: Update 11306: task edges-pos-ontonotes, batch 306 (11306): mcc: 0.8551, acc: 0.7987, precision: 0.8772, recall: 0.8393, f1: 0.8578, edges-pos-ontonotes_loss: 0.0148
09/16 12:56:07 PM: Update 11320: task edges-pos-ontonotes, batch 320 (11320): mcc: 0.8553, acc: 0.7990, precision: 0.8776, recall: 0.8394, f1: 0.8581, edges-pos-ontonotes_loss: 0.0148
09/16 12:56:17 PM: Update 11380: task edges-pos-ontonotes, batch 380 (11380): mcc: 0.8554, acc: 0.7994, precision: 0.8781, recall: 0.8391, f1: 0.8581, edges-pos-ontonotes_loss: 0.0149
09/16 12:56:27 PM: Update 11445: task edges-pos-ontonotes, batch 445 (11445): mcc: 0.8556, acc: 0.7999, precision: 0.8786, recall: 0.8389, f1: 0.8583, edges-pos-ontonotes_loss: 0.0149
09/16 12:56:37 PM: Update 11511: task edges-pos-ontonotes, batch 511 (11511): mcc: 0.8559, acc: 0.8007, precision: 0.8789, recall: 0.8392, f1: 0.8586, edges-pos-ontonotes_loss: 0.0149
09/16 12:56:47 PM: Update 11580: task edges-pos-ontonotes, batch 580 (11580): mcc: 0.8565, acc: 0.8018, precision: 0.8796, recall: 0.8397, f1: 0.8592, edges-pos-ontonotes_loss: 0.0148
09/16 12:56:57 PM: Update 11633: task edges-pos-ontonotes, batch 633 (11633): mcc: 0.8569, acc: 0.8026, precision: 0.8799, recall: 0.8401, f1: 0.8595, edges-pos-ontonotes_loss: 0.0148
09/16 12:57:07 PM: Update 11705: task edges-pos-ontonotes, batch 705 (11705): mcc: 0.8572, acc: 0.8031, precision: 0.8803, recall: 0.8403, f1: 0.8599, edges-pos-ontonotes_loss: 0.0148
09/16 12:57:17 PM: Update 11776: task edges-pos-ontonotes, batch 776 (11776): mcc: 0.8575, acc: 0.8037, precision: 0.8807, recall: 0.8406, f1: 0.8602, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:27 PM: Update 11845: task edges-pos-ontonotes, batch 845 (11845): mcc: 0.8580, acc: 0.8045, precision: 0.8811, recall: 0.8412, f1: 0.8607, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:37 PM: Update 11908: task edges-pos-ontonotes, batch 908 (11908): mcc: 0.8581, acc: 0.8046, precision: 0.8812, recall: 0.8411, f1: 0.8607, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:47 PM: Update 11960: task edges-pos-ontonotes, batch 960 (11960): mcc: 0.8583, acc: 0.8051, precision: 0.8816, recall: 0.8413, f1: 0.8610, edges-pos-ontonotes_loss: 0.0147
09/16 12:57:53 PM: ***** Step 12000 / Validation 12 *****
09/16 12:57:53 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 12:57:53 PM: Validating...
09/16 12:57:57 PM: Evaluate: task edges-pos-ontonotes, batch 43 (157): mcc: 0.8590, acc: 0.8306, precision: 0.8785, recall: 0.8456, f1: 0.8617, edges-pos-ontonotes_loss: 0.0143
09/16 12:58:07 PM: Evaluate: task edges-pos-ontonotes, batch 123 (157): mcc: 0.8672, acc: 0.8398, precision: 0.8860, recall: 0.8541, f1: 0.8697, edges-pos-ontonotes_loss: 0.0136
09/16 12:58:13 PM: Updating LR scheduler:
09/16 12:58:13 PM: 	Best result seen so far for macro_avg: 0.871
09/16 12:58:13 PM: 	# validation passes without improvement: 0
09/16 12:58:13 PM: edges-pos-ontonotes_loss: training: 0.014681 validation: 0.013461
09/16 12:58:13 PM: macro_avg: validation: 0.870791
09/16 12:58:13 PM: micro_avg: validation: 0.000000
09/16 12:58:13 PM: edges-pos-ontonotes_mcc: training: 0.858345 validation: 0.868258
09/16 12:58:13 PM: edges-pos-ontonotes_acc: training: 0.805194 validation: 0.840122
09/16 12:58:13 PM: edges-pos-ontonotes_precision: training: 0.881587 validation: 0.887632
09/16 12:58:13 PM: edges-pos-ontonotes_recall: training: 0.841330 validation: 0.854577
09/16 12:58:13 PM: edges-pos-ontonotes_f1: training: 0.860988 validation: 0.870791
09/16 12:58:13 PM: Global learning rate: 5e-05
09/16 12:58:13 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 12:58:18 PM: Update 12030: task edges-pos-ontonotes, batch 30 (12030): mcc: 0.8623, acc: 0.8108, precision: 0.8852, recall: 0.8454, f1: 0.8649, edges-pos-ontonotes_loss: 0.0137
09/16 12:58:28 PM: Update 12083: task edges-pos-ontonotes, batch 83 (12083): mcc: 0.8627, acc: 0.8104, precision: 0.8857, recall: 0.8457, f1: 0.8653, edges-pos-ontonotes_loss: 0.0143
09/16 12:58:38 PM: Update 12138: task edges-pos-ontonotes, batch 138 (12138): mcc: 0.8626, acc: 0.8107, precision: 0.8856, recall: 0.8457, f1: 0.8652, edges-pos-ontonotes_loss: 0.0144
09/16 12:58:48 PM: Update 12175: task edges-pos-ontonotes, batch 175 (12175): mcc: 0.8633, acc: 0.8118, precision: 0.8868, recall: 0.8459, f1: 0.8659, edges-pos-ontonotes_loss: 0.0143
09/16 12:58:58 PM: Update 12234: task edges-pos-ontonotes, batch 234 (12234): mcc: 0.8635, acc: 0.8123, precision: 0.8865, recall: 0.8465, f1: 0.8661, edges-pos-ontonotes_loss: 0.0143
09/16 12:59:08 PM: Update 12283: task edges-pos-ontonotes, batch 283 (12283): mcc: 0.8627, acc: 0.8118, precision: 0.8855, recall: 0.8459, f1: 0.8653, edges-pos-ontonotes_loss: 0.0141
09/16 12:59:18 PM: Update 12361: task edges-pos-ontonotes, batch 361 (12361): mcc: 0.8638, acc: 0.8126, precision: 0.8864, recall: 0.8473, f1: 0.8664, edges-pos-ontonotes_loss: 0.0138
09/16 12:59:29 PM: Update 12447: task edges-pos-ontonotes, batch 447 (12447): mcc: 0.8651, acc: 0.8134, precision: 0.8878, recall: 0.8484, f1: 0.8676, edges-pos-ontonotes_loss: 0.0135
09/16 12:59:39 PM: Update 12528: task edges-pos-ontonotes, batch 528 (12528): mcc: 0.8659, acc: 0.8137, precision: 0.8884, recall: 0.8492, f1: 0.8684, edges-pos-ontonotes_loss: 0.0132
09/16 12:59:49 PM: Update 12590: task edges-pos-ontonotes, batch 590 (12590): mcc: 0.8670, acc: 0.8147, precision: 0.8895, recall: 0.8504, f1: 0.8695, edges-pos-ontonotes_loss: 0.0130
09/16 12:59:59 PM: Update 12696: task edges-pos-ontonotes, batch 696 (12696): mcc: 0.8694, acc: 0.8173, precision: 0.8912, recall: 0.8532, f1: 0.8718, edges-pos-ontonotes_loss: 0.0126
09/16 01:00:09 PM: Update 12801: task edges-pos-ontonotes, batch 801 (12801): mcc: 0.8717, acc: 0.8200, precision: 0.8930, recall: 0.8561, f1: 0.8742, edges-pos-ontonotes_loss: 0.0123
09/16 01:00:19 PM: Update 12905: task edges-pos-ontonotes, batch 905 (12905): mcc: 0.8734, acc: 0.8219, precision: 0.8943, recall: 0.8580, f1: 0.8758, edges-pos-ontonotes_loss: 0.0121
09/16 01:00:26 PM: ***** Step 13000 / Validation 13 *****
09/16 01:00:26 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:00:26 PM: Validating...
09/16 01:00:29 PM: Evaluate: task edges-pos-ontonotes, batch 34 (157): mcc: 0.8430, acc: 0.8159, precision: 0.8700, recall: 0.8229, f1: 0.8458, edges-pos-ontonotes_loss: 0.0145
09/16 01:00:39 PM: Evaluate: task edges-pos-ontonotes, batch 116 (157): mcc: 0.8497, acc: 0.8213, precision: 0.8755, recall: 0.8305, f1: 0.8524, edges-pos-ontonotes_loss: 0.0140
09/16 01:00:47 PM: Updating LR scheduler:
09/16 01:00:47 PM: 	Best result seen so far for macro_avg: 0.871
09/16 01:00:47 PM: 	# validation passes without improvement: 1
09/16 01:00:47 PM: edges-pos-ontonotes_loss: training: 0.011993 validation: 0.014184
09/16 01:00:47 PM: macro_avg: validation: 0.851107
09/16 01:00:47 PM: micro_avg: validation: 0.000000
09/16 01:00:47 PM: edges-pos-ontonotes_mcc: training: 0.874322 validation: 0.848428
09/16 01:00:47 PM: edges-pos-ontonotes_acc: training: 0.822975 validation: 0.817031
09/16 01:00:47 PM: edges-pos-ontonotes_precision: training: 0.895257 validation: 0.877234
09/16 01:00:47 PM: edges-pos-ontonotes_recall: training: 0.858884 validation: 0.826492
09/16 01:00:47 PM: edges-pos-ontonotes_f1: training: 0.876694 validation: 0.851107
09/16 01:00:47 PM: Global learning rate: 5e-05
09/16 01:00:47 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:00:49 PM: Update 13026: task edges-pos-ontonotes, batch 26 (13026): mcc: 0.8861, acc: 0.8376, precision: 0.9027, recall: 0.8743, f1: 0.8883, edges-pos-ontonotes_loss: 0.0110
09/16 01:00:59 PM: Update 13162: task edges-pos-ontonotes, batch 162 (13162): mcc: 0.8907, acc: 0.8442, precision: 0.9055, recall: 0.8807, f1: 0.8929, edges-pos-ontonotes_loss: 0.0106
09/16 01:01:09 PM: Update 13302: task edges-pos-ontonotes, batch 302 (13302): mcc: 0.8836, acc: 0.8365, precision: 0.9009, recall: 0.8713, f1: 0.8858, edges-pos-ontonotes_loss: 0.0110
09/16 01:01:20 PM: Update 13399: task edges-pos-ontonotes, batch 399 (13399): mcc: 0.8796, acc: 0.8331, precision: 0.8979, recall: 0.8666, f1: 0.8820, edges-pos-ontonotes_loss: 0.0112
09/16 01:01:41 PM: Update 13511: task edges-pos-ontonotes, batch 511 (13511): mcc: 0.8776, acc: 0.8315, precision: 0.8961, recall: 0.8644, f1: 0.8799, edges-pos-ontonotes_loss: 0.0114
09/16 01:01:51 PM: Update 13583: task edges-pos-ontonotes, batch 583 (13583): mcc: 0.8678, acc: 0.8188, precision: 0.8883, recall: 0.8530, f1: 0.8703, edges-pos-ontonotes_loss: 0.0120
09/16 01:02:01 PM: Update 13661: task edges-pos-ontonotes, batch 661 (13661): mcc: 0.8629, acc: 0.8124, precision: 0.8850, recall: 0.8467, f1: 0.8654, edges-pos-ontonotes_loss: 0.0126
09/16 01:02:11 PM: Update 13733: task edges-pos-ontonotes, batch 733 (13733): mcc: 0.8597, acc: 0.8078, precision: 0.8828, recall: 0.8427, f1: 0.8623, edges-pos-ontonotes_loss: 0.0129
09/16 01:02:21 PM: Update 13795: task edges-pos-ontonotes, batch 795 (13795): mcc: 0.8575, acc: 0.8044, precision: 0.8817, recall: 0.8397, f1: 0.8602, edges-pos-ontonotes_loss: 0.0132
09/16 01:02:31 PM: Update 13848: task edges-pos-ontonotes, batch 848 (13848): mcc: 0.8560, acc: 0.8021, precision: 0.8807, recall: 0.8376, f1: 0.8586, edges-pos-ontonotes_loss: 0.0134
09/16 01:02:41 PM: Update 13962: task edges-pos-ontonotes, batch 962 (13962): mcc: 0.8565, acc: 0.8030, precision: 0.8809, recall: 0.8385, f1: 0.8592, edges-pos-ontonotes_loss: 0.0134
09/16 01:02:46 PM: ***** Step 14000 / Validation 14 *****
09/16 01:02:46 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:02:46 PM: Validating...
09/16 01:02:51 PM: Evaluate: task edges-pos-ontonotes, batch 48 (157): mcc: 0.8696, acc: 0.8353, precision: 0.9001, recall: 0.8452, f1: 0.8718, edges-pos-ontonotes_loss: 0.0129
09/16 01:03:01 PM: Evaluate: task edges-pos-ontonotes, batch 127 (157): mcc: 0.8648, acc: 0.8288, precision: 0.8930, recall: 0.8428, f1: 0.8672, edges-pos-ontonotes_loss: 0.0130
09/16 01:03:06 PM: Updating LR scheduler:
09/16 01:03:06 PM: 	Best result seen so far for macro_avg: 0.871
09/16 01:03:06 PM: 	# validation passes without improvement: 2
09/16 01:03:06 PM: edges-pos-ontonotes_loss: training: 0.013345 validation: 0.013242
09/16 01:03:06 PM: macro_avg: validation: 0.864967
09/16 01:03:06 PM: micro_avg: validation: 0.000000
09/16 01:03:06 PM: edges-pos-ontonotes_mcc: training: 0.856642 validation: 0.862572
09/16 01:03:06 PM: edges-pos-ontonotes_acc: training: 0.803123 validation: 0.825762
09/16 01:03:06 PM: edges-pos-ontonotes_precision: training: 0.880880 validation: 0.891266
09/16 01:03:06 PM: edges-pos-ontonotes_recall: training: 0.838738 validation: 0.840175
09/16 01:03:06 PM: edges-pos-ontonotes_f1: training: 0.859293 validation: 0.864967
09/16 01:03:06 PM: Global learning rate: 5e-05
09/16 01:03:06 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:03:11 PM: Update 14069: task edges-pos-ontonotes, batch 69 (14069): mcc: 0.8669, acc: 0.8190, precision: 0.8857, recall: 0.8539, f1: 0.8695, edges-pos-ontonotes_loss: 0.0125
09/16 01:03:21 PM: Update 14159: task edges-pos-ontonotes, batch 159 (14159): mcc: 0.8664, acc: 0.8189, precision: 0.8857, recall: 0.8529, f1: 0.8690, edges-pos-ontonotes_loss: 0.0123
09/16 01:03:31 PM: Update 14255: task edges-pos-ontonotes, batch 255 (14255): mcc: 0.8663, acc: 0.8170, precision: 0.8872, recall: 0.8513, f1: 0.8689, edges-pos-ontonotes_loss: 0.0126
09/16 01:03:41 PM: Update 14341: task edges-pos-ontonotes, batch 341 (14341): mcc: 0.8656, acc: 0.8142, precision: 0.8871, recall: 0.8500, f1: 0.8682, edges-pos-ontonotes_loss: 0.0127
09/16 01:03:51 PM: Update 14432: task edges-pos-ontonotes, batch 432 (14432): mcc: 0.8654, acc: 0.8134, precision: 0.8870, recall: 0.8497, f1: 0.8680, edges-pos-ontonotes_loss: 0.0128
09/16 01:04:01 PM: Update 14489: task edges-pos-ontonotes, batch 489 (14489): mcc: 0.8635, acc: 0.8104, precision: 0.8856, recall: 0.8475, f1: 0.8661, edges-pos-ontonotes_loss: 0.0129
09/16 01:04:11 PM: Update 14563: task edges-pos-ontonotes, batch 563 (14563): mcc: 0.8616, acc: 0.8077, precision: 0.8839, recall: 0.8454, f1: 0.8642, edges-pos-ontonotes_loss: 0.0132
09/16 01:04:21 PM: Update 14638: task edges-pos-ontonotes, batch 638 (14638): mcc: 0.8606, acc: 0.8063, precision: 0.8829, recall: 0.8444, f1: 0.8632, edges-pos-ontonotes_loss: 0.0134
09/16 01:04:31 PM: Update 14696: task edges-pos-ontonotes, batch 696 (14696): mcc: 0.8603, acc: 0.8057, precision: 0.8825, recall: 0.8441, f1: 0.8629, edges-pos-ontonotes_loss: 0.0135
09/16 01:04:41 PM: Update 14750: task edges-pos-ontonotes, batch 750 (14750): mcc: 0.8599, acc: 0.8053, precision: 0.8821, recall: 0.8438, f1: 0.8625, edges-pos-ontonotes_loss: 0.0135
09/16 01:04:52 PM: Update 14781: task edges-pos-ontonotes, batch 781 (14781): mcc: 0.8599, acc: 0.8053, precision: 0.8822, recall: 0.8437, f1: 0.8625, edges-pos-ontonotes_loss: 0.0136
09/16 01:05:02 PM: Update 14843: task edges-pos-ontonotes, batch 843 (14843): mcc: 0.8597, acc: 0.8053, precision: 0.8821, recall: 0.8435, f1: 0.8623, edges-pos-ontonotes_loss: 0.0137
09/16 01:05:12 PM: Update 14913: task edges-pos-ontonotes, batch 913 (14913): mcc: 0.8597, acc: 0.8055, precision: 0.8822, recall: 0.8433, f1: 0.8623, edges-pos-ontonotes_loss: 0.0137
09/16 01:05:22 PM: Update 14977: task edges-pos-ontonotes, batch 977 (14977): mcc: 0.8595, acc: 0.8055, precision: 0.8821, recall: 0.8431, f1: 0.8621, edges-pos-ontonotes_loss: 0.0138
09/16 01:05:25 PM: ***** Step 15000 / Validation 15 *****
09/16 01:05:25 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:05:25 PM: Validating...
09/16 01:05:32 PM: Evaluate: task edges-pos-ontonotes, batch 70 (157): mcc: 0.8698, acc: 0.8463, precision: 0.8889, recall: 0.8564, f1: 0.8723, edges-pos-ontonotes_loss: 0.0131
09/16 01:05:42 PM: Evaluate: task edges-pos-ontonotes, batch 141 (157): mcc: 0.8719, acc: 0.8458, precision: 0.8903, recall: 0.8589, f1: 0.8743, edges-pos-ontonotes_loss: 0.0129
09/16 01:05:44 PM: Best result seen so far for edges-pos-ontonotes.
09/16 01:05:44 PM: Best result seen so far for macro.
09/16 01:05:44 PM: Updating LR scheduler:
09/16 01:05:44 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:05:44 PM: 	# validation passes without improvement: 0
09/16 01:05:44 PM: edges-pos-ontonotes_loss: training: 0.013791 validation: 0.012944
09/16 01:05:44 PM: macro_avg: validation: 0.874469
09/16 01:05:44 PM: micro_avg: validation: 0.000000
09/16 01:05:44 PM: edges-pos-ontonotes_mcc: training: 0.859578 validation: 0.871994
09/16 01:05:44 PM: edges-pos-ontonotes_acc: training: 0.805664 validation: 0.845858
09/16 01:05:44 PM: edges-pos-ontonotes_precision: training: 0.882067 validation: 0.890504
09/16 01:05:44 PM: edges-pos-ontonotes_recall: training: 0.843237 validation: 0.859001
09/16 01:05:44 PM: edges-pos-ontonotes_f1: training: 0.862215 validation: 0.874469
09/16 01:05:44 PM: Global learning rate: 5e-05
09/16 01:05:44 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:05:52 PM: Update 15049: task edges-pos-ontonotes, batch 49 (15049): mcc: 0.8599, acc: 0.8091, precision: 0.8836, recall: 0.8424, f1: 0.8625, edges-pos-ontonotes_loss: 0.0145
09/16 01:06:09 PM: Update 15093: task edges-pos-ontonotes, batch 93 (15093): mcc: 0.8586, acc: 0.8075, precision: 0.8824, recall: 0.8409, f1: 0.8612, edges-pos-ontonotes_loss: 0.0147
09/16 01:06:20 PM: Update 15163: task edges-pos-ontonotes, batch 163 (15163): mcc: 0.8594, acc: 0.8086, precision: 0.8832, recall: 0.8419, f1: 0.8621, edges-pos-ontonotes_loss: 0.0147
09/16 01:06:30 PM: Update 15230: task edges-pos-ontonotes, batch 230 (15230): mcc: 0.8598, acc: 0.8093, precision: 0.8835, recall: 0.8423, f1: 0.8624, edges-pos-ontonotes_loss: 0.0147
09/16 01:06:40 PM: Update 15298: task edges-pos-ontonotes, batch 298 (15298): mcc: 0.8605, acc: 0.8100, precision: 0.8840, recall: 0.8431, f1: 0.8631, edges-pos-ontonotes_loss: 0.0145
09/16 01:06:50 PM: Update 15371: task edges-pos-ontonotes, batch 371 (15371): mcc: 0.8611, acc: 0.8106, precision: 0.8846, recall: 0.8436, f1: 0.8637, edges-pos-ontonotes_loss: 0.0145
09/16 01:07:00 PM: Update 15426: task edges-pos-ontonotes, batch 426 (15426): mcc: 0.8612, acc: 0.8107, precision: 0.8849, recall: 0.8437, f1: 0.8638, edges-pos-ontonotes_loss: 0.0145
09/16 01:07:10 PM: Update 15495: task edges-pos-ontonotes, batch 495 (15495): mcc: 0.8612, acc: 0.8106, precision: 0.8849, recall: 0.8437, f1: 0.8638, edges-pos-ontonotes_loss: 0.0145
09/16 01:07:20 PM: Update 15564: task edges-pos-ontonotes, batch 564 (15564): mcc: 0.8614, acc: 0.8108, precision: 0.8849, recall: 0.8440, f1: 0.8640, edges-pos-ontonotes_loss: 0.0144
09/16 01:07:30 PM: Update 15628: task edges-pos-ontonotes, batch 628 (15628): mcc: 0.8617, acc: 0.8111, precision: 0.8851, recall: 0.8443, f1: 0.8642, edges-pos-ontonotes_loss: 0.0144
09/16 01:07:40 PM: Update 15701: task edges-pos-ontonotes, batch 701 (15701): mcc: 0.8618, acc: 0.8112, precision: 0.8852, recall: 0.8445, f1: 0.8644, edges-pos-ontonotes_loss: 0.0144
09/16 01:07:50 PM: Update 15759: task edges-pos-ontonotes, batch 759 (15759): mcc: 0.8615, acc: 0.8107, precision: 0.8848, recall: 0.8442, f1: 0.8640, edges-pos-ontonotes_loss: 0.0143
09/16 01:08:01 PM: Update 15843: task edges-pos-ontonotes, batch 843 (15843): mcc: 0.8624, acc: 0.8116, precision: 0.8855, recall: 0.8453, f1: 0.8649, edges-pos-ontonotes_loss: 0.0141
09/16 01:08:11 PM: Update 15926: task edges-pos-ontonotes, batch 926 (15926): mcc: 0.8631, acc: 0.8122, precision: 0.8860, recall: 0.8462, f1: 0.8656, edges-pos-ontonotes_loss: 0.0139
09/16 01:08:19 PM: ***** Step 16000 / Validation 16 *****
09/16 01:08:19 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:08:19 PM: Validating...
09/16 01:08:21 PM: Evaluate: task edges-pos-ontonotes, batch 16 (157): mcc: 0.8604, acc: 0.8224, precision: 0.8874, recall: 0.8397, f1: 0.8629, edges-pos-ontonotes_loss: 0.0139
09/16 01:08:31 PM: Evaluate: task edges-pos-ontonotes, batch 106 (157): mcc: 0.8665, acc: 0.8266, precision: 0.8950, recall: 0.8442, f1: 0.8688, edges-pos-ontonotes_loss: 0.0135
09/16 01:08:39 PM: Updating LR scheduler:
09/16 01:08:39 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:08:39 PM: 	# validation passes without improvement: 1
09/16 01:08:39 PM: edges-pos-ontonotes_loss: training: 0.013744 validation: 0.013501
09/16 01:08:39 PM: macro_avg: validation: 0.867672
09/16 01:08:39 PM: micro_avg: validation: 0.000000
09/16 01:08:39 PM: edges-pos-ontonotes_mcc: training: 0.863796 validation: 0.865415
09/16 01:08:39 PM: edges-pos-ontonotes_acc: training: 0.812708 validation: 0.825031
09/16 01:08:39 PM: edges-pos-ontonotes_precision: training: 0.886647 validation: 0.896500
09/16 01:08:39 PM: edges-pos-ontonotes_recall: training: 0.846937 validation: 0.840640
09/16 01:08:39 PM: edges-pos-ontonotes_f1: training: 0.866337 validation: 0.867672
09/16 01:08:39 PM: Global learning rate: 5e-05
09/16 01:08:39 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:08:41 PM: Update 16019: task edges-pos-ontonotes, batch 19 (16019): mcc: 0.8747, acc: 0.8215, precision: 0.8974, recall: 0.8576, f1: 0.8770, edges-pos-ontonotes_loss: 0.0117
09/16 01:08:51 PM: Update 16101: task edges-pos-ontonotes, batch 101 (16101): mcc: 0.8841, acc: 0.8328, precision: 0.9029, recall: 0.8704, f1: 0.8863, edges-pos-ontonotes_loss: 0.0107
09/16 01:09:01 PM: Update 16212: task edges-pos-ontonotes, batch 212 (16212): mcc: 0.8886, acc: 0.8386, precision: 0.9063, recall: 0.8757, f1: 0.8908, edges-pos-ontonotes_loss: 0.0103
09/16 01:09:11 PM: Update 16320: task edges-pos-ontonotes, batch 320 (16320): mcc: 0.8907, acc: 0.8414, precision: 0.9079, recall: 0.8782, f1: 0.8928, edges-pos-ontonotes_loss: 0.0101
09/16 01:09:21 PM: Update 16431: task edges-pos-ontonotes, batch 431 (16431): mcc: 0.8901, acc: 0.8412, precision: 0.9076, recall: 0.8774, f1: 0.8923, edges-pos-ontonotes_loss: 0.0104
09/16 01:09:31 PM: Update 16566: task edges-pos-ontonotes, batch 566 (16566): mcc: 0.8907, acc: 0.8422, precision: 0.9079, recall: 0.8781, f1: 0.8928, edges-pos-ontonotes_loss: 0.0104
09/16 01:09:41 PM: Update 16694: task edges-pos-ontonotes, batch 694 (16694): mcc: 0.8898, acc: 0.8416, precision: 0.9072, recall: 0.8771, f1: 0.8919, edges-pos-ontonotes_loss: 0.0104
09/16 01:09:51 PM: Update 16874: task edges-pos-ontonotes, batch 874 (16874): mcc: 0.8870, acc: 0.8396, precision: 0.9047, recall: 0.8742, f1: 0.8892, edges-pos-ontonotes_loss: 0.0108
09/16 01:10:01 PM: Update 16980: task edges-pos-ontonotes, batch 980 (16980): mcc: 0.8841, acc: 0.8367, precision: 0.9024, recall: 0.8709, f1: 0.8864, edges-pos-ontonotes_loss: 0.0109
09/16 01:10:05 PM: ***** Step 17000 / Validation 17 *****
09/16 01:10:05 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:10:05 PM: Validating...
09/16 01:10:11 PM: Evaluate: task edges-pos-ontonotes, batch 65 (157): mcc: 0.8703, acc: 0.8446, precision: 0.8910, recall: 0.8553, f1: 0.8728, edges-pos-ontonotes_loss: 0.0134
09/16 01:10:21 PM: Evaluate: task edges-pos-ontonotes, batch 127 (157): mcc: 0.8605, acc: 0.8289, precision: 0.8823, recall: 0.8449, f1: 0.8632, edges-pos-ontonotes_loss: 0.0139
09/16 01:10:26 PM: Updating LR scheduler:
09/16 01:10:26 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:10:26 PM: 	# validation passes without improvement: 2
09/16 01:10:26 PM: edges-pos-ontonotes_loss: training: 0.010980 validation: 0.014101
09/16 01:10:26 PM: macro_avg: validation: 0.860601
09/16 01:10:26 PM: micro_avg: validation: 0.000000
09/16 01:10:26 PM: edges-pos-ontonotes_mcc: training: 0.881893 validation: 0.857945
09/16 01:10:26 PM: edges-pos-ontonotes_acc: training: 0.833763 validation: 0.825010
09/16 01:10:26 PM: edges-pos-ontonotes_precision: training: 0.900506 validation: 0.880975
09/16 01:10:26 PM: edges-pos-ontonotes_recall: training: 0.868398 validation: 0.841148
09/16 01:10:26 PM: edges-pos-ontonotes_f1: training: 0.884160 validation: 0.860601
09/16 01:10:26 PM: Global learning rate: 5e-05
09/16 01:10:26 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:10:31 PM: Update 17043: task edges-pos-ontonotes, batch 43 (17043): mcc: 0.8432, acc: 0.7865, precision: 0.8671, recall: 0.8263, f1: 0.8462, edges-pos-ontonotes_loss: 0.0163
09/16 01:10:42 PM: Update 17116: task edges-pos-ontonotes, batch 116 (17116): mcc: 0.8411, acc: 0.7830, precision: 0.8679, recall: 0.8213, f1: 0.8440, edges-pos-ontonotes_loss: 0.0166
09/16 01:10:52 PM: Update 17190: task edges-pos-ontonotes, batch 190 (17190): mcc: 0.8425, acc: 0.7840, precision: 0.8705, recall: 0.8216, f1: 0.8453, edges-pos-ontonotes_loss: 0.0163
09/16 01:11:02 PM: Update 17266: task edges-pos-ontonotes, batch 266 (17266): mcc: 0.8430, acc: 0.7845, precision: 0.8713, recall: 0.8217, f1: 0.8458, edges-pos-ontonotes_loss: 0.0165
09/16 01:11:12 PM: Update 17350: task edges-pos-ontonotes, batch 350 (17350): mcc: 0.8438, acc: 0.7857, precision: 0.8719, recall: 0.8228, f1: 0.8466, edges-pos-ontonotes_loss: 0.0160
09/16 01:11:22 PM: Update 17462: task edges-pos-ontonotes, batch 462 (17462): mcc: 0.8478, acc: 0.7913, precision: 0.8745, recall: 0.8279, f1: 0.8505, edges-pos-ontonotes_loss: 0.0152
09/16 01:11:32 PM: Update 17575: task edges-pos-ontonotes, batch 575 (17575): mcc: 0.8504, acc: 0.7951, precision: 0.8761, recall: 0.8314, f1: 0.8532, edges-pos-ontonotes_loss: 0.0146
09/16 01:11:45 PM: Update 17614: task edges-pos-ontonotes, batch 614 (17614): mcc: 0.8511, acc: 0.7963, precision: 0.8765, recall: 0.8323, f1: 0.8539, edges-pos-ontonotes_loss: 0.0144
09/16 01:11:56 PM: Update 17670: task edges-pos-ontonotes, batch 670 (17670): mcc: 0.8520, acc: 0.7975, precision: 0.8770, recall: 0.8336, f1: 0.8548, edges-pos-ontonotes_loss: 0.0144
09/16 01:12:06 PM: Update 17765: task edges-pos-ontonotes, batch 765 (17765): mcc: 0.8537, acc: 0.7993, precision: 0.8783, recall: 0.8356, f1: 0.8564, edges-pos-ontonotes_loss: 0.0142
09/16 01:12:16 PM: Update 17841: task edges-pos-ontonotes, batch 841 (17841): mcc: 0.8551, acc: 0.8009, precision: 0.8793, recall: 0.8373, f1: 0.8578, edges-pos-ontonotes_loss: 0.0141
09/16 01:12:27 PM: Update 17927: task edges-pos-ontonotes, batch 927 (17927): mcc: 0.8561, acc: 0.8018, precision: 0.8802, recall: 0.8383, f1: 0.8587, edges-pos-ontonotes_loss: 0.0139
09/16 01:12:37 PM: Update 17992: task edges-pos-ontonotes, batch 992 (17992): mcc: 0.8552, acc: 0.8003, precision: 0.8793, recall: 0.8375, f1: 0.8579, edges-pos-ontonotes_loss: 0.0140
09/16 01:12:38 PM: ***** Step 18000 / Validation 18 *****
09/16 01:12:38 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:12:38 PM: Validating...
09/16 01:12:47 PM: Evaluate: task edges-pos-ontonotes, batch 91 (157): mcc: 0.8754, acc: 0.8531, precision: 0.8934, recall: 0.8628, f1: 0.8778, edges-pos-ontonotes_loss: 0.0125
09/16 01:12:57 PM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.8716, acc: 0.8467, precision: 0.8908, recall: 0.8581, f1: 0.8741, edges-pos-ontonotes_loss: 0.0129
09/16 01:12:58 PM: Updating LR scheduler:
09/16 01:12:58 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:12:58 PM: 	# validation passes without improvement: 3
09/16 01:12:58 PM: edges-pos-ontonotes_loss: training: 0.014034 validation: 0.012849
09/16 01:12:58 PM: macro_avg: validation: 0.874111
09/16 01:12:58 PM: micro_avg: validation: 0.000000
09/16 01:12:58 PM: edges-pos-ontonotes_mcc: training: 0.855203 validation: 0.871643
09/16 01:12:58 PM: edges-pos-ontonotes_acc: training: 0.800306 validation: 0.846704
09/16 01:12:58 PM: edges-pos-ontonotes_precision: training: 0.879280 validation: 0.890787
09/16 01:12:58 PM: edges-pos-ontonotes_recall: training: 0.837511 validation: 0.858048
09/16 01:12:58 PM: edges-pos-ontonotes_f1: training: 0.857888 validation: 0.874111
09/16 01:12:58 PM: Global learning rate: 5e-05
09/16 01:12:58 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:13:08 PM: Update 18066: task edges-pos-ontonotes, batch 66 (18066): mcc: 0.8553, acc: 0.7979, precision: 0.8772, recall: 0.8397, f1: 0.8580, edges-pos-ontonotes_loss: 0.0145
09/16 01:13:18 PM: Update 18139: task edges-pos-ontonotes, batch 139 (18139): mcc: 0.8556, acc: 0.8000, precision: 0.8778, recall: 0.8397, f1: 0.8583, edges-pos-ontonotes_loss: 0.0145
09/16 01:13:28 PM: Update 18214: task edges-pos-ontonotes, batch 214 (18214): mcc: 0.8568, acc: 0.8015, precision: 0.8784, recall: 0.8414, f1: 0.8595, edges-pos-ontonotes_loss: 0.0144
09/16 01:13:38 PM: Update 18268: task edges-pos-ontonotes, batch 268 (18268): mcc: 0.8569, acc: 0.8021, precision: 0.8788, recall: 0.8412, f1: 0.8596, edges-pos-ontonotes_loss: 0.0145
09/16 01:13:48 PM: Update 18336: task edges-pos-ontonotes, batch 336 (18336): mcc: 0.8572, acc: 0.8030, precision: 0.8793, recall: 0.8414, f1: 0.8599, edges-pos-ontonotes_loss: 0.0146
09/16 01:13:58 PM: Update 18403: task edges-pos-ontonotes, batch 403 (18403): mcc: 0.8572, acc: 0.8033, precision: 0.8793, recall: 0.8413, f1: 0.8599, edges-pos-ontonotes_loss: 0.0146
09/16 01:14:08 PM: Update 18463: task edges-pos-ontonotes, batch 463 (18463): mcc: 0.8573, acc: 0.8038, precision: 0.8797, recall: 0.8412, f1: 0.8600, edges-pos-ontonotes_loss: 0.0146
09/16 01:14:18 PM: Update 18512: task edges-pos-ontonotes, batch 512 (18512): mcc: 0.8580, acc: 0.8047, precision: 0.8802, recall: 0.8419, f1: 0.8606, edges-pos-ontonotes_loss: 0.0146
09/16 01:14:28 PM: Update 18561: task edges-pos-ontonotes, batch 561 (18561): mcc: 0.8581, acc: 0.8051, precision: 0.8804, recall: 0.8420, f1: 0.8608, edges-pos-ontonotes_loss: 0.0146
09/16 01:14:38 PM: Update 18622: task edges-pos-ontonotes, batch 622 (18622): mcc: 0.8585, acc: 0.8058, precision: 0.8807, recall: 0.8424, f1: 0.8612, edges-pos-ontonotes_loss: 0.0145
09/16 01:14:48 PM: Update 18694: task edges-pos-ontonotes, batch 694 (18694): mcc: 0.8588, acc: 0.8065, precision: 0.8812, recall: 0.8427, f1: 0.8615, edges-pos-ontonotes_loss: 0.0145
09/16 01:14:58 PM: Update 18763: task edges-pos-ontonotes, batch 763 (18763): mcc: 0.8589, acc: 0.8067, precision: 0.8814, recall: 0.8427, f1: 0.8616, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:09 PM: Update 18827: task edges-pos-ontonotes, batch 827 (18827): mcc: 0.8592, acc: 0.8072, precision: 0.8817, recall: 0.8429, f1: 0.8618, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:19 PM: Update 18878: task edges-pos-ontonotes, batch 878 (18878): mcc: 0.8591, acc: 0.8073, precision: 0.8817, recall: 0.8427, f1: 0.8618, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:29 PM: Update 18947: task edges-pos-ontonotes, batch 947 (18947): mcc: 0.8594, acc: 0.8077, precision: 0.8820, recall: 0.8430, f1: 0.8620, edges-pos-ontonotes_loss: 0.0145
09/16 01:15:36 PM: ***** Step 19000 / Validation 19 *****
09/16 01:15:36 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:15:36 PM: Validating...
09/16 01:15:39 PM: Evaluate: task edges-pos-ontonotes, batch 26 (157): mcc: 0.8642, acc: 0.8398, precision: 0.8834, recall: 0.8508, f1: 0.8668, edges-pos-ontonotes_loss: 0.0138
09/16 01:15:49 PM: Evaluate: task edges-pos-ontonotes, batch 113 (157): mcc: 0.8706, acc: 0.8463, precision: 0.8894, recall: 0.8574, f1: 0.8731, edges-pos-ontonotes_loss: 0.0131
09/16 01:15:56 PM: Updating LR scheduler:
09/16 01:15:56 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:15:56 PM: 	# validation passes without improvement: 0
09/16 01:15:56 PM: edges-pos-ontonotes_loss: training: 0.014473 validation: 0.013092
09/16 01:15:56 PM: macro_avg: validation: 0.873695
09/16 01:15:56 PM: micro_avg: validation: 0.000000
09/16 01:15:56 PM: edges-pos-ontonotes_mcc: training: 0.859649 validation: 0.871229
09/16 01:15:56 PM: edges-pos-ontonotes_acc: training: 0.808030 validation: 0.845508
09/16 01:15:56 PM: edges-pos-ontonotes_precision: training: 0.882203 validation: 0.890846
09/16 01:15:56 PM: edges-pos-ontonotes_recall: training: 0.843243 validation: 0.857191
09/16 01:15:56 PM: edges-pos-ontonotes_f1: training: 0.862283 validation: 0.873695
09/16 01:15:56 PM: Global learning rate: 2.5e-05
09/16 01:15:56 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:15:59 PM: Update 19020: task edges-pos-ontonotes, batch 20 (19020): mcc: 0.8641, acc: 0.8131, precision: 0.8884, recall: 0.8458, f1: 0.8666, edges-pos-ontonotes_loss: 0.0141
09/16 01:16:09 PM: Update 19090: task edges-pos-ontonotes, batch 90 (19090): mcc: 0.8618, acc: 0.8104, precision: 0.8850, recall: 0.8447, f1: 0.8644, edges-pos-ontonotes_loss: 0.0145
09/16 01:16:19 PM: Update 19159: task edges-pos-ontonotes, batch 159 (19159): mcc: 0.8627, acc: 0.8115, precision: 0.8855, recall: 0.8460, f1: 0.8653, edges-pos-ontonotes_loss: 0.0143
09/16 01:16:33 PM: Update 19179: task edges-pos-ontonotes, batch 179 (19179): mcc: 0.8623, acc: 0.8111, precision: 0.8848, recall: 0.8458, f1: 0.8648, edges-pos-ontonotes_loss: 0.0143
09/16 01:16:43 PM: Update 19252: task edges-pos-ontonotes, batch 252 (19252): mcc: 0.8632, acc: 0.8126, precision: 0.8847, recall: 0.8477, f1: 0.8658, edges-pos-ontonotes_loss: 0.0139
09/16 01:16:53 PM: Update 19338: task edges-pos-ontonotes, batch 338 (19338): mcc: 0.8654, acc: 0.8147, precision: 0.8867, recall: 0.8500, f1: 0.8680, edges-pos-ontonotes_loss: 0.0134
09/16 01:17:03 PM: Update 19415: task edges-pos-ontonotes, batch 415 (19415): mcc: 0.8665, acc: 0.8153, precision: 0.8879, recall: 0.8509, f1: 0.8690, edges-pos-ontonotes_loss: 0.0131
09/16 01:17:14 PM: Update 19492: task edges-pos-ontonotes, batch 492 (19492): mcc: 0.8675, acc: 0.8160, precision: 0.8891, recall: 0.8517, f1: 0.8700, edges-pos-ontonotes_loss: 0.0129
09/16 01:17:24 PM: Update 19609: task edges-pos-ontonotes, batch 609 (19609): mcc: 0.8705, acc: 0.8193, precision: 0.8913, recall: 0.8552, f1: 0.8729, edges-pos-ontonotes_loss: 0.0124
09/16 01:17:34 PM: Update 19718: task edges-pos-ontonotes, batch 718 (19718): mcc: 0.8731, acc: 0.8223, precision: 0.8935, recall: 0.8583, f1: 0.8756, edges-pos-ontonotes_loss: 0.0120
09/16 01:17:44 PM: Update 19817: task edges-pos-ontonotes, batch 817 (19817): mcc: 0.8747, acc: 0.8241, precision: 0.8946, recall: 0.8602, f1: 0.8771, edges-pos-ontonotes_loss: 0.0118
09/16 01:17:54 PM: Update 19953: task edges-pos-ontonotes, batch 953 (19953): mcc: 0.8756, acc: 0.8251, precision: 0.8955, recall: 0.8612, f1: 0.8780, edges-pos-ontonotes_loss: 0.0118
09/16 01:17:56 PM: ***** Step 20000 / Validation 20 *****
09/16 01:17:56 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:17:56 PM: Validating...
09/16 01:18:04 PM: Evaluate: task edges-pos-ontonotes, batch 74 (157): mcc: 0.8640, acc: 0.8247, precision: 0.9043, recall: 0.8308, f1: 0.8660, edges-pos-ontonotes_loss: 0.0134
09/16 01:18:14 PM: Evaluate: task edges-pos-ontonotes, batch 143 (157): mcc: 0.8532, acc: 0.8108, precision: 0.8965, recall: 0.8175, f1: 0.8552, edges-pos-ontonotes_loss: 0.0137
09/16 01:18:16 PM: Updating LR scheduler:
09/16 01:18:16 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:18:16 PM: 	# validation passes without improvement: 1
09/16 01:18:16 PM: edges-pos-ontonotes_loss: training: 0.011701 validation: 0.013759
09/16 01:18:16 PM: macro_avg: validation: 0.853971
09/16 01:18:16 PM: micro_avg: validation: 0.000000
09/16 01:18:16 PM: edges-pos-ontonotes_mcc: training: 0.875966 validation: 0.851974
09/16 01:18:16 PM: edges-pos-ontonotes_acc: training: 0.825458 validation: 0.809126
09/16 01:18:16 PM: edges-pos-ontonotes_precision: training: 0.895755 validation: 0.895705
09/16 01:18:16 PM: edges-pos-ontonotes_recall: training: 0.861570 validation: 0.815952
09/16 01:18:16 PM: edges-pos-ontonotes_f1: training: 0.878330 validation: 0.853971
09/16 01:18:16 PM: Global learning rate: 2.5e-05
09/16 01:18:16 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:18:24 PM: Update 20104: task edges-pos-ontonotes, batch 104 (20104): mcc: 0.8929, acc: 0.8453, precision: 0.9113, recall: 0.8792, f1: 0.8950, edges-pos-ontonotes_loss: 0.0104
09/16 01:18:34 PM: Update 20259: task edges-pos-ontonotes, batch 259 (20259): mcc: 0.8796, acc: 0.8312, precision: 0.9015, recall: 0.8630, f1: 0.8818, edges-pos-ontonotes_loss: 0.0112
09/16 01:18:44 PM: Update 20423: task edges-pos-ontonotes, batch 423 (20423): mcc: 0.8752, acc: 0.8282, precision: 0.8969, recall: 0.8589, f1: 0.8775, edges-pos-ontonotes_loss: 0.0116
09/16 01:18:54 PM: Update 20486: task edges-pos-ontonotes, batch 486 (20486): mcc: 0.8666, acc: 0.8176, precision: 0.8899, recall: 0.8492, f1: 0.8691, edges-pos-ontonotes_loss: 0.0121
09/16 01:19:04 PM: Update 20562: task edges-pos-ontonotes, batch 562 (20562): mcc: 0.8599, acc: 0.8088, precision: 0.8839, recall: 0.8421, f1: 0.8625, edges-pos-ontonotes_loss: 0.0128
09/16 01:19:14 PM: Update 20644: task edges-pos-ontonotes, batch 644 (20644): mcc: 0.8565, acc: 0.8042, precision: 0.8813, recall: 0.8380, f1: 0.8591, edges-pos-ontonotes_loss: 0.0133
09/16 01:19:25 PM: Update 20714: task edges-pos-ontonotes, batch 714 (20714): mcc: 0.8540, acc: 0.8006, precision: 0.8794, recall: 0.8350, f1: 0.8566, edges-pos-ontonotes_loss: 0.0136
09/16 01:19:35 PM: Update 20778: task edges-pos-ontonotes, batch 778 (20778): mcc: 0.8526, acc: 0.7986, precision: 0.8782, recall: 0.8336, f1: 0.8553, edges-pos-ontonotes_loss: 0.0137
09/16 01:19:45 PM: Update 20894: task edges-pos-ontonotes, batch 894 (20894): mcc: 0.8539, acc: 0.8004, precision: 0.8790, recall: 0.8353, f1: 0.8566, edges-pos-ontonotes_loss: 0.0136
09/16 01:19:54 PM: ***** Step 21000 / Validation 21 *****
09/16 01:19:54 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:19:54 PM: Validating...
09/16 01:19:55 PM: Evaluate: task edges-pos-ontonotes, batch 12 (157): mcc: 0.8609, acc: 0.8369, precision: 0.8824, recall: 0.8455, f1: 0.8636, edges-pos-ontonotes_loss: 0.0133
09/16 01:20:05 PM: Evaluate: task edges-pos-ontonotes, batch 106 (157): mcc: 0.8702, acc: 0.8431, precision: 0.8919, recall: 0.8542, f1: 0.8726, edges-pos-ontonotes_loss: 0.0127
09/16 01:20:13 PM: Updating LR scheduler:
09/16 01:20:13 PM: 	Best result seen so far for macro_avg: 0.874
09/16 01:20:13 PM: 	# validation passes without improvement: 2
09/16 01:20:13 PM: edges-pos-ontonotes_loss: training: 0.013436 validation: 0.013121
09/16 01:20:13 PM: macro_avg: validation: 0.864973
09/16 01:20:13 PM: micro_avg: validation: 0.000000
09/16 01:20:13 PM: edges-pos-ontonotes_mcc: training: 0.855119 validation: 0.862450
09/16 01:20:13 PM: edges-pos-ontonotes_acc: training: 0.802204 validation: 0.832185
09/16 01:20:13 PM: edges-pos-ontonotes_precision: training: 0.879663 validation: 0.886997
09/16 01:20:13 PM: edges-pos-ontonotes_recall: training: 0.836982 validation: 0.844016
09/16 01:20:13 PM: edges-pos-ontonotes_f1: training: 0.857792 validation: 0.864973
09/16 01:20:13 PM: Global learning rate: 2.5e-05
09/16 01:20:13 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:20:15 PM: Update 21024: task edges-pos-ontonotes, batch 24 (21024): mcc: 0.8647, acc: 0.8179, precision: 0.8850, recall: 0.8503, f1: 0.8673, edges-pos-ontonotes_loss: 0.0127
09/16 01:20:25 PM: Update 21103: task edges-pos-ontonotes, batch 103 (21103): mcc: 0.8622, acc: 0.8140, precision: 0.8834, recall: 0.8469, f1: 0.8648, edges-pos-ontonotes_loss: 0.0131
09/16 01:20:35 PM: Update 21182: task edges-pos-ontonotes, batch 182 (21182): mcc: 0.8614, acc: 0.8105, precision: 0.8826, recall: 0.8462, f1: 0.8640, edges-pos-ontonotes_loss: 0.0132
09/16 01:20:45 PM: Update 21276: task edges-pos-ontonotes, batch 276 (21276): mcc: 0.8629, acc: 0.8113, precision: 0.8840, recall: 0.8477, f1: 0.8655, edges-pos-ontonotes_loss: 0.0132
09/16 01:20:55 PM: Update 21364: task edges-pos-ontonotes, batch 364 (21364): mcc: 0.8641, acc: 0.8119, precision: 0.8853, recall: 0.8488, f1: 0.8667, edges-pos-ontonotes_loss: 0.0131
09/16 01:21:09 PM: Update 21387: task edges-pos-ontonotes, batch 387 (21387): mcc: 0.8642, acc: 0.8118, precision: 0.8855, recall: 0.8488, f1: 0.8668, edges-pos-ontonotes_loss: 0.0130
09/16 01:21:19 PM: Update 21455: task edges-pos-ontonotes, batch 455 (21455): mcc: 0.8610, acc: 0.8065, precision: 0.8831, recall: 0.8451, f1: 0.8637, edges-pos-ontonotes_loss: 0.0133
09/16 01:21:29 PM: Update 21522: task edges-pos-ontonotes, batch 522 (21522): mcc: 0.8594, acc: 0.8040, precision: 0.8816, recall: 0.8433, f1: 0.8620, edges-pos-ontonotes_loss: 0.0135
09/16 01:21:39 PM: Update 21597: task edges-pos-ontonotes, batch 597 (21597): mcc: 0.8589, acc: 0.8033, precision: 0.8810, recall: 0.8430, f1: 0.8616, edges-pos-ontonotes_loss: 0.0137
09/16 01:21:49 PM: Update 21671: task edges-pos-ontonotes, batch 671 (21671): mcc: 0.8586, acc: 0.8029, precision: 0.8807, recall: 0.8426, f1: 0.8612, edges-pos-ontonotes_loss: 0.0138
09/16 01:22:00 PM: Update 21723: task edges-pos-ontonotes, batch 723 (21723): mcc: 0.8586, acc: 0.8031, precision: 0.8809, recall: 0.8426, f1: 0.8613, edges-pos-ontonotes_loss: 0.0138
09/16 01:22:10 PM: Update 21788: task edges-pos-ontonotes, batch 788 (21788): mcc: 0.8583, acc: 0.8030, precision: 0.8807, recall: 0.8421, f1: 0.8610, edges-pos-ontonotes_loss: 0.0139
09/16 01:22:20 PM: Update 21855: task edges-pos-ontonotes, batch 855 (21855): mcc: 0.8582, acc: 0.8033, precision: 0.8806, recall: 0.8421, f1: 0.8609, edges-pos-ontonotes_loss: 0.0139
09/16 01:22:30 PM: Update 21917: task edges-pos-ontonotes, batch 917 (21917): mcc: 0.8586, acc: 0.8039, precision: 0.8810, recall: 0.8425, f1: 0.8613, edges-pos-ontonotes_loss: 0.0140
09/16 01:22:40 PM: Update 21991: task edges-pos-ontonotes, batch 991 (21991): mcc: 0.8588, acc: 0.8043, precision: 0.8811, recall: 0.8426, f1: 0.8614, edges-pos-ontonotes_loss: 0.0140
09/16 01:22:41 PM: ***** Step 22000 / Validation 22 *****
09/16 01:22:41 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:22:41 PM: Validating...
09/16 01:22:50 PM: Evaluate: task edges-pos-ontonotes, batch 89 (157): mcc: 0.8737, acc: 0.8500, precision: 0.8937, recall: 0.8591, f1: 0.8761, edges-pos-ontonotes_loss: 0.0128
09/16 01:23:00 PM: Evaluate: task edges-pos-ontonotes, batch 155 (157): mcc: 0.8732, acc: 0.8477, precision: 0.8932, recall: 0.8587, f1: 0.8756, edges-pos-ontonotes_loss: 0.0128
09/16 01:23:00 PM: Best result seen so far for edges-pos-ontonotes.
09/16 01:23:00 PM: Best result seen so far for macro.
09/16 01:23:00 PM: Updating LR scheduler:
09/16 01:23:00 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:23:00 PM: 	# validation passes without improvement: 0
09/16 01:23:00 PM: edges-pos-ontonotes_loss: training: 0.014011 validation: 0.012828
09/16 01:23:00 PM: macro_avg: validation: 0.875607
09/16 01:23:00 PM: micro_avg: validation: 0.000000
09/16 01:23:00 PM: edges-pos-ontonotes_mcc: training: 0.858822 validation: 0.873190
09/16 01:23:00 PM: edges-pos-ontonotes_acc: training: 0.804399 validation: 0.847699
09/16 01:23:00 PM: edges-pos-ontonotes_precision: training: 0.881205 validation: 0.893177
09/16 01:23:00 PM: edges-pos-ontonotes_recall: training: 0.842615 validation: 0.858715
09/16 01:23:00 PM: edges-pos-ontonotes_f1: training: 0.861478 validation: 0.875607
09/16 01:23:00 PM: Global learning rate: 2.5e-05
09/16 01:23:00 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:23:10 PM: Update 22047: task edges-pos-ontonotes, batch 47 (22047): mcc: 0.8568, acc: 0.8063, precision: 0.8807, recall: 0.8392, f1: 0.8595, edges-pos-ontonotes_loss: 0.0150
09/16 01:23:20 PM: Update 22119: task edges-pos-ontonotes, batch 119 (22119): mcc: 0.8593, acc: 0.8083, precision: 0.8827, recall: 0.8420, f1: 0.8619, edges-pos-ontonotes_loss: 0.0148
09/16 01:23:30 PM: Update 22188: task edges-pos-ontonotes, batch 188 (22188): mcc: 0.8592, acc: 0.8076, precision: 0.8829, recall: 0.8417, f1: 0.8618, edges-pos-ontonotes_loss: 0.0146
09/16 01:23:41 PM: Update 22258: task edges-pos-ontonotes, batch 258 (22258): mcc: 0.8605, acc: 0.8096, precision: 0.8839, recall: 0.8433, f1: 0.8631, edges-pos-ontonotes_loss: 0.0144
09/16 01:23:52 PM: Update 22326: task edges-pos-ontonotes, batch 326 (22326): mcc: 0.8607, acc: 0.8099, precision: 0.8840, recall: 0.8435, f1: 0.8633, edges-pos-ontonotes_loss: 0.0144
09/16 01:24:02 PM: Update 22393: task edges-pos-ontonotes, batch 393 (22393): mcc: 0.8610, acc: 0.8103, precision: 0.8842, recall: 0.8439, f1: 0.8636, edges-pos-ontonotes_loss: 0.0144
09/16 01:24:12 PM: Update 22461: task edges-pos-ontonotes, batch 461 (22461): mcc: 0.8613, acc: 0.8106, precision: 0.8845, recall: 0.8442, f1: 0.8639, edges-pos-ontonotes_loss: 0.0144
09/16 01:24:22 PM: Update 22535: task edges-pos-ontonotes, batch 535 (22535): mcc: 0.8617, acc: 0.8109, precision: 0.8848, recall: 0.8447, f1: 0.8643, edges-pos-ontonotes_loss: 0.0144
09/16 01:24:32 PM: Update 22600: task edges-pos-ontonotes, batch 600 (22600): mcc: 0.8617, acc: 0.8108, precision: 0.8846, recall: 0.8448, f1: 0.8642, edges-pos-ontonotes_loss: 0.0144
09/16 01:24:42 PM: Update 22651: task edges-pos-ontonotes, batch 651 (22651): mcc: 0.8609, acc: 0.8101, precision: 0.8838, recall: 0.8442, f1: 0.8635, edges-pos-ontonotes_loss: 0.0144
09/16 01:24:52 PM: Update 22727: task edges-pos-ontonotes, batch 727 (22727): mcc: 0.8619, acc: 0.8111, precision: 0.8844, recall: 0.8455, f1: 0.8645, edges-pos-ontonotes_loss: 0.0142
09/16 01:25:03 PM: Update 22814: task edges-pos-ontonotes, batch 814 (22814): mcc: 0.8631, acc: 0.8122, precision: 0.8854, recall: 0.8468, f1: 0.8657, edges-pos-ontonotes_loss: 0.0139
09/16 01:25:13 PM: Update 22906: task edges-pos-ontonotes, batch 906 (22906): mcc: 0.8641, acc: 0.8131, precision: 0.8863, recall: 0.8479, f1: 0.8666, edges-pos-ontonotes_loss: 0.0137
09/16 01:25:23 PM: Update 22979: task edges-pos-ontonotes, batch 979 (22979): mcc: 0.8646, acc: 0.8135, precision: 0.8869, recall: 0.8483, f1: 0.8672, edges-pos-ontonotes_loss: 0.0135
09/16 01:25:24 PM: ***** Step 23000 / Validation 23 *****
09/16 01:25:24 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:25:24 PM: Validating...
09/16 01:25:33 PM: Evaluate: task edges-pos-ontonotes, batch 83 (157): mcc: 0.8710, acc: 0.8389, precision: 0.9006, recall: 0.8474, f1: 0.8732, edges-pos-ontonotes_loss: 0.0133
09/16 01:25:43 PM: Evaluate: task edges-pos-ontonotes, batch 150 (157): mcc: 0.8690, acc: 0.8347, precision: 0.9002, recall: 0.8439, f1: 0.8712, edges-pos-ontonotes_loss: 0.0132
09/16 01:25:44 PM: Updating LR scheduler:
09/16 01:25:44 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:25:44 PM: 	# validation passes without improvement: 1
09/16 01:25:44 PM: edges-pos-ontonotes_loss: training: 0.013459 validation: 0.013187
09/16 01:25:44 PM: macro_avg: validation: 0.870563
09/16 01:25:44 PM: micro_avg: validation: 0.000000
09/16 01:25:44 PM: edges-pos-ontonotes_mcc: training: 0.864967 validation: 0.868382
09/16 01:25:44 PM: edges-pos-ontonotes_acc: training: 0.813940 validation: 0.833910
09/16 01:25:44 PM: edges-pos-ontonotes_precision: training: 0.887105 validation: 0.899918
09/16 01:25:44 PM: edges-pos-ontonotes_recall: training: 0.848747 validation: 0.843064
09/16 01:25:44 PM: edges-pos-ontonotes_f1: training: 0.867502 validation: 0.870563
09/16 01:25:44 PM: Global learning rate: 2.5e-05
09/16 01:25:44 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:25:53 PM: Update 23106: task edges-pos-ontonotes, batch 106 (23106): mcc: 0.8919, acc: 0.8445, precision: 0.9087, recall: 0.8798, f1: 0.8940, edges-pos-ontonotes_loss: 0.0101
09/16 01:26:03 PM: Update 23221: task edges-pos-ontonotes, batch 221 (23221): mcc: 0.8937, acc: 0.8465, precision: 0.9100, recall: 0.8821, f1: 0.8958, edges-pos-ontonotes_loss: 0.0099
09/16 01:26:17 PM: Update 23265: task edges-pos-ontonotes, batch 265 (23265): mcc: 0.8933, acc: 0.8457, precision: 0.9100, recall: 0.8813, f1: 0.8954, edges-pos-ontonotes_loss: 0.0099
09/16 01:26:27 PM: Update 23392: task edges-pos-ontonotes, batch 392 (23392): mcc: 0.8918, acc: 0.8442, precision: 0.9088, recall: 0.8794, f1: 0.8939, edges-pos-ontonotes_loss: 0.0104
09/16 01:26:37 PM: Update 23517: task edges-pos-ontonotes, batch 517 (23517): mcc: 0.8920, acc: 0.8446, precision: 0.9091, recall: 0.8797, f1: 0.8941, edges-pos-ontonotes_loss: 0.0104
09/16 01:26:47 PM: Update 23653: task edges-pos-ontonotes, batch 653 (23653): mcc: 0.8891, acc: 0.8413, precision: 0.9066, recall: 0.8764, f1: 0.8912, edges-pos-ontonotes_loss: 0.0107
09/16 01:26:57 PM: Update 23816: task edges-pos-ontonotes, batch 816 (23816): mcc: 0.8863, acc: 0.8391, precision: 0.9046, recall: 0.8729, f1: 0.8885, edges-pos-ontonotes_loss: 0.0109
09/16 01:27:07 PM: Update 23917: task edges-pos-ontonotes, batch 917 (23917): mcc: 0.8822, acc: 0.8346, precision: 0.9015, recall: 0.8679, f1: 0.8844, edges-pos-ontonotes_loss: 0.0111
09/16 01:27:17 PM: Update 23992: task edges-pos-ontonotes, batch 992 (23992): mcc: 0.8760, acc: 0.8269, precision: 0.8963, recall: 0.8610, f1: 0.8783, edges-pos-ontonotes_loss: 0.0115
09/16 01:27:18 PM: ***** Step 24000 / Validation 24 *****
09/16 01:27:18 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:27:18 PM: Validating...
09/16 01:27:27 PM: Evaluate: task edges-pos-ontonotes, batch 89 (157): mcc: 0.8717, acc: 0.8379, precision: 0.9005, recall: 0.8487, f1: 0.8739, edges-pos-ontonotes_loss: 0.0129
09/16 01:27:37 PM: Evaluate: task edges-pos-ontonotes, batch 154 (157): mcc: 0.8633, acc: 0.8266, precision: 0.8932, recall: 0.8398, f1: 0.8657, edges-pos-ontonotes_loss: 0.0133
09/16 01:27:38 PM: Updating LR scheduler:
09/16 01:27:38 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:27:38 PM: 	# validation passes without improvement: 2
09/16 01:27:38 PM: edges-pos-ontonotes_loss: training: 0.011598 validation: 0.013366
09/16 01:27:38 PM: macro_avg: validation: 0.865187
09/16 01:27:38 PM: micro_avg: validation: 0.000000
09/16 01:27:38 PM: edges-pos-ontonotes_mcc: training: 0.875450 validation: 0.862839
09/16 01:27:38 PM: edges-pos-ontonotes_acc: training: 0.826228 validation: 0.826100
09/16 01:27:38 PM: edges-pos-ontonotes_precision: training: 0.895961 validation: 0.892821
09/16 01:27:38 PM: edges-pos-ontonotes_recall: training: 0.860378 validation: 0.839212
09/16 01:27:38 PM: edges-pos-ontonotes_f1: training: 0.877809 validation: 0.865187
09/16 01:27:38 PM: Global learning rate: 2.5e-05
09/16 01:27:38 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:27:47 PM: Update 24066: task edges-pos-ontonotes, batch 66 (24066): mcc: 0.8395, acc: 0.7806, precision: 0.8700, recall: 0.8163, f1: 0.8423, edges-pos-ontonotes_loss: 0.0162
09/16 01:27:57 PM: Update 24139: task edges-pos-ontonotes, batch 139 (24139): mcc: 0.8420, acc: 0.7839, precision: 0.8707, recall: 0.8205, f1: 0.8449, edges-pos-ontonotes_loss: 0.0161
09/16 01:28:07 PM: Update 24216: task edges-pos-ontonotes, batch 216 (24216): mcc: 0.8425, acc: 0.7842, precision: 0.8712, recall: 0.8210, f1: 0.8453, edges-pos-ontonotes_loss: 0.0161
09/16 01:28:17 PM: Update 24313: task edges-pos-ontonotes, batch 313 (24313): mcc: 0.8464, acc: 0.7898, precision: 0.8736, recall: 0.8260, f1: 0.8491, edges-pos-ontonotes_loss: 0.0151
09/16 01:28:28 PM: Update 24425: task edges-pos-ontonotes, batch 425 (24425): mcc: 0.8497, acc: 0.7947, precision: 0.8756, recall: 0.8305, f1: 0.8525, edges-pos-ontonotes_loss: 0.0144
09/16 01:28:38 PM: Update 24529: task edges-pos-ontonotes, batch 529 (24529): mcc: 0.8524, acc: 0.7986, precision: 0.8772, recall: 0.8341, f1: 0.8551, edges-pos-ontonotes_loss: 0.0140
09/16 01:28:48 PM: Update 24592: task edges-pos-ontonotes, batch 592 (24592): mcc: 0.8538, acc: 0.8003, precision: 0.8781, recall: 0.8359, f1: 0.8565, edges-pos-ontonotes_loss: 0.0139
09/16 01:28:58 PM: Update 24693: task edges-pos-ontonotes, batch 693 (24693): mcc: 0.8555, acc: 0.8021, precision: 0.8793, recall: 0.8380, f1: 0.8582, edges-pos-ontonotes_loss: 0.0138
09/16 01:29:08 PM: Update 24780: task edges-pos-ontonotes, batch 780 (24780): mcc: 0.8567, acc: 0.8035, precision: 0.8803, recall: 0.8395, f1: 0.8594, edges-pos-ontonotes_loss: 0.0137
09/16 01:29:18 PM: Update 24842: task edges-pos-ontonotes, batch 842 (24842): mcc: 0.8577, acc: 0.8045, precision: 0.8810, recall: 0.8406, f1: 0.8603, edges-pos-ontonotes_loss: 0.0136
09/16 01:29:28 PM: Update 24881: task edges-pos-ontonotes, batch 881 (24881): mcc: 0.8568, acc: 0.8032, precision: 0.8803, recall: 0.8397, f1: 0.8595, edges-pos-ontonotes_loss: 0.0137
09/16 01:29:38 PM: Update 24940: task edges-pos-ontonotes, batch 940 (24940): mcc: 0.8561, acc: 0.8018, precision: 0.8796, recall: 0.8389, f1: 0.8588, edges-pos-ontonotes_loss: 0.0137
09/16 01:29:47 PM: ***** Step 25000 / Validation 25 *****
09/16 01:29:47 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:29:47 PM: Validating...
09/16 01:29:48 PM: Evaluate: task edges-pos-ontonotes, batch 13 (157): mcc: 0.8717, acc: 0.8491, precision: 0.8907, recall: 0.8582, f1: 0.8742, edges-pos-ontonotes_loss: 0.0128
09/16 01:29:58 PM: Evaluate: task edges-pos-ontonotes, batch 106 (157): mcc: 0.8774, acc: 0.8546, precision: 0.8968, recall: 0.8633, f1: 0.8797, edges-pos-ontonotes_loss: 0.0124
09/16 01:30:06 PM: Updating LR scheduler:
09/16 01:30:06 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:30:06 PM: 	# validation passes without improvement: 3
09/16 01:30:06 PM: edges-pos-ontonotes_loss: training: 0.013778 validation: 0.012748
09/16 01:30:06 PM: macro_avg: validation: 0.874881
09/16 01:30:06 PM: micro_avg: validation: 0.000000
09/16 01:30:06 PM: edges-pos-ontonotes_mcc: training: 0.856083 validation: 0.872457
09/16 01:30:06 PM: edges-pos-ontonotes_acc: training: 0.801669 validation: 0.847667
09/16 01:30:06 PM: edges-pos-ontonotes_precision: training: 0.879473 validation: 0.892822
09/16 01:30:06 PM: edges-pos-ontonotes_recall: training: 0.839016 validation: 0.857646
09/16 01:30:06 PM: edges-pos-ontonotes_f1: training: 0.858768 validation: 0.874881
09/16 01:30:06 PM: Global learning rate: 2.5e-05
09/16 01:30:06 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:30:08 PM: Update 25015: task edges-pos-ontonotes, batch 15 (25015): mcc: 0.8563, acc: 0.8007, precision: 0.8786, recall: 0.8402, f1: 0.8590, edges-pos-ontonotes_loss: 0.0141
09/16 01:30:18 PM: Update 25060: task edges-pos-ontonotes, batch 60 (25060): mcc: 0.8581, acc: 0.8025, precision: 0.8802, recall: 0.8422, f1: 0.8608, edges-pos-ontonotes_loss: 0.0143
09/16 01:30:29 PM: Update 25123: task edges-pos-ontonotes, batch 123 (25123): mcc: 0.8563, acc: 0.8009, precision: 0.8773, recall: 0.8415, f1: 0.8590, edges-pos-ontonotes_loss: 0.0145
09/16 01:30:44 PM: Update 25160: task edges-pos-ontonotes, batch 160 (25160): mcc: 0.8565, acc: 0.8011, precision: 0.8779, recall: 0.8413, f1: 0.8592, edges-pos-ontonotes_loss: 0.0145
09/16 01:30:54 PM: Update 25228: task edges-pos-ontonotes, batch 228 (25228): mcc: 0.8567, acc: 0.8027, precision: 0.8790, recall: 0.8406, f1: 0.8594, edges-pos-ontonotes_loss: 0.0145
09/16 01:31:04 PM: Update 25293: task edges-pos-ontonotes, batch 293 (25293): mcc: 0.8567, acc: 0.8032, precision: 0.8788, recall: 0.8407, f1: 0.8594, edges-pos-ontonotes_loss: 0.0146
09/16 01:31:14 PM: Update 25360: task edges-pos-ontonotes, batch 360 (25360): mcc: 0.8571, acc: 0.8037, precision: 0.8793, recall: 0.8411, f1: 0.8598, edges-pos-ontonotes_loss: 0.0146
09/16 01:31:25 PM: Update 25427: task edges-pos-ontonotes, batch 427 (25427): mcc: 0.8579, acc: 0.8049, precision: 0.8802, recall: 0.8418, f1: 0.8606, edges-pos-ontonotes_loss: 0.0146
09/16 01:31:35 PM: Update 25474: task edges-pos-ontonotes, batch 474 (25474): mcc: 0.8578, acc: 0.8050, precision: 0.8802, recall: 0.8416, f1: 0.8605, edges-pos-ontonotes_loss: 0.0146
09/16 01:31:45 PM: Update 25544: task edges-pos-ontonotes, batch 544 (25544): mcc: 0.8584, acc: 0.8059, precision: 0.8810, recall: 0.8421, f1: 0.8611, edges-pos-ontonotes_loss: 0.0146
09/16 01:31:55 PM: Update 25614: task edges-pos-ontonotes, batch 614 (25614): mcc: 0.8585, acc: 0.8063, precision: 0.8811, recall: 0.8421, f1: 0.8612, edges-pos-ontonotes_loss: 0.0146
09/16 01:32:05 PM: Update 25681: task edges-pos-ontonotes, batch 681 (25681): mcc: 0.8588, acc: 0.8067, precision: 0.8814, recall: 0.8423, f1: 0.8614, edges-pos-ontonotes_loss: 0.0146
09/16 01:32:15 PM: Update 25753: task edges-pos-ontonotes, batch 753 (25753): mcc: 0.8592, acc: 0.8073, precision: 0.8819, recall: 0.8426, f1: 0.8618, edges-pos-ontonotes_loss: 0.0145
09/16 01:32:25 PM: Update 25811: task edges-pos-ontonotes, batch 811 (25811): mcc: 0.8595, acc: 0.8078, precision: 0.8822, recall: 0.8429, f1: 0.8621, edges-pos-ontonotes_loss: 0.0145
09/16 01:32:35 PM: Update 25876: task edges-pos-ontonotes, batch 876 (25876): mcc: 0.8597, acc: 0.8081, precision: 0.8824, recall: 0.8432, f1: 0.8624, edges-pos-ontonotes_loss: 0.0145
09/16 01:32:45 PM: Update 25937: task edges-pos-ontonotes, batch 937 (25937): mcc: 0.8599, acc: 0.8083, precision: 0.8826, recall: 0.8434, f1: 0.8626, edges-pos-ontonotes_loss: 0.0145
09/16 01:32:55 PM: ***** Step 26000 / Validation 26 *****
09/16 01:32:55 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:32:55 PM: Validating...
09/16 01:32:56 PM: Evaluate: task edges-pos-ontonotes, batch 6 (157): mcc: 0.8842, acc: 0.8655, precision: 0.8985, recall: 0.8748, f1: 0.8865, edges-pos-ontonotes_loss: 0.0124
09/16 01:33:06 PM: Evaluate: task edges-pos-ontonotes, batch 102 (157): mcc: 0.8733, acc: 0.8496, precision: 0.8919, recall: 0.8601, f1: 0.8757, edges-pos-ontonotes_loss: 0.0128
09/16 01:33:14 PM: Updating LR scheduler:
09/16 01:33:14 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:33:14 PM: 	# validation passes without improvement: 0
09/16 01:33:14 PM: edges-pos-ontonotes_loss: training: 0.014461 validation: 0.012916
09/16 01:33:14 PM: macro_avg: validation: 0.874654
09/16 01:33:14 PM: micro_avg: validation: 0.000000
09/16 01:33:14 PM: edges-pos-ontonotes_mcc: training: 0.860013 validation: 0.872200
09/16 01:33:14 PM: edges-pos-ontonotes_acc: training: 0.808367 validation: 0.846948
09/16 01:33:14 PM: edges-pos-ontonotes_precision: training: 0.882679 validation: 0.891470
09/16 01:33:14 PM: edges-pos-ontonotes_recall: training: 0.843485 validation: 0.858461
09/16 01:33:14 PM: edges-pos-ontonotes_f1: training: 0.862637 validation: 0.874654
09/16 01:33:14 PM: Global learning rate: 1.25e-05
09/16 01:33:14 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:33:16 PM: Update 26011: task edges-pos-ontonotes, batch 11 (26011): mcc: 0.8576, acc: 0.8052, precision: 0.8796, recall: 0.8419, f1: 0.8603, edges-pos-ontonotes_loss: 0.0144
09/16 01:33:26 PM: Update 26084: task edges-pos-ontonotes, batch 84 (26084): mcc: 0.8609, acc: 0.8092, precision: 0.8818, recall: 0.8459, f1: 0.8635, edges-pos-ontonotes_loss: 0.0143
09/16 01:33:36 PM: Update 26152: task edges-pos-ontonotes, batch 152 (26152): mcc: 0.8636, acc: 0.8138, precision: 0.8836, recall: 0.8495, f1: 0.8662, edges-pos-ontonotes_loss: 0.0136
09/16 01:33:46 PM: Update 26231: task edges-pos-ontonotes, batch 231 (26231): mcc: 0.8652, acc: 0.8151, precision: 0.8852, recall: 0.8510, f1: 0.8678, edges-pos-ontonotes_loss: 0.0132
09/16 01:33:56 PM: Update 26314: task edges-pos-ontonotes, batch 314 (26314): mcc: 0.8671, acc: 0.8170, precision: 0.8870, recall: 0.8530, f1: 0.8697, edges-pos-ontonotes_loss: 0.0129
09/16 01:34:06 PM: Update 26396: task edges-pos-ontonotes, batch 396 (26396): mcc: 0.8684, acc: 0.8180, precision: 0.8882, recall: 0.8542, f1: 0.8709, edges-pos-ontonotes_loss: 0.0127
09/16 01:34:16 PM: Update 26492: task edges-pos-ontonotes, batch 492 (26492): mcc: 0.8710, acc: 0.8208, precision: 0.8904, recall: 0.8572, f1: 0.8735, edges-pos-ontonotes_loss: 0.0123
09/16 01:34:26 PM: Update 26604: task edges-pos-ontonotes, batch 604 (26604): mcc: 0.8740, acc: 0.8242, precision: 0.8929, recall: 0.8605, f1: 0.8764, edges-pos-ontonotes_loss: 0.0119
09/16 01:34:36 PM: Update 26712: task edges-pos-ontonotes, batch 712 (26712): mcc: 0.8762, acc: 0.8266, precision: 0.8947, recall: 0.8630, f1: 0.8785, edges-pos-ontonotes_loss: 0.0116
09/16 01:34:46 PM: Update 26817: task edges-pos-ontonotes, batch 817 (26817): mcc: 0.8771, acc: 0.8276, precision: 0.8955, recall: 0.8639, f1: 0.8794, edges-pos-ontonotes_loss: 0.0116
09/16 01:34:56 PM: Update 26944: task edges-pos-ontonotes, batch 944 (26944): mcc: 0.8779, acc: 0.8284, precision: 0.8963, recall: 0.8647, f1: 0.8802, edges-pos-ontonotes_loss: 0.0115
09/16 01:35:01 PM: ***** Step 27000 / Validation 27 *****
09/16 01:35:01 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:35:01 PM: Validating...
09/16 01:35:06 PM: Evaluate: task edges-pos-ontonotes, batch 58 (157): mcc: 0.8646, acc: 0.8403, precision: 0.8882, recall: 0.8470, f1: 0.8671, edges-pos-ontonotes_loss: 0.0133
09/16 01:35:16 PM: Evaluate: task edges-pos-ontonotes, batch 132 (157): mcc: 0.8623, acc: 0.8335, precision: 0.8898, recall: 0.8410, f1: 0.8647, edges-pos-ontonotes_loss: 0.0132
09/16 01:35:20 PM: Updating LR scheduler:
09/16 01:35:20 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:35:20 PM: 	# validation passes without improvement: 1
09/16 01:35:20 PM: edges-pos-ontonotes_loss: training: 0.011480 validation: 0.013365
09/16 01:35:20 PM: macro_avg: validation: 0.863596
09/16 01:35:20 PM: micro_avg: validation: 0.000000
09/16 01:35:20 PM: edges-pos-ontonotes_mcc: training: 0.878337 validation: 0.861180
09/16 01:35:20 PM: edges-pos-ontonotes_acc: training: 0.828991 validation: 0.830968
09/16 01:35:20 PM: edges-pos-ontonotes_precision: training: 0.896726 validation: 0.890106
09/16 01:35:20 PM: edges-pos-ontonotes_recall: training: 0.865204 validation: 0.838619
09/16 01:35:20 PM: edges-pos-ontonotes_f1: training: 0.880683 validation: 0.863596
09/16 01:35:20 PM: Global learning rate: 1.25e-05
09/16 01:35:20 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:35:27 PM: Update 27071: task edges-pos-ontonotes, batch 71 (27071): mcc: 0.8707, acc: 0.8173, precision: 0.8935, recall: 0.8536, f1: 0.8731, edges-pos-ontonotes_loss: 0.0120
09/16 01:35:37 PM: Update 27239: task edges-pos-ontonotes, batch 239 (27239): mcc: 0.8661, acc: 0.8156, precision: 0.8907, recall: 0.8475, f1: 0.8686, edges-pos-ontonotes_loss: 0.0122
09/16 01:35:52 PM: Update 27351: task edges-pos-ontonotes, batch 351 (27351): mcc: 0.8669, acc: 0.8179, precision: 0.8914, recall: 0.8484, f1: 0.8694, edges-pos-ontonotes_loss: 0.0119
09/16 01:36:02 PM: Update 27424: task edges-pos-ontonotes, batch 424 (27424): mcc: 0.8561, acc: 0.8044, precision: 0.8832, recall: 0.8355, f1: 0.8587, edges-pos-ontonotes_loss: 0.0128
09/16 01:36:12 PM: Update 27476: task edges-pos-ontonotes, batch 476 (27476): mcc: 0.8530, acc: 0.8003, precision: 0.8803, recall: 0.8323, f1: 0.8556, edges-pos-ontonotes_loss: 0.0132
09/16 01:36:23 PM: Update 27535: task edges-pos-ontonotes, batch 535 (27535): mcc: 0.8511, acc: 0.7978, precision: 0.8784, recall: 0.8305, f1: 0.8538, edges-pos-ontonotes_loss: 0.0135
09/16 01:36:33 PM: Update 27604: task edges-pos-ontonotes, batch 604 (27604): mcc: 0.8495, acc: 0.7956, precision: 0.8769, recall: 0.8288, f1: 0.8522, edges-pos-ontonotes_loss: 0.0138
09/16 01:36:43 PM: Update 27664: task edges-pos-ontonotes, batch 664 (27664): mcc: 0.8484, acc: 0.7939, precision: 0.8759, recall: 0.8278, f1: 0.8512, edges-pos-ontonotes_loss: 0.0141
09/16 01:36:53 PM: Update 27740: task edges-pos-ontonotes, batch 740 (27740): mcc: 0.8485, acc: 0.7938, precision: 0.8758, recall: 0.8280, f1: 0.8512, edges-pos-ontonotes_loss: 0.0141
09/16 01:37:03 PM: Update 27822: task edges-pos-ontonotes, batch 822 (27822): mcc: 0.8498, acc: 0.7956, precision: 0.8766, recall: 0.8297, f1: 0.8525, edges-pos-ontonotes_loss: 0.0139
09/16 01:37:13 PM: Update 27923: task edges-pos-ontonotes, batch 923 (27923): mcc: 0.8511, acc: 0.7976, precision: 0.8774, recall: 0.8316, f1: 0.8539, edges-pos-ontonotes_loss: 0.0137
09/16 01:37:22 PM: ***** Step 28000 / Validation 28 *****
09/16 01:37:22 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:37:22 PM: Validating...
09/16 01:37:23 PM: Evaluate: task edges-pos-ontonotes, batch 4 (157): mcc: 0.8756, acc: 0.8500, precision: 0.8926, recall: 0.8641, f1: 0.8781, edges-pos-ontonotes_loss: 0.0123
09/16 01:37:33 PM: Evaluate: task edges-pos-ontonotes, batch 100 (157): mcc: 0.8727, acc: 0.8486, precision: 0.8941, recall: 0.8569, f1: 0.8751, edges-pos-ontonotes_loss: 0.0125
09/16 01:37:41 PM: Updating LR scheduler:
09/16 01:37:41 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:37:41 PM: 	# validation passes without improvement: 2
09/16 01:37:41 PM: edges-pos-ontonotes_loss: training: 0.013619 validation: 0.013007
09/16 01:37:41 PM: macro_avg: validation: 0.865165
09/16 01:37:41 PM: micro_avg: validation: 0.000000
09/16 01:37:41 PM: edges-pos-ontonotes_mcc: training: 0.852322 validation: 0.862692
09/16 01:37:41 PM: edges-pos-ontonotes_acc: training: 0.799251 validation: 0.833455
09/16 01:37:41 PM: edges-pos-ontonotes_precision: training: 0.877944 validation: 0.888797
09/16 01:37:41 PM: edges-pos-ontonotes_recall: training: 0.833267 validation: 0.842757
09/16 01:37:41 PM: edges-pos-ontonotes_f1: training: 0.855022 validation: 0.865165
09/16 01:37:41 PM: Global learning rate: 1.25e-05
09/16 01:37:41 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:37:43 PM: Update 28013: task edges-pos-ontonotes, batch 13 (28013): mcc: 0.8607, acc: 0.8100, precision: 0.8829, recall: 0.8446, f1: 0.8633, edges-pos-ontonotes_loss: 0.0150
09/16 01:37:53 PM: Update 28105: task edges-pos-ontonotes, batch 105 (28105): mcc: 0.8642, acc: 0.8124, precision: 0.8861, recall: 0.8482, f1: 0.8667, edges-pos-ontonotes_loss: 0.0131
09/16 01:38:03 PM: Update 28197: task edges-pos-ontonotes, batch 197 (28197): mcc: 0.8633, acc: 0.8111, precision: 0.8846, recall: 0.8480, f1: 0.8659, edges-pos-ontonotes_loss: 0.0133
09/16 01:38:13 PM: Update 28283: task edges-pos-ontonotes, batch 283 (28283): mcc: 0.8641, acc: 0.8116, precision: 0.8853, recall: 0.8488, f1: 0.8667, edges-pos-ontonotes_loss: 0.0132
09/16 01:38:23 PM: Update 28340: task edges-pos-ontonotes, batch 340 (28340): mcc: 0.8603, acc: 0.8062, precision: 0.8825, recall: 0.8442, f1: 0.8629, edges-pos-ontonotes_loss: 0.0134
09/16 01:38:33 PM: Update 28414: task edges-pos-ontonotes, batch 414 (28414): mcc: 0.8586, acc: 0.8032, precision: 0.8812, recall: 0.8423, f1: 0.8613, edges-pos-ontonotes_loss: 0.0136
09/16 01:38:43 PM: Update 28482: task edges-pos-ontonotes, batch 482 (28482): mcc: 0.8571, acc: 0.8005, precision: 0.8798, recall: 0.8408, f1: 0.8598, edges-pos-ontonotes_loss: 0.0138
09/16 01:38:53 PM: Update 28549: task edges-pos-ontonotes, batch 549 (28549): mcc: 0.8565, acc: 0.7996, precision: 0.8791, recall: 0.8403, f1: 0.8592, edges-pos-ontonotes_loss: 0.0139
09/16 01:39:05 PM: Update 28620: task edges-pos-ontonotes, batch 620 (28620): mcc: 0.8564, acc: 0.7993, precision: 0.8787, recall: 0.8403, f1: 0.8591, edges-pos-ontonotes_loss: 0.0139
09/16 01:39:15 PM: Update 28682: task edges-pos-ontonotes, batch 682 (28682): mcc: 0.8563, acc: 0.7995, precision: 0.8789, recall: 0.8399, f1: 0.8590, edges-pos-ontonotes_loss: 0.0140
09/16 01:39:25 PM: Update 28743: task edges-pos-ontonotes, batch 743 (28743): mcc: 0.8566, acc: 0.8004, precision: 0.8794, recall: 0.8401, f1: 0.8593, edges-pos-ontonotes_loss: 0.0140
09/16 01:39:35 PM: Update 28811: task edges-pos-ontonotes, batch 811 (28811): mcc: 0.8568, acc: 0.8011, precision: 0.8795, recall: 0.8404, f1: 0.8595, edges-pos-ontonotes_loss: 0.0141
09/16 01:39:45 PM: Update 28881: task edges-pos-ontonotes, batch 881 (28881): mcc: 0.8570, acc: 0.8016, precision: 0.8798, recall: 0.8405, f1: 0.8597, edges-pos-ontonotes_loss: 0.0141
09/16 01:39:56 PM: Update 28933: task edges-pos-ontonotes, batch 933 (28933): mcc: 0.8571, acc: 0.8020, precision: 0.8798, recall: 0.8407, f1: 0.8598, edges-pos-ontonotes_loss: 0.0141
09/16 01:40:06 PM: Update 28994: task edges-pos-ontonotes, batch 994 (28994): mcc: 0.8572, acc: 0.8024, precision: 0.8799, recall: 0.8407, f1: 0.8598, edges-pos-ontonotes_loss: 0.0142
09/16 01:40:07 PM: ***** Step 29000 / Validation 29 *****
09/16 01:40:07 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:40:07 PM: Validating...
09/16 01:40:16 PM: Evaluate: task edges-pos-ontonotes, batch 91 (157): mcc: 0.8742, acc: 0.8509, precision: 0.8927, recall: 0.8612, f1: 0.8767, edges-pos-ontonotes_loss: 0.0126
09/16 01:40:26 PM: Evaluate: task edges-pos-ontonotes, batch 156 (157): mcc: 0.8730, acc: 0.8471, precision: 0.8912, recall: 0.8603, f1: 0.8755, edges-pos-ontonotes_loss: 0.0128
09/16 01:40:26 PM: Updating LR scheduler:
09/16 01:40:26 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:40:26 PM: 	# validation passes without improvement: 3
09/16 01:40:26 PM: edges-pos-ontonotes_loss: training: 0.014160 validation: 0.012763
09/16 01:40:26 PM: macro_avg: validation: 0.875533
09/16 01:40:26 PM: micro_avg: validation: 0.000000
09/16 01:40:26 PM: edges-pos-ontonotes_mcc: training: 0.857196 validation: 0.873074
09/16 01:40:26 PM: edges-pos-ontonotes_acc: training: 0.802466 validation: 0.847159
09/16 01:40:26 PM: edges-pos-ontonotes_precision: training: 0.879935 validation: 0.891245
09/16 01:40:26 PM: edges-pos-ontonotes_recall: training: 0.840712 validation: 0.860366
09/16 01:40:26 PM: edges-pos-ontonotes_f1: training: 0.859876 validation: 0.875533
09/16 01:40:26 PM: Global learning rate: 1.25e-05
09/16 01:40:26 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:40:36 PM: Update 29067: task edges-pos-ontonotes, batch 67 (29067): mcc: 0.8618, acc: 0.8110, precision: 0.8852, recall: 0.8445, f1: 0.8643, edges-pos-ontonotes_loss: 0.0148
09/16 01:40:46 PM: Update 29138: task edges-pos-ontonotes, batch 138 (29138): mcc: 0.8626, acc: 0.8113, precision: 0.8858, recall: 0.8454, f1: 0.8651, edges-pos-ontonotes_loss: 0.0147
09/16 01:40:56 PM: Update 29207: task edges-pos-ontonotes, batch 207 (29207): mcc: 0.8609, acc: 0.8097, precision: 0.8842, recall: 0.8436, f1: 0.8634, edges-pos-ontonotes_loss: 0.0146
09/16 01:41:14 PM: Update 29246: task edges-pos-ontonotes, batch 246 (29246): mcc: 0.8612, acc: 0.8103, precision: 0.8846, recall: 0.8440, f1: 0.8638, edges-pos-ontonotes_loss: 0.0145
09/16 01:41:24 PM: Update 29307: task edges-pos-ontonotes, batch 307 (29307): mcc: 0.8610, acc: 0.8101, precision: 0.8843, recall: 0.8439, f1: 0.8636, edges-pos-ontonotes_loss: 0.0145
09/16 01:41:34 PM: Update 29379: task edges-pos-ontonotes, batch 379 (29379): mcc: 0.8605, acc: 0.8092, precision: 0.8836, recall: 0.8435, f1: 0.8631, edges-pos-ontonotes_loss: 0.0146
09/16 01:41:44 PM: Update 29447: task edges-pos-ontonotes, batch 447 (29447): mcc: 0.8605, acc: 0.8091, precision: 0.8836, recall: 0.8435, f1: 0.8631, edges-pos-ontonotes_loss: 0.0145
09/16 01:41:54 PM: Update 29507: task edges-pos-ontonotes, batch 507 (29507): mcc: 0.8610, acc: 0.8097, precision: 0.8841, recall: 0.8440, f1: 0.8636, edges-pos-ontonotes_loss: 0.0145
09/16 01:42:05 PM: Update 29560: task edges-pos-ontonotes, batch 560 (29560): mcc: 0.8607, acc: 0.8092, precision: 0.8837, recall: 0.8437, f1: 0.8633, edges-pos-ontonotes_loss: 0.0145
09/16 01:42:15 PM: Update 29640: task edges-pos-ontonotes, batch 640 (29640): mcc: 0.8617, acc: 0.8105, precision: 0.8843, recall: 0.8453, f1: 0.8643, edges-pos-ontonotes_loss: 0.0142
09/16 01:42:25 PM: Update 29723: task edges-pos-ontonotes, batch 723 (29723): mcc: 0.8626, acc: 0.8115, precision: 0.8848, recall: 0.8465, f1: 0.8652, edges-pos-ontonotes_loss: 0.0140
09/16 01:42:35 PM: Update 29807: task edges-pos-ontonotes, batch 807 (29807): mcc: 0.8637, acc: 0.8125, precision: 0.8857, recall: 0.8476, f1: 0.8662, edges-pos-ontonotes_loss: 0.0137
09/16 01:42:45 PM: Update 29883: task edges-pos-ontonotes, batch 883 (29883): mcc: 0.8646, acc: 0.8134, precision: 0.8864, recall: 0.8487, f1: 0.8671, edges-pos-ontonotes_loss: 0.0135
09/16 01:42:55 PM: Update 29998: task edges-pos-ontonotes, batch 998 (29998): mcc: 0.8663, acc: 0.8153, precision: 0.8877, recall: 0.8508, f1: 0.8689, edges-pos-ontonotes_loss: 0.0132
09/16 01:42:55 PM: ***** Step 30000 / Validation 30 *****
09/16 01:42:55 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:42:55 PM: Validating...
09/16 01:43:05 PM: Evaluate: task edges-pos-ontonotes, batch 94 (157): mcc: 0.8723, acc: 0.8401, precision: 0.9024, recall: 0.8482, f1: 0.8745, edges-pos-ontonotes_loss: 0.0130
09/16 01:43:15 PM: Updating LR scheduler:
09/16 01:43:15 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:43:15 PM: 	# validation passes without improvement: 0
09/16 01:43:15 PM: edges-pos-ontonotes_loss: training: 0.013184 validation: 0.013046
09/16 01:43:15 PM: macro_avg: validation: 0.870597
09/16 01:43:15 PM: micro_avg: validation: 0.000000
09/16 01:43:15 PM: edges-pos-ontonotes_mcc: training: 0.866357 validation: 0.868440
09/16 01:43:15 PM: edges-pos-ontonotes_acc: training: 0.815328 validation: 0.833762
09/16 01:43:15 PM: edges-pos-ontonotes_precision: training: 0.887757 validation: 0.900594
09/16 01:43:15 PM: edges-pos-ontonotes_recall: training: 0.850793 validation: 0.842535
09/16 01:43:15 PM: edges-pos-ontonotes_f1: training: 0.868882 validation: 0.870597
09/16 01:43:15 PM: Global learning rate: 6.25e-06
09/16 01:43:15 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:43:15 PM: Update 30006: task edges-pos-ontonotes, batch 6 (30006): mcc: 0.8937, acc: 0.8437, precision: 0.9060, recall: 0.8858, f1: 0.8958, edges-pos-ontonotes_loss: 0.0099
09/16 01:43:25 PM: Update 30109: task edges-pos-ontonotes, batch 109 (30109): mcc: 0.8925, acc: 0.8447, precision: 0.9073, recall: 0.8823, f1: 0.8946, edges-pos-ontonotes_loss: 0.0101
09/16 01:43:35 PM: Update 30210: task edges-pos-ontonotes, batch 210 (30210): mcc: 0.8919, acc: 0.8444, precision: 0.9074, recall: 0.8811, f1: 0.8940, edges-pos-ontonotes_loss: 0.0103
09/16 01:43:46 PM: Update 30326: task edges-pos-ontonotes, batch 326 (30326): mcc: 0.8900, acc: 0.8424, precision: 0.9061, recall: 0.8786, f1: 0.8922, edges-pos-ontonotes_loss: 0.0105
09/16 01:43:56 PM: Update 30466: task edges-pos-ontonotes, batch 466 (30466): mcc: 0.8898, acc: 0.8422, precision: 0.9065, recall: 0.8779, f1: 0.8920, edges-pos-ontonotes_loss: 0.0107
09/16 01:44:06 PM: Update 30569: task edges-pos-ontonotes, batch 569 (30569): mcc: 0.8851, acc: 0.8363, precision: 0.9034, recall: 0.8718, f1: 0.8873, edges-pos-ontonotes_loss: 0.0110
09/16 01:44:16 PM: Update 30708: task edges-pos-ontonotes, batch 708 (30708): mcc: 0.8816, acc: 0.8321, precision: 0.9009, recall: 0.8676, f1: 0.8839, edges-pos-ontonotes_loss: 0.0113
09/16 01:44:26 PM: Update 30813: task edges-pos-ontonotes, batch 813 (30813): mcc: 0.8789, acc: 0.8292, precision: 0.8990, recall: 0.8641, f1: 0.8812, edges-pos-ontonotes_loss: 0.0115
09/16 01:44:36 PM: Update 30894: task edges-pos-ontonotes, batch 894 (30894): mcc: 0.8730, acc: 0.8222, precision: 0.8953, recall: 0.8564, f1: 0.8754, edges-pos-ontonotes_loss: 0.0119
09/16 01:44:46 PM: Update 30966: task edges-pos-ontonotes, batch 966 (30966): mcc: 0.8683, acc: 0.8165, precision: 0.8918, recall: 0.8506, f1: 0.8707, edges-pos-ontonotes_loss: 0.0122
09/16 01:44:50 PM: ***** Step 31000 / Validation 31 *****
09/16 01:44:50 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:44:50 PM: Validating...
09/16 01:44:56 PM: Evaluate: task edges-pos-ontonotes, batch 58 (157): mcc: 0.8700, acc: 0.8447, precision: 0.8918, recall: 0.8539, f1: 0.8725, edges-pos-ontonotes_loss: 0.0129
09/16 01:45:06 PM: Evaluate: task edges-pos-ontonotes, batch 132 (157): mcc: 0.8634, acc: 0.8349, precision: 0.8886, recall: 0.8444, f1: 0.8659, edges-pos-ontonotes_loss: 0.0129
09/16 01:45:10 PM: Updating LR scheduler:
09/16 01:45:10 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:45:10 PM: 	# validation passes without improvement: 1
09/16 01:45:10 PM: edges-pos-ontonotes_loss: training: 0.012392 validation: 0.013116
09/16 01:45:10 PM: macro_avg: validation: 0.864111
09/16 01:45:10 PM: micro_avg: validation: 0.000000
09/16 01:45:10 PM: edges-pos-ontonotes_mcc: training: 0.866568 validation: 0.861621
09/16 01:45:10 PM: edges-pos-ontonotes_acc: training: 0.814469 validation: 0.831857
09/16 01:45:10 PM: edges-pos-ontonotes_precision: training: 0.890439 validation: 0.887903
09/16 01:45:10 PM: edges-pos-ontonotes_recall: training: 0.848616 validation: 0.841561
09/16 01:45:10 PM: edges-pos-ontonotes_f1: training: 0.869025 validation: 0.864111
09/16 01:45:10 PM: Global learning rate: 6.25e-06
09/16 01:45:10 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:45:16 PM: Update 31046: task edges-pos-ontonotes, batch 46 (31046): mcc: 0.8439, acc: 0.7885, precision: 0.8716, recall: 0.8232, f1: 0.8467, edges-pos-ontonotes_loss: 0.0158
09/16 01:45:26 PM: Update 31106: task edges-pos-ontonotes, batch 106 (31106): mcc: 0.8427, acc: 0.7861, precision: 0.8702, recall: 0.8222, f1: 0.8455, edges-pos-ontonotes_loss: 0.0162
09/16 01:45:36 PM: Update 31184: task edges-pos-ontonotes, batch 184 (31184): mcc: 0.8443, acc: 0.7878, precision: 0.8716, recall: 0.8240, f1: 0.8471, edges-pos-ontonotes_loss: 0.0155
09/16 01:45:46 PM: Update 31296: task edges-pos-ontonotes, batch 296 (31296): mcc: 0.8499, acc: 0.7952, precision: 0.8754, recall: 0.8310, f1: 0.8526, edges-pos-ontonotes_loss: 0.0144
09/16 01:45:56 PM: Update 31411: task edges-pos-ontonotes, batch 411 (31411): mcc: 0.8526, acc: 0.7994, precision: 0.8773, recall: 0.8345, f1: 0.8554, edges-pos-ontonotes_loss: 0.0138
09/16 01:46:09 PM: Update 31454: task edges-pos-ontonotes, batch 454 (31454): mcc: 0.8537, acc: 0.8010, precision: 0.8781, recall: 0.8358, f1: 0.8564, edges-pos-ontonotes_loss: 0.0137
09/16 01:46:20 PM: Update 31545: task edges-pos-ontonotes, batch 545 (31545): mcc: 0.8557, acc: 0.8034, precision: 0.8798, recall: 0.8380, f1: 0.8584, edges-pos-ontonotes_loss: 0.0136
09/16 01:46:30 PM: Update 31629: task edges-pos-ontonotes, batch 629 (31629): mcc: 0.8569, acc: 0.8047, precision: 0.8806, recall: 0.8395, f1: 0.8595, edges-pos-ontonotes_loss: 0.0136
09/16 01:46:40 PM: Update 31726: task edges-pos-ontonotes, batch 726 (31726): mcc: 0.8579, acc: 0.8057, precision: 0.8814, recall: 0.8407, f1: 0.8606, edges-pos-ontonotes_loss: 0.0135
09/16 01:46:50 PM: Update 31789: task edges-pos-ontonotes, batch 789 (31789): mcc: 0.8572, acc: 0.8045, precision: 0.8810, recall: 0.8398, f1: 0.8599, edges-pos-ontonotes_loss: 0.0136
09/16 01:47:00 PM: Update 31858: task edges-pos-ontonotes, batch 858 (31858): mcc: 0.8560, acc: 0.8023, precision: 0.8798, recall: 0.8385, f1: 0.8586, edges-pos-ontonotes_loss: 0.0137
09/16 01:47:10 PM: Update 31928: task edges-pos-ontonotes, batch 928 (31928): mcc: 0.8556, acc: 0.8011, precision: 0.8795, recall: 0.8380, f1: 0.8582, edges-pos-ontonotes_loss: 0.0137
09/16 01:47:20 PM: Update 31990: task edges-pos-ontonotes, batch 990 (31990): mcc: 0.8552, acc: 0.8001, precision: 0.8791, recall: 0.8376, f1: 0.8579, edges-pos-ontonotes_loss: 0.0138
09/16 01:47:22 PM: ***** Step 32000 / Validation 32 *****
09/16 01:47:22 PM: edges-pos-ontonotes: trained on 1000 batches, 0.290 epochs
09/16 01:47:22 PM: Validating...
09/16 01:47:30 PM: Evaluate: task edges-pos-ontonotes, batch 83 (157): mcc: 0.8746, acc: 0.8404, precision: 0.9059, recall: 0.8493, f1: 0.8767, edges-pos-ontonotes_loss: 0.0124
09/16 01:47:40 PM: Evaluate: task edges-pos-ontonotes, batch 150 (157): mcc: 0.8699, acc: 0.8350, precision: 0.8990, recall: 0.8468, f1: 0.8721, edges-pos-ontonotes_loss: 0.0127
09/16 01:47:41 PM: Updating LR scheduler:
09/16 01:47:41 PM: 	Best result seen so far for macro_avg: 0.876
09/16 01:47:41 PM: 	# validation passes without improvement: 2
09/16 01:47:41 PM: Ran out of early stopping patience. Stopping training.
09/16 01:47:41 PM: edges-pos-ontonotes_loss: training: 0.013816 validation: 0.012737
09/16 01:47:41 PM: macro_avg: validation: 0.871688
09/16 01:47:41 PM: micro_avg: validation: 0.000000
09/16 01:47:41 PM: edges-pos-ontonotes_mcc: training: 0.854947 validation: 0.869453
09/16 01:47:41 PM: edges-pos-ontonotes_acc: training: 0.799741 validation: 0.834534
09/16 01:47:41 PM: edges-pos-ontonotes_precision: training: 0.878954 validation: 0.898764
09/16 01:47:41 PM: edges-pos-ontonotes_recall: training: 0.837334 validation: 0.846196
09/16 01:47:41 PM: edges-pos-ontonotes_f1: training: 0.857639 validation: 0.871688
09/16 01:47:41 PM: Global learning rate: 6.25e-06
09/16 01:47:41 PM: Saving checkpoints to: ./experiments/pos-ontonotes-sst-only/run
09/16 01:47:41 PM: Stopped training after 32 validation checks
09/16 01:47:41 PM: Trained edges-pos-ontonotes for 32000 batches or 9.265 epochs
09/16 01:47:41 PM: ***** VALIDATION RESULTS *****
09/16 01:47:41 PM: edges-pos-ontonotes_f1 (for best val pass 22): edges-pos-ontonotes_loss: 0.01283, macro_avg: 0.87561, micro_avg: 0.00000, edges-pos-ontonotes_mcc: 0.87319, edges-pos-ontonotes_acc: 0.84770, edges-pos-ontonotes_precision: 0.89318, edges-pos-ontonotes_recall: 0.85872, edges-pos-ontonotes_f1: 0.87561
09/16 01:47:41 PM: micro_avg (for best val pass 1): edges-pos-ontonotes_loss: 0.01973, macro_avg: 0.82424, micro_avg: 0.00000, edges-pos-ontonotes_mcc: 0.82269, edges-pos-ontonotes_acc: 0.74887, edges-pos-ontonotes_precision: 0.88360, edges-pos-ontonotes_recall: 0.77236, edges-pos-ontonotes_f1: 0.82424
09/16 01:47:41 PM: macro_avg (for best val pass 22): edges-pos-ontonotes_loss: 0.01283, macro_avg: 0.87561, micro_avg: 0.00000, edges-pos-ontonotes_mcc: 0.87319, edges-pos-ontonotes_acc: 0.84770, edges-pos-ontonotes_precision: 0.89318, edges-pos-ontonotes_recall: 0.85872, edges-pos-ontonotes_f1: 0.87561
09/16 01:47:41 PM: Evaluating...
09/16 01:47:41 PM: Loaded model state from ./experiments/pos-ontonotes-sst-only/run/edges-pos-ontonotes/model_state_target_train_val_22.best.th
09/16 01:47:41 PM: Evaluating on: edges-pos-ontonotes, split: val
09/16 01:48:11 PM: 	Task edges-pos-ontonotes: batch 204
09/16 01:48:42 PM: 	Task edges-pos-ontonotes: batch 430
09/16 01:48:47 PM: Task 'edges-pos-ontonotes': sorting predictions by 'idx'
09/16 01:48:47 PM: Finished evaluating on: edges-pos-ontonotes
09/16 01:48:48 PM: Task 'edges-pos-ontonotes': joining predictions with input split 'val'
09/16 01:49:00 PM: Task 'edges-pos-ontonotes': Wrote predictions to ./experiments/pos-ontonotes-sst-only/run
09/16 01:49:00 PM: Wrote all preds for split 'val' to ./experiments/pos-ontonotes-sst-only/run
09/16 01:49:00 PM: Evaluating on: edges-pos-ontonotes, split: test
09/16 01:49:30 PM: 	Task edges-pos-ontonotes: batch 197
09/16 01:49:49 PM: Task 'edges-pos-ontonotes': sorting predictions by 'idx'
09/16 01:49:49 PM: Finished evaluating on: edges-pos-ontonotes
09/16 01:49:49 PM: Task 'edges-pos-ontonotes': joining predictions with input split 'test'
09/16 01:49:58 PM: Task 'edges-pos-ontonotes': Wrote predictions to ./experiments/pos-ontonotes-sst-only/run
09/16 01:49:58 PM: Wrote all preds for split 'test' to ./experiments/pos-ontonotes-sst-only/run
09/16 01:49:58 PM: Writing results for split 'val' to ./experiments/pos-ontonotes-sst-only/results.tsv
09/16 01:49:58 PM: micro_avg: 0.000, macro_avg: 0.876, edges-pos-ontonotes_mcc: 0.874, edges-pos-ontonotes_acc: 0.849, edges-pos-ontonotes_precision: 0.893, edges-pos-ontonotes_recall: 0.860, edges-pos-ontonotes_f1: 0.876
09/16 01:49:59 PM: Done!
09/16 01:49:59 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
