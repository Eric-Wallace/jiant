09/16 09:13:23 AM: Git branch: master
09/16 09:13:23 AM: Git SHA: 3ca0f74688379229ab3eec908a215358ad18b3f4
09/16 09:13:23 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-coref-top/",
  "exp_name": "experiments/ner-ontonotes-coref-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-coref-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/coref",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-coref-top__run",
  "run_dir": "./experiments/ner-ontonotes-coref-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:13:23 AM: Saved config to ./experiments/ner-ontonotes-coref-top/run/params.conf
09/16 09:13:23 AM: Using random seed 1234
09/16 09:13:58 AM: Using GPU 0
09/16 09:13:58 AM: Loading tasks...
09/16 09:13:58 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-coref-top/
09/16 09:13:58 AM: 	Creating task edges-ner-ontonotes from scratch.
09/16 09:13:59 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 09:14:00 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 09:14:00 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 09:14:00 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 09:14:00 AM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 09:14:00 AM: 	Building vocab from scratch.
09/16 09:14:00 AM: 	Counting units for task edges-ner-ontonotes.
09/16 09:14:02 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 09:14:03 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:04 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:14:04 AM: 	Saved vocab to ./experiments/ner-ontonotes-coref-top/vocab
09/16 09:14:04 AM: Loading token dictionary from ./experiments/ner-ontonotes-coref-top/vocab.
09/16 09:14:04 AM: 	Loaded vocab from ./experiments/ner-ontonotes-coref-top/vocab
09/16 09:14:04 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 09:14:04 AM: 	Vocab namespace tokens: size 22840
09/16 09:14:04 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:14:04 AM: 	Vocab namespace chars: size 77
09/16 09:14:04 AM: 	Finished building vocab.
09/16 09:14:04 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 09:14:18 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-coref-top/preproc/edges-ner-ontonotes__train_data
09/16 09:14:18 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 09:14:20 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-coref-top/preproc/edges-ner-ontonotes__val_data
09/16 09:14:20 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 09:14:21 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-coref-top/preproc/edges-ner-ontonotes__test_data
09/16 09:14:21 AM: 	Finished indexing tasks
09/16 09:14:21 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 09:14:21 AM: 	  Training on 
09/16 09:14:21 AM: 	  Evaluating on edges-ner-ontonotes
09/16 09:14:21 AM: 	Finished loading tasks in 23.827s
09/16 09:14:21 AM: 	 Tasks: ['edges-ner-ontonotes']
09/16 09:14:21 AM: Building model...
09/16 09:14:21 AM: Using BERT model (bert-base-uncased).
09/16 09:14:21 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:14:21 AM: models/coref
09/16 09:14:21 AM: loading configuration file models/coref/config.json
09/16 09:14:21 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:14:21 AM: loading weights file models/coref/pytorch_model.bin
09/16 09:14:26 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmprsosdy_2
09/16 09:14:29 AM: copying /tmp/tmprsosdy_2 to cache at ./experiments/ner-ontonotes-coref-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: creating metadata file for ./experiments/ner-ontonotes-coref-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: removing temp file /tmp/tmprsosdy_2
09/16 09:14:29 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-coref-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:30 AM: Initializing parameters
09/16 09:14:30 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:14:30 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 09:14:54 AM: Model specification:
09/16 09:14:54 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 09:14:54 AM: Model parameters:
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:54 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:54 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:14:54 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:14:54 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 09:14:54 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 09:14:54 AM: Total number of parameters: 109688338 (1.09688e+08)
09/16 09:14:54 AM: Number of trainable parameters: 206098 (206098)
09/16 09:14:54 AM: Finished building model in 32.685s
09/16 09:14:54 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 09:14:58 AM: patience = 9
09/16 09:14:59 AM: val_interval = 1000
09/16 09:14:59 AM: max_vals = 250
09/16 09:14:59 AM: cuda_device = 0
09/16 09:14:59 AM: grad_norm = 5.0
09/16 09:14:59 AM: grad_clipping = None
09/16 09:14:59 AM: lr_decay = 0.99
09/16 09:14:59 AM: min_lr = 1e-06
09/16 09:14:59 AM: keep_all_checkpoints = 0
09/16 09:14:59 AM: val_data_limit = 5000
09/16 09:14:59 AM: max_epochs = -1
09/16 09:14:59 AM: dec_val_scale = 250
09/16 09:14:59 AM: training_data_fraction = 1
09/16 09:14:59 AM: type = adam
09/16 09:14:59 AM: parameter_groups = None
09/16 09:14:59 AM: Number of trainable parameters: 206098
09/16 09:14:59 AM: infer_type_and_cast = True
09/16 09:14:59 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:59 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:59 AM: lr = 0.0001
09/16 09:14:59 AM: amsgrad = True
09/16 09:14:59 AM: type = reduce_on_plateau
09/16 09:14:59 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:59 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:59 AM: mode = max
09/16 09:14:59 AM: factor = 0.5
09/16 09:14:59 AM: patience = 3
09/16 09:14:59 AM: threshold = 0.0001
09/16 09:14:59 AM: threshold_mode = abs
09/16 09:14:59 AM: verbose = True
09/16 09:14:59 AM: type = adam
09/16 09:14:59 AM: parameter_groups = None
09/16 09:14:59 AM: Number of trainable parameters: 206098
09/16 09:14:59 AM: infer_type_and_cast = True
09/16 09:14:59 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:59 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:59 AM: lr = 0.0001
09/16 09:14:59 AM: amsgrad = True
09/16 09:14:59 AM: type = reduce_on_plateau
09/16 09:14:59 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:59 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:59 AM: mode = max
09/16 09:14:59 AM: factor = 0.5
09/16 09:14:59 AM: patience = 3
09/16 09:14:59 AM: threshold = 0.0001
09/16 09:14:59 AM: threshold_mode = abs
09/16 09:14:59 AM: verbose = True
09/16 09:14:59 AM: Starting training without restoring from a checkpoint.
09/16 09:14:59 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 09:14:59 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 09:15:09 AM: Update 78: task edges-ner-ontonotes, batch 78 (78): mcc: 0.0409, acc: 0.0312, precision: 0.0943, recall: 0.0940, f1: 0.0942, edges-ner-ontonotes_loss: 0.3294
09/16 09:15:19 AM: Update 228: task edges-ner-ontonotes, batch 228 (228): mcc: 0.2806, acc: 0.2082, precision: 0.4151, recall: 0.2314, f1: 0.2972, edges-ner-ontonotes_loss: 0.1986
09/16 09:15:29 AM: Update 330: task edges-ner-ontonotes, batch 330 (330): mcc: 0.4075, acc: 0.3046, precision: 0.5760, recall: 0.3231, f1: 0.4140, edges-ner-ontonotes_loss: 0.1679
09/16 09:15:39 AM: Update 463: task edges-ner-ontonotes, batch 463 (463): mcc: 0.5249, acc: 0.4078, precision: 0.6997, recall: 0.4241, f1: 0.5281, edges-ner-ontonotes_loss: 0.1429
09/16 09:15:49 AM: Update 589: task edges-ner-ontonotes, batch 589 (589): mcc: 0.5919, acc: 0.4739, precision: 0.7590, recall: 0.4894, f1: 0.5951, edges-ner-ontonotes_loss: 0.1273
09/16 09:15:59 AM: Update 694: task edges-ner-ontonotes, batch 694 (694): mcc: 0.6297, acc: 0.5123, precision: 0.7909, recall: 0.5275, f1: 0.6329, edges-ner-ontonotes_loss: 0.1179
09/16 09:16:09 AM: Update 820: task edges-ner-ontonotes, batch 820 (820): mcc: 0.6654, acc: 0.5504, precision: 0.8183, recall: 0.5655, f1: 0.6688, edges-ner-ontonotes_loss: 0.1086
09/16 09:16:21 AM: Update 940: task edges-ner-ontonotes, batch 940 (940): mcc: 0.6926, acc: 0.5815, precision: 0.8370, recall: 0.5964, f1: 0.6965, edges-ner-ontonotes_loss: 0.1012
09/16 09:16:25 AM: ***** Step 1000 / Validation 1 *****
09/16 09:16:25 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:16:25 AM: Validating...
09/16 09:16:31 AM: Evaluate: task edges-ner-ontonotes, batch 52 (157): mcc: 0.8024, acc: 0.7315, precision: 0.8872, recall: 0.7438, f1: 0.8092, edges-ner-ontonotes_loss: 0.0665
09/16 09:16:45 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.8400, acc: 0.7735, precision: 0.9169, recall: 0.7844, f1: 0.8455, edges-ner-ontonotes_loss: 0.0564
09/16 09:16:50 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:16:51 AM: Best result seen so far for micro.
09/16 09:16:51 AM: Best result seen so far for macro.
09/16 09:16:51 AM: Updating LR scheduler:
09/16 09:16:51 AM: 	Best result seen so far for macro_avg: 0.851
09/16 09:16:51 AM: 	# validation passes without improvement: 0
09/16 09:16:51 AM: edges-ner-ontonotes_loss: training: 0.098294 validation: 0.053492
09/16 09:16:51 AM: macro_avg: validation: 0.851192
09/16 09:16:51 AM: micro_avg: validation: 0.000000
09/16 09:16:51 AM: edges-ner-ontonotes_mcc: training: 0.703482 validation: 0.845948
09/16 09:16:51 AM: edges-ner-ontonotes_acc: training: 0.594243 validation: 0.779951
09/16 09:16:51 AM: edges-ner-ontonotes_precision: training: 0.843952 validation: 0.922301
09/16 09:16:51 AM: edges-ner-ontonotes_recall: training: 0.609168 validation: 0.790264
09/16 09:16:51 AM: edges-ner-ontonotes_f1: training: 0.707593 validation: 0.851192
09/16 09:16:51 AM: Global learning rate: 0.0001
09/16 09:16:51 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:16:55 AM: Update 1048: task edges-ner-ontonotes, batch 48 (1048): mcc: 0.8578, acc: 0.7837, precision: 0.9355, recall: 0.7998, f1: 0.8624, edges-ner-ontonotes_loss: 0.0478
09/16 09:17:05 AM: Update 1170: task edges-ner-ontonotes, batch 170 (1170): mcc: 0.8602, acc: 0.7910, precision: 0.9326, recall: 0.8067, f1: 0.8651, edges-ner-ontonotes_loss: 0.0471
09/16 09:17:15 AM: Update 1269: task edges-ner-ontonotes, batch 269 (1269): mcc: 0.8571, acc: 0.7874, precision: 0.9305, recall: 0.8029, f1: 0.8620, edges-ner-ontonotes_loss: 0.0477
09/16 09:17:25 AM: Update 1400: task edges-ner-ontonotes, batch 400 (1400): mcc: 0.8467, acc: 0.7742, precision: 0.9234, recall: 0.7906, f1: 0.8519, edges-ner-ontonotes_loss: 0.0514
09/16 09:17:35 AM: Update 1530: task edges-ner-ontonotes, batch 530 (1530): mcc: 0.8430, acc: 0.7700, precision: 0.9204, recall: 0.7868, f1: 0.8483, edges-ner-ontonotes_loss: 0.0522
09/16 09:17:45 AM: Update 1654: task edges-ner-ontonotes, batch 654 (1654): mcc: 0.8434, acc: 0.7704, precision: 0.9198, recall: 0.7879, f1: 0.8487, edges-ner-ontonotes_loss: 0.0515
09/16 09:17:55 AM: Update 1802: task edges-ner-ontonotes, batch 802 (1802): mcc: 0.8446, acc: 0.7722, precision: 0.9200, recall: 0.7898, f1: 0.8500, edges-ner-ontonotes_loss: 0.0505
09/16 09:18:05 AM: Update 1913: task edges-ner-ontonotes, batch 913 (1913): mcc: 0.8470, acc: 0.7753, precision: 0.9209, recall: 0.7934, f1: 0.8524, edges-ner-ontonotes_loss: 0.0496
09/16 09:18:12 AM: ***** Step 2000 / Validation 2 *****
09/16 09:18:12 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:18:12 AM: Validating...
09/16 09:18:16 AM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.8453, acc: 0.7854, precision: 0.9111, recall: 0.7990, f1: 0.8514, edges-ner-ontonotes_loss: 0.0476
09/16 09:18:26 AM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.8852, acc: 0.8335, precision: 0.9404, recall: 0.8446, f1: 0.8899, edges-ner-ontonotes_loss: 0.0385
09/16 09:18:30 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:18:30 AM: Best result seen so far for macro.
09/16 09:18:30 AM: Updating LR scheduler:
09/16 09:18:30 AM: 	Best result seen so far for macro_avg: 0.892
09/16 09:18:30 AM: 	# validation passes without improvement: 0
09/16 09:18:30 AM: edges-ner-ontonotes_loss: training: 0.048810 validation: 0.037391
09/16 09:18:30 AM: macro_avg: validation: 0.891929
09/16 09:18:30 AM: micro_avg: validation: 0.000000
09/16 09:18:30 AM: edges-ner-ontonotes_mcc: training: 0.849210 validation: 0.887434
09/16 09:18:30 AM: edges-ner-ontonotes_acc: training: 0.778812 validation: 0.833258
09/16 09:18:30 AM: edges-ner-ontonotes_precision: training: 0.920769 validation: 0.943796
09/16 09:18:30 AM: edges-ner-ontonotes_recall: training: 0.797435 validation: 0.845466
09/16 09:18:30 AM: edges-ner-ontonotes_f1: training: 0.854675 validation: 0.891929
09/16 09:18:30 AM: Global learning rate: 0.0001
09/16 09:18:30 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:18:36 AM: Update 2068: task edges-ner-ontonotes, batch 68 (2068): mcc: 0.8822, acc: 0.8244, precision: 0.9344, recall: 0.8447, f1: 0.8873, edges-ner-ontonotes_loss: 0.0369
09/16 09:18:46 AM: Update 2183: task edges-ner-ontonotes, batch 183 (2183): mcc: 0.8807, acc: 0.8248, precision: 0.9314, recall: 0.8447, f1: 0.8859, edges-ner-ontonotes_loss: 0.0372
09/16 09:18:57 AM: Update 2307: task edges-ner-ontonotes, batch 307 (2307): mcc: 0.8861, acc: 0.8305, precision: 0.9353, recall: 0.8508, f1: 0.8910, edges-ner-ontonotes_loss: 0.0361
09/16 09:19:07 AM: Update 2436: task edges-ner-ontonotes, batch 436 (2436): mcc: 0.8888, acc: 0.8345, precision: 0.9366, recall: 0.8546, f1: 0.8937, edges-ner-ontonotes_loss: 0.0351
09/16 09:19:17 AM: Update 2534: task edges-ner-ontonotes, batch 534 (2534): mcc: 0.8904, acc: 0.8365, precision: 0.9378, recall: 0.8565, f1: 0.8953, edges-ner-ontonotes_loss: 0.0348
09/16 09:19:27 AM: Update 2663: task edges-ner-ontonotes, batch 663 (2663): mcc: 0.8919, acc: 0.8379, precision: 0.9382, recall: 0.8587, f1: 0.8967, edges-ner-ontonotes_loss: 0.0345
09/16 09:19:37 AM: Update 2788: task edges-ner-ontonotes, batch 788 (2788): mcc: 0.8932, acc: 0.8395, precision: 0.9386, recall: 0.8609, f1: 0.8980, edges-ner-ontonotes_loss: 0.0341
09/16 09:19:47 AM: Update 2884: task edges-ner-ontonotes, batch 884 (2884): mcc: 0.8901, acc: 0.8356, precision: 0.9361, recall: 0.8575, f1: 0.8951, edges-ner-ontonotes_loss: 0.0352
09/16 09:19:56 AM: ***** Step 3000 / Validation 3 *****
09/16 09:19:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:19:56 AM: Validating...
09/16 09:19:57 AM: Evaluate: task edges-ner-ontonotes, batch 11 (157): mcc: 0.7987, acc: 0.7274, precision: 0.8704, recall: 0.7518, f1: 0.8068, edges-ner-ontonotes_loss: 0.0546
09/16 09:20:07 AM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.8842, acc: 0.8372, precision: 0.9303, recall: 0.8519, f1: 0.8894, edges-ner-ontonotes_loss: 0.0366
09/16 09:20:14 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:20:14 AM: Best result seen so far for macro.
09/16 09:20:14 AM: Updating LR scheduler:
09/16 09:20:14 AM: 	Best result seen so far for macro_avg: 0.903
09/16 09:20:14 AM: 	# validation passes without improvement: 0
09/16 09:20:14 AM: edges-ner-ontonotes_loss: training: 0.036421 validation: 0.032772
09/16 09:20:14 AM: macro_avg: validation: 0.902938
09/16 09:20:14 AM: micro_avg: validation: 0.000000
09/16 09:20:14 AM: edges-ner-ontonotes_mcc: training: 0.886751 validation: 0.898405
09/16 09:20:14 AM: edges-ner-ontonotes_acc: training: 0.831389 validation: 0.852138
09/16 09:20:14 AM: edges-ner-ontonotes_precision: training: 0.933705 validation: 0.943329
09/16 09:20:14 AM: edges-ner-ontonotes_recall: training: 0.853527 validation: 0.865863
09/16 09:20:14 AM: edges-ner-ontonotes_f1: training: 0.891817 validation: 0.902938
09/16 09:20:14 AM: Global learning rate: 0.0001
09/16 09:20:14 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:20:17 AM: Update 3031: task edges-ner-ontonotes, batch 31 (3031): mcc: 0.8665, acc: 0.8037, precision: 0.9245, recall: 0.8252, f1: 0.8720, edges-ner-ontonotes_loss: 0.0444
09/16 09:20:27 AM: Update 3146: task edges-ner-ontonotes, batch 146 (3146): mcc: 0.8637, acc: 0.8039, precision: 0.9169, recall: 0.8272, f1: 0.8697, edges-ner-ontonotes_loss: 0.0438
09/16 09:20:37 AM: Update 3298: task edges-ner-ontonotes, batch 298 (3298): mcc: 0.8713, acc: 0.8133, precision: 0.9209, recall: 0.8371, f1: 0.8770, edges-ner-ontonotes_loss: 0.0412
09/16 09:20:47 AM: Update 3426: task edges-ner-ontonotes, batch 426 (3426): mcc: 0.8745, acc: 0.8171, precision: 0.9229, recall: 0.8413, f1: 0.8802, edges-ner-ontonotes_loss: 0.0398
09/16 09:20:57 AM: Update 3551: task edges-ner-ontonotes, batch 551 (3551): mcc: 0.8783, acc: 0.8225, precision: 0.9246, recall: 0.8465, f1: 0.8838, edges-ner-ontonotes_loss: 0.0386
09/16 09:21:07 AM: Update 3680: task edges-ner-ontonotes, batch 680 (3680): mcc: 0.8823, acc: 0.8280, precision: 0.9271, recall: 0.8516, f1: 0.8877, edges-ner-ontonotes_loss: 0.0375
09/16 09:21:17 AM: Update 3779: task edges-ner-ontonotes, batch 779 (3779): mcc: 0.8853, acc: 0.8320, precision: 0.9284, recall: 0.8559, f1: 0.8907, edges-ner-ontonotes_loss: 0.0368
09/16 09:21:27 AM: Update 3905: task edges-ner-ontonotes, batch 905 (3905): mcc: 0.8891, acc: 0.8365, precision: 0.9308, recall: 0.8605, f1: 0.8943, edges-ner-ontonotes_loss: 0.0356
09/16 09:21:35 AM: ***** Step 4000 / Validation 4 *****
09/16 09:21:35 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:21:35 AM: Validating...
09/16 09:21:37 AM: Evaluate: task edges-ner-ontonotes, batch 28 (157): mcc: 0.8476, acc: 0.7905, precision: 0.8969, recall: 0.8163, f1: 0.8547, edges-ner-ontonotes_loss: 0.0484
09/16 09:21:48 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.9014, acc: 0.8608, precision: 0.9374, recall: 0.8770, f1: 0.9062, edges-ner-ontonotes_loss: 0.0337
09/16 09:21:54 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:21:54 AM: Best result seen so far for macro.
09/16 09:21:54 AM: Updating LR scheduler:
09/16 09:21:54 AM: 	Best result seen so far for macro_avg: 0.913
09/16 09:21:54 AM: 	# validation passes without improvement: 0
09/16 09:21:54 AM: edges-ner-ontonotes_loss: training: 0.034857 validation: 0.030596
09/16 09:21:54 AM: macro_avg: validation: 0.913273
09/16 09:21:54 AM: micro_avg: validation: 0.000000
09/16 09:21:54 AM: edges-ner-ontonotes_mcc: training: 0.891454 validation: 0.908857
09/16 09:21:54 AM: edges-ner-ontonotes_acc: training: 0.839336 validation: 0.868972
09/16 09:21:54 AM: edges-ner-ontonotes_precision: training: 0.932216 validation: 0.943273
09/16 09:21:54 AM: edges-ner-ontonotes_recall: training: 0.863540 validation: 0.885123
09/16 09:21:54 AM: edges-ner-ontonotes_f1: training: 0.896564 validation: 0.913273
09/16 09:21:54 AM: Global learning rate: 0.0001
09/16 09:21:54 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:21:58 AM: Update 4048: task edges-ner-ontonotes, batch 48 (4048): mcc: 0.9160, acc: 0.8713, precision: 0.9461, recall: 0.8956, f1: 0.9202, edges-ner-ontonotes_loss: 0.0268
09/16 09:22:08 AM: Update 4148: task edges-ner-ontonotes, batch 148 (4148): mcc: 0.9113, acc: 0.8649, precision: 0.9445, recall: 0.8885, f1: 0.9156, edges-ner-ontonotes_loss: 0.0281
09/16 09:22:18 AM: Update 4273: task edges-ner-ontonotes, batch 273 (4273): mcc: 0.9119, acc: 0.8660, precision: 0.9447, recall: 0.8894, f1: 0.9162, edges-ner-ontonotes_loss: 0.0280
09/16 09:22:28 AM: Update 4384: task edges-ner-ontonotes, batch 384 (4384): mcc: 0.9081, acc: 0.8609, precision: 0.9413, recall: 0.8855, f1: 0.9126, edges-ner-ontonotes_loss: 0.0291
09/16 09:22:38 AM: Update 4511: task edges-ner-ontonotes, batch 511 (4511): mcc: 0.9003, acc: 0.8505, precision: 0.9368, recall: 0.8755, f1: 0.9051, edges-ner-ontonotes_loss: 0.0326
09/16 09:22:48 AM: Update 4636: task edges-ner-ontonotes, batch 636 (4636): mcc: 0.8948, acc: 0.8443, precision: 0.9329, recall: 0.8691, f1: 0.8999, edges-ner-ontonotes_loss: 0.0347
09/16 09:22:58 AM: Update 4761: task edges-ner-ontonotes, batch 761 (4761): mcc: 0.8931, acc: 0.8425, precision: 0.9314, recall: 0.8673, f1: 0.8982, edges-ner-ontonotes_loss: 0.0352
09/16 09:23:08 AM: Update 4909: task edges-ner-ontonotes, batch 909 (4909): mcc: 0.8927, acc: 0.8422, precision: 0.9309, recall: 0.8671, f1: 0.8979, edges-ner-ontonotes_loss: 0.0352
09/16 09:23:17 AM: ***** Step 5000 / Validation 5 *****
09/16 09:23:17 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:23:17 AM: Validating...
09/16 09:23:18 AM: Evaluate: task edges-ner-ontonotes, batch 14 (157): mcc: 0.8435, acc: 0.7798, precision: 0.8989, recall: 0.8070, f1: 0.8505, edges-ner-ontonotes_loss: 0.0435
09/16 09:23:28 AM: Evaluate: task edges-ner-ontonotes, batch 101 (157): mcc: 0.9038, acc: 0.8636, precision: 0.9410, recall: 0.8779, f1: 0.9083, edges-ner-ontonotes_loss: 0.0318
09/16 09:23:35 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:23:35 AM: Best result seen so far for macro.
09/16 09:23:35 AM: Updating LR scheduler:
09/16 09:23:35 AM: 	Best result seen so far for macro_avg: 0.920
09/16 09:23:35 AM: 	# validation passes without improvement: 0
09/16 09:23:35 AM: edges-ner-ontonotes_loss: training: 0.035225 validation: 0.028352
09/16 09:23:35 AM: macro_avg: validation: 0.919612
09/16 09:23:35 AM: micro_avg: validation: 0.000000
09/16 09:23:35 AM: edges-ner-ontonotes_mcc: training: 0.892344 validation: 0.915540
09/16 09:23:35 AM: edges-ner-ontonotes_acc: training: 0.841819 validation: 0.877161
09/16 09:23:35 AM: edges-ner-ontonotes_precision: training: 0.930448 validation: 0.949306
09/16 09:23:35 AM: edges-ner-ontonotes_recall: training: 0.866849 validation: 0.891720
09/16 09:23:35 AM: edges-ner-ontonotes_f1: training: 0.897523 validation: 0.919612
09/16 09:23:35 AM: Global learning rate: 0.0001
09/16 09:23:35 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:23:38 AM: Update 5032: task edges-ner-ontonotes, batch 32 (5032): mcc: 0.8909, acc: 0.8435, precision: 0.9287, recall: 0.8659, f1: 0.8962, edges-ner-ontonotes_loss: 0.0351
09/16 09:23:48 AM: Update 5159: task edges-ner-ontonotes, batch 159 (5159): mcc: 0.8976, acc: 0.8518, precision: 0.9318, recall: 0.8753, f1: 0.9027, edges-ner-ontonotes_loss: 0.0330
09/16 09:23:58 AM: Update 5293: task edges-ner-ontonotes, batch 293 (5293): mcc: 0.9004, acc: 0.8541, precision: 0.9338, recall: 0.8784, f1: 0.9053, edges-ner-ontonotes_loss: 0.0315
09/16 09:24:08 AM: Update 5400: task edges-ner-ontonotes, batch 400 (5400): mcc: 0.9045, acc: 0.8586, precision: 0.9363, recall: 0.8838, f1: 0.9093, edges-ner-ontonotes_loss: 0.0302
09/16 09:24:18 AM: Update 5522: task edges-ner-ontonotes, batch 522 (5522): mcc: 0.9077, acc: 0.8628, precision: 0.9385, recall: 0.8875, f1: 0.9123, edges-ner-ontonotes_loss: 0.0294
09/16 09:24:28 AM: Update 5617: task edges-ner-ontonotes, batch 617 (5617): mcc: 0.9089, acc: 0.8642, precision: 0.9396, recall: 0.8887, f1: 0.9134, edges-ner-ontonotes_loss: 0.0289
09/16 09:24:38 AM: Update 5735: task edges-ner-ontonotes, batch 735 (5735): mcc: 0.9098, acc: 0.8652, precision: 0.9400, recall: 0.8900, f1: 0.9143, edges-ner-ontonotes_loss: 0.0286
09/16 09:24:48 AM: Update 5865: task edges-ner-ontonotes, batch 865 (5865): mcc: 0.9103, acc: 0.8657, precision: 0.9403, recall: 0.8906, f1: 0.9148, edges-ner-ontonotes_loss: 0.0285
09/16 09:24:59 AM: Update 5972: task edges-ner-ontonotes, batch 972 (5972): mcc: 0.9087, acc: 0.8638, precision: 0.9393, recall: 0.8887, f1: 0.9133, edges-ner-ontonotes_loss: 0.0291
09/16 09:25:01 AM: ***** Step 6000 / Validation 6 *****
09/16 09:25:01 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:25:01 AM: Validating...
09/16 09:25:09 AM: Evaluate: task edges-ner-ontonotes, batch 70 (157): mcc: 0.8966, acc: 0.8555, precision: 0.9310, recall: 0.8741, f1: 0.9017, edges-ner-ontonotes_loss: 0.0345
09/16 09:25:19 AM: Evaluate: task edges-ner-ontonotes, batch 149 (157): mcc: 0.9170, acc: 0.8795, precision: 0.9477, recall: 0.8959, f1: 0.9211, edges-ner-ontonotes_loss: 0.0279
09/16 09:25:20 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:25:20 AM: Best result seen so far for macro.
09/16 09:25:20 AM: Updating LR scheduler:
09/16 09:25:20 AM: 	Best result seen so far for macro_avg: 0.921
09/16 09:25:20 AM: 	# validation passes without improvement: 0
09/16 09:25:20 AM: edges-ner-ontonotes_loss: training: 0.029474 validation: 0.027644
09/16 09:25:20 AM: macro_avg: validation: 0.921333
09/16 09:25:20 AM: micro_avg: validation: 0.000000
09/16 09:25:20 AM: edges-ner-ontonotes_mcc: training: 0.907773 validation: 0.917234
09/16 09:25:20 AM: edges-ner-ontonotes_acc: training: 0.862509 validation: 0.879588
09/16 09:25:20 AM: edges-ner-ontonotes_precision: training: 0.938732 validation: 0.947584
09/16 09:25:20 AM: edges-ner-ontonotes_recall: training: 0.887457 validation: 0.896497
09/16 09:25:20 AM: edges-ner-ontonotes_f1: training: 0.912374 validation: 0.921333
09/16 09:25:20 AM: Global learning rate: 0.0001
09/16 09:25:20 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:25:29 AM: Update 6115: task edges-ner-ontonotes, batch 115 (6115): mcc: 0.8770, acc: 0.8247, precision: 0.9160, recall: 0.8523, f1: 0.8830, edges-ner-ontonotes_loss: 0.0406
09/16 09:25:39 AM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.8779, acc: 0.8249, precision: 0.9183, recall: 0.8516, f1: 0.8837, edges-ner-ontonotes_loss: 0.0410
09/16 09:25:49 AM: Update 6378: task edges-ner-ontonotes, batch 378 (6378): mcc: 0.8824, acc: 0.8308, precision: 0.9215, recall: 0.8571, f1: 0.8881, edges-ner-ontonotes_loss: 0.0384
09/16 09:25:59 AM: Update 6525: task edges-ner-ontonotes, batch 525 (6525): mcc: 0.8858, acc: 0.8355, precision: 0.9233, recall: 0.8615, f1: 0.8913, edges-ner-ontonotes_loss: 0.0371
09/16 09:26:09 AM: Update 6635: task edges-ner-ontonotes, batch 635 (6635): mcc: 0.8894, acc: 0.8403, precision: 0.9257, recall: 0.8660, f1: 0.8948, edges-ner-ontonotes_loss: 0.0358
09/16 09:26:19 AM: Update 6766: task edges-ner-ontonotes, batch 766 (6766): mcc: 0.8924, acc: 0.8443, precision: 0.9274, recall: 0.8698, f1: 0.8977, edges-ner-ontonotes_loss: 0.0350
09/16 09:26:29 AM: Update 6865: task edges-ner-ontonotes, batch 865 (6865): mcc: 0.8941, acc: 0.8469, precision: 0.9284, recall: 0.8720, f1: 0.8993, edges-ner-ontonotes_loss: 0.0343
09/16 09:26:39 AM: Update 6992: task edges-ner-ontonotes, batch 992 (6992): mcc: 0.8975, acc: 0.8511, precision: 0.9306, recall: 0.8761, f1: 0.9026, edges-ner-ontonotes_loss: 0.0332
09/16 09:26:40 AM: ***** Step 7000 / Validation 7 *****
09/16 09:26:40 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:26:40 AM: Validating...
09/16 09:26:49 AM: Evaluate: task edges-ner-ontonotes, batch 82 (157): mcc: 0.9047, acc: 0.8649, precision: 0.9378, recall: 0.8825, f1: 0.9094, edges-ner-ontonotes_loss: 0.0335
09/16 09:26:59 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:26:59 AM: Best result seen so far for macro.
09/16 09:26:59 AM: Updating LR scheduler:
09/16 09:26:59 AM: 	Best result seen so far for macro_avg: 0.925
09/16 09:26:59 AM: 	# validation passes without improvement: 0
09/16 09:26:59 AM: edges-ner-ontonotes_loss: training: 0.033159 validation: 0.027437
09/16 09:26:59 AM: macro_avg: validation: 0.924582
09/16 09:26:59 AM: micro_avg: validation: 0.000000
09/16 09:26:59 AM: edges-ner-ontonotes_mcc: training: 0.897822 validation: 0.920564
09/16 09:26:59 AM: edges-ner-ontonotes_acc: training: 0.851446 validation: 0.884592
09/16 09:26:59 AM: edges-ner-ontonotes_precision: training: 0.930880 validation: 0.947620
09/16 09:26:59 AM: edges-ner-ontonotes_recall: training: 0.876550 validation: 0.902639
09/16 09:26:59 AM: edges-ner-ontonotes_f1: training: 0.902899 validation: 0.924582
09/16 09:26:59 AM: Global learning rate: 0.0001
09/16 09:26:59 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:26:59 AM: Update 7006: task edges-ner-ontonotes, batch 6 (7006): mcc: 0.9346, acc: 0.9004, precision: 0.9499, recall: 0.9267, f1: 0.9382, edges-ner-ontonotes_loss: 0.0221
09/16 09:27:09 AM: Update 7128: task edges-ner-ontonotes, batch 128 (7128): mcc: 0.9209, acc: 0.8796, precision: 0.9459, recall: 0.9050, f1: 0.9250, edges-ner-ontonotes_loss: 0.0249
09/16 09:27:19 AM: Update 7236: task edges-ner-ontonotes, batch 236 (7236): mcc: 0.9185, acc: 0.8773, precision: 0.9446, recall: 0.9018, f1: 0.9227, edges-ner-ontonotes_loss: 0.0256
09/16 09:27:29 AM: Update 7360: task edges-ner-ontonotes, batch 360 (7360): mcc: 0.9196, acc: 0.8781, precision: 0.9456, recall: 0.9029, f1: 0.9237, edges-ner-ontonotes_loss: 0.0252
09/16 09:27:41 AM: Update 7477: task edges-ner-ontonotes, batch 477 (7477): mcc: 0.9183, acc: 0.8766, precision: 0.9445, recall: 0.9015, f1: 0.9225, edges-ner-ontonotes_loss: 0.0258
09/16 09:27:51 AM: Update 7603: task edges-ner-ontonotes, batch 603 (7603): mcc: 0.9101, acc: 0.8660, precision: 0.9392, recall: 0.8913, f1: 0.9146, edges-ner-ontonotes_loss: 0.0290
09/16 09:28:01 AM: Update 7728: task edges-ner-ontonotes, batch 728 (7728): mcc: 0.9057, acc: 0.8605, precision: 0.9362, recall: 0.8862, f1: 0.9105, edges-ner-ontonotes_loss: 0.0309
09/16 09:28:11 AM: Update 7850: task edges-ner-ontonotes, batch 850 (7850): mcc: 0.9038, acc: 0.8580, precision: 0.9350, recall: 0.8837, f1: 0.9086, edges-ner-ontonotes_loss: 0.0318
09/16 09:28:21 AM: Update 7999: task edges-ner-ontonotes, batch 999 (7999): mcc: 0.9030, acc: 0.8571, precision: 0.9343, recall: 0.8828, f1: 0.9078, edges-ner-ontonotes_loss: 0.0320
09/16 09:28:21 AM: ***** Step 8000 / Validation 8 *****
09/16 09:28:21 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:28:21 AM: Validating...
09/16 09:28:31 AM: Evaluate: task edges-ner-ontonotes, batch 90 (157): mcc: 0.9133, acc: 0.8755, precision: 0.9472, recall: 0.8895, f1: 0.9175, edges-ner-ontonotes_loss: 0.0297
09/16 09:28:40 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:28:40 AM: Best result seen so far for macro.
09/16 09:28:40 AM: Updating LR scheduler:
09/16 09:28:40 AM: 	Best result seen so far for macro_avg: 0.925
09/16 09:28:40 AM: 	# validation passes without improvement: 0
09/16 09:28:40 AM: edges-ner-ontonotes_loss: training: 0.031981 validation: 0.026289
09/16 09:28:40 AM: macro_avg: validation: 0.925330
09/16 09:28:40 AM: micro_avg: validation: 0.000000
09/16 09:28:40 AM: edges-ner-ontonotes_mcc: training: 0.902943 validation: 0.921543
09/16 09:28:40 AM: edges-ner-ontonotes_acc: training: 0.857052 validation: 0.885729
09/16 09:28:40 AM: edges-ner-ontonotes_precision: training: 0.934252 validation: 0.954018
09/16 09:28:40 AM: edges-ner-ontonotes_recall: training: 0.882804 validation: 0.898317
09/16 09:28:40 AM: edges-ner-ontonotes_f1: training: 0.907800 validation: 0.925330
09/16 09:28:40 AM: Global learning rate: 0.0001
09/16 09:28:40 AM: Saving checkpoints to: ./experiments/ner-ontonotes-coref-top/run
09/16 09:28:41 AM: Update 8015: task edges-ner-ontonotes, batch 15 (8015): mcc: 0.9016, acc: 0.8619, precision: 0.9330, recall: 0.8815, f1: 0.9065, edges-ner-ontonotes_loss: 0.0302
09/16 09:28:51 AM: Update 8129: task edges-ner-ontonotes, batch 129 (8129): mcc: 0.9025, acc: 0.8559, precision: 0.9347, recall: 0.8816, f1: 0.9074, edges-ner-ontonotes_loss: 0.0315
09/16 09:29:01 AM: Update 8265: task edges-ner-ontonotes, batch 265 (8265): mcc: 0.9066, acc: 0.8621, precision: 0.9362, recall: 0.8877, f1: 0.9113, edges-ner-ontonotes_loss: 0.0304
09/16 09:29:11 AM: Update 8393: task edges-ner-ontonotes, batch 393 (8393): mcc: 0.9069, acc: 0.8629, precision: 0.9360, recall: 0.8885, f1: 0.9116, edges-ner-ontonotes_loss: 0.0301
09/16 09:29:22 AM: Update 8498: task edges-ner-ontonotes, batch 498 (8498): mcc: 0.9096, acc: 0.8661, precision: 0.9374, recall: 0.8921, f1: 0.9142, edges-ner-ontonotes_loss: 0.0292
09/16 09:29:32 AM: Update 8619: task edges-ner-ontonotes, batch 619 (8619): mcc: 0.9129, acc: 0.8701, precision: 0.9396, recall: 0.8961, f1: 0.9174, edges-ner-ontonotes_loss: 0.0282
09/16 09:29:42 AM: Update 8722: task edges-ner-ontonotes, batch 722 (8722): mcc: 0.9142, acc: 0.8718, precision: 0.9404, recall: 0.8978, f1: 0.9186, edges-ner-ontonotes_loss: 0.0275
09/16 09:29:52 AM: Update 8845: task edges-ner-ontonotes, batch 845 (8845): mcc: 0.9156, acc: 0.8736, precision: 0.9412, recall: 0.8996, f1: 0.9199, edges-ner-ontonotes_loss: 0.0272
09/16 09:30:02 AM: Update 8973: task edges-ner-ontonotes, batch 973 (8973): mcc: 0.9159, acc: 0.8739, precision: 0.9413, recall: 0.9000, f1: 0.9202, edges-ner-ontonotes_loss: 0.0269
09/16 09:30:04 AM: ***** Step 9000 / Validation 9 *****
09/16 09:30:04 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:30:04 AM: Validating...
09/16 09:30:12 AM: Evaluate: task edges-ner-ontonotes, batch 65 (157): mcc: 0.9006, acc: 0.8618, precision: 0.9324, recall: 0.8802, f1: 0.9056, edges-ner-ontonotes_loss: 0.0348
