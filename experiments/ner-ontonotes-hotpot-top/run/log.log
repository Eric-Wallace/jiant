09/16 09:13:23 AM: Git branch: master
09/16 09:13:23 AM: Git SHA: 3ca0f74688379229ab3eec908a215358ad18b3f4
09/16 09:13:23 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-hotpot-top/",
  "exp_name": "experiments/ner-ontonotes-hotpot-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-hotpot-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/hotpot",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-hotpot-top__run",
  "run_dir": "./experiments/ner-ontonotes-hotpot-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:13:23 AM: Saved config to ./experiments/ner-ontonotes-hotpot-top/run/params.conf
09/16 09:13:23 AM: Using random seed 1234
09/16 09:13:58 AM: Using GPU 0
09/16 09:13:58 AM: Loading tasks...
09/16 09:13:58 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-hotpot-top/
09/16 09:13:58 AM: 	Creating task edges-ner-ontonotes from scratch.
09/16 09:13:59 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 09:13:59 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 09:13:59 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 09:14:00 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 09:14:00 AM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 09:14:00 AM: 	Building vocab from scratch.
09/16 09:14:00 AM: 	Counting units for task edges-ner-ontonotes.
09/16 09:14:02 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 09:14:03 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:03 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:14:04 AM: 	Saved vocab to ./experiments/ner-ontonotes-hotpot-top/vocab
09/16 09:14:04 AM: Loading token dictionary from ./experiments/ner-ontonotes-hotpot-top/vocab.
09/16 09:14:04 AM: 	Loaded vocab from ./experiments/ner-ontonotes-hotpot-top/vocab
09/16 09:14:04 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 09:14:04 AM: 	Vocab namespace tokens: size 22840
09/16 09:14:04 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:14:04 AM: 	Vocab namespace chars: size 77
09/16 09:14:04 AM: 	Finished building vocab.
09/16 09:14:04 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 09:14:14 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-hotpot-top/preproc/edges-ner-ontonotes__train_data
09/16 09:14:14 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 09:14:16 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-hotpot-top/preproc/edges-ner-ontonotes__val_data
09/16 09:14:16 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 09:14:17 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-hotpot-top/preproc/edges-ner-ontonotes__test_data
09/16 09:14:17 AM: 	Finished indexing tasks
09/16 09:14:17 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 09:14:17 AM: 	  Training on 
09/16 09:14:17 AM: 	  Evaluating on edges-ner-ontonotes
09/16 09:14:17 AM: 	Finished loading tasks in 19.016s
09/16 09:14:17 AM: 	 Tasks: ['edges-ner-ontonotes']
09/16 09:14:17 AM: Building model...
09/16 09:14:17 AM: Using BERT model (bert-base-uncased).
09/16 09:14:17 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:14:17 AM: models/hotpot
09/16 09:14:17 AM: loading configuration file models/hotpot/config.json
09/16 09:14:17 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:14:17 AM: loading weights file models/hotpot/pytorch_model.bin
09/16 09:14:22 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpgpcjysuf
09/16 09:14:29 AM: copying /tmp/tmpgpcjysuf to cache at ./experiments/ner-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: creating metadata file for ./experiments/ner-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: removing temp file /tmp/tmpgpcjysuf
09/16 09:14:29 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-hotpot-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:30 AM: Initializing parameters
09/16 09:14:30 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:14:30 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 09:14:53 AM: Model specification:
09/16 09:14:53 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 09:14:53 AM: Model parameters:
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:14:53 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 09:14:53 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 09:14:53 AM: Total number of parameters: 109688338 (1.09688e+08)
09/16 09:14:53 AM: Number of trainable parameters: 206098 (206098)
09/16 09:14:53 AM: Finished building model in 36.442s
09/16 09:14:53 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 09:14:56 AM: patience = 9
09/16 09:14:56 AM: val_interval = 1000
09/16 09:14:56 AM: max_vals = 250
09/16 09:14:56 AM: cuda_device = 0
09/16 09:14:56 AM: grad_norm = 5.0
09/16 09:14:56 AM: grad_clipping = None
09/16 09:14:56 AM: lr_decay = 0.99
09/16 09:14:56 AM: min_lr = 1e-06
09/16 09:14:56 AM: keep_all_checkpoints = 0
09/16 09:14:56 AM: val_data_limit = 5000
09/16 09:14:56 AM: max_epochs = -1
09/16 09:14:56 AM: dec_val_scale = 250
09/16 09:14:56 AM: training_data_fraction = 1
09/16 09:14:56 AM: type = adam
09/16 09:14:56 AM: parameter_groups = None
09/16 09:14:56 AM: Number of trainable parameters: 206098
09/16 09:14:56 AM: infer_type_and_cast = True
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: lr = 0.0001
09/16 09:14:56 AM: amsgrad = True
09/16 09:14:56 AM: type = reduce_on_plateau
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: mode = max
09/16 09:14:56 AM: factor = 0.5
09/16 09:14:56 AM: patience = 3
09/16 09:14:56 AM: threshold = 0.0001
09/16 09:14:56 AM: threshold_mode = abs
09/16 09:14:56 AM: verbose = True
09/16 09:14:56 AM: type = adam
09/16 09:14:56 AM: parameter_groups = None
09/16 09:14:56 AM: Number of trainable parameters: 206098
09/16 09:14:56 AM: infer_type_and_cast = True
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: lr = 0.0001
09/16 09:14:56 AM: amsgrad = True
09/16 09:14:56 AM: type = reduce_on_plateau
09/16 09:14:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:14:56 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:14:56 AM: mode = max
09/16 09:14:56 AM: factor = 0.5
09/16 09:14:56 AM: patience = 3
09/16 09:14:56 AM: threshold = 0.0001
09/16 09:14:56 AM: threshold_mode = abs
09/16 09:14:56 AM: verbose = True
09/16 09:14:56 AM: Starting training without restoring from a checkpoint.
09/16 09:14:56 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 09:14:56 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 09:15:06 AM: Update 41: task edges-ner-ontonotes, batch 41 (41): mcc: -0.0089, acc: 0.0017, precision: 0.0490, recall: 0.0769, f1: 0.0598, edges-ner-ontonotes_loss: 0.3774
09/16 09:15:16 AM: Update 201: task edges-ner-ontonotes, batch 201 (201): mcc: 0.0366, acc: 0.0260, precision: 0.1130, recall: 0.0424, f1: 0.0617, edges-ner-ontonotes_loss: 0.2108
09/16 09:15:26 AM: Update 318: task edges-ner-ontonotes, batch 318 (318): mcc: 0.0959, acc: 0.0546, precision: 0.2292, recall: 0.0650, f1: 0.1012, edges-ner-ontonotes_loss: 0.1893
09/16 09:15:36 AM: Update 458: task edges-ner-ontonotes, batch 458 (458): mcc: 0.1703, acc: 0.0895, precision: 0.3834, recall: 0.0963, f1: 0.1540, edges-ner-ontonotes_loss: 0.1739
09/16 09:15:46 AM: Update 596: task edges-ner-ontonotes, batch 596 (596): mcc: 0.2355, acc: 0.1235, precision: 0.5060, recall: 0.1288, f1: 0.2053, edges-ner-ontonotes_loss: 0.1632
09/16 09:15:56 AM: Update 709: task edges-ner-ontonotes, batch 709 (709): mcc: 0.2813, acc: 0.1502, precision: 0.5779, recall: 0.1560, f1: 0.2456, edges-ner-ontonotes_loss: 0.1568
09/16 09:16:06 AM: Update 853: task edges-ner-ontonotes, batch 853 (853): mcc: 0.3324, acc: 0.1840, precision: 0.6447, recall: 0.1906, f1: 0.2942, edges-ner-ontonotes_loss: 0.1491
09/16 09:16:16 AM: Update 968: task edges-ner-ontonotes, batch 968 (968): mcc: 0.3699, acc: 0.2121, precision: 0.6843, recall: 0.2197, f1: 0.3326, edges-ner-ontonotes_loss: 0.1438
09/16 09:16:19 AM: ***** Step 1000 / Validation 1 *****
09/16 09:16:19 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:16:19 AM: Validating...
09/16 09:16:33 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.5425, acc: 0.3756, precision: 0.8022, recall: 0.3891, f1: 0.5241, edges-ner-ontonotes_loss: 0.1177
09/16 09:16:45 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.5691, acc: 0.4060, precision: 0.8139, recall: 0.4204, f1: 0.5544, edges-ner-ontonotes_loss: 0.1111
09/16 09:16:50 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:16:51 AM: Best result seen so far for micro.
09/16 09:16:51 AM: Best result seen so far for macro.
09/16 09:16:51 AM: Updating LR scheduler:
09/16 09:16:51 AM: 	Best result seen so far for macro_avg: 0.571
09/16 09:16:51 AM: 	# validation passes without improvement: 0
09/16 09:16:51 AM: edges-ner-ontonotes_loss: training: 0.142597 validation: 0.107455
09/16 09:16:51 AM: macro_avg: validation: 0.570733
09/16 09:16:51 AM: micro_avg: validation: 0.000000
09/16 09:16:51 AM: edges-ner-ontonotes_mcc: training: 0.378349 validation: 0.584989
09/16 09:16:51 AM: edges-ner-ontonotes_acc: training: 0.218556 validation: 0.421216
09/16 09:16:51 AM: edges-ner-ontonotes_precision: training: 0.692613 validation: 0.827879
09/16 09:16:51 AM: edges-ner-ontonotes_recall: training: 0.226533 validation: 0.435472
09/16 09:16:51 AM: edges-ner-ontonotes_f1: training: 0.341403 validation: 0.570733
09/16 09:16:51 AM: Global learning rate: 0.0001
09/16 09:16:51 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:16:55 AM: Update 1049: task edges-ner-ontonotes, batch 49 (1049): mcc: 0.5834, acc: 0.4128, precision: 0.8345, recall: 0.4294, f1: 0.5670, edges-ner-ontonotes_loss: 0.1003
09/16 09:17:05 AM: Update 1187: task edges-ner-ontonotes, batch 187 (1187): mcc: 0.5980, acc: 0.4327, precision: 0.8356, recall: 0.4499, f1: 0.5849, edges-ner-ontonotes_loss: 0.0999
09/16 09:17:15 AM: Update 1296: task edges-ner-ontonotes, batch 296 (1296): mcc: 0.5890, acc: 0.4246, precision: 0.8269, recall: 0.4419, f1: 0.5760, edges-ner-ontonotes_loss: 0.1015
09/16 09:17:25 AM: Update 1436: task edges-ner-ontonotes, batch 436 (1436): mcc: 0.5756, acc: 0.4110, precision: 0.8173, recall: 0.4280, f1: 0.5618, edges-ner-ontonotes_loss: 0.1048
09/16 09:17:35 AM: Update 1562: task edges-ner-ontonotes, batch 562 (1562): mcc: 0.5738, acc: 0.4096, precision: 0.8152, recall: 0.4265, f1: 0.5600, edges-ner-ontonotes_loss: 0.1055
09/16 09:17:45 AM: Update 1732: task edges-ner-ontonotes, batch 732 (1732): mcc: 0.5838, acc: 0.4214, precision: 0.8209, recall: 0.4377, f1: 0.5709, edges-ner-ontonotes_loss: 0.1038
09/16 09:17:55 AM: Update 1870: task edges-ner-ontonotes, batch 870 (1870): mcc: 0.5896, acc: 0.4286, precision: 0.8225, recall: 0.4453, f1: 0.5778, edges-ner-ontonotes_loss: 0.1025
09/16 09:18:05 AM: ***** Step 2000 / Validation 2 *****
09/16 09:18:05 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:18:05 AM: Validating...
09/16 09:18:05 AM: Evaluate: task edges-ner-ontonotes, batch 9 (157): mcc: 0.5826, acc: 0.4507, precision: 0.7826, recall: 0.4592, f1: 0.5788, edges-ner-ontonotes_loss: 0.1114
09/16 09:18:15 AM: Evaluate: task edges-ner-ontonotes, batch 101 (157): mcc: 0.7020, acc: 0.5682, precision: 0.8759, recall: 0.5833, f1: 0.7003, edges-ner-ontonotes_loss: 0.0845
09/16 09:18:22 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:18:22 AM: Best result seen so far for macro.
09/16 09:18:22 AM: Updating LR scheduler:
09/16 09:18:22 AM: 	Best result seen so far for macro_avg: 0.691
09/16 09:18:22 AM: 	# validation passes without improvement: 0
09/16 09:18:22 AM: edges-ner-ontonotes_loss: training: 0.101024 validation: 0.083766
09/16 09:18:22 AM: macro_avg: validation: 0.690675
09/16 09:18:22 AM: micro_avg: validation: 0.000000
09/16 09:18:22 AM: edges-ner-ontonotes_mcc: training: 0.597975 validation: 0.692297
09/16 09:18:22 AM: edges-ner-ontonotes_acc: training: 0.439566 validation: 0.556794
09/16 09:18:22 AM: edges-ner-ontonotes_precision: training: 0.823145 validation: 0.867561
09/16 09:18:22 AM: edges-ner-ontonotes_recall: training: 0.457243 validation: 0.573703
09/16 09:18:22 AM: edges-ner-ontonotes_f1: training: 0.587911 validation: 0.690675
09/16 09:18:22 AM: Global learning rate: 0.0001
09/16 09:18:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:18:25 AM: Update 2039: task edges-ner-ontonotes, batch 39 (2039): mcc: 0.6648, acc: 0.5213, precision: 0.8396, recall: 0.5494, f1: 0.6642, edges-ner-ontonotes_loss: 0.0869
09/16 09:18:35 AM: Update 2173: task edges-ner-ontonotes, batch 173 (2173): mcc: 0.6694, acc: 0.5291, precision: 0.8433, recall: 0.5541, f1: 0.6688, edges-ner-ontonotes_loss: 0.0862
09/16 09:18:46 AM: Update 2284: task edges-ner-ontonotes, batch 284 (2284): mcc: 0.6755, acc: 0.5375, precision: 0.8433, recall: 0.5640, f1: 0.6759, edges-ner-ontonotes_loss: 0.0852
09/16 09:18:56 AM: Update 2416: task edges-ner-ontonotes, batch 416 (2416): mcc: 0.6821, acc: 0.5472, precision: 0.8429, recall: 0.5748, f1: 0.6835, edges-ner-ontonotes_loss: 0.0836
09/16 09:19:06 AM: Update 2524: task edges-ner-ontonotes, batch 524 (2524): mcc: 0.6853, acc: 0.5513, precision: 0.8438, recall: 0.5793, f1: 0.6870, edges-ner-ontonotes_loss: 0.0828
09/16 09:19:16 AM: Update 2664: task edges-ner-ontonotes, batch 664 (2664): mcc: 0.6879, acc: 0.5552, precision: 0.8441, recall: 0.5834, f1: 0.6900, edges-ner-ontonotes_loss: 0.0823
09/16 09:19:26 AM: Update 2800: task edges-ner-ontonotes, batch 800 (2800): mcc: 0.6912, acc: 0.5598, precision: 0.8446, recall: 0.5885, f1: 0.6936, edges-ner-ontonotes_loss: 0.0815
09/16 09:19:36 AM: Update 2907: task edges-ner-ontonotes, batch 907 (2907): mcc: 0.6825, acc: 0.5497, precision: 0.8384, recall: 0.5788, f1: 0.6848, edges-ner-ontonotes_loss: 0.0834
09/16 09:19:42 AM: ***** Step 3000 / Validation 3 *****
09/16 09:19:42 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:19:42 AM: Validating...
09/16 09:19:46 AM: Evaluate: task edges-ner-ontonotes, batch 39 (157): mcc: 0.7049, acc: 0.5871, precision: 0.8591, recall: 0.6002, f1: 0.7067, edges-ner-ontonotes_loss: 0.0827
09/16 09:19:56 AM: Evaluate: task edges-ner-ontonotes, batch 117 (157): mcc: 0.7359, acc: 0.6182, precision: 0.8831, recall: 0.6333, f1: 0.7376, edges-ner-ontonotes_loss: 0.0755
09/16 09:20:01 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:20:01 AM: Best result seen so far for macro.
09/16 09:20:01 AM: Updating LR scheduler:
09/16 09:20:01 AM: 	Best result seen so far for macro_avg: 0.730
09/16 09:20:01 AM: 	# validation passes without improvement: 0
09/16 09:20:01 AM: edges-ner-ontonotes_loss: training: 0.084542 validation: 0.075907
09/16 09:20:01 AM: macro_avg: validation: 0.730152
09/16 09:20:01 AM: micro_avg: validation: 0.000000
09/16 09:20:01 AM: edges-ner-ontonotes_mcc: training: 0.678842 validation: 0.728755
09/16 09:20:01 AM: edges-ner-ontonotes_acc: training: 0.545262 validation: 0.608204
09/16 09:20:01 AM: edges-ner-ontonotes_precision: training: 0.836099 validation: 0.879568
09/16 09:20:01 AM: edges-ner-ontonotes_recall: training: 0.574479 validation: 0.624128
09/16 09:20:01 AM: edges-ner-ontonotes_f1: training: 0.681028 validation: 0.730152
09/16 09:20:01 AM: Global learning rate: 0.0001
09/16 09:20:01 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:20:06 AM: Update 3081: task edges-ner-ontonotes, batch 81 (3081): mcc: 0.6440, acc: 0.5032, precision: 0.8191, recall: 0.5305, f1: 0.6440, edges-ner-ontonotes_loss: 0.0959
09/16 09:20:16 AM: Update 3208: task edges-ner-ontonotes, batch 208 (3208): mcc: 0.6497, acc: 0.5120, precision: 0.8215, recall: 0.5379, f1: 0.6501, edges-ner-ontonotes_loss: 0.0928
09/16 09:20:26 AM: Update 3374: task edges-ner-ontonotes, batch 374 (3374): mcc: 0.6647, acc: 0.5274, precision: 0.8321, recall: 0.5544, f1: 0.6654, edges-ner-ontonotes_loss: 0.0893
09/16 09:20:36 AM: Update 3497: task edges-ner-ontonotes, batch 497 (3497): mcc: 0.6707, acc: 0.5368, precision: 0.8335, recall: 0.5631, f1: 0.6721, edges-ner-ontonotes_loss: 0.0876
09/16 09:20:46 AM: Update 3630: task edges-ner-ontonotes, batch 630 (3630): mcc: 0.6780, acc: 0.5466, precision: 0.8359, recall: 0.5733, f1: 0.6801, edges-ner-ontonotes_loss: 0.0861
09/16 09:20:56 AM: Update 3739: task edges-ner-ontonotes, batch 739 (3739): mcc: 0.6821, acc: 0.5517, precision: 0.8375, recall: 0.5787, f1: 0.6844, edges-ner-ontonotes_loss: 0.0850
09/16 09:21:06 AM: Update 3869: task edges-ner-ontonotes, batch 869 (3869): mcc: 0.6887, acc: 0.5600, precision: 0.8394, recall: 0.5881, f1: 0.6916, edges-ner-ontonotes_loss: 0.0835
09/16 09:21:16 AM: ***** Step 4000 / Validation 4 *****
09/16 09:21:16 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:21:16 AM: Validating...
09/16 09:21:16 AM: Evaluate: task edges-ner-ontonotes, batch 5 (157): mcc: 0.6027, acc: 0.5015, precision: 0.7457, recall: 0.5164, f1: 0.6102, edges-ner-ontonotes_loss: 0.1105
09/16 09:21:26 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.7528, acc: 0.6512, precision: 0.8786, recall: 0.6650, f1: 0.7570, edges-ner-ontonotes_loss: 0.0747
09/16 09:21:34 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:21:34 AM: Best result seen so far for macro.
09/16 09:21:34 AM: Updating LR scheduler:
09/16 09:21:34 AM: 	Best result seen so far for macro_avg: 0.761
09/16 09:21:34 AM: 	# validation passes without improvement: 0
09/16 09:21:34 AM: edges-ner-ontonotes_loss: training: 0.081945 validation: 0.070566
09/16 09:21:34 AM: macro_avg: validation: 0.761322
09/16 09:21:34 AM: micro_avg: validation: 0.000000
09/16 09:21:34 AM: edges-ner-ontonotes_mcc: training: 0.696081 validation: 0.757259
09/16 09:21:34 AM: edges-ner-ontonotes_acc: training: 0.568810 validation: 0.653700
09/16 09:21:34 AM: edges-ner-ontonotes_precision: training: 0.842785 validation: 0.882777
09/16 09:21:34 AM: edges-ner-ontonotes_recall: training: 0.597791 validation: 0.669245
09/16 09:21:34 AM: edges-ner-ontonotes_f1: training: 0.699456 validation: 0.761322
09/16 09:21:34 AM: Global learning rate: 0.0001
09/16 09:21:34 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:21:36 AM: Update 4026: task edges-ner-ontonotes, batch 26 (4026): mcc: 0.7370, acc: 0.6203, precision: 0.8535, recall: 0.6581, f1: 0.7432, edges-ner-ontonotes_loss: 0.0719
09/16 09:21:46 AM: Update 4128: task edges-ner-ontonotes, batch 128 (4128): mcc: 0.7293, acc: 0.6116, precision: 0.8534, recall: 0.6452, f1: 0.7348, edges-ner-ontonotes_loss: 0.0730
09/16 09:21:56 AM: Update 4256: task edges-ner-ontonotes, batch 256 (4256): mcc: 0.7297, acc: 0.6130, precision: 0.8522, recall: 0.6469, f1: 0.7354, edges-ner-ontonotes_loss: 0.0730
09/16 09:22:07 AM: Update 4374: task edges-ner-ontonotes, batch 374 (4374): mcc: 0.7234, acc: 0.6055, precision: 0.8482, recall: 0.6394, f1: 0.7291, edges-ner-ontonotes_loss: 0.0739
09/16 09:22:17 AM: Update 4512: task edges-ner-ontonotes, batch 512 (4512): mcc: 0.7074, acc: 0.5847, precision: 0.8385, recall: 0.6198, f1: 0.7128, edges-ner-ontonotes_loss: 0.0788
09/16 09:22:27 AM: Update 4650: task edges-ner-ontonotes, batch 650 (4650): mcc: 0.6971, acc: 0.5725, precision: 0.8320, recall: 0.6078, f1: 0.7024, edges-ner-ontonotes_loss: 0.0816
09/16 09:22:37 AM: Update 4792: task edges-ner-ontonotes, batch 792 (4792): mcc: 0.6969, acc: 0.5720, precision: 0.8332, recall: 0.6064, f1: 0.7019, edges-ner-ontonotes_loss: 0.0818
09/16 09:22:47 AM: Update 4953: task edges-ner-ontonotes, batch 953 (4953): mcc: 0.6972, acc: 0.5724, precision: 0.8340, recall: 0.6063, f1: 0.7022, edges-ner-ontonotes_loss: 0.0816
09/16 09:22:52 AM: ***** Step 5000 / Validation 5 *****
09/16 09:22:53 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:22:53 AM: Validating...
09/16 09:22:57 AM: Evaluate: task edges-ner-ontonotes, batch 40 (157): mcc: 0.7452, acc: 0.6455, precision: 0.8655, recall: 0.6625, f1: 0.7505, edges-ner-ontonotes_loss: 0.0717
09/16 09:23:07 AM: Evaluate: task edges-ner-ontonotes, batch 117 (157): mcc: 0.7759, acc: 0.6752, precision: 0.8963, recall: 0.6901, f1: 0.7798, edges-ner-ontonotes_loss: 0.0653
09/16 09:23:12 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:23:12 AM: Best result seen so far for macro.
09/16 09:23:12 AM: Updating LR scheduler:
09/16 09:23:12 AM: 	Best result seen so far for macro_avg: 0.773
09/16 09:23:12 AM: 	# validation passes without improvement: 0
09/16 09:23:12 AM: edges-ner-ontonotes_loss: training: 0.081616 validation: 0.065791
09/16 09:23:12 AM: macro_avg: validation: 0.773388
09/16 09:23:12 AM: micro_avg: validation: 0.000000
09/16 09:23:12 AM: edges-ner-ontonotes_mcc: training: 0.696545 validation: 0.769733
09/16 09:23:12 AM: edges-ner-ontonotes_acc: training: 0.571627 validation: 0.665150
09/16 09:23:12 AM: edges-ner-ontonotes_precision: training: 0.833758 validation: 0.894129
09/16 09:23:12 AM: edges-ner-ontonotes_recall: training: 0.605406 validation: 0.681377
09/16 09:23:12 AM: edges-ner-ontonotes_f1: training: 0.701466 validation: 0.773388
09/16 09:23:12 AM: Global learning rate: 0.0001
09/16 09:23:12 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:23:17 AM: Update 5070: task edges-ner-ontonotes, batch 70 (5070): mcc: 0.7080, acc: 0.5891, precision: 0.8390, recall: 0.6205, f1: 0.7134, edges-ner-ontonotes_loss: 0.0773
09/16 09:23:27 AM: Update 5205: task edges-ner-ontonotes, batch 205 (5205): mcc: 0.7066, acc: 0.5879, precision: 0.8350, recall: 0.6214, f1: 0.7125, edges-ner-ontonotes_loss: 0.0773
09/16 09:23:37 AM: Update 5327: task edges-ner-ontonotes, batch 327 (5327): mcc: 0.7168, acc: 0.5997, precision: 0.8426, recall: 0.6326, f1: 0.7226, edges-ner-ontonotes_loss: 0.0754
09/16 09:23:47 AM: Update 5461: task edges-ner-ontonotes, batch 461 (5461): mcc: 0.7264, acc: 0.6116, precision: 0.8480, recall: 0.6446, f1: 0.7324, edges-ner-ontonotes_loss: 0.0734
09/16 09:23:57 AM: Update 5589: task edges-ner-ontonotes, batch 589 (5589): mcc: 0.7312, acc: 0.6168, precision: 0.8515, recall: 0.6499, f1: 0.7372, edges-ner-ontonotes_loss: 0.0723
09/16 09:24:07 AM: Update 5685: task edges-ner-ontonotes, batch 685 (5685): mcc: 0.7314, acc: 0.6172, precision: 0.8510, recall: 0.6506, f1: 0.7374, edges-ner-ontonotes_loss: 0.0721
09/16 09:24:17 AM: Update 5826: task edges-ner-ontonotes, batch 826 (5826): mcc: 0.7344, acc: 0.6206, precision: 0.8528, recall: 0.6542, f1: 0.7404, edges-ner-ontonotes_loss: 0.0715
09/16 09:24:27 AM: Update 5943: task edges-ner-ontonotes, batch 943 (5943): mcc: 0.7311, acc: 0.6172, precision: 0.8501, recall: 0.6509, f1: 0.7373, edges-ner-ontonotes_loss: 0.0720
09/16 09:24:32 AM: ***** Step 6000 / Validation 6 *****
09/16 09:24:32 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:24:32 AM: Validating...
09/16 09:24:37 AM: Evaluate: task edges-ner-ontonotes, batch 57 (157): mcc: 0.7700, acc: 0.6722, precision: 0.8866, recall: 0.6878, f1: 0.7747, edges-ner-ontonotes_loss: 0.0682
09/16 09:24:47 AM: Evaluate: task edges-ner-ontonotes, batch 134 (157): mcc: 0.7838, acc: 0.6807, precision: 0.9048, recall: 0.6967, f1: 0.7872, edges-ner-ontonotes_loss: 0.0638
09/16 09:24:50 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:24:50 AM: Best result seen so far for macro.
09/16 09:24:50 AM: Updating LR scheduler:
09/16 09:24:50 AM: 	Best result seen so far for macro_avg: 0.785
09/16 09:24:50 AM: 	# validation passes without improvement: 0
09/16 09:24:50 AM: edges-ner-ontonotes_loss: training: 0.073081 validation: 0.063775
09/16 09:24:50 AM: macro_avg: validation: 0.784635
09/16 09:24:50 AM: micro_avg: validation: 0.000000
09/16 09:24:50 AM: edges-ner-ontonotes_mcc: training: 0.727293 validation: 0.780960
09/16 09:24:50 AM: edges-ner-ontonotes_acc: training: 0.612453 validation: 0.676903
09/16 09:24:50 AM: edges-ner-ontonotes_precision: training: 0.847419 validation: 0.901407
09/16 09:24:50 AM: edges-ner-ontonotes_recall: training: 0.646523 validation: 0.694647
09/16 09:24:50 AM: edges-ner-ontonotes_f1: training: 0.733463 validation: 0.784635
09/16 09:24:50 AM: Global learning rate: 0.0001
09/16 09:24:50 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:24:57 AM: Update 6102: task edges-ner-ontonotes, batch 102 (6102): mcc: 0.6717, acc: 0.5414, precision: 0.8107, recall: 0.5817, f1: 0.6773, edges-ner-ontonotes_loss: 0.0883
09/16 09:25:08 AM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.6712, acc: 0.5414, precision: 0.8102, recall: 0.5812, f1: 0.6768, edges-ner-ontonotes_loss: 0.0890
09/16 09:25:18 AM: Update 6390: task edges-ner-ontonotes, batch 390 (6390): mcc: 0.6837, acc: 0.5567, precision: 0.8215, recall: 0.5934, f1: 0.6890, edges-ner-ontonotes_loss: 0.0848
09/16 09:25:29 AM: Update 6538: task edges-ner-ontonotes, batch 538 (6538): mcc: 0.6880, acc: 0.5626, precision: 0.8247, recall: 0.5981, f1: 0.6934, edges-ner-ontonotes_loss: 0.0831
09/16 09:25:39 AM: Update 6672: task edges-ner-ontonotes, batch 672 (6672): mcc: 0.6963, acc: 0.5740, precision: 0.8291, recall: 0.6086, f1: 0.7019, edges-ner-ontonotes_loss: 0.0815
09/16 09:25:49 AM: Update 6812: task edges-ner-ontonotes, batch 812 (6812): mcc: 0.7034, acc: 0.5825, precision: 0.8333, recall: 0.6172, f1: 0.7092, edges-ner-ontonotes_loss: 0.0799
09/16 09:25:59 AM: Update 6917: task edges-ner-ontonotes, batch 917 (6917): mcc: 0.7073, acc: 0.5874, precision: 0.8357, recall: 0.6219, f1: 0.7131, edges-ner-ontonotes_loss: 0.0788
09/16 09:26:06 AM: ***** Step 7000 / Validation 7 *****
09/16 09:26:06 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:26:06 AM: Validating...
09/16 09:26:09 AM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.7275, acc: 0.6324, precision: 0.8429, recall: 0.6505, f1: 0.7343, edges-ner-ontonotes_loss: 0.0784
09/16 09:26:21 AM: Evaluate: task edges-ner-ontonotes, batch 114 (157): mcc: 0.7742, acc: 0.6815, precision: 0.8799, recall: 0.7006, f1: 0.7801, edges-ner-ontonotes_loss: 0.0665
09/16 09:26:26 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:26:27 AM: Best result seen so far for macro.
09/16 09:26:27 AM: Updating LR scheduler:
09/16 09:26:27 AM: 	Best result seen so far for macro_avg: 0.788
09/16 09:26:27 AM: 	# validation passes without improvement: 0
09/16 09:26:27 AM: edges-ner-ontonotes_loss: training: 0.077849 validation: 0.063425
09/16 09:26:27 AM: macro_avg: validation: 0.787644
09/16 09:26:27 AM: micro_avg: validation: 0.000000
09/16 09:26:27 AM: edges-ner-ontonotes_mcc: training: 0.711654 validation: 0.782021
09/16 09:26:27 AM: edges-ner-ontonotes_acc: training: 0.592896 validation: 0.688960
09/16 09:26:27 AM: edges-ner-ontonotes_precision: training: 0.838291 validation: 0.886538
09/16 09:26:27 AM: edges-ner-ontonotes_recall: training: 0.627249 validation: 0.708599
09/16 09:26:27 AM: edges-ner-ontonotes_f1: training: 0.717575 validation: 0.787644
09/16 09:26:27 AM: Global learning rate: 0.0001
09/16 09:26:27 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:26:31 AM: Update 7053: task edges-ner-ontonotes, batch 53 (7053): mcc: 0.7589, acc: 0.6564, precision: 0.8559, recall: 0.6940, f1: 0.7665, edges-ner-ontonotes_loss: 0.0657
09/16 09:26:41 AM: Update 7164: task edges-ner-ontonotes, batch 164 (7164): mcc: 0.7538, acc: 0.6466, precision: 0.8561, recall: 0.6850, f1: 0.7611, edges-ner-ontonotes_loss: 0.0666
09/16 09:26:51 AM: Update 7295: task edges-ner-ontonotes, batch 295 (7295): mcc: 0.7517, acc: 0.6441, precision: 0.8569, recall: 0.6807, f1: 0.7587, edges-ner-ontonotes_loss: 0.0669
09/16 09:27:01 AM: Update 7426: task edges-ner-ontonotes, batch 426 (7426): mcc: 0.7495, acc: 0.6420, precision: 0.8541, recall: 0.6793, f1: 0.7567, edges-ner-ontonotes_loss: 0.0672
09/16 09:27:12 AM: Update 7540: task edges-ner-ontonotes, batch 540 (7540): mcc: 0.7383, acc: 0.6282, precision: 0.8459, recall: 0.6665, f1: 0.7456, edges-ner-ontonotes_loss: 0.0704
09/16 09:27:22 AM: Update 7670: task edges-ner-ontonotes, batch 670 (7670): mcc: 0.7272, acc: 0.6134, precision: 0.8409, recall: 0.6517, f1: 0.7343, edges-ner-ontonotes_loss: 0.0738
09/16 09:27:32 AM: Update 7787: task edges-ner-ontonotes, batch 787 (7787): mcc: 0.7217, acc: 0.6070, precision: 0.8370, recall: 0.6454, f1: 0.7288, edges-ner-ontonotes_loss: 0.0755
09/16 09:27:42 AM: Update 7952: task edges-ner-ontonotes, batch 952 (7952): mcc: 0.7204, acc: 0.6049, precision: 0.8375, recall: 0.6427, f1: 0.7273, edges-ner-ontonotes_loss: 0.0759
09/16 09:27:45 AM: ***** Step 8000 / Validation 8 *****
09/16 09:27:45 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:27:45 AM: Validating...
09/16 09:27:52 AM: Evaluate: task edges-ner-ontonotes, batch 66 (157): mcc: 0.7811, acc: 0.6786, precision: 0.8952, recall: 0.6999, f1: 0.7856, edges-ner-ontonotes_loss: 0.0630
09/16 09:28:02 AM: Evaluate: task edges-ner-ontonotes, batch 142 (157): mcc: 0.7793, acc: 0.6722, precision: 0.9039, recall: 0.6898, f1: 0.7825, edges-ner-ontonotes_loss: 0.0619
09/16 09:28:04 AM: Updating LR scheduler:
09/16 09:28:04 AM: 	Best result seen so far for macro_avg: 0.788
09/16 09:28:04 AM: 	# validation passes without improvement: 1
09/16 09:28:04 AM: edges-ner-ontonotes_loss: training: 0.075845 validation: 0.062495
09/16 09:28:04 AM: macro_avg: validation: 0.777615
09/16 09:28:04 AM: micro_avg: validation: 0.000000
09/16 09:28:04 AM: edges-ner-ontonotes_mcc: training: 0.720294 validation: 0.774385
09/16 09:28:04 AM: edges-ner-ontonotes_acc: training: 0.604600 validation: 0.666060
09/16 09:28:04 AM: edges-ner-ontonotes_precision: training: 0.837620 validation: 0.900339
09/16 09:28:04 AM: edges-ner-ontonotes_recall: training: 0.642461 validation: 0.684334
09/16 09:28:04 AM: edges-ner-ontonotes_f1: training: 0.727174 validation: 0.777615
09/16 09:28:04 AM: Global learning rate: 0.0001
09/16 09:28:04 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:28:12 AM: Update 8094: task edges-ner-ontonotes, batch 94 (8094): mcc: 0.7081, acc: 0.5897, precision: 0.8319, recall: 0.6263, f1: 0.7146, edges-ner-ontonotes_loss: 0.0765
09/16 09:28:23 AM: Update 8228: task edges-ner-ontonotes, batch 228 (8228): mcc: 0.7258, acc: 0.6085, precision: 0.8449, recall: 0.6459, f1: 0.7321, edges-ner-ontonotes_loss: 0.0738
09/16 09:28:33 AM: Update 8364: task edges-ner-ontonotes, batch 364 (8364): mcc: 0.7250, acc: 0.6095, precision: 0.8413, recall: 0.6476, f1: 0.7318, edges-ner-ontonotes_loss: 0.0737
09/16 09:28:43 AM: Update 8478: task edges-ner-ontonotes, batch 478 (8478): mcc: 0.7282, acc: 0.6133, precision: 0.8437, recall: 0.6511, f1: 0.7350, edges-ner-ontonotes_loss: 0.0728
09/16 09:28:53 AM: Update 8609: task edges-ner-ontonotes, batch 609 (8609): mcc: 0.7376, acc: 0.6245, precision: 0.8500, recall: 0.6621, f1: 0.7444, edges-ner-ontonotes_loss: 0.0708
09/16 09:29:03 AM: Update 8723: task edges-ner-ontonotes, batch 723 (8723): mcc: 0.7403, acc: 0.6283, precision: 0.8514, recall: 0.6656, f1: 0.7471, edges-ner-ontonotes_loss: 0.0701
09/16 09:29:13 AM: Update 8858: task edges-ner-ontonotes, batch 858 (8858): mcc: 0.7431, acc: 0.6317, precision: 0.8525, recall: 0.6695, f1: 0.7500, edges-ner-ontonotes_loss: 0.0693
09/16 09:29:23 AM: Update 8993: task edges-ner-ontonotes, batch 993 (8993): mcc: 0.7449, acc: 0.6338, precision: 0.8530, recall: 0.6721, f1: 0.7518, edges-ner-ontonotes_loss: 0.0689
09/16 09:29:24 AM: ***** Step 9000 / Validation 9 *****
09/16 09:29:24 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:29:24 AM: Validating...
09/16 09:29:33 AM: Evaluate: task edges-ner-ontonotes, batch 82 (157): mcc: 0.7743, acc: 0.6741, precision: 0.8895, recall: 0.6929, f1: 0.7790, edges-ner-ontonotes_loss: 0.0668
09/16 09:29:43 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:29:43 AM: Best result seen so far for macro.
09/16 09:29:43 AM: Updating LR scheduler:
09/16 09:29:43 AM: 	Best result seen so far for macro_avg: 0.797
09/16 09:29:43 AM: 	# validation passes without improvement: 0
09/16 09:29:43 AM: edges-ner-ontonotes_loss: training: 0.068959 validation: 0.059757
09/16 09:29:43 AM: macro_avg: validation: 0.797410
09/16 09:29:43 AM: micro_avg: validation: 0.000000
09/16 09:29:43 AM: edges-ner-ontonotes_mcc: training: 0.744814 validation: 0.792898
09/16 09:29:43 AM: edges-ner-ontonotes_acc: training: 0.633766 validation: 0.696012
09/16 09:29:43 AM: edges-ner-ontonotes_precision: training: 0.853069 validation: 0.902308
09/16 09:29:43 AM: edges-ner-ontonotes_recall: training: 0.671966 validation: 0.714362
09/16 09:29:43 AM: edges-ner-ontonotes_f1: training: 0.751764 validation: 0.797410
09/16 09:29:43 AM: Global learning rate: 0.0001
09/16 09:29:43 AM: Saving checkpoints to: ./experiments/ner-ontonotes-hotpot-top/run
09/16 09:29:44 AM: Update 9006: task edges-ner-ontonotes, batch 6 (9006): mcc: 0.7184, acc: 0.6130, precision: 0.8184, recall: 0.6550, f1: 0.7276, edges-ner-ontonotes_loss: 0.0756
09/16 09:29:54 AM: Update 9114: task edges-ner-ontonotes, batch 114 (9114): mcc: 0.6812, acc: 0.5600, precision: 0.8044, recall: 0.6025, f1: 0.6889, edges-ner-ontonotes_loss: 0.0833
09/16 09:30:04 AM: Update 9256: task edges-ner-ontonotes, batch 256 (9256): mcc: 0.6818, acc: 0.5589, precision: 0.8078, recall: 0.6009, f1: 0.6891, edges-ner-ontonotes_loss: 0.0852
09/16 09:30:14 AM: Update 9389: task edges-ner-ontonotes, batch 389 (9389): mcc: 0.6854, acc: 0.5623, precision: 0.8123, recall: 0.6032, f1: 0.6924, edges-ner-ontonotes_loss: 0.0840
