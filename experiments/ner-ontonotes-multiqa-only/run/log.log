09/16 03:34:04 PM: Git branch: master
09/16 03:34:04 PM: Git SHA: 93c1dfd555f3458ddbb66d458dfeca984f2d8527
09/16 03:34:04 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-multiqa-only/",
  "exp_name": "experiments/ner-ontonotes-multiqa-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-multiqa-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/multiqa",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/ner-ontonotes-multiqa-only__run",
  "run_dir": "./experiments/ner-ontonotes-multiqa-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 03:34:04 PM: Saved config to ./experiments/ner-ontonotes-multiqa-only/run/params.conf
09/16 03:34:04 PM: Using random seed 1234
09/16 03:34:28 PM: Using GPU 0
09/16 03:34:28 PM: Loading tasks...
09/16 03:34:28 PM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-multiqa-only/
09/16 03:34:28 PM: 	Creating task edges-ner-ontonotes from scratch.
09/16 03:34:30 PM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 03:34:30 PM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 03:34:30 PM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 03:34:30 PM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 03:34:30 PM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 03:34:30 PM: 	Building vocab from scratch.
09/16 03:34:30 PM: 	Counting units for task edges-ner-ontonotes.
09/16 03:34:33 PM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 03:34:34 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 03:34:34 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 03:34:35 PM: 	Saved vocab to ./experiments/ner-ontonotes-multiqa-only/vocab
09/16 03:34:35 PM: Loading token dictionary from ./experiments/ner-ontonotes-multiqa-only/vocab.
09/16 03:34:35 PM: 	Loaded vocab from ./experiments/ner-ontonotes-multiqa-only/vocab
09/16 03:34:35 PM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 03:34:35 PM: 	Vocab namespace tokens: size 22840
09/16 03:34:35 PM: 	Vocab namespace bert_uncased: size 30524
09/16 03:34:35 PM: 	Vocab namespace chars: size 77
09/16 03:34:35 PM: 	Finished building vocab.
09/16 03:34:35 PM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 03:34:53 PM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-multiqa-only/preproc/edges-ner-ontonotes__train_data
09/16 03:34:53 PM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 03:34:55 PM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-multiqa-only/preproc/edges-ner-ontonotes__val_data
09/16 03:34:55 PM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 03:34:56 PM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-multiqa-only/preproc/edges-ner-ontonotes__test_data
09/16 03:34:56 PM: 	Finished indexing tasks
09/16 03:34:56 PM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 03:34:56 PM: 	  Training on 
09/16 03:34:56 PM: 	  Evaluating on edges-ner-ontonotes
09/16 03:34:56 PM: 	Finished loading tasks in 27.567s
09/16 03:34:56 PM: 	 Tasks: ['edges-ner-ontonotes']
09/16 03:34:56 PM: Building model...
09/16 03:34:56 PM: Using BERT model (bert-base-uncased).
09/16 03:34:56 PM: LOADING A FUNETUNED MODEL from: 
09/16 03:34:56 PM: models/multiqa
09/16 03:34:56 PM: loading configuration file models/multiqa/config.json
09/16 03:34:56 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 03:34:56 PM: loading weights file models/multiqa/pytorch_model.bin
09/16 03:35:02 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp4a2nchsv
09/16 03:35:07 PM: copying /tmp/tmp4a2nchsv to cache at ./experiments/ner-ontonotes-multiqa-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 03:35:07 PM: creating metadata file for ./experiments/ner-ontonotes-multiqa-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 03:35:07 PM: removing temp file /tmp/tmp4a2nchsv
09/16 03:35:07 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-multiqa-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 03:35:07 PM: Initializing parameters
09/16 03:35:07 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 03:35:07 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 03:35:07 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 03:35:07 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 03:35:07 PM:    _text_field_embedder.model.pooler.dense.bias
09/16 03:35:07 PM:    _text_field_embedder.model.pooler.dense.weight
09/16 03:35:07 PM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 03:35:22 PM: Model specification:
09/16 03:35:22 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 03:35:22 PM: Model parameters:
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 03:35:22 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 03:35:22 PM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 03:35:22 PM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 03:35:22 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 03:35:22 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 03:35:22 PM: Total number of parameters: 109688338 (1.09688e+08)
09/16 03:35:22 PM: Number of trainable parameters: 206098 (206098)
09/16 03:35:22 PM: Finished building model in 25.635s
09/16 03:35:22 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 03:35:26 PM: patience = 9
09/16 03:35:27 PM: val_interval = 1000
09/16 03:35:27 PM: max_vals = 250
09/16 03:35:27 PM: cuda_device = 0
09/16 03:35:27 PM: grad_norm = 5.0
09/16 03:35:27 PM: grad_clipping = None
09/16 03:35:27 PM: lr_decay = 0.99
09/16 03:35:27 PM: min_lr = 1e-06
09/16 03:35:27 PM: keep_all_checkpoints = 0
09/16 03:35:27 PM: val_data_limit = 5000
09/16 03:35:27 PM: max_epochs = -1
09/16 03:35:27 PM: dec_val_scale = 250
09/16 03:35:27 PM: training_data_fraction = 1
09/16 03:35:27 PM: type = adam
09/16 03:35:27 PM: parameter_groups = None
09/16 03:35:27 PM: Number of trainable parameters: 206098
09/16 03:35:27 PM: infer_type_and_cast = True
09/16 03:35:27 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 03:35:27 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 03:35:27 PM: lr = 0.0001
09/16 03:35:27 PM: amsgrad = True
09/16 03:35:27 PM: type = reduce_on_plateau
09/16 03:35:27 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 03:35:27 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 03:35:27 PM: mode = max
09/16 03:35:27 PM: factor = 0.5
09/16 03:35:27 PM: patience = 3
09/16 03:35:27 PM: threshold = 0.0001
09/16 03:35:27 PM: threshold_mode = abs
09/16 03:35:27 PM: verbose = True
09/16 03:35:27 PM: type = adam
09/16 03:35:27 PM: parameter_groups = None
09/16 03:35:27 PM: Number of trainable parameters: 206098
09/16 03:35:27 PM: infer_type_and_cast = True
09/16 03:35:27 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 03:35:27 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 03:35:27 PM: lr = 0.0001
09/16 03:35:27 PM: amsgrad = True
09/16 03:35:27 PM: type = reduce_on_plateau
09/16 03:35:27 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 03:35:27 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 03:35:27 PM: mode = max
09/16 03:35:27 PM: factor = 0.5
09/16 03:35:27 PM: patience = 3
09/16 03:35:27 PM: threshold = 0.0001
09/16 03:35:27 PM: threshold_mode = abs
09/16 03:35:27 PM: verbose = True
09/16 03:35:27 PM: Starting training without restoring from a checkpoint.
09/16 03:35:27 PM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 03:35:27 PM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 03:35:37 PM: Update 284: task edges-ner-ontonotes, batch 284 (284): mcc: 0.4140, acc: 0.3729, precision: 0.4752, recall: 0.4155, f1: 0.4434, edges-ner-ontonotes_loss: 0.1827
09/16 03:35:47 PM: Update 496: task edges-ner-ontonotes, batch 496 (496): mcc: 0.5673, acc: 0.5060, precision: 0.6527, recall: 0.5312, f1: 0.5857, edges-ner-ontonotes_loss: 0.1370
09/16 03:35:57 PM: Update 729: task edges-ner-ontonotes, batch 729 (729): mcc: 0.6491, acc: 0.5805, precision: 0.7385, recall: 0.6016, f1: 0.6631, edges-ner-ontonotes_loss: 0.1128
09/16 03:36:07 PM: Update 955: task edges-ner-ontonotes, batch 955 (955): mcc: 0.6921, acc: 0.6222, precision: 0.7776, recall: 0.6438, f1: 0.7044, edges-ner-ontonotes_loss: 0.0988
09/16 03:36:08 PM: ***** Step 1000 / Validation 1 *****
09/16 03:36:08 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:36:08 PM: Validating...
09/16 03:36:15 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:36:15 PM: Best result seen so far for micro.
09/16 03:36:15 PM: Best result seen so far for macro.
09/16 03:36:15 PM: Updating LR scheduler:
09/16 03:36:15 PM: 	Best result seen so far for macro_avg: 0.821
09/16 03:36:15 PM: 	# validation passes without improvement: 0
09/16 03:36:15 PM: edges-ner-ontonotes_loss: training: 0.096862 validation: 0.058286
09/16 03:36:15 PM: macro_avg: validation: 0.821171
09/16 03:36:15 PM: micro_avg: validation: 0.000000
09/16 03:36:15 PM: edges-ner-ontonotes_mcc: training: 0.697850 validation: 0.813688
09/16 03:36:15 PM: edges-ner-ontonotes_acc: training: 0.627714 validation: 0.746057
09/16 03:36:15 PM: edges-ner-ontonotes_precision: training: 0.782717 validation: 0.883493
09/16 03:36:15 PM: edges-ner-ontonotes_recall: training: 0.649454 validation: 0.767061
09/16 03:36:15 PM: edges-ner-ontonotes_f1: training: 0.709885 validation: 0.821171
09/16 03:36:15 PM: Global learning rate: 0.0001
09/16 03:36:15 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:36:17 PM: Update 1044: task edges-ner-ontonotes, batch 44 (1044): mcc: 0.8308, acc: 0.7661, precision: 0.8897, recall: 0.7924, f1: 0.8382, edges-ner-ontonotes_loss: 0.0483
09/16 03:36:27 PM: Update 1259: task edges-ner-ontonotes, batch 259 (1259): mcc: 0.8276, acc: 0.7611, precision: 0.8904, recall: 0.7859, f1: 0.8349, edges-ner-ontonotes_loss: 0.0490
09/16 03:36:37 PM: Update 1552: task edges-ner-ontonotes, batch 552 (1552): mcc: 0.8199, acc: 0.7539, precision: 0.8887, recall: 0.7735, f1: 0.8271, edges-ner-ontonotes_loss: 0.0548
09/16 03:36:47 PM: Update 1797: task edges-ner-ontonotes, batch 797 (1797): mcc: 0.8287, acc: 0.7669, precision: 0.8946, recall: 0.7840, f1: 0.8357, edges-ner-ontonotes_loss: 0.0522
09/16 03:36:56 PM: ***** Step 2000 / Validation 2 *****
09/16 03:36:56 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:36:56 PM: Validating...
09/16 03:36:57 PM: Evaluate: task edges-ner-ontonotes, batch 32 (157): mcc: 0.8182, acc: 0.7740, precision: 0.8805, recall: 0.7778, f1: 0.8260, edges-ner-ontonotes_loss: 0.0565
09/16 03:37:01 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:37:01 PM: Best result seen so far for macro.
09/16 03:37:01 PM: Updating LR scheduler:
09/16 03:37:01 PM: 	Best result seen so far for macro_avg: 0.843
09/16 03:37:01 PM: 	# validation passes without improvement: 0
09/16 03:37:01 PM: edges-ner-ontonotes_loss: training: 0.050671 validation: 0.049392
09/16 03:37:01 PM: macro_avg: validation: 0.843041
09/16 03:37:01 PM: micro_avg: validation: 0.000000
09/16 03:37:01 PM: edges-ner-ontonotes_mcc: training: 0.833043 validation: 0.835805
09/16 03:37:01 PM: edges-ner-ontonotes_acc: training: 0.773709 validation: 0.785866
09/16 03:37:01 PM: edges-ner-ontonotes_precision: training: 0.896607 validation: 0.892903
09/16 03:37:01 PM: edges-ner-ontonotes_recall: training: 0.790076 validation: 0.798453
09/16 03:37:01 PM: edges-ner-ontonotes_f1: training: 0.839977 validation: 0.843041
09/16 03:37:01 PM: Global learning rate: 0.0001
09/16 03:37:01 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:37:07 PM: Update 2099: task edges-ner-ontonotes, batch 99 (2099): mcc: 0.8630, acc: 0.8132, precision: 0.9172, recall: 0.8255, f1: 0.8689, edges-ner-ontonotes_loss: 0.0413
09/16 03:37:17 PM: Update 2307: task edges-ner-ontonotes, batch 307 (2307): mcc: 0.8522, acc: 0.8001, precision: 0.9034, recall: 0.8186, f1: 0.8589, edges-ner-ontonotes_loss: 0.0429
09/16 03:37:27 PM: Update 2524: task edges-ner-ontonotes, batch 524 (2524): mcc: 0.8535, acc: 0.8014, precision: 0.9009, recall: 0.8233, f1: 0.8604, edges-ner-ontonotes_loss: 0.0420
09/16 03:37:37 PM: Update 2795: task edges-ner-ontonotes, batch 795 (2795): mcc: 0.8545, acc: 0.8018, precision: 0.9004, recall: 0.8256, f1: 0.8614, edges-ner-ontonotes_loss: 0.0414
09/16 03:37:45 PM: ***** Step 3000 / Validation 3 *****
09/16 03:37:45 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:37:45 PM: Validating...
09/16 03:37:47 PM: Evaluate: task edges-ner-ontonotes, batch 31 (157): mcc: 0.8340, acc: 0.7838, precision: 0.8808, recall: 0.8064, f1: 0.8420, edges-ner-ontonotes_loss: 0.0545
09/16 03:37:50 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:37:50 PM: Best result seen so far for macro.
09/16 03:37:50 PM: Updating LR scheduler:
09/16 03:37:50 PM: 	Best result seen so far for macro_avg: 0.852
09/16 03:37:50 PM: 	# validation passes without improvement: 0
09/16 03:37:50 PM: edges-ner-ontonotes_loss: training: 0.043646 validation: 0.047361
09/16 03:37:50 PM: macro_avg: validation: 0.852123
09/16 03:37:50 PM: micro_avg: validation: 0.000000
09/16 03:37:50 PM: edges-ner-ontonotes_mcc: training: 0.850915 validation: 0.845253
09/16 03:37:50 PM: edges-ner-ontonotes_acc: training: 0.798076 validation: 0.796785
09/16 03:37:50 PM: edges-ner-ontonotes_precision: training: 0.898652 validation: 0.900110
09/16 03:37:50 PM: edges-ner-ontonotes_recall: training: 0.820678 validation: 0.808993
09/16 03:37:50 PM: edges-ner-ontonotes_f1: training: 0.857897 validation: 0.852123
09/16 03:37:50 PM: Global learning rate: 0.0001
09/16 03:37:50 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:37:57 PM: Update 3138: task edges-ner-ontonotes, batch 138 (3138): mcc: 0.8406, acc: 0.7890, precision: 0.8959, recall: 0.8044, f1: 0.8477, edges-ner-ontonotes_loss: 0.0501
09/16 03:38:08 PM: Update 3426: task edges-ner-ontonotes, batch 426 (3426): mcc: 0.8649, acc: 0.8221, precision: 0.9104, recall: 0.8354, f1: 0.8713, edges-ner-ontonotes_loss: 0.0421
09/16 03:38:18 PM: Update 3705: task edges-ner-ontonotes, batch 705 (3705): mcc: 0.8650, acc: 0.8209, precision: 0.9106, recall: 0.8354, f1: 0.8714, edges-ner-ontonotes_loss: 0.0414
09/16 03:38:28 PM: Update 3928: task edges-ner-ontonotes, batch 928 (3928): mcc: 0.8638, acc: 0.8187, precision: 0.9074, recall: 0.8362, f1: 0.8703, edges-ner-ontonotes_loss: 0.0410
09/16 03:38:30 PM: ***** Step 4000 / Validation 4 *****
09/16 03:38:34 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:38:34 PM: Validating...
09/16 03:38:38 PM: Evaluate: task edges-ner-ontonotes, batch 138 (157): mcc: 0.8500, acc: 0.8089, precision: 0.8941, recall: 0.8233, f1: 0.8572, edges-ner-ontonotes_loss: 0.0465
09/16 03:38:41 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:38:41 PM: Best result seen so far for macro.
09/16 03:38:41 PM: Updating LR scheduler:
09/16 03:38:41 PM: 	Best result seen so far for macro_avg: 0.859
09/16 03:38:41 PM: 	# validation passes without improvement: 0
09/16 03:38:41 PM: edges-ner-ontonotes_loss: training: 0.040581 validation: 0.045530
09/16 03:38:41 PM: macro_avg: validation: 0.859102
09/16 03:38:41 PM: micro_avg: validation: 0.000000
09/16 03:38:41 PM: edges-ner-ontonotes_mcc: training: 0.864545 validation: 0.851903
09/16 03:38:41 PM: edges-ner-ontonotes_acc: training: 0.819330 validation: 0.811344
09/16 03:38:41 PM: edges-ner-ontonotes_precision: training: 0.907509 validation: 0.894460
09/16 03:38:41 PM: edges-ner-ontonotes_recall: training: 0.837369 validation: 0.826433
09/16 03:38:41 PM: edges-ner-ontonotes_f1: training: 0.871029 validation: 0.859102
09/16 03:38:41 PM: Global learning rate: 0.0001
09/16 03:38:41 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:38:50 PM: Update 4188: task edges-ner-ontonotes, batch 188 (4188): mcc: 0.8647, acc: 0.8171, precision: 0.8999, recall: 0.8449, f1: 0.8715, edges-ner-ontonotes_loss: 0.0370
09/16 03:39:00 PM: Update 4425: task edges-ner-ontonotes, batch 425 (4425): mcc: 0.8634, acc: 0.8151, precision: 0.9016, recall: 0.8409, f1: 0.8702, edges-ner-ontonotes_loss: 0.0390
09/16 03:39:11 PM: Update 4669: task edges-ner-ontonotes, batch 669 (4669): mcc: 0.8581, acc: 0.8093, precision: 0.9004, recall: 0.8322, f1: 0.8650, edges-ner-ontonotes_loss: 0.0430
09/16 03:39:21 PM: Update 4974: task edges-ner-ontonotes, batch 974 (4974): mcc: 0.8651, acc: 0.8195, precision: 0.9058, recall: 0.8399, f1: 0.8716, edges-ner-ontonotes_loss: 0.0409
09/16 03:39:24 PM: ***** Step 5000 / Validation 5 *****
09/16 03:39:24 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:39:24 PM: Validating...
09/16 03:39:29 PM: Updating LR scheduler:
09/16 03:39:29 PM: 	Best result seen so far for macro_avg: 0.859
09/16 03:39:29 PM: 	# validation passes without improvement: 1
09/16 03:39:29 PM: edges-ner-ontonotes_loss: training: 0.041014 validation: 0.044533
09/16 03:39:29 PM: macro_avg: validation: 0.853254
09/16 03:39:29 PM: micro_avg: validation: 0.000000
09/16 03:39:29 PM: edges-ner-ontonotes_mcc: training: 0.864481 validation: 0.846245
09/16 03:39:29 PM: edges-ner-ontonotes_acc: training: 0.818729 validation: 0.804519
09/16 03:39:29 PM: edges-ner-ontonotes_precision: training: 0.905475 validation: 0.897964
09/16 03:39:29 PM: edges-ner-ontonotes_recall: training: 0.839172 validation: 0.812784
09/16 03:39:29 PM: edges-ner-ontonotes_f1: training: 0.871064 validation: 0.853254
09/16 03:39:29 PM: Global learning rate: 0.0001
09/16 03:39:29 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:39:31 PM: Update 5070: task edges-ner-ontonotes, batch 70 (5070): mcc: 0.8563, acc: 0.8106, precision: 0.9005, recall: 0.8289, f1: 0.8632, edges-ner-ontonotes_loss: 0.0418
09/16 03:39:41 PM: Update 5296: task edges-ner-ontonotes, batch 296 (5296): mcc: 0.8718, acc: 0.8302, precision: 0.9109, recall: 0.8475, f1: 0.8781, edges-ner-ontonotes_loss: 0.0379
09/16 03:39:51 PM: Update 5571: task edges-ner-ontonotes, batch 571 (5571): mcc: 0.8697, acc: 0.8252, precision: 0.9063, recall: 0.8481, f1: 0.8762, edges-ner-ontonotes_loss: 0.0376
09/16 03:40:01 PM: Update 5780: task edges-ner-ontonotes, batch 780 (5780): mcc: 0.8700, acc: 0.8247, precision: 0.9057, recall: 0.8492, f1: 0.8765, edges-ner-ontonotes_loss: 0.0372
09/16 03:40:11 PM: ***** Step 6000 / Validation 6 *****
09/16 03:40:11 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:40:11 PM: Validating...
09/16 03:40:11 PM: Evaluate: task edges-ner-ontonotes, batch 19 (157): mcc: 0.8195, acc: 0.7700, precision: 0.8675, recall: 0.7922, f1: 0.8281, edges-ner-ontonotes_loss: 0.0537
09/16 03:40:15 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:40:17 PM: Best result seen so far for macro.
09/16 03:40:17 PM: Updating LR scheduler:
09/16 03:40:17 PM: 	Best result seen so far for macro_avg: 0.860
09/16 03:40:17 PM: 	# validation passes without improvement: 0
09/16 03:40:17 PM: edges-ner-ontonotes_loss: training: 0.038007 validation: 0.044730
09/16 03:40:17 PM: macro_avg: validation: 0.860400
09/16 03:40:17 PM: micro_avg: validation: 0.000000
09/16 03:40:17 PM: edges-ner-ontonotes_mcc: training: 0.868580 validation: 0.853693
09/16 03:40:17 PM: edges-ner-ontonotes_acc: training: 0.822640 validation: 0.812709
09/16 03:40:17 PM: edges-ner-ontonotes_precision: training: 0.904972 validation: 0.903621
09/16 03:40:17 PM: edges-ner-ontonotes_recall: training: 0.847215 validation: 0.821125
09/16 03:40:17 PM: edges-ner-ontonotes_f1: training: 0.875142 validation: 0.860400
09/16 03:40:17 PM: Global learning rate: 0.0001
09/16 03:40:17 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:40:21 PM: Update 6121: task edges-ner-ontonotes, batch 121 (6121): mcc: 0.8480, acc: 0.8018, precision: 0.8951, recall: 0.8186, f1: 0.8552, edges-ner-ontonotes_loss: 0.0479
09/16 03:40:33 PM: Update 6366: task edges-ner-ontonotes, batch 366 (6366): mcc: 0.8614, acc: 0.8177, precision: 0.9041, recall: 0.8347, f1: 0.8680, edges-ner-ontonotes_loss: 0.0432
09/16 03:40:44 PM: Update 6602: task edges-ner-ontonotes, batch 602 (6602): mcc: 0.8689, acc: 0.8277, precision: 0.9088, recall: 0.8442, f1: 0.8753, edges-ner-ontonotes_loss: 0.0404
09/16 03:40:54 PM: Update 6851: task edges-ner-ontonotes, batch 851 (6851): mcc: 0.8721, acc: 0.8313, precision: 0.9106, recall: 0.8485, f1: 0.8784, edges-ner-ontonotes_loss: 0.0392
09/16 03:41:00 PM: ***** Step 7000 / Validation 7 *****
09/16 03:41:02 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:41:02 PM: Validating...
09/16 03:41:04 PM: Evaluate: task edges-ner-ontonotes, batch 73 (157): mcc: 0.8492, acc: 0.8110, precision: 0.8940, recall: 0.8219, f1: 0.8565, edges-ner-ontonotes_loss: 0.0497
09/16 03:41:07 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:41:07 PM: Best result seen so far for macro.
09/16 03:41:07 PM: Updating LR scheduler:
09/16 03:41:07 PM: 	Best result seen so far for macro_avg: 0.863
09/16 03:41:07 PM: 	# validation passes without improvement: 0
09/16 03:41:07 PM: edges-ner-ontonotes_loss: training: 0.038836 validation: 0.044184
09/16 03:41:07 PM: macro_avg: validation: 0.863174
09/16 03:41:07 PM: micro_avg: validation: 0.000000
09/16 03:41:07 PM: edges-ner-ontonotes_mcc: training: 0.871616 validation: 0.855922
09/16 03:41:07 PM: edges-ner-ontonotes_acc: training: 0.829569 validation: 0.816576
09/16 03:41:07 PM: edges-ner-ontonotes_precision: training: 0.908897 validation: 0.892533
09/16 03:41:07 PM: edges-ner-ontonotes_recall: training: 0.849083 validation: 0.835684
09/16 03:41:07 PM: edges-ner-ontonotes_f1: training: 0.877972 validation: 0.863174
09/16 03:41:07 PM: Global learning rate: 0.0001
09/16 03:41:07 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:41:14 PM: Update 7164: task edges-ner-ontonotes, batch 164 (7164): mcc: 0.8743, acc: 0.8300, precision: 0.9044, recall: 0.8583, f1: 0.8808, edges-ner-ontonotes_loss: 0.0353
09/16 03:41:24 PM: Update 7435: task edges-ner-ontonotes, batch 435 (7435): mcc: 0.8733, acc: 0.8278, precision: 0.9048, recall: 0.8562, f1: 0.8798, edges-ner-ontonotes_loss: 0.0355
09/16 03:41:34 PM: Update 7662: task edges-ner-ontonotes, batch 662 (7662): mcc: 0.8675, acc: 0.8216, precision: 0.9025, recall: 0.8476, f1: 0.8742, edges-ner-ontonotes_loss: 0.0390
09/16 03:41:44 PM: Update 7902: task edges-ner-ontonotes, batch 902 (7902): mcc: 0.8679, acc: 0.8228, precision: 0.9036, recall: 0.8472, f1: 0.8745, edges-ner-ontonotes_loss: 0.0394
09/16 03:41:48 PM: ***** Step 8000 / Validation 8 *****
09/16 03:41:48 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:41:48 PM: Validating...
09/16 03:41:52 PM: Updating LR scheduler:
09/16 03:41:52 PM: 	Best result seen so far for macro_avg: 0.863
09/16 03:41:52 PM: 	# validation passes without improvement: 1
09/16 03:41:52 PM: edges-ner-ontonotes_loss: training: 0.038744 validation: 0.045190
09/16 03:41:52 PM: macro_avg: validation: 0.855569
09/16 03:41:52 PM: micro_avg: validation: 0.000000
09/16 03:41:52 PM: edges-ner-ontonotes_mcc: training: 0.870068 validation: 0.848457
09/16 03:41:52 PM: edges-ner-ontonotes_acc: training: 0.825825 validation: 0.808993
09/16 03:41:52 PM: edges-ner-ontonotes_precision: training: 0.905441 validation: 0.896280
09/16 03:41:52 PM: edges-ner-ontonotes_recall: training: 0.849519 validation: 0.818395
09/16 03:41:52 PM: edges-ner-ontonotes_f1: training: 0.876589 validation: 0.855569
09/16 03:41:52 PM: Global learning rate: 0.0001
09/16 03:41:52 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:41:54 PM: Update 8063: task edges-ner-ontonotes, batch 63 (8063): mcc: 0.8893, acc: 0.8560, precision: 0.9175, recall: 0.8736, f1: 0.8950, edges-ner-ontonotes_loss: 0.0333
09/16 03:42:04 PM: Update 8288: task edges-ner-ontonotes, batch 288 (8288): mcc: 0.8805, acc: 0.8415, precision: 0.9146, recall: 0.8601, f1: 0.8865, edges-ner-ontonotes_loss: 0.0356
09/16 03:42:15 PM: Update 8520: task edges-ner-ontonotes, batch 520 (8520): mcc: 0.8767, acc: 0.8349, precision: 0.9106, recall: 0.8570, f1: 0.8830, edges-ner-ontonotes_loss: 0.0360
09/16 03:42:25 PM: Update 8761: task edges-ner-ontonotes, batch 761 (8761): mcc: 0.8769, acc: 0.8342, precision: 0.9094, recall: 0.8584, f1: 0.8831, edges-ner-ontonotes_loss: 0.0354
09/16 03:42:33 PM: ***** Step 9000 / Validation 9 *****
09/16 03:42:33 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:42:33 PM: Validating...
09/16 03:42:35 PM: Evaluate: task edges-ner-ontonotes, batch 58 (157): mcc: 0.8564, acc: 0.8191, precision: 0.8958, recall: 0.8335, f1: 0.8635, edges-ner-ontonotes_loss: 0.0474
09/16 03:42:38 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:42:38 PM: Best result seen so far for macro.
09/16 03:42:38 PM: Updating LR scheduler:
09/16 03:42:38 PM: 	Best result seen so far for macro_avg: 0.868
09/16 03:42:38 PM: 	# validation passes without improvement: 0
09/16 03:42:38 PM: edges-ner-ontonotes_loss: training: 0.035279 validation: 0.044020
09/16 03:42:38 PM: macro_avg: validation: 0.867967
09/16 03:42:38 PM: micro_avg: validation: 0.000000
09/16 03:42:38 PM: edges-ner-ontonotes_mcc: training: 0.876130 validation: 0.861099
09/16 03:42:38 PM: edges-ner-ontonotes_acc: training: 0.832723 validation: 0.820291
09/16 03:42:38 PM: edges-ner-ontonotes_precision: training: 0.908316 validation: 0.899943
09/16 03:42:38 PM: edges-ner-ontonotes_recall: training: 0.857997 validation: 0.838186
09/16 03:42:38 PM: edges-ner-ontonotes_f1: training: 0.882440 validation: 0.867967
09/16 03:42:38 PM: Global learning rate: 0.0001
09/16 03:42:38 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:42:45 PM: Update 9142: task edges-ner-ontonotes, batch 142 (9142): mcc: 0.8566, acc: 0.8100, precision: 0.8945, recall: 0.8351, f1: 0.8638, edges-ner-ontonotes_loss: 0.0438
09/16 03:42:55 PM: Update 9405: task edges-ner-ontonotes, batch 405 (9405): mcc: 0.8605, acc: 0.8147, precision: 0.9000, recall: 0.8370, f1: 0.8673, edges-ner-ontonotes_loss: 0.0437
09/16 03:43:05 PM: Update 9651: task edges-ner-ontonotes, batch 651 (9651): mcc: 0.8721, acc: 0.8305, precision: 0.9084, recall: 0.8505, f1: 0.8785, edges-ner-ontonotes_loss: 0.0397
09/16 03:43:15 PM: Update 9938: task edges-ner-ontonotes, batch 938 (9938): mcc: 0.8747, acc: 0.8334, precision: 0.9102, recall: 0.8536, f1: 0.8810, edges-ner-ontonotes_loss: 0.0385
09/16 03:43:20 PM: ***** Step 10000 / Validation 10 *****
09/16 03:43:20 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:43:20 PM: Validating...
09/16 03:43:24 PM: Updating LR scheduler:
09/16 03:43:24 PM: 	Best result seen so far for macro_avg: 0.868
09/16 03:43:24 PM: 	# validation passes without improvement: 1
09/16 03:43:24 PM: edges-ner-ontonotes_loss: training: 0.038378 validation: 0.043822
09/16 03:43:24 PM: macro_avg: validation: 0.863213
09/16 03:43:24 PM: micro_avg: validation: 0.000000
09/16 03:43:24 PM: edges-ner-ontonotes_mcc: training: 0.874474 validation: 0.855929
09/16 03:43:24 PM: edges-ner-ontonotes_acc: training: 0.833095 validation: 0.819760
09/16 03:43:24 PM: edges-ner-ontonotes_precision: training: 0.909646 validation: 0.891754
09/16 03:43:24 PM: edges-ner-ontonotes_recall: training: 0.853652 validation: 0.836442
09/16 03:43:24 PM: edges-ner-ontonotes_f1: training: 0.880760 validation: 0.863213
09/16 03:43:24 PM: Global learning rate: 0.0001
09/16 03:43:24 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:43:25 PM: Update 10027: task edges-ner-ontonotes, batch 27 (10027): mcc: 0.8779, acc: 0.8271, precision: 0.9080, recall: 0.8616, f1: 0.8842, edges-ner-ontonotes_loss: 0.0355
09/16 03:43:36 PM: Update 10276: task edges-ner-ontonotes, batch 276 (10276): mcc: 0.8739, acc: 0.8282, precision: 0.9035, recall: 0.8585, f1: 0.8804, edges-ner-ontonotes_loss: 0.0348
09/16 03:43:46 PM: Update 10559: task edges-ner-ontonotes, batch 559 (10559): mcc: 0.8747, acc: 0.8292, precision: 0.9048, recall: 0.8587, f1: 0.8811, edges-ner-ontonotes_loss: 0.0347
09/16 03:43:56 PM: Update 10795: task edges-ner-ontonotes, batch 795 (10795): mcc: 0.8696, acc: 0.8238, precision: 0.9017, recall: 0.8522, f1: 0.8762, edges-ner-ontonotes_loss: 0.0376
09/16 03:44:05 PM: ***** Step 11000 / Validation 11 *****
09/16 03:44:05 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:44:05 PM: Validating...
09/16 03:44:06 PM: Evaluate: task edges-ner-ontonotes, batch 47 (157): mcc: 0.8664, acc: 0.8354, precision: 0.9013, recall: 0.8466, f1: 0.8731, edges-ner-ontonotes_loss: 0.0430
09/16 03:44:09 PM: Updating LR scheduler:
09/16 03:44:09 PM: 	Best result seen so far for macro_avg: 0.868
09/16 03:44:09 PM: 	# validation passes without improvement: 2
09/16 03:44:09 PM: edges-ner-ontonotes_loss: training: 0.038241 validation: 0.044122
09/16 03:44:09 PM: macro_avg: validation: 0.857392
09/16 03:44:09 PM: micro_avg: validation: 0.000000
09/16 03:44:09 PM: edges-ner-ontonotes_mcc: training: 0.870037 validation: 0.850430
09/16 03:44:09 PM: edges-ner-ontonotes_acc: training: 0.825314 validation: 0.810661
09/16 03:44:09 PM: edges-ner-ontonotes_precision: training: 0.902732 validation: 0.899010
09/16 03:44:09 PM: edges-ner-ontonotes_recall: training: 0.852063 validation: 0.819457
09/16 03:44:09 PM: edges-ner-ontonotes_f1: training: 0.876666 validation: 0.857392
09/16 03:44:09 PM: Global learning rate: 0.0001
09/16 03:44:09 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:44:16 PM: Update 11201: task edges-ner-ontonotes, batch 201 (11201): mcc: 0.8969, acc: 0.8639, precision: 0.9263, recall: 0.8793, f1: 0.9022, edges-ner-ontonotes_loss: 0.0319
09/16 03:44:26 PM: Update 11425: task edges-ner-ontonotes, batch 425 (11425): mcc: 0.8837, acc: 0.8459, precision: 0.9161, recall: 0.8646, f1: 0.8896, edges-ner-ontonotes_loss: 0.0342
09/16 03:44:36 PM: Update 11654: task edges-ner-ontonotes, batch 654 (11654): mcc: 0.8814, acc: 0.8416, precision: 0.9124, recall: 0.8639, f1: 0.8875, edges-ner-ontonotes_loss: 0.0345
09/16 03:44:46 PM: Update 11892: task edges-ner-ontonotes, batch 892 (11892): mcc: 0.8799, acc: 0.8393, precision: 0.9100, recall: 0.8634, f1: 0.8861, edges-ner-ontonotes_loss: 0.0343
09/16 03:44:50 PM: ***** Step 12000 / Validation 12 *****
09/16 03:44:50 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:44:50 PM: Validating...
09/16 03:44:55 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:44:55 PM: Best result seen so far for macro.
09/16 03:44:55 PM: Updating LR scheduler:
09/16 03:44:55 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:44:55 PM: 	# validation passes without improvement: 0
09/16 03:44:55 PM: edges-ner-ontonotes_loss: training: 0.034355 validation: 0.043680
09/16 03:44:55 PM: macro_avg: validation: 0.869125
09/16 03:44:55 PM: micro_avg: validation: 0.000000
09/16 03:44:55 PM: edges-ner-ontonotes_mcc: training: 0.879602 validation: 0.862291
09/16 03:44:55 PM: edges-ner-ontonotes_acc: training: 0.838415 validation: 0.824462
09/16 03:44:55 PM: edges-ner-ontonotes_precision: training: 0.909559 validation: 0.900423
09/16 03:44:55 PM: edges-ner-ontonotes_recall: training: 0.863248 validation: 0.839930
09/16 03:44:55 PM: edges-ner-ontonotes_f1: training: 0.885798 validation: 0.869125
09/16 03:44:55 PM: Global learning rate: 0.0001
09/16 03:44:55 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:44:56 PM: Update 12019: task edges-ner-ontonotes, batch 19 (12019): mcc: 0.8833, acc: 0.8450, precision: 0.9135, recall: 0.8663, f1: 0.8893, edges-ner-ontonotes_loss: 0.0320
09/16 03:45:06 PM: Update 12243: task edges-ner-ontonotes, batch 243 (12243): mcc: 0.8660, acc: 0.8193, precision: 0.9006, recall: 0.8465, f1: 0.8728, edges-ner-ontonotes_loss: 0.0392
09/16 03:45:16 PM: Update 12492: task edges-ner-ontonotes, batch 492 (12492): mcc: 0.8652, acc: 0.8201, precision: 0.9018, recall: 0.8440, f1: 0.8720, edges-ner-ontonotes_loss: 0.0412
09/16 03:45:27 PM: Update 12762: task edges-ner-ontonotes, batch 762 (12762): mcc: 0.8745, acc: 0.8331, precision: 0.9082, recall: 0.8551, f1: 0.8809, edges-ner-ontonotes_loss: 0.0380
09/16 03:45:35 PM: ***** Step 13000 / Validation 13 *****
09/16 03:45:35 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:45:35 PM: Validating...
09/16 03:45:37 PM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.8766, acc: 0.8495, precision: 0.9069, recall: 0.8603, f1: 0.8830, edges-ner-ontonotes_loss: 0.0408
09/16 03:45:40 PM: Updating LR scheduler:
09/16 03:45:40 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:45:40 PM: 	# validation passes without improvement: 1
09/16 03:45:40 PM: edges-ner-ontonotes_loss: training: 0.037524 validation: 0.043246
09/16 03:45:40 PM: macro_avg: validation: 0.864803
09/16 03:45:40 PM: micro_avg: validation: 0.000000
09/16 03:45:40 PM: edges-ner-ontonotes_mcc: training: 0.875295 validation: 0.857626
09/16 03:45:40 PM: edges-ner-ontonotes_acc: training: 0.834211 validation: 0.824082
09/16 03:45:40 PM: edges-ner-ontonotes_precision: training: 0.908328 validation: 0.893770
09/16 03:45:40 PM: edges-ner-ontonotes_recall: training: 0.856437 validation: 0.837655
09/16 03:45:40 PM: edges-ner-ontonotes_f1: training: 0.881620 validation: 0.864803
09/16 03:45:40 PM: Global learning rate: 0.0001
09/16 03:45:40 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:45:47 PM: Update 13143: task edges-ner-ontonotes, batch 143 (13143): mcc: 0.8795, acc: 0.8391, precision: 0.9083, recall: 0.8643, f1: 0.8858, edges-ner-ontonotes_loss: 0.0339
09/16 03:45:58 PM: Update 13388: task edges-ner-ontonotes, batch 388 (13388): mcc: 0.8771, acc: 0.8345, precision: 0.9056, recall: 0.8624, f1: 0.8835, edges-ner-ontonotes_loss: 0.0342
09/16 03:46:08 PM: Update 13684: task edges-ner-ontonotes, batch 684 (13684): mcc: 0.8777, acc: 0.8337, precision: 0.9065, recall: 0.8627, f1: 0.8840, edges-ner-ontonotes_loss: 0.0341
09/16 03:46:18 PM: Update 13919: task edges-ner-ontonotes, batch 919 (13919): mcc: 0.8720, acc: 0.8273, precision: 0.9029, recall: 0.8555, f1: 0.8786, edges-ner-ontonotes_loss: 0.0368
09/16 03:46:20 PM: ***** Step 14000 / Validation 14 *****
09/16 03:46:20 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:46:20 PM: Validating...
09/16 03:46:24 PM: Updating LR scheduler:
09/16 03:46:25 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:46:25 PM: 	# validation passes without improvement: 2
09/16 03:46:25 PM: edges-ner-ontonotes_loss: training: 0.037332 validation: 0.045039
09/16 03:46:25 PM: macro_avg: validation: 0.860138
09/16 03:46:25 PM: micro_avg: validation: 0.000000
09/16 03:46:25 PM: edges-ner-ontonotes_mcc: training: 0.871410 validation: 0.853178
09/16 03:46:25 PM: edges-ner-ontonotes_acc: training: 0.826826 validation: 0.815287
09/16 03:46:25 PM: edges-ner-ontonotes_precision: training: 0.903000 validation: 0.899041
09/16 03:46:25 PM: edges-ner-ontonotes_recall: training: 0.854352 validation: 0.824462
09/16 03:46:25 PM: edges-ner-ontonotes_f1: training: 0.878003 validation: 0.860138
09/16 03:46:25 PM: Global learning rate: 0.0001
09/16 03:46:25 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:46:28 PM: Update 14017: task edges-ner-ontonotes, batch 17 (14017): mcc: 0.8814, acc: 0.8366, precision: 0.9077, recall: 0.8684, f1: 0.8876, edges-ner-ontonotes_loss: 0.0384
09/16 03:46:38 PM: Update 14318: task edges-ner-ontonotes, batch 318 (14318): mcc: 0.8962, acc: 0.8628, precision: 0.9220, recall: 0.8821, f1: 0.9016, edges-ner-ontonotes_loss: 0.0328
09/16 03:46:49 PM: Update 14631: task edges-ner-ontonotes, batch 631 (14631): mcc: 0.8880, acc: 0.8515, precision: 0.9169, recall: 0.8717, f1: 0.8937, edges-ner-ontonotes_loss: 0.0337
09/16 03:46:59 PM: Update 14944: task edges-ner-ontonotes, batch 944 (14944): mcc: 0.8836, acc: 0.8442, precision: 0.9122, recall: 0.8681, f1: 0.8896, edges-ner-ontonotes_loss: 0.0339
09/16 03:47:01 PM: ***** Step 15000 / Validation 15 *****
09/16 03:47:01 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:47:01 PM: Validating...
09/16 03:47:06 PM: Updating LR scheduler:
09/16 03:47:06 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:47:06 PM: 	# validation passes without improvement: 3
09/16 03:47:06 PM: edges-ner-ontonotes_loss: training: 0.033849 validation: 0.043737
09/16 03:47:06 PM: macro_avg: validation: 0.869056
09/16 03:47:06 PM: micro_avg: validation: 0.000000
09/16 03:47:06 PM: edges-ner-ontonotes_mcc: training: 0.882937 validation: 0.861989
09/16 03:47:06 PM: edges-ner-ontonotes_acc: training: 0.843164 validation: 0.822642
09/16 03:47:06 PM: edges-ner-ontonotes_precision: training: 0.911690 validation: 0.894851
09/16 03:47:06 PM: edges-ner-ontonotes_recall: training: 0.867391 validation: 0.844707
09/16 03:47:06 PM: edges-ner-ontonotes_f1: training: 0.888989 validation: 0.869056
09/16 03:47:06 PM: Global learning rate: 0.0001
09/16 03:47:06 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:47:09 PM: Update 15089: task edges-ner-ontonotes, batch 89 (15089): mcc: 0.8738, acc: 0.8293, precision: 0.9018, recall: 0.8599, f1: 0.8803, edges-ner-ontonotes_loss: 0.0345
09/16 03:47:20 PM: Update 15342: task edges-ner-ontonotes, batch 342 (15342): mcc: 0.8712, acc: 0.8263, precision: 0.9019, recall: 0.8551, f1: 0.8779, edges-ner-ontonotes_loss: 0.0374
09/16 03:47:30 PM: Update 15584: task edges-ner-ontonotes, batch 584 (15584): mcc: 0.8678, acc: 0.8235, precision: 0.9011, recall: 0.8495, f1: 0.8746, edges-ner-ontonotes_loss: 0.0397
09/16 03:47:41 PM: Update 15874: task edges-ner-ontonotes, batch 874 (15874): mcc: 0.8762, acc: 0.8352, precision: 0.9075, recall: 0.8588, f1: 0.8825, edges-ner-ontonotes_loss: 0.0372
09/16 03:47:45 PM: ***** Step 16000 / Validation 16 *****
09/16 03:47:45 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:47:45 PM: Validating...
09/16 03:47:49 PM: Updating LR scheduler:
09/16 03:47:49 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:47:49 PM: 	# validation passes without improvement: 0
09/16 03:47:49 PM: edges-ner-ontonotes_loss: training: 0.036792 validation: 0.043260
09/16 03:47:49 PM: macro_avg: validation: 0.864540
09/16 03:47:49 PM: micro_avg: validation: 0.000000
09/16 03:47:49 PM: edges-ner-ontonotes_mcc: training: 0.877152 validation: 0.857570
09/16 03:47:49 PM: edges-ner-ontonotes_acc: training: 0.836566 validation: 0.820367
09/16 03:47:49 PM: edges-ner-ontonotes_precision: training: 0.908543 validation: 0.898438
09/16 03:47:49 PM: edges-ner-ontonotes_recall: training: 0.859675 validation: 0.833106
09/16 03:47:49 PM: edges-ner-ontonotes_f1: training: 0.883434 validation: 0.864540
09/16 03:47:49 PM: Global learning rate: 5e-05
09/16 03:47:49 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:47:51 PM: Update 16050: task edges-ner-ontonotes, batch 50 (16050): mcc: 0.8865, acc: 0.8482, precision: 0.9170, recall: 0.8690, f1: 0.8923, edges-ner-ontonotes_loss: 0.0327
09/16 03:48:01 PM: Update 16279: task edges-ner-ontonotes, batch 279 (16279): mcc: 0.8746, acc: 0.8332, precision: 0.9048, recall: 0.8586, f1: 0.8811, edges-ner-ontonotes_loss: 0.0358
09/16 03:48:11 PM: Update 16528: task edges-ner-ontonotes, batch 528 (16528): mcc: 0.8759, acc: 0.8331, precision: 0.9048, recall: 0.8609, f1: 0.8823, edges-ner-ontonotes_loss: 0.0348
09/16 03:48:21 PM: Update 16812: task edges-ner-ontonotes, batch 812 (16812): mcc: 0.8766, acc: 0.8328, precision: 0.9051, recall: 0.8620, f1: 0.8830, edges-ner-ontonotes_loss: 0.0344
09/16 03:48:30 PM: ***** Step 17000 / Validation 17 *****
09/16 03:48:30 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:48:30 PM: Validating...
09/16 03:48:31 PM: Evaluate: task edges-ner-ontonotes, batch 55 (157): mcc: 0.8760, acc: 0.8480, precision: 0.9071, recall: 0.8588, f1: 0.8823, edges-ner-ontonotes_loss: 0.0423
09/16 03:48:34 PM: Updating LR scheduler:
09/16 03:48:34 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:48:34 PM: 	# validation passes without improvement: 1
09/16 03:48:34 PM: edges-ner-ontonotes_loss: training: 0.036405 validation: 0.043499
09/16 03:48:34 PM: macro_avg: validation: 0.863853
09/16 03:48:34 PM: micro_avg: validation: 0.000000
09/16 03:48:34 PM: edges-ner-ontonotes_mcc: training: 0.872962 validation: 0.857004
09/16 03:48:34 PM: edges-ner-ontonotes_acc: training: 0.828503 validation: 0.820822
09/16 03:48:34 PM: edges-ner-ontonotes_precision: training: 0.903096 validation: 0.900947
09/16 03:48:34 PM: edges-ner-ontonotes_recall: training: 0.857144 validation: 0.829694
09/16 03:48:34 PM: edges-ner-ontonotes_f1: training: 0.879521 validation: 0.863853
09/16 03:48:34 PM: Global learning rate: 5e-05
09/16 03:48:34 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:48:41 PM: Update 17142: task edges-ner-ontonotes, batch 142 (17142): mcc: 0.8675, acc: 0.8217, precision: 0.9068, recall: 0.8436, f1: 0.8740, edges-ner-ontonotes_loss: 0.0414
09/16 03:48:52 PM: Update 17430: task edges-ner-ontonotes, batch 430 (17430): mcc: 0.8887, acc: 0.8519, precision: 0.9182, recall: 0.8717, f1: 0.8944, edges-ner-ontonotes_loss: 0.0344
09/16 03:49:02 PM: Update 17728: task edges-ner-ontonotes, batch 728 (17728): mcc: 0.8856, acc: 0.8476, precision: 0.9164, recall: 0.8679, f1: 0.8915, edges-ner-ontonotes_loss: 0.0344
09/16 03:49:12 PM: Update 17956: task edges-ner-ontonotes, batch 956 (17956): mcc: 0.8825, acc: 0.8430, precision: 0.9121, recall: 0.8661, f1: 0.8885, edges-ner-ontonotes_loss: 0.0345
09/16 03:49:14 PM: ***** Step 18000 / Validation 18 *****
09/16 03:49:14 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:49:14 PM: Validating...
09/16 03:49:18 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:49:18 PM: Best result seen so far for macro.
09/16 03:49:18 PM: Updating LR scheduler:
09/16 03:49:18 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:49:18 PM: 	# validation passes without improvement: 0
09/16 03:49:18 PM: edges-ner-ontonotes_loss: training: 0.034293 validation: 0.043299
09/16 03:49:18 PM: macro_avg: validation: 0.869277
09/16 03:49:18 PM: micro_avg: validation: 0.000000
09/16 03:49:18 PM: edges-ner-ontonotes_mcc: training: 0.882728 validation: 0.862248
09/16 03:49:18 PM: edges-ner-ontonotes_acc: training: 0.843119 validation: 0.825068
09/16 03:49:18 PM: edges-ner-ontonotes_precision: training: 0.912276 validation: 0.895745
09/16 03:49:18 PM: edges-ner-ontonotes_recall: training: 0.866437 validation: 0.844328
09/16 03:49:18 PM: edges-ner-ontonotes_f1: training: 0.888766 validation: 0.869277
09/16 03:49:18 PM: Global learning rate: 5e-05
09/16 03:49:18 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:49:22 PM: Update 18100: task edges-ner-ontonotes, batch 100 (18100): mcc: 0.8729, acc: 0.8274, precision: 0.9005, recall: 0.8596, f1: 0.8795, edges-ner-ontonotes_loss: 0.0339
09/16 03:49:34 PM: Update 18369: task edges-ner-ontonotes, batch 369 (18369): mcc: 0.8749, acc: 0.8295, precision: 0.9031, recall: 0.8607, f1: 0.8814, edges-ner-ontonotes_loss: 0.0338
09/16 03:49:44 PM: Update 18663: task edges-ner-ontonotes, batch 663 (18663): mcc: 0.8676, acc: 0.8217, precision: 0.9003, recall: 0.8498, f1: 0.8743, edges-ner-ontonotes_loss: 0.0384
09/16 03:49:54 PM: Update 18932: task edges-ner-ontonotes, batch 932 (18932): mcc: 0.8751, acc: 0.8327, precision: 0.9060, recall: 0.8584, f1: 0.8815, edges-ner-ontonotes_loss: 0.0364
09/16 03:49:58 PM: ***** Step 19000 / Validation 19 *****
09/16 03:49:58 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:49:58 PM: Validating...
09/16 03:50:02 PM: Updating LR scheduler:
09/16 03:50:02 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:50:02 PM: 	# validation passes without improvement: 1
09/16 03:50:02 PM: edges-ner-ontonotes_loss: training: 0.036209 validation: 0.043339
09/16 03:50:02 PM: macro_avg: validation: 0.860869
09/16 03:50:02 PM: micro_avg: validation: 0.000000
09/16 03:50:02 PM: edges-ner-ontonotes_mcc: training: 0.875941 validation: 0.854065
09/16 03:50:02 PM: edges-ner-ontonotes_acc: training: 0.833930 validation: 0.814907
09/16 03:50:02 PM: edges-ner-ontonotes_precision: training: 0.906769 validation: 0.901910
09/16 03:50:02 PM: edges-ner-ontonotes_recall: training: 0.859136 validation: 0.823400
09/16 03:50:02 PM: edges-ner-ontonotes_f1: training: 0.882310 validation: 0.860869
09/16 03:50:02 PM: Global learning rate: 5e-05
09/16 03:50:02 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:50:04 PM: Update 19038: task edges-ner-ontonotes, batch 38 (19038): mcc: 0.8798, acc: 0.8390, precision: 0.9175, recall: 0.8560, f1: 0.8857, edges-ner-ontonotes_loss: 0.0350
09/16 03:50:15 PM: Update 19299: task edges-ner-ontonotes, batch 299 (19299): mcc: 0.8833, acc: 0.8442, precision: 0.9143, recall: 0.8656, f1: 0.8893, edges-ner-ontonotes_loss: 0.0345
09/16 03:50:25 PM: Update 19585: task edges-ner-ontonotes, batch 585 (19585): mcc: 0.8802, acc: 0.8388, precision: 0.9091, recall: 0.8647, f1: 0.8864, edges-ner-ontonotes_loss: 0.0343
09/16 03:50:35 PM: Update 19843: task edges-ner-ontonotes, batch 843 (19843): mcc: 0.8793, acc: 0.8367, precision: 0.9083, recall: 0.8639, f1: 0.8855, edges-ner-ontonotes_loss: 0.0341
09/16 03:50:43 PM: ***** Step 20000 / Validation 20 *****
09/16 03:50:43 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:50:43 PM: Validating...
09/16 03:50:45 PM: Evaluate: task edges-ner-ontonotes, batch 91 (157): mcc: 0.8637, acc: 0.8231, precision: 0.9090, recall: 0.8344, f1: 0.8701, edges-ner-ontonotes_loss: 0.0455
09/16 03:50:48 PM: Updating LR scheduler:
09/16 03:50:48 PM: 	Best result seen so far for macro_avg: 0.869
09/16 03:50:48 PM: 	# validation passes without improvement: 2
09/16 03:50:48 PM: edges-ner-ontonotes_loss: training: 0.035055 validation: 0.042562
09/16 03:50:48 PM: macro_avg: validation: 0.864826
09/16 03:50:48 PM: micro_avg: validation: 0.000000
09/16 03:50:48 PM: edges-ner-ontonotes_mcc: training: 0.877199 validation: 0.858204
09/16 03:50:48 PM: edges-ner-ontonotes_acc: training: 0.834148 validation: 0.817637
09/16 03:50:48 PM: edges-ner-ontonotes_precision: training: 0.906706 validation: 0.905232
09/16 03:50:48 PM: edges-ner-ontonotes_recall: training: 0.861536 validation: 0.827874
09/16 03:50:48 PM: edges-ner-ontonotes_f1: training: 0.883544 validation: 0.864826
09/16 03:50:48 PM: Global learning rate: 5e-05
09/16 03:50:48 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:50:57 PM: Update 20229: task edges-ner-ontonotes, batch 229 (20229): mcc: 0.8629, acc: 0.8159, precision: 0.9009, recall: 0.8405, f1: 0.8697, edges-ner-ontonotes_loss: 0.0428
09/16 03:51:09 PM: Update 20542: task edges-ner-ontonotes, batch 542 (20542): mcc: 0.8815, acc: 0.8419, precision: 0.9133, recall: 0.8631, f1: 0.8875, edges-ner-ontonotes_loss: 0.0364
09/16 03:51:20 PM: Update 20855: task edges-ner-ontonotes, batch 855 (20855): mcc: 0.8820, acc: 0.8431, precision: 0.9135, recall: 0.8639, f1: 0.8880, edges-ner-ontonotes_loss: 0.0357
09/16 03:51:25 PM: ***** Step 21000 / Validation 21 *****
09/16 03:51:25 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:51:25 PM: Validating...
09/16 03:51:30 PM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.8631, acc: 0.8254, precision: 0.8969, recall: 0.8447, f1: 0.8700, edges-ner-ontonotes_loss: 0.0436
09/16 03:51:31 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:51:31 PM: Best result seen so far for macro.
09/16 03:51:31 PM: Updating LR scheduler:
09/16 03:51:31 PM: 	Best result seen so far for macro_avg: 0.871
09/16 03:51:31 PM: 	# validation passes without improvement: 0
09/16 03:51:31 PM: edges-ner-ontonotes_loss: training: 0.035359 validation: 0.042876
09/16 03:51:31 PM: macro_avg: validation: 0.871186
09/16 03:51:31 PM: micro_avg: validation: 0.000000
09/16 03:51:31 PM: edges-ner-ontonotes_mcc: training: 0.881077 validation: 0.864302
09/16 03:51:31 PM: edges-ner-ontonotes_acc: training: 0.841332 validation: 0.827040
09/16 03:51:31 PM: edges-ner-ontonotes_precision: training: 0.911824 validation: 0.898606
09/16 03:51:31 PM: edges-ner-ontonotes_recall: training: 0.863805 validation: 0.845390
09/16 03:51:31 PM: edges-ner-ontonotes_f1: training: 0.887165 validation: 0.871186
09/16 03:51:31 PM: Global learning rate: 5e-05
09/16 03:51:31 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:51:40 PM: Update 21215: task edges-ner-ontonotes, batch 215 (21215): mcc: 0.8771, acc: 0.8337, precision: 0.9046, recall: 0.8633, f1: 0.8835, edges-ner-ontonotes_loss: 0.0338
09/16 03:51:50 PM: Update 21507: task edges-ner-ontonotes, batch 507 (21507): mcc: 0.8772, acc: 0.8334, precision: 0.9048, recall: 0.8633, f1: 0.8836, edges-ner-ontonotes_loss: 0.0341
09/16 03:52:02 PM: Update 21785: task edges-ner-ontonotes, batch 785 (21785): mcc: 0.8718, acc: 0.8275, precision: 0.9027, recall: 0.8553, f1: 0.8784, edges-ner-ontonotes_loss: 0.0374
09/16 03:52:08 PM: ***** Step 22000 / Validation 22 *****
09/16 03:52:08 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:52:08 PM: Validating...
09/16 03:52:12 PM: Evaluate: task edges-ner-ontonotes, batch 120 (157): mcc: 0.8509, acc: 0.8148, precision: 0.8959, recall: 0.8232, f1: 0.8580, edges-ner-ontonotes_loss: 0.0450
09/16 03:52:13 PM: Updating LR scheduler:
09/16 03:52:13 PM: 	Best result seen so far for macro_avg: 0.871
09/16 03:52:13 PM: 	# validation passes without improvement: 1
09/16 03:52:13 PM: edges-ner-ontonotes_loss: training: 0.036234 validation: 0.043083
09/16 03:52:13 PM: macro_avg: validation: 0.862869
09/16 03:52:13 PM: micro_avg: validation: 0.000000
09/16 03:52:13 PM: edges-ner-ontonotes_mcc: training: 0.875947 validation: 0.856035
09/16 03:52:13 PM: edges-ner-ontonotes_acc: training: 0.833586 validation: 0.818623
09/16 03:52:13 PM: edges-ner-ontonotes_precision: training: 0.905934 validation: 0.901313
09/16 03:52:13 PM: edges-ner-ontonotes_recall: training: 0.859954 validation: 0.827570
09/16 03:52:13 PM: edges-ner-ontonotes_f1: training: 0.882345 validation: 0.862869
09/16 03:52:13 PM: Global learning rate: 5e-05
09/16 03:52:13 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:52:22 PM: Update 22208: task edges-ner-ontonotes, batch 208 (22208): mcc: 0.8899, acc: 0.8548, precision: 0.9186, recall: 0.8736, f1: 0.8955, edges-ner-ontonotes_loss: 0.0325
09/16 03:52:32 PM: Update 22445: task edges-ner-ontonotes, batch 445 (22445): mcc: 0.8844, acc: 0.8466, precision: 0.9143, recall: 0.8675, f1: 0.8903, edges-ner-ontonotes_loss: 0.0338
09/16 03:52:43 PM: Update 22724: task edges-ner-ontonotes, batch 724 (22724): mcc: 0.8825, acc: 0.8423, precision: 0.9106, recall: 0.8675, f1: 0.8886, edges-ner-ontonotes_loss: 0.0335
09/16 03:52:51 PM: ***** Step 23000 / Validation 23 *****
09/16 03:52:51 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:52:51 PM: Validating...
09/16 03:52:53 PM: Evaluate: task edges-ner-ontonotes, batch 82 (157): mcc: 0.8637, acc: 0.8280, precision: 0.8998, recall: 0.8430, f1: 0.8705, edges-ner-ontonotes_loss: 0.0467
09/16 03:52:54 PM: Best result seen so far for edges-ner-ontonotes.
09/16 03:52:54 PM: Best result seen so far for macro.
09/16 03:52:54 PM: Updating LR scheduler:
09/16 03:52:54 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:52:54 PM: 	# validation passes without improvement: 0
09/16 03:52:54 PM: edges-ner-ontonotes_loss: training: 0.033417 validation: 0.042704
09/16 03:52:54 PM: macro_avg: validation: 0.872853
09/16 03:52:54 PM: micro_avg: validation: 0.000000
09/16 03:52:54 PM: edges-ner-ontonotes_mcc: training: 0.881393 validation: 0.866130
09/16 03:52:54 PM: edges-ner-ontonotes_acc: training: 0.840132 validation: 0.829618
09/16 03:52:54 PM: edges-ner-ontonotes_precision: training: 0.909251 validation: 0.901901
09/16 03:52:54 PM: edges-ner-ontonotes_recall: training: 0.866879 validation: 0.845617
09/16 03:52:54 PM: edges-ner-ontonotes_f1: training: 0.887560 validation: 0.872853
09/16 03:52:54 PM: Global learning rate: 5e-05
09/16 03:52:54 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:53:03 PM: Update 23216: task edges-ner-ontonotes, batch 216 (23216): mcc: 0.8601, acc: 0.8146, precision: 0.8968, recall: 0.8394, f1: 0.8672, edges-ner-ontonotes_loss: 0.0428
09/16 03:53:13 PM: Update 23490: task edges-ner-ontonotes, batch 490 (23490): mcc: 0.8720, acc: 0.8306, precision: 0.9049, recall: 0.8536, f1: 0.8785, edges-ner-ontonotes_loss: 0.0391
09/16 03:53:23 PM: Update 23784: task edges-ner-ontonotes, batch 784 (23784): mcc: 0.8781, acc: 0.8386, precision: 0.9097, recall: 0.8604, f1: 0.8843, edges-ner-ontonotes_loss: 0.0368
09/16 03:53:32 PM: ***** Step 24000 / Validation 24 *****
09/16 03:53:32 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:53:32 PM: Validating...
09/16 03:53:33 PM: Evaluate: task edges-ner-ontonotes, batch 45 (157): mcc: 0.8537, acc: 0.8274, precision: 0.8818, recall: 0.8418, f1: 0.8614, edges-ner-ontonotes_loss: 0.0457
09/16 03:53:36 PM: Updating LR scheduler:
09/16 03:53:36 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:53:36 PM: 	# validation passes without improvement: 1
09/16 03:53:36 PM: edges-ner-ontonotes_loss: training: 0.036205 validation: 0.042262
09/16 03:53:36 PM: macro_avg: validation: 0.868801
09/16 03:53:36 PM: micro_avg: validation: 0.000000
09/16 03:53:36 PM: edges-ner-ontonotes_mcc: training: 0.878890 validation: 0.861758
09/16 03:53:36 PM: edges-ner-ontonotes_acc: training: 0.839664 validation: 0.828784
09/16 03:53:36 PM: edges-ner-ontonotes_precision: training: 0.910067 validation: 0.895588
09/16 03:53:36 PM: edges-ner-ontonotes_recall: training: 0.861436 validation: 0.843570
09/16 03:53:36 PM: edges-ner-ontonotes_f1: training: 0.885084 validation: 0.868801
09/16 03:53:36 PM: Global learning rate: 5e-05
09/16 03:53:36 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:53:43 PM: Update 24213: task edges-ner-ontonotes, batch 213 (24213): mcc: 0.8777, acc: 0.8331, precision: 0.9037, recall: 0.8655, f1: 0.8842, edges-ner-ontonotes_loss: 0.0336
09/16 03:53:53 PM: Update 24455: task edges-ner-ontonotes, batch 455 (24455): mcc: 0.8784, acc: 0.8339, precision: 0.9052, recall: 0.8652, f1: 0.8847, edges-ner-ontonotes_loss: 0.0332
09/16 03:54:03 PM: Update 24719: task edges-ner-ontonotes, batch 719 (24719): mcc: 0.8754, acc: 0.8309, precision: 0.9043, recall: 0.8605, f1: 0.8819, edges-ner-ontonotes_loss: 0.0353
09/16 03:54:13 PM: Update 24976: task edges-ner-ontonotes, batch 976 (24976): mcc: 0.8745, acc: 0.8307, precision: 0.9048, recall: 0.8584, f1: 0.8810, edges-ner-ontonotes_loss: 0.0364
09/16 03:54:13 PM: ***** Step 25000 / Validation 25 *****
09/16 03:54:15 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:54:15 PM: Validating...
09/16 03:54:19 PM: Updating LR scheduler:
09/16 03:54:19 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:54:19 PM: 	# validation passes without improvement: 2
09/16 03:54:19 PM: edges-ner-ontonotes_loss: training: 0.036332 validation: 0.043230
09/16 03:54:19 PM: macro_avg: validation: 0.864004
09/16 03:54:19 PM: micro_avg: validation: 0.000000
09/16 03:54:19 PM: edges-ner-ontonotes_mcc: training: 0.874901 validation: 0.857458
09/16 03:54:19 PM: edges-ner-ontonotes_acc: training: 0.831310 validation: 0.817637
09/16 03:54:19 PM: edges-ner-ontonotes_precision: training: 0.905074 validation: 0.906614
09/16 03:54:19 PM: edges-ner-ontonotes_recall: training: 0.858840 validation: 0.825220
09/16 03:54:19 PM: edges-ner-ontonotes_f1: training: 0.881351 validation: 0.864004
09/16 03:54:19 PM: Global learning rate: 5e-05
09/16 03:54:19 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:54:23 PM: Update 25148: task edges-ner-ontonotes, batch 148 (25148): mcc: 0.9007, acc: 0.8690, precision: 0.9268, recall: 0.8858, f1: 0.9058, edges-ner-ontonotes_loss: 0.0312
09/16 03:54:33 PM: Update 25408: task edges-ner-ontonotes, batch 408 (25408): mcc: 0.8894, acc: 0.8539, precision: 0.9180, recall: 0.8732, f1: 0.8951, edges-ner-ontonotes_loss: 0.0327
09/16 03:54:43 PM: Update 25652: task edges-ner-ontonotes, batch 652 (25652): mcc: 0.8844, acc: 0.8463, precision: 0.9131, recall: 0.8687, f1: 0.8903, edges-ner-ontonotes_loss: 0.0334
09/16 03:54:53 PM: Update 25928: task edges-ner-ontonotes, batch 928 (25928): mcc: 0.8829, acc: 0.8427, precision: 0.9110, recall: 0.8680, f1: 0.8890, edges-ner-ontonotes_loss: 0.0332
09/16 03:54:55 PM: ***** Step 26000 / Validation 26 *****
09/16 03:54:55 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:54:55 PM: Validating...
09/16 03:55:00 PM: Updating LR scheduler:
09/16 03:55:00 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:55:00 PM: 	# validation passes without improvement: 3
09/16 03:55:00 PM: edges-ner-ontonotes_loss: training: 0.033213 validation: 0.042747
09/16 03:55:00 PM: macro_avg: validation: 0.869671
09/16 03:55:00 PM: micro_avg: validation: 0.000000
09/16 03:55:00 PM: edges-ner-ontonotes_mcc: training: 0.882840 validation: 0.862936
09/16 03:55:00 PM: edges-ner-ontonotes_acc: training: 0.842373 validation: 0.826585
09/16 03:55:00 PM: edges-ner-ontonotes_precision: training: 0.910817 validation: 0.902471
09/16 03:55:00 PM: edges-ner-ontonotes_recall: training: 0.868056 validation: 0.839172
09/16 03:55:00 PM: edges-ner-ontonotes_f1: training: 0.888923 validation: 0.869671
09/16 03:55:00 PM: Global learning rate: 5e-05
09/16 03:55:00 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:55:03 PM: Update 26071: task edges-ner-ontonotes, batch 71 (26071): mcc: 0.8765, acc: 0.8344, precision: 0.9023, recall: 0.8645, f1: 0.8830, edges-ner-ontonotes_loss: 0.0344
09/16 03:55:13 PM: Update 26325: task edges-ner-ontonotes, batch 325 (26325): mcc: 0.8690, acc: 0.8244, precision: 0.9005, recall: 0.8523, f1: 0.8757, edges-ner-ontonotes_loss: 0.0391
09/16 03:55:23 PM: Update 26641: task edges-ner-ontonotes, batch 641 (26641): mcc: 0.8737, acc: 0.8314, precision: 0.9050, recall: 0.8566, f1: 0.8801, edges-ner-ontonotes_loss: 0.0379
09/16 03:55:33 PM: Update 26925: task edges-ner-ontonotes, batch 925 (26925): mcc: 0.8785, acc: 0.8379, precision: 0.9090, recall: 0.8618, f1: 0.8847, edges-ner-ontonotes_loss: 0.0363
09/16 03:55:35 PM: ***** Step 27000 / Validation 27 *****
09/16 03:55:35 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:55:35 PM: Validating...
09/16 03:55:39 PM: Updating LR scheduler:
09/16 03:55:41 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:55:41 PM: 	# validation passes without improvement: 0
09/16 03:55:41 PM: edges-ner-ontonotes_loss: training: 0.035929 validation: 0.042556
09/16 03:55:41 PM: macro_avg: validation: 0.867335
09/16 03:55:41 PM: micro_avg: validation: 0.000000
09/16 03:55:41 PM: edges-ner-ontonotes_mcc: training: 0.879801 validation: 0.860512
09/16 03:55:41 PM: edges-ner-ontonotes_acc: training: 0.839739 validation: 0.824765
09/16 03:55:41 PM: edges-ner-ontonotes_precision: training: 0.909994 validation: 0.901038
09/16 03:55:41 PM: edges-ner-ontonotes_recall: training: 0.863198 validation: 0.836063
09/16 03:55:41 PM: edges-ner-ontonotes_f1: training: 0.885978 validation: 0.867335
09/16 03:55:41 PM: Global learning rate: 2.5e-05
09/16 03:55:41 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:55:43 PM: Update 27062: task edges-ner-ontonotes, batch 62 (27062): mcc: 0.8836, acc: 0.8462, precision: 0.9135, recall: 0.8669, f1: 0.8896, edges-ner-ontonotes_loss: 0.0348
09/16 03:55:53 PM: Update 27322: task edges-ner-ontonotes, batch 322 (27322): mcc: 0.8778, acc: 0.8345, precision: 0.9051, recall: 0.8642, f1: 0.8842, edges-ner-ontonotes_loss: 0.0343
09/16 03:56:03 PM: Update 27572: task edges-ner-ontonotes, batch 572 (27572): mcc: 0.8800, acc: 0.8364, precision: 0.9066, recall: 0.8669, f1: 0.8863, edges-ner-ontonotes_loss: 0.0337
09/16 03:56:13 PM: Update 27842: task edges-ner-ontonotes, batch 842 (27842): mcc: 0.8759, acc: 0.8310, precision: 0.9047, recall: 0.8611, f1: 0.8823, edges-ner-ontonotes_loss: 0.0354
09/16 03:56:18 PM: ***** Step 28000 / Validation 28 *****
09/16 03:56:18 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:56:18 PM: Validating...
09/16 03:56:22 PM: Updating LR scheduler:
09/16 03:56:22 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:56:22 PM: 	# validation passes without improvement: 1
09/16 03:56:22 PM: edges-ner-ontonotes_loss: training: 0.036707 validation: 0.042741
09/16 03:56:22 PM: macro_avg: validation: 0.865211
09/16 03:56:22 PM: micro_avg: validation: 0.000000
09/16 03:56:22 PM: edges-ner-ontonotes_mcc: training: 0.873809 validation: 0.858538
09/16 03:56:22 PM: edges-ner-ontonotes_acc: training: 0.828945 validation: 0.821277
09/16 03:56:22 PM: edges-ner-ontonotes_precision: training: 0.903367 validation: 0.904266
09/16 03:56:22 PM: edges-ner-ontonotes_recall: training: 0.858459 validation: 0.829390
09/16 03:56:22 PM: edges-ner-ontonotes_f1: training: 0.880341 validation: 0.865211
09/16 03:56:22 PM: Global learning rate: 2.5e-05
09/16 03:56:22 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:56:24 PM: Update 28009: task edges-ner-ontonotes, batch 9 (28009): mcc: 0.8503, acc: 0.7978, precision: 0.8935, recall: 0.8244, f1: 0.8576, edges-ner-ontonotes_loss: 0.0461
09/16 03:56:36 PM: Update 28322: task edges-ner-ontonotes, batch 322 (28322): mcc: 0.8978, acc: 0.8652, precision: 0.9229, recall: 0.8842, f1: 0.9031, edges-ner-ontonotes_loss: 0.0313
09/16 03:56:46 PM: Update 28631: task edges-ner-ontonotes, batch 631 (28631): mcc: 0.8898, acc: 0.8541, precision: 0.9173, recall: 0.8747, f1: 0.8955, edges-ner-ontonotes_loss: 0.0327
09/16 03:56:56 PM: Update 28864: task edges-ner-ontonotes, batch 864 (28864): mcc: 0.8862, acc: 0.8482, precision: 0.9136, recall: 0.8717, f1: 0.8922, edges-ner-ontonotes_loss: 0.0330
09/16 03:57:02 PM: ***** Step 29000 / Validation 29 *****
09/16 03:57:02 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:57:02 PM: Validating...
09/16 03:57:06 PM: Evaluate: task edges-ner-ontonotes, batch 131 (157): mcc: 0.8568, acc: 0.8184, precision: 0.8936, recall: 0.8362, f1: 0.8640, edges-ner-ontonotes_loss: 0.0447
09/16 03:57:06 PM: Updating LR scheduler:
09/16 03:57:06 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:57:06 PM: 	# validation passes without improvement: 2
09/16 03:57:06 PM: edges-ner-ontonotes_loss: training: 0.033041 validation: 0.042672
09/16 03:57:06 PM: macro_avg: validation: 0.870212
09/16 03:57:06 PM: micro_avg: validation: 0.000000
09/16 03:57:06 PM: edges-ner-ontonotes_mcc: training: 0.885216 validation: 0.863368
09/16 03:57:06 PM: edges-ner-ontonotes_acc: training: 0.846118 validation: 0.825675
09/16 03:57:06 PM: edges-ner-ontonotes_precision: training: 0.912763 validation: 0.899887
09/16 03:57:06 PM: edges-ner-ontonotes_recall: training: 0.870594 validation: 0.842432
09/16 03:57:06 PM: edges-ner-ontonotes_f1: training: 0.891180 validation: 0.870212
09/16 03:57:06 PM: Global learning rate: 2.5e-05
09/16 03:57:06 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:57:16 PM: Update 29267: task edges-ner-ontonotes, batch 267 (29267): mcc: 0.8769, acc: 0.8323, precision: 0.9052, recall: 0.8625, f1: 0.8833, edges-ner-ontonotes_loss: 0.0337
09/16 03:57:26 PM: Update 29565: task edges-ner-ontonotes, batch 565 (29565): mcc: 0.8685, acc: 0.8242, precision: 0.9014, recall: 0.8505, f1: 0.8752, edges-ner-ontonotes_loss: 0.0387
09/16 03:57:37 PM: Update 29878: task edges-ner-ontonotes, batch 878 (29878): mcc: 0.8782, acc: 0.8376, precision: 0.9086, recall: 0.8615, f1: 0.8844, edges-ner-ontonotes_loss: 0.0359
09/16 03:57:41 PM: ***** Step 30000 / Validation 30 *****
09/16 03:57:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:57:41 PM: Validating...
09/16 03:57:45 PM: Updating LR scheduler:
09/16 03:57:45 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:57:45 PM: 	# validation passes without improvement: 3
09/16 03:57:45 PM: edges-ner-ontonotes_loss: training: 0.035709 validation: 0.042112
09/16 03:57:45 PM: macro_avg: validation: 0.868227
09/16 03:57:45 PM: micro_avg: validation: 0.000000
09/16 03:57:45 PM: edges-ner-ontonotes_mcc: training: 0.878760 validation: 0.861473
09/16 03:57:45 PM: edges-ner-ontonotes_acc: training: 0.838451 validation: 0.826509
09/16 03:57:45 PM: edges-ner-ontonotes_precision: training: 0.909648 validation: 0.902347
09/16 03:57:45 PM: edges-ner-ontonotes_recall: training: 0.861598 validation: 0.836594
09/16 03:57:45 PM: edges-ner-ontonotes_f1: training: 0.884971 validation: 0.868227
09/16 03:57:45 PM: Global learning rate: 2.5e-05
09/16 03:57:45 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:57:47 PM: Update 30050: task edges-ner-ontonotes, batch 50 (30050): mcc: 0.8890, acc: 0.8550, precision: 0.9176, recall: 0.8729, f1: 0.8947, edges-ner-ontonotes_loss: 0.0335
09/16 03:57:57 PM: Update 30290: task edges-ner-ontonotes, batch 290 (30290): mcc: 0.8812, acc: 0.8405, precision: 0.9111, recall: 0.8647, f1: 0.8873, edges-ner-ontonotes_loss: 0.0339
09/16 03:58:07 PM: Update 30568: task edges-ner-ontonotes, batch 568 (30568): mcc: 0.8809, acc: 0.8383, precision: 0.9086, recall: 0.8666, f1: 0.8871, edges-ner-ontonotes_loss: 0.0334
09/16 03:58:17 PM: Update 30825: task edges-ner-ontonotes, batch 825 (30825): mcc: 0.8804, acc: 0.8369, precision: 0.9084, recall: 0.8659, f1: 0.8866, edges-ner-ontonotes_loss: 0.0333
09/16 03:58:22 PM: ***** Step 31000 / Validation 31 *****
09/16 03:58:22 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:58:22 PM: Validating...
09/16 03:58:26 PM: Updating LR scheduler:
09/16 03:58:26 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:58:26 PM: 	# validation passes without improvement: 0
09/16 03:58:26 PM: edges-ner-ontonotes_loss: training: 0.035013 validation: 0.042545
09/16 03:58:26 PM: macro_avg: validation: 0.867631
09/16 03:58:26 PM: micro_avg: validation: 0.000000
09/16 03:58:26 PM: edges-ner-ontonotes_mcc: training: 0.877214 validation: 0.861047
09/16 03:58:26 PM: edges-ner-ontonotes_acc: training: 0.833535 validation: 0.823779
09/16 03:58:26 PM: edges-ner-ontonotes_precision: training: 0.906981 validation: 0.905858
09/16 03:58:26 PM: edges-ner-ontonotes_recall: training: 0.861298 validation: 0.832499
09/16 03:58:26 PM: edges-ner-ontonotes_f1: training: 0.883550 validation: 0.867631
09/16 03:58:26 PM: Global learning rate: 1.25e-05
09/16 03:58:26 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:58:27 PM: Update 31035: task edges-ner-ontonotes, batch 35 (31035): mcc: 0.8619, acc: 0.8128, precision: 0.9022, recall: 0.8374, f1: 0.8686, edges-ner-ontonotes_loss: 0.0450
09/16 03:58:37 PM: Update 31301: task edges-ner-ontonotes, batch 301 (31301): mcc: 0.8822, acc: 0.8432, precision: 0.9126, recall: 0.8650, f1: 0.8882, edges-ner-ontonotes_loss: 0.0368
09/16 03:58:47 PM: Update 31583: task edges-ner-ontonotes, batch 583 (31583): mcc: 0.8859, acc: 0.8486, precision: 0.9162, recall: 0.8685, f1: 0.8917, edges-ner-ontonotes_loss: 0.0345
09/16 03:58:57 PM: Update 31853: task edges-ner-ontonotes, batch 853 (31853): mcc: 0.8836, acc: 0.8449, precision: 0.9138, recall: 0.8665, f1: 0.8895, edges-ner-ontonotes_loss: 0.0345
09/16 03:59:01 PM: ***** Step 32000 / Validation 32 *****
09/16 03:59:01 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:59:01 PM: Validating...
09/16 03:59:06 PM: Updating LR scheduler:
09/16 03:59:06 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:59:06 PM: 	# validation passes without improvement: 1
09/16 03:59:06 PM: edges-ner-ontonotes_loss: training: 0.034225 validation: 0.042288
09/16 03:59:06 PM: macro_avg: validation: 0.871525
09/16 03:59:06 PM: micro_avg: validation: 0.000000
09/16 03:59:06 PM: edges-ner-ontonotes_mcc: training: 0.883087 validation: 0.864641
09/16 03:59:06 PM: edges-ner-ontonotes_acc: training: 0.843995 validation: 0.826812
09/16 03:59:06 PM: edges-ner-ontonotes_precision: training: 0.912636 validation: 0.898470
09/16 03:59:06 PM: edges-ner-ontonotes_recall: training: 0.866756 validation: 0.846148
09/16 03:59:06 PM: edges-ner-ontonotes_f1: training: 0.889105 validation: 0.871525
09/16 03:59:06 PM: Global learning rate: 1.25e-05
09/16 03:59:06 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:59:07 PM: Update 32021: task edges-ner-ontonotes, batch 21 (32021): mcc: 0.8953, acc: 0.8557, precision: 0.9200, recall: 0.8824, f1: 0.9008, edges-ner-ontonotes_loss: 0.0293
09/16 03:59:17 PM: Update 32278: task edges-ner-ontonotes, batch 278 (32278): mcc: 0.8806, acc: 0.8361, precision: 0.9072, recall: 0.8675, f1: 0.8869, edges-ner-ontonotes_loss: 0.0330
09/16 03:59:27 PM: Update 32568: task edges-ner-ontonotes, batch 568 (32568): mcc: 0.8723, acc: 0.8269, precision: 0.9025, recall: 0.8564, f1: 0.8789, edges-ner-ontonotes_loss: 0.0371
09/16 03:59:37 PM: Update 32856: task edges-ner-ontonotes, batch 856 (32856): mcc: 0.8763, acc: 0.8336, precision: 0.9061, recall: 0.8604, f1: 0.8827, edges-ner-ontonotes_loss: 0.0363
09/16 03:59:42 PM: ***** Step 33000 / Validation 33 *****
09/16 03:59:42 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 03:59:42 PM: Validating...
09/16 03:59:46 PM: Updating LR scheduler:
09/16 03:59:47 PM: 	Best result seen so far for macro_avg: 0.873
09/16 03:59:47 PM: 	# validation passes without improvement: 2
09/16 03:59:47 PM: Ran out of early stopping patience. Stopping training.
09/16 03:59:47 PM: edges-ner-ontonotes_loss: training: 0.035691 validation: 0.042418
09/16 03:59:47 PM: macro_avg: validation: 0.866279
09/16 03:59:47 PM: micro_avg: validation: 0.000000
09/16 03:59:47 PM: edges-ner-ontonotes_mcc: training: 0.878602 validation: 0.859697
09/16 03:59:47 PM: edges-ner-ontonotes_acc: training: 0.837171 validation: 0.821429
09/16 03:59:47 PM: edges-ner-ontonotes_precision: training: 0.908248 validation: 0.905968
09/16 03:59:47 PM: edges-ner-ontonotes_recall: training: 0.862655 validation: 0.829921
09/16 03:59:47 PM: edges-ner-ontonotes_f1: training: 0.884865 validation: 0.866279
09/16 03:59:47 PM: Global learning rate: 1.25e-05
09/16 03:59:47 PM: Saving checkpoints to: ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:59:47 PM: Stopped training after 33 validation checks
09/16 03:59:47 PM: Trained edges-ner-ontonotes for 33000 batches or 21.236 epochs
09/16 03:59:47 PM: ***** VALIDATION RESULTS *****
09/16 03:59:47 PM: edges-ner-ontonotes_f1 (for best val pass 23): edges-ner-ontonotes_loss: 0.04270, macro_avg: 0.87285, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.86613, edges-ner-ontonotes_acc: 0.82962, edges-ner-ontonotes_precision: 0.90190, edges-ner-ontonotes_recall: 0.84562, edges-ner-ontonotes_f1: 0.87285
09/16 03:59:47 PM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.05829, macro_avg: 0.82117, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.81369, edges-ner-ontonotes_acc: 0.74606, edges-ner-ontonotes_precision: 0.88349, edges-ner-ontonotes_recall: 0.76706, edges-ner-ontonotes_f1: 0.82117
09/16 03:59:47 PM: macro_avg (for best val pass 23): edges-ner-ontonotes_loss: 0.04270, macro_avg: 0.87285, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.86613, edges-ner-ontonotes_acc: 0.82962, edges-ner-ontonotes_precision: 0.90190, edges-ner-ontonotes_recall: 0.84562, edges-ner-ontonotes_f1: 0.87285
09/16 03:59:47 PM: Evaluating...
09/16 03:59:47 PM: Loaded model state from ./experiments/ner-ontonotes-multiqa-only/run/edges-ner-ontonotes/model_state_target_train_val_23.best.th
09/16 03:59:47 PM: Evaluating on: edges-ner-ontonotes, split: val
09/16 03:59:54 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 03:59:54 PM: Finished evaluating on: edges-ner-ontonotes
09/16 03:59:54 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
09/16 03:59:54 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:59:54 PM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-multiqa-only/run
09/16 03:59:55 PM: Evaluating on: edges-ner-ontonotes, split: test
09/16 03:59:59 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 03:59:59 PM: Finished evaluating on: edges-ner-ontonotes
09/16 03:59:59 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
09/16 04:00:00 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-multiqa-only/run
09/16 04:00:00 PM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-multiqa-only/run
09/16 04:00:00 PM: Writing results for split 'val' to ./experiments/ner-ontonotes-multiqa-only/results.tsv
09/16 04:00:00 PM: micro_avg: 0.000, macro_avg: 0.874, edges-ner-ontonotes_mcc: 0.868, edges-ner-ontonotes_acc: 0.831, edges-ner-ontonotes_precision: 0.903, edges-ner-ontonotes_recall: 0.847, edges-ner-ontonotes_f1: 0.874
09/16 04:00:00 PM: Done!
09/16 04:00:00 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
