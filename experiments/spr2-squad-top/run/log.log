09/16 08:49:21 AM: Git branch: master
09/16 08:49:21 AM: Git SHA: 1f47e9f0cf1d882ab41f98d47f94e0f8258db16b
09/16 08:49:21 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/spr2-squad-top/",
  "exp_name": "experiments/spr2-squad-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/spr2-squad-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/squad",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/spr2-squad-top__run",
  "run_dir": "./experiments/spr2-squad-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-spr2",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 08:49:21 AM: Saved config to ./experiments/spr2-squad-top/run/params.conf
09/16 08:49:21 AM: Using random seed 1234
09/16 08:49:23 AM: Using GPU 0
09/16 08:49:23 AM: Loading tasks...
09/16 08:49:23 AM: Writing pre-preprocessed tasks to ./experiments/spr2-squad-top/
09/16 08:49:23 AM: 	Creating task edges-spr2 from scratch.
09/16 08:49:23 AM: Read=2226, Skip=0, Total=2226 from ./probing_data/edges/spr2/edges.train.json.retokenized.bert-base-uncased
09/16 08:49:23 AM: Read=291, Skip=0, Total=291 from ./probing_data/edges/spr2/edges.dev.json.retokenized.bert-base-uncased
09/16 08:49:23 AM: Read=276, Skip=0, Total=276 from ./probing_data/edges/spr2/edges.test.json.retokenized.bert-base-uncased
09/16 08:49:23 AM: 	Task 'edges-spr2': |train|=2226 |val|=291 |test|=276
09/16 08:49:23 AM: 	Finished loading tasks: edges-spr2.
09/16 08:49:23 AM: 	Building vocab from scratch.
09/16 08:49:23 AM: 	Counting units for task edges-spr2.
09/16 08:49:23 AM: 	Task 'edges-spr2': adding vocab namespace 'edges-spr2_labels'
09/16 08:49:24 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 08:49:24 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 08:49:25 AM: 	Saved vocab to ./experiments/spr2-squad-top/vocab
09/16 08:49:25 AM: Loading token dictionary from ./experiments/spr2-squad-top/vocab.
09/16 08:49:25 AM: 	Loaded vocab from ./experiments/spr2-squad-top/vocab
09/16 08:49:25 AM: 	Vocab namespace tokens: size 7943
09/16 08:49:25 AM: 	Vocab namespace edges-spr2_labels: size 20
09/16 08:49:25 AM: 	Vocab namespace bert_uncased: size 30524
09/16 08:49:25 AM: 	Vocab namespace chars: size 65
09/16 08:49:25 AM: 	Finished building vocab.
09/16 08:49:25 AM: 	Task edges-spr2 (train): Indexing from scratch.
09/16 08:49:25 AM: 	Task edges-spr2 (train): Saved 2226 instances to ./experiments/spr2-squad-top/preproc/edges-spr2__train_data
09/16 08:49:25 AM: 	Task edges-spr2 (val): Indexing from scratch.
09/16 08:49:25 AM: 	Task edges-spr2 (val): Saved 291 instances to ./experiments/spr2-squad-top/preproc/edges-spr2__val_data
09/16 08:49:25 AM: 	Task edges-spr2 (test): Indexing from scratch.
09/16 08:49:25 AM: 	Task edges-spr2 (test): Saved 276 instances to ./experiments/spr2-squad-top/preproc/edges-spr2__test_data
09/16 08:49:25 AM: 	Finished indexing tasks
09/16 08:49:25 AM: 	Creating trimmed target-only version of edges-spr2 train.
09/16 08:49:25 AM: 	  Training on 
09/16 08:49:25 AM: 	  Evaluating on edges-spr2
09/16 08:49:25 AM: 	Finished loading tasks in 2.624s
09/16 08:49:25 AM: 	 Tasks: ['edges-spr2']
09/16 08:49:25 AM: Building model...
09/16 08:49:25 AM: Using BERT model (bert-base-uncased).
09/16 08:49:25 AM: LOADING A FUNETUNED MODEL from: 
09/16 08:49:25 AM: models/squad
09/16 08:49:25 AM: loading configuration file models/squad/config.json
09/16 08:49:25 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 08:49:25 AM: loading weights file models/squad/pytorch_model.bin
09/16 08:49:36 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpusw52_mj
09/16 08:49:39 AM: copying /tmp/tmpusw52_mj to cache at ./experiments/spr2-squad-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 08:49:39 AM: creating metadata file for ./experiments/spr2-squad-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 08:49:39 AM: removing temp file /tmp/tmpusw52_mj
09/16 08:49:39 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/spr2-squad-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 08:49:39 AM: Initializing parameters
09/16 08:49:39 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 08:49:39 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 08:49:39 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 08:49:39 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 08:49:39 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 08:49:39 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 08:49:39 AM: 	Task 'edges-spr2' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-spr2"
}
09/16 08:49:45 AM: Model specification:
09/16 08:49:45 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-spr2_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=20, bias=True)
      )
    )
  )
)
09/16 08:49:45 AM: Model parameters:
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 08:49:45 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 08:49:45 AM: 	edges-spr2_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 08:49:45 AM: 	edges-spr2_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 08:49:45 AM: 	edges-spr2_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 08:49:45 AM: 	edges-spr2_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 08:49:45 AM: 	edges-spr2_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/16 08:49:45 AM: 	edges-spr2_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 08:49:45 AM: 	edges-spr2_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/16 08:49:45 AM: 	edges-spr2_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 08:49:45 AM: 	edges-spr2_mdl.classifier.classifier.4.weight: Trainable parameter, count 5120 with torch.Size([20, 256])
09/16 08:49:45 AM: 	edges-spr2_mdl.classifier.classifier.4.bias: Trainable parameter, count 20 with torch.Size([20])
09/16 08:49:45 AM: Total number of parameters: 110144020 (1.10144e+08)
09/16 08:49:45 AM: Number of trainable parameters: 661780 (661780)
09/16 08:49:45 AM: Finished building model in 19.547s
09/16 08:49:45 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-spr2 

09/16 08:49:46 AM: patience = 9
09/16 08:49:46 AM: val_interval = 100
09/16 08:49:46 AM: max_vals = 100
09/16 08:49:46 AM: cuda_device = 0
09/16 08:49:46 AM: grad_norm = 5.0
09/16 08:49:46 AM: grad_clipping = None
09/16 08:49:46 AM: lr_decay = 0.99
09/16 08:49:46 AM: min_lr = 1e-06
09/16 08:49:46 AM: keep_all_checkpoints = 0
09/16 08:49:46 AM: val_data_limit = 5000
09/16 08:49:46 AM: max_epochs = -1
09/16 08:49:46 AM: dec_val_scale = 250
09/16 08:49:46 AM: training_data_fraction = 1
09/16 08:49:46 AM: type = adam
09/16 08:49:46 AM: parameter_groups = None
09/16 08:49:46 AM: Number of trainable parameters: 661780
09/16 08:49:46 AM: infer_type_and_cast = True
09/16 08:49:46 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 08:49:46 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 08:49:46 AM: lr = 0.0001
09/16 08:49:46 AM: amsgrad = True
09/16 08:49:46 AM: type = reduce_on_plateau
09/16 08:49:46 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 08:49:46 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 08:49:46 AM: mode = max
09/16 08:49:46 AM: factor = 0.5
09/16 08:49:46 AM: patience = 3
09/16 08:49:46 AM: threshold = 0.0001
09/16 08:49:46 AM: threshold_mode = abs
09/16 08:49:46 AM: verbose = True
09/16 08:49:46 AM: type = adam
09/16 08:49:46 AM: parameter_groups = None
09/16 08:49:46 AM: Number of trainable parameters: 661780
09/16 08:49:46 AM: infer_type_and_cast = True
09/16 08:49:46 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 08:49:46 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 08:49:46 AM: lr = 0.0001
09/16 08:49:46 AM: amsgrad = True
09/16 08:49:46 AM: type = reduce_on_plateau
09/16 08:49:46 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 08:49:46 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 08:49:46 AM: mode = max
09/16 08:49:46 AM: factor = 0.5
09/16 08:49:46 AM: patience = 3
09/16 08:49:46 AM: threshold = 0.0001
09/16 08:49:46 AM: threshold_mode = abs
09/16 08:49:46 AM: verbose = True
09/16 08:49:46 AM: Starting training without restoring from a checkpoint.
09/16 08:49:46 AM: Training examples per task, before any subsampling: {'edges-spr2': 2226}
09/16 08:49:46 AM: Beginning training with stopping criteria based on metric: edges-spr2_f1
09/16 08:49:55 AM: ***** Step 100 / Validation 1 *****
09/16 08:49:55 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:49:55 AM: Validating...
09/16 08:49:56 AM: Evaluate: task edges-spr2, batch 6 (10): mcc: 0.6755, acc: 0.0488, precision: 0.8007, recall: 0.8019, f1: 0.8013, edges-spr2_loss: 0.3404
09/16 08:49:56 AM: Best result seen so far for edges-spr2.
09/16 08:49:56 AM: Best result seen so far for micro.
09/16 08:49:56 AM: Best result seen so far for macro.
09/16 08:49:56 AM: Updating LR scheduler:
09/16 08:49:56 AM: 	Best result seen so far for macro_avg: 0.788
09/16 08:49:56 AM: 	# validation passes without improvement: 0
09/16 08:49:56 AM: edges-spr2_loss: training: 0.393361 validation: 0.368580
09/16 08:49:56 AM: macro_avg: validation: 0.787616
09/16 08:49:56 AM: micro_avg: validation: 0.000000
09/16 08:49:56 AM: edges-spr2_mcc: training: 0.586315 validation: 0.659923
09/16 08:49:56 AM: edges-spr2_acc: training: 0.024813 validation: 0.052381
09/16 08:49:56 AM: edges-spr2_precision: training: 0.752327 validation: 0.782819
09/16 08:49:56 AM: edges-spr2_recall: training: 0.716005 validation: 0.792473
09/16 08:49:56 AM: edges-spr2_f1: training: 0.733717 validation: 0.787616
09/16 08:49:56 AM: Global learning rate: 0.0001
09/16 08:49:56 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:50:05 AM: ***** Step 200 / Validation 2 *****
09/16 08:50:05 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:50:05 AM: Validating...
09/16 08:50:06 AM: Evaluate: task edges-spr2, batch 9 (10): mcc: 0.6766, acc: 0.0797, precision: 0.8389, recall: 0.7433, f1: 0.7882, edges-spr2_loss: 0.3303
09/16 08:50:06 AM: Updating LR scheduler:
09/16 08:50:06 AM: 	Best result seen so far for macro_avg: 0.788
09/16 08:50:06 AM: 	# validation passes without improvement: 1
09/16 08:50:06 AM: edges-spr2_loss: training: 0.345397 validation: 0.352984
09/16 08:50:06 AM: macro_avg: validation: 0.787100
09/16 08:50:06 AM: micro_avg: validation: 0.000000
09/16 08:50:06 AM: edges-spr2_mcc: training: 0.652700 validation: 0.675106
09/16 08:50:06 AM: edges-spr2_acc: training: 0.049750 validation: 0.079365
09/16 08:50:06 AM: edges-spr2_precision: training: 0.800200 validation: 0.837935
09/16 08:50:06 AM: edges-spr2_recall: training: 0.751409 validation: 0.742079
09/16 08:50:06 AM: edges-spr2_f1: training: 0.775038 validation: 0.787100
09/16 08:50:06 AM: Global learning rate: 0.0001
09/16 08:50:06 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:50:15 AM: ***** Step 300 / Validation 3 *****
09/16 08:50:15 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:50:15 AM: Validating...
09/16 08:50:16 AM: Evaluate: task edges-spr2, batch 7 (10): mcc: 0.6940, acc: 0.0573, precision: 0.8397, recall: 0.7747, f1: 0.8059, edges-spr2_loss: 0.3113
09/16 08:50:16 AM: Best result seen so far for edges-spr2.
09/16 08:50:16 AM: Best result seen so far for macro.
09/16 08:50:16 AM: Updating LR scheduler:
09/16 08:50:16 AM: 	Best result seen so far for macro_avg: 0.794
09/16 08:50:16 AM: 	# validation passes without improvement: 0
09/16 08:50:16 AM: edges-spr2_loss: training: 0.329302 validation: 0.346340
09/16 08:50:16 AM: macro_avg: validation: 0.794105
09/16 08:50:16 AM: micro_avg: validation: 0.000000
09/16 08:50:16 AM: edges-spr2_mcc: training: 0.668092 validation: 0.681043
09/16 08:50:16 AM: edges-spr2_acc: training: 0.059182 validation: 0.058730
09/16 08:50:16 AM: edges-spr2_precision: training: 0.812685 validation: 0.829206
09/16 08:50:16 AM: edges-spr2_recall: training: 0.758382 validation: 0.761854
09/16 08:50:16 AM: edges-spr2_f1: training: 0.784595 validation: 0.794105
09/16 08:50:16 AM: Global learning rate: 0.0001
09/16 08:50:16 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:50:25 AM: ***** Step 400 / Validation 4 *****
09/16 08:50:25 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:50:25 AM: Validating...
09/16 08:50:26 AM: Best result seen so far for edges-spr2.
09/16 08:50:26 AM: Best result seen so far for macro.
09/16 08:50:26 AM: Updating LR scheduler:
09/16 08:50:26 AM: 	Best result seen so far for macro_avg: 0.806
09/16 08:50:26 AM: 	# validation passes without improvement: 0
09/16 08:50:26 AM: edges-spr2_loss: training: 0.321329 validation: 0.338623
09/16 08:50:26 AM: macro_avg: validation: 0.806378
09/16 08:50:26 AM: micro_avg: validation: 0.000000
09/16 08:50:26 AM: edges-spr2_mcc: training: 0.675555 validation: 0.696626
09/16 08:50:26 AM: edges-spr2_acc: training: 0.058898 validation: 0.071429
09/16 08:50:26 AM: edges-spr2_precision: training: 0.814956 validation: 0.828918
09/16 08:50:26 AM: edges-spr2_recall: training: 0.766277 validation: 0.785031
09/16 08:50:26 AM: edges-spr2_f1: training: 0.789867 validation: 0.806378
09/16 08:50:26 AM: Global learning rate: 0.0001
09/16 08:50:26 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:50:26 AM: Update 405: task edges-spr2, batch 5 (405): mcc: 0.6808, acc: 0.0438, precision: 0.8102, recall: 0.7816, f1: 0.7956, edges-spr2_loss: 0.3160
09/16 08:50:35 AM: ***** Step 500 / Validation 5 *****
09/16 08:50:35 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:50:35 AM: Validating...
09/16 08:50:36 AM: Updating LR scheduler:
09/16 08:50:36 AM: 	Best result seen so far for macro_avg: 0.806
09/16 08:50:36 AM: 	# validation passes without improvement: 1
09/16 08:50:36 AM: edges-spr2_loss: training: 0.316560 validation: 0.335004
09/16 08:50:36 AM: macro_avg: validation: 0.805166
09/16 08:50:36 AM: micro_avg: validation: 0.000000
09/16 08:50:36 AM: edges-spr2_mcc: training: 0.678815 validation: 0.697314
09/16 08:50:36 AM: edges-spr2_acc: training: 0.061621 validation: 0.087302
09/16 08:50:36 AM: edges-spr2_precision: training: 0.817146 validation: 0.837236
09/16 08:50:36 AM: edges-spr2_recall: training: 0.768986 validation: 0.775462
09/16 08:50:36 AM: edges-spr2_f1: training: 0.792335 validation: 0.805166
09/16 08:50:36 AM: Global learning rate: 0.0001
09/16 08:50:36 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:50:36 AM: Update 503: task edges-spr2, batch 3 (503): mcc: 0.6708, acc: 0.0735, precision: 0.8156, recall: 0.7604, f1: 0.7870, edges-spr2_loss: 0.3178
09/16 08:50:45 AM: ***** Step 600 / Validation 6 *****
09/16 08:50:45 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:50:45 AM: Validating...
09/16 08:50:46 AM: Updating LR scheduler:
09/16 08:50:46 AM: 	Best result seen so far for macro_avg: 0.806
09/16 08:50:46 AM: 	# validation passes without improvement: 2
09/16 08:50:46 AM: edges-spr2_loss: training: 0.314445 validation: 0.332786
09/16 08:50:46 AM: macro_avg: validation: 0.800090
09/16 08:50:46 AM: micro_avg: validation: 0.000000
09/16 08:50:46 AM: edges-spr2_mcc: training: 0.682877 validation: 0.694838
09/16 08:50:46 AM: edges-spr2_acc: training: 0.062271 validation: 0.080952
09/16 08:50:46 AM: edges-spr2_precision: training: 0.819091 validation: 0.850575
09/16 08:50:46 AM: edges-spr2_recall: training: 0.771941 validation: 0.755263
09/16 08:50:46 AM: edges-spr2_f1: training: 0.794818 validation: 0.800090
09/16 08:50:46 AM: Global learning rate: 0.0001
09/16 08:50:46 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:50:46 AM: Update 604: task edges-spr2, batch 4 (604): mcc: 0.6816, acc: 0.0536, precision: 0.8394, recall: 0.7555, f1: 0.7952, edges-spr2_loss: 0.3161
09/16 08:50:55 AM: ***** Step 700 / Validation 7 *****
09/16 08:50:55 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:50:55 AM: Validating...
09/16 08:50:56 AM: Updating LR scheduler:
09/16 08:50:56 AM: 	Best result seen so far for macro_avg: 0.806
09/16 08:50:56 AM: 	# validation passes without improvement: 3
09/16 08:50:56 AM: edges-spr2_loss: training: 0.311097 validation: 0.329680
09/16 08:50:56 AM: macro_avg: validation: 0.804567
09/16 08:50:56 AM: micro_avg: validation: 0.000000
09/16 08:50:56 AM: edges-spr2_mcc: training: 0.683621 validation: 0.697447
09/16 08:50:56 AM: edges-spr2_acc: training: 0.057429 validation: 0.088889
09/16 08:50:56 AM: edges-spr2_precision: training: 0.820861 validation: 0.840435
09/16 08:50:56 AM: edges-spr2_recall: training: 0.771222 validation: 0.771635
09/16 08:50:56 AM: edges-spr2_f1: training: 0.795268 validation: 0.804567
09/16 08:50:56 AM: Global learning rate: 0.0001
09/16 08:50:56 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:50:56 AM: Update 701: task edges-spr2, batch 1 (701): mcc: 0.6075, acc: 0.0485, precision: 0.7630, recall: 0.7422, f1: 0.7525, edges-spr2_loss: 0.3725
09/16 08:51:05 AM: ***** Step 800 / Validation 8 *****
09/16 08:51:05 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:51:05 AM: Validating...
09/16 08:51:06 AM: Best result seen so far for edges-spr2.
09/16 08:51:06 AM: Best result seen so far for macro.
09/16 08:51:06 AM: Updating LR scheduler:
09/16 08:51:06 AM: 	Best result seen so far for macro_avg: 0.811
09/16 08:51:06 AM: 	# validation passes without improvement: 0
09/16 08:51:06 AM: edges-spr2_loss: training: 0.308772 validation: 0.329413
09/16 08:51:06 AM: macro_avg: validation: 0.810501
09/16 08:51:06 AM: micro_avg: validation: 0.000000
09/16 08:51:06 AM: edges-spr2_mcc: training: 0.686241 validation: 0.700767
09/16 08:51:06 AM: edges-spr2_acc: training: 0.062741 validation: 0.061905
09/16 08:51:06 AM: edges-spr2_precision: training: 0.823192 validation: 0.823852
09/16 08:51:06 AM: edges-spr2_recall: training: 0.771827 validation: 0.797576
09/16 08:51:06 AM: edges-spr2_f1: training: 0.796682 validation: 0.810501
09/16 08:51:06 AM: Global learning rate: 0.0001
09/16 08:51:06 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:51:06 AM: Update 806: task edges-spr2, batch 6 (806): mcc: 0.6994, acc: 0.0476, precision: 0.8176, recall: 0.8034, f1: 0.8105, edges-spr2_loss: 0.3093
09/16 08:51:14 AM: ***** Step 900 / Validation 9 *****
09/16 08:51:14 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:51:14 AM: Validating...
09/16 08:51:15 AM: Updating LR scheduler:
09/16 08:51:15 AM: 	Best result seen so far for macro_avg: 0.811
09/16 08:51:15 AM: 	# validation passes without improvement: 1
09/16 08:51:15 AM: edges-spr2_loss: training: 0.308329 validation: 0.327526
09/16 08:51:15 AM: macro_avg: validation: 0.807714
09/16 08:51:15 AM: micro_avg: validation: 0.000000
09/16 08:51:15 AM: edges-spr2_mcc: training: 0.686730 validation: 0.702297
09/16 08:51:15 AM: edges-spr2_acc: training: 0.063116 validation: 0.074603
09/16 08:51:15 AM: edges-spr2_precision: training: 0.822845 validation: 0.843518
09/16 08:51:15 AM: edges-spr2_recall: training: 0.772322 validation: 0.774825
09/16 08:51:15 AM: edges-spr2_f1: training: 0.796783 validation: 0.807714
09/16 08:51:15 AM: Global learning rate: 0.0001
09/16 08:51:15 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:51:17 AM: Update 911: task edges-spr2, batch 11 (911): mcc: 0.6744, acc: 0.0426, precision: 0.8074, recall: 0.7817, f1: 0.7944, edges-spr2_loss: 0.3163
09/16 08:51:24 AM: ***** Step 1000 / Validation 10 *****
09/16 08:51:24 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:51:24 AM: Validating...
09/16 08:51:25 AM: Updating LR scheduler:
09/16 08:51:25 AM: 	Best result seen so far for macro_avg: 0.811
09/16 08:51:25 AM: 	# validation passes without improvement: 2
09/16 08:51:25 AM: edges-spr2_loss: training: 0.306957 validation: 0.328027
09/16 08:51:25 AM: macro_avg: validation: 0.807348
09/16 08:51:25 AM: micro_avg: validation: 0.000000
09/16 08:51:25 AM: edges-spr2_mcc: training: 0.686421 validation: 0.701332
09/16 08:51:25 AM: edges-spr2_acc: training: 0.065362 validation: 0.066667
09/16 08:51:25 AM: edges-spr2_precision: training: 0.820780 validation: 0.841717
09/16 08:51:25 AM: edges-spr2_recall: training: 0.776548 validation: 0.775675
09/16 08:51:25 AM: edges-spr2_f1: training: 0.798052 validation: 0.807348
09/16 08:51:25 AM: Global learning rate: 0.0001
09/16 08:51:25 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:51:27 AM: Update 1020: task edges-spr2, batch 20 (1020): mcc: 0.6875, acc: 0.0669, precision: 0.8230, recall: 0.7746, f1: 0.7981, edges-spr2_loss: 0.3142
09/16 08:51:33 AM: ***** Step 1100 / Validation 11 *****
09/16 08:51:33 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:51:33 AM: Validating...
09/16 08:51:34 AM: Updating LR scheduler:
09/16 08:51:34 AM: 	Best result seen so far for macro_avg: 0.811
09/16 08:51:34 AM: 	# validation passes without improvement: 3
09/16 08:51:34 AM: edges-spr2_loss: training: 0.306717 validation: 0.325497
09/16 08:51:34 AM: macro_avg: validation: 0.807186
09/16 08:51:34 AM: micro_avg: validation: 0.000000
09/16 08:51:34 AM: edges-spr2_mcc: training: 0.688444 validation: 0.703250
09/16 08:51:34 AM: edges-spr2_acc: training: 0.064807 validation: 0.079365
09/16 08:51:34 AM: edges-spr2_precision: training: 0.823587 validation: 0.849260
09/16 08:51:34 AM: edges-spr2_recall: training: 0.774024 validation: 0.769084
09/16 08:51:34 AM: edges-spr2_f1: training: 0.798037 validation: 0.807186
09/16 08:51:34 AM: Global learning rate: 0.0001
09/16 08:51:34 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:51:37 AM: Update 1123: task edges-spr2, batch 23 (1123): mcc: 0.6869, acc: 0.0602, precision: 0.8224, recall: 0.7739, f1: 0.7974, edges-spr2_loss: 0.3073
09/16 08:51:43 AM: ***** Step 1200 / Validation 12 *****
09/16 08:51:43 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:51:43 AM: Validating...
09/16 08:51:44 AM: Best result seen so far for edges-spr2.
09/16 08:51:44 AM: Best result seen so far for macro.
09/16 08:51:44 AM: Updating LR scheduler:
09/16 08:51:44 AM: 	Best result seen so far for macro_avg: 0.812
09/16 08:51:44 AM: 	# validation passes without improvement: 0
09/16 08:51:44 AM: edges-spr2_loss: training: 0.304573 validation: 0.323437
09/16 08:51:44 AM: macro_avg: validation: 0.811569
09/16 08:51:44 AM: micro_avg: validation: 0.000000
09/16 08:51:44 AM: edges-spr2_mcc: training: 0.688799 validation: 0.703815
09/16 08:51:44 AM: edges-spr2_acc: training: 0.062264 validation: 0.063492
09/16 08:51:44 AM: edges-spr2_precision: training: 0.822115 validation: 0.830441
09/16 08:51:44 AM: edges-spr2_recall: training: 0.777148 validation: 0.793536
09/16 08:51:44 AM: edges-spr2_f1: training: 0.798999 validation: 0.811569
09/16 08:51:44 AM: Global learning rate: 0.0001
09/16 08:51:44 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:51:47 AM: Update 1237: task edges-spr2, batch 37 (1237): mcc: 0.6917, acc: 0.0730, precision: 0.8250, recall: 0.7771, f1: 0.8004, edges-spr2_loss: 0.3024
09/16 08:51:52 AM: ***** Step 1300 / Validation 13 *****
09/16 08:51:52 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:51:52 AM: Validating...
09/16 08:51:53 AM: Updating LR scheduler:
09/16 08:51:53 AM: 	Best result seen so far for macro_avg: 0.812
09/16 08:51:53 AM: 	# validation passes without improvement: 1
09/16 08:51:53 AM: edges-spr2_loss: training: 0.303078 validation: 0.325194
09/16 08:51:53 AM: macro_avg: validation: 0.807080
09/16 08:51:53 AM: micro_avg: validation: 0.000000
09/16 08:51:53 AM: edges-spr2_mcc: training: 0.693197 validation: 0.702463
09/16 08:51:53 AM: edges-spr2_acc: training: 0.069597 validation: 0.077778
09/16 08:51:53 AM: edges-spr2_precision: training: 0.826322 validation: 0.846963
09/16 08:51:53 AM: edges-spr2_recall: training: 0.777547 validation: 0.770785
09/16 08:51:53 AM: edges-spr2_f1: training: 0.801193 validation: 0.807080
09/16 08:51:53 AM: Global learning rate: 0.0001
09/16 08:51:53 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:51:57 AM: Update 1345: task edges-spr2, batch 45 (1345): mcc: 0.6929, acc: 0.0718, precision: 0.8257, recall: 0.7785, f1: 0.8014, edges-spr2_loss: 0.3024
09/16 08:52:01 AM: ***** Step 1400 / Validation 14 *****
09/16 08:52:01 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:52:01 AM: Validating...
09/16 08:52:02 AM: Updating LR scheduler:
09/16 08:52:02 AM: 	Best result seen so far for macro_avg: 0.812
09/16 08:52:02 AM: 	# validation passes without improvement: 2
09/16 08:52:02 AM: edges-spr2_loss: training: 0.300967 validation: 0.325450
09/16 08:52:02 AM: macro_avg: validation: 0.806725
09/16 08:52:02 AM: micro_avg: validation: 0.000000
09/16 08:52:02 AM: edges-spr2_mcc: training: 0.695830 validation: 0.701939
09/16 08:52:02 AM: edges-spr2_acc: training: 0.071844 validation: 0.063492
09/16 08:52:02 AM: edges-spr2_precision: training: 0.826584 validation: 0.846693
09/16 08:52:02 AM: edges-spr2_recall: training: 0.782141 validation: 0.770359
09/16 08:52:02 AM: edges-spr2_f1: training: 0.803749 validation: 0.806725
09/16 08:52:02 AM: Global learning rate: 0.0001
09/16 08:52:02 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:52:07 AM: Update 1462: task edges-spr2, batch 62 (1462): mcc: 0.6934, acc: 0.0705, precision: 0.8278, recall: 0.7768, f1: 0.8015, edges-spr2_loss: 0.3013
09/16 08:52:10 AM: ***** Step 1500 / Validation 15 *****
09/16 08:52:10 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:52:10 AM: Validating...
09/16 08:52:11 AM: Best result seen so far for edges-spr2.
09/16 08:52:11 AM: Best result seen so far for macro.
09/16 08:52:11 AM: Updating LR scheduler:
09/16 08:52:11 AM: 	Best result seen so far for macro_avg: 0.813
09/16 08:52:11 AM: 	# validation passes without improvement: 0
09/16 08:52:11 AM: edges-spr2_loss: training: 0.301506 validation: 0.322963
09/16 08:52:11 AM: macro_avg: validation: 0.812685
09/16 08:52:11 AM: micro_avg: validation: 0.000000
09/16 08:52:11 AM: edges-spr2_mcc: training: 0.692068 validation: 0.706834
09/16 08:52:11 AM: edges-spr2_acc: training: 0.068045 validation: 0.085714
09/16 08:52:11 AM: edges-spr2_precision: training: 0.826744 validation: 0.836560
09/16 08:52:11 AM: edges-spr2_recall: training: 0.776528 validation: 0.790134
09/16 08:52:11 AM: edges-spr2_f1: training: 0.800849 validation: 0.812685
09/16 08:52:11 AM: Global learning rate: 0.0001
09/16 08:52:11 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:52:17 AM: Update 1572: task edges-spr2, batch 72 (1572): mcc: 0.6948, acc: 0.0721, precision: 0.8272, recall: 0.7787, f1: 0.8022, edges-spr2_loss: 0.2999
09/16 08:52:19 AM: ***** Step 1600 / Validation 16 *****
09/16 08:52:19 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:52:19 AM: Validating...
09/16 08:52:20 AM: Updating LR scheduler:
09/16 08:52:20 AM: 	Best result seen so far for macro_avg: 0.813
09/16 08:52:20 AM: 	# validation passes without improvement: 1
09/16 08:52:20 AM: edges-spr2_loss: training: 0.299930 validation: 0.322728
09/16 08:52:20 AM: macro_avg: validation: 0.812425
09/16 08:52:20 AM: micro_avg: validation: 0.000000
09/16 08:52:20 AM: edges-spr2_mcc: training: 0.695531 validation: 0.705690
09/16 08:52:20 AM: edges-spr2_acc: training: 0.073258 validation: 0.060317
09/16 08:52:20 AM: edges-spr2_precision: training: 0.827914 validation: 0.833408
09/16 08:52:20 AM: edges-spr2_recall: training: 0.779036 validation: 0.792473
09/16 08:52:20 AM: edges-spr2_f1: training: 0.802732 validation: 0.812425
09/16 08:52:20 AM: Global learning rate: 0.0001
09/16 08:52:20 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:52:27 AM: Update 1682: task edges-spr2, batch 82 (1682): mcc: 0.6941, acc: 0.0707, precision: 0.8256, recall: 0.7801, f1: 0.8022, edges-spr2_loss: 0.3003
09/16 08:52:28 AM: ***** Step 1700 / Validation 17 *****
09/16 08:52:28 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:52:28 AM: Validating...
09/16 08:52:29 AM: Updating LR scheduler:
09/16 08:52:29 AM: 	Best result seen so far for macro_avg: 0.813
09/16 08:52:29 AM: 	# validation passes without improvement: 2
09/16 08:52:29 AM: edges-spr2_loss: training: 0.299542 validation: 0.320461
09/16 08:52:29 AM: macro_avg: validation: 0.808081
09/16 08:52:29 AM: micro_avg: validation: 0.000000
09/16 08:52:29 AM: edges-spr2_mcc: training: 0.694666 validation: 0.703268
09/16 08:52:29 AM: edges-spr2_acc: training: 0.071692 validation: 0.068254
09/16 08:52:29 AM: edges-spr2_precision: training: 0.827165 validation: 0.845332
09/16 08:52:29 AM: edges-spr2_recall: training: 0.778884 validation: 0.773974
09/16 08:52:29 AM: edges-spr2_f1: training: 0.802299 validation: 0.808081
09/16 08:52:29 AM: Global learning rate: 0.0001
09/16 08:52:29 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:52:37 AM: Update 1799: task edges-spr2, batch 99 (1799): mcc: 0.6956, acc: 0.0658, precision: 0.8259, recall: 0.7820, f1: 0.8034, edges-spr2_loss: 0.2989
09/16 08:52:37 AM: ***** Step 1800 / Validation 18 *****
09/16 08:52:37 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:52:37 AM: Validating...
09/16 08:52:38 AM: Updating LR scheduler:
09/16 08:52:38 AM: 	Best result seen so far for macro_avg: 0.813
09/16 08:52:38 AM: 	# validation passes without improvement: 3
09/16 08:52:38 AM: edges-spr2_loss: training: 0.299240 validation: 0.323473
09/16 08:52:38 AM: macro_avg: validation: 0.800181
09/16 08:52:38 AM: micro_avg: validation: 0.000000
09/16 08:52:38 AM: edges-spr2_mcc: training: 0.695211 validation: 0.696676
09/16 08:52:38 AM: edges-spr2_acc: training: 0.065524 validation: 0.085714
09/16 08:52:38 AM: edges-spr2_precision: training: 0.825837 validation: 0.856242
09/16 08:52:38 AM: edges-spr2_recall: training: 0.781488 validation: 0.751010
09/16 08:52:38 AM: edges-spr2_f1: training: 0.803051 validation: 0.800181
09/16 08:52:38 AM: Global learning rate: 0.0001
09/16 08:52:38 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:52:47 AM: ***** Step 1900 / Validation 19 *****
09/16 08:52:47 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:52:47 AM: Validating...
09/16 08:52:47 AM: Evaluate: task edges-spr2, batch 3 (10): mcc: 0.7118, acc: 0.0603, precision: 0.8411, recall: 0.8021, f1: 0.8211, edges-spr2_loss: 0.2974
09/16 08:52:48 AM: Best result seen so far for edges-spr2.
09/16 08:52:48 AM: Best result seen so far for macro.
09/16 08:52:48 AM: Updating LR scheduler:
09/16 08:52:48 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:52:48 AM: 	# validation passes without improvement: 0
09/16 08:52:48 AM: edges-spr2_loss: training: 0.301081 validation: 0.321098
09/16 08:52:48 AM: macro_avg: validation: 0.813604
09/16 08:52:48 AM: micro_avg: validation: 0.000000
09/16 08:52:48 AM: edges-spr2_mcc: training: 0.692268 validation: 0.707569
09/16 08:52:48 AM: edges-spr2_acc: training: 0.066703 validation: 0.061905
09/16 08:52:48 AM: edges-spr2_precision: training: 0.826119 validation: 0.834713
09/16 08:52:48 AM: edges-spr2_recall: training: 0.777292 validation: 0.793536
09/16 08:52:48 AM: edges-spr2_f1: training: 0.800962 validation: 0.813604
09/16 08:52:48 AM: Global learning rate: 0.0001
09/16 08:52:48 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:52:56 AM: ***** Step 2000 / Validation 20 *****
09/16 08:52:56 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:52:56 AM: Validating...
09/16 08:52:57 AM: Updating LR scheduler:
09/16 08:52:57 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:52:57 AM: 	# validation passes without improvement: 1
09/16 08:52:57 AM: edges-spr2_loss: training: 0.297644 validation: 0.322223
09/16 08:52:57 AM: macro_avg: validation: 0.810463
09/16 08:52:57 AM: micro_avg: validation: 0.000000
09/16 08:52:57 AM: edges-spr2_mcc: training: 0.698052 validation: 0.702604
09/16 08:52:57 AM: edges-spr2_acc: training: 0.071005 validation: 0.076190
09/16 08:52:57 AM: edges-spr2_precision: training: 0.831113 validation: 0.831395
09/16 08:52:57 AM: edges-spr2_recall: training: 0.779370 validation: 0.790559
09/16 08:52:57 AM: edges-spr2_f1: training: 0.804410 validation: 0.810463
09/16 08:52:57 AM: Global learning rate: 0.0001
09/16 08:52:57 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:52:57 AM: Update 2007: task edges-spr2, batch 7 (2007): mcc: 0.7127, acc: 0.0731, precision: 0.8313, recall: 0.7995, f1: 0.8151, edges-spr2_loss: 0.2908
09/16 08:53:05 AM: ***** Step 2100 / Validation 21 *****
09/16 08:53:05 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:53:05 AM: Validating...
09/16 08:53:06 AM: Updating LR scheduler:
09/16 08:53:06 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:53:06 AM: 	# validation passes without improvement: 2
09/16 08:53:06 AM: edges-spr2_loss: training: 0.298347 validation: 0.321263
09/16 08:53:06 AM: macro_avg: validation: 0.809118
09/16 08:53:06 AM: micro_avg: validation: 0.000000
09/16 08:53:06 AM: edges-spr2_mcc: training: 0.696978 validation: 0.704096
09/16 08:53:06 AM: edges-spr2_acc: training: 0.072146 validation: 0.069841
09/16 08:53:06 AM: edges-spr2_precision: training: 0.829205 validation: 0.843563
09/16 08:53:06 AM: edges-spr2_recall: training: 0.780073 validation: 0.777376
09/16 08:53:06 AM: edges-spr2_f1: training: 0.803889 validation: 0.809118
09/16 08:53:06 AM: Global learning rate: 0.0001
09/16 08:53:06 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:53:07 AM: Update 2115: task edges-spr2, batch 15 (2115): mcc: 0.6792, acc: 0.0714, precision: 0.8135, recall: 0.7711, f1: 0.7917, edges-spr2_loss: 0.3083
09/16 08:53:14 AM: ***** Step 2200 / Validation 22 *****
09/16 08:53:14 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:53:14 AM: Validating...
09/16 08:53:15 AM: Updating LR scheduler:
09/16 08:53:15 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:53:15 AM: 	# validation passes without improvement: 3
09/16 08:53:15 AM: edges-spr2_loss: training: 0.300630 validation: 0.320853
09/16 08:53:15 AM: macro_avg: validation: 0.810674
09/16 08:53:15 AM: micro_avg: validation: 0.000000
09/16 08:53:15 AM: edges-spr2_mcc: training: 0.692566 validation: 0.705650
09/16 08:53:15 AM: edges-spr2_acc: training: 0.072211 validation: 0.061905
09/16 08:53:15 AM: edges-spr2_precision: training: 0.825175 validation: 0.841961
09/16 08:53:15 AM: edges-spr2_recall: training: 0.778672 validation: 0.781629
09/16 08:53:15 AM: edges-spr2_f1: training: 0.801250 validation: 0.810674
09/16 08:53:15 AM: Global learning rate: 0.0001
09/16 08:53:15 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:53:17 AM: Update 2229: task edges-spr2, batch 29 (2229): mcc: 0.7025, acc: 0.0712, precision: 0.8328, recall: 0.7848, f1: 0.8081, edges-spr2_loss: 0.2927
09/16 08:53:23 AM: ***** Step 2300 / Validation 23 *****
09/16 08:53:23 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:53:23 AM: Validating...
09/16 08:53:24 AM: Best result seen so far for edges-spr2.
09/16 08:53:24 AM: Best result seen so far for macro.
09/16 08:53:24 AM: Updating LR scheduler:
09/16 08:53:24 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:53:24 AM: 	# validation passes without improvement: 0
09/16 08:53:24 AM: edges-spr2_loss: training: 0.295811 validation: 0.320659
09/16 08:53:24 AM: macro_avg: validation: 0.814250
09/16 08:53:24 AM: micro_avg: validation: 0.000000
09/16 08:53:24 AM: edges-spr2_mcc: training: 0.699616 validation: 0.708465
09/16 08:53:24 AM: edges-spr2_acc: training: 0.073595 validation: 0.063492
09/16 08:53:24 AM: edges-spr2_precision: training: 0.831253 validation: 0.834897
09/16 08:53:24 AM: edges-spr2_recall: training: 0.781703 validation: 0.794599
09/16 08:53:24 AM: edges-spr2_f1: training: 0.805717 validation: 0.814250
09/16 08:53:24 AM: Global learning rate: 0.0001
09/16 08:53:24 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:53:27 AM: Update 2333: task edges-spr2, batch 33 (2333): mcc: 0.6914, acc: 0.0678, precision: 0.8241, recall: 0.7773, f1: 0.8000, edges-spr2_loss: 0.3033
09/16 08:53:33 AM: ***** Step 2400 / Validation 24 *****
09/16 08:53:33 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:53:33 AM: Validating...
09/16 08:53:34 AM: Updating LR scheduler:
09/16 08:53:34 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:53:34 AM: 	# validation passes without improvement: 1
09/16 08:53:34 AM: edges-spr2_loss: training: 0.294272 validation: 0.321590
09/16 08:53:34 AM: macro_avg: validation: 0.812741
09/16 08:53:34 AM: micro_avg: validation: 0.000000
09/16 08:53:34 AM: edges-spr2_mcc: training: 0.700984 validation: 0.708776
09/16 08:53:34 AM: edges-spr2_acc: training: 0.075708 validation: 0.069841
09/16 08:53:34 AM: edges-spr2_precision: training: 0.830732 validation: 0.843707
09/16 08:53:34 AM: edges-spr2_recall: training: 0.783541 validation: 0.783968
09/16 08:53:34 AM: edges-spr2_f1: training: 0.806446 validation: 0.812741
09/16 08:53:34 AM: Global learning rate: 0.0001
09/16 08:53:34 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:53:37 AM: Update 2445: task edges-spr2, batch 45 (2445): mcc: 0.6971, acc: 0.0708, precision: 0.8310, recall: 0.7777, f1: 0.8035, edges-spr2_loss: 0.2984
09/16 08:53:42 AM: ***** Step 2500 / Validation 25 *****
09/16 08:53:42 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:53:42 AM: Validating...
09/16 08:53:43 AM: Updating LR scheduler:
09/16 08:53:43 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:53:43 AM: 	# validation passes without improvement: 2
09/16 08:53:43 AM: edges-spr2_loss: training: 0.298869 validation: 0.321827
09/16 08:53:43 AM: macro_avg: validation: 0.812184
09/16 08:53:43 AM: micro_avg: validation: 0.000000
09/16 08:53:43 AM: edges-spr2_mcc: training: 0.695563 validation: 0.707350
09/16 08:53:43 AM: edges-spr2_acc: training: 0.069009 validation: 0.063492
09/16 08:53:43 AM: edges-spr2_precision: training: 0.828327 validation: 0.841038
09/16 08:53:43 AM: edges-spr2_recall: training: 0.779369 validation: 0.785243
09/16 08:53:43 AM: edges-spr2_f1: training: 0.803103 validation: 0.812184
09/16 08:53:43 AM: Global learning rate: 0.0001
09/16 08:53:43 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:53:47 AM: Update 2549: task edges-spr2, batch 49 (2549): mcc: 0.6947, acc: 0.0689, precision: 0.8269, recall: 0.7789, f1: 0.8022, edges-spr2_loss: 0.2986
09/16 08:53:52 AM: ***** Step 2600 / Validation 26 *****
09/16 08:53:52 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:53:52 AM: Validating...
09/16 08:53:53 AM: Updating LR scheduler:
09/16 08:53:53 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:53:53 AM: 	# validation passes without improvement: 3
09/16 08:53:53 AM: edges-spr2_loss: training: 0.294727 validation: 0.320644
09/16 08:53:53 AM: macro_avg: validation: 0.814173
09/16 08:53:53 AM: micro_avg: validation: 0.000000
09/16 08:53:53 AM: edges-spr2_mcc: training: 0.699572 validation: 0.707177
09/16 08:53:53 AM: edges-spr2_acc: training: 0.073088 validation: 0.060317
09/16 08:53:53 AM: edges-spr2_precision: training: 0.830597 validation: 0.830093
09/16 08:53:53 AM: edges-spr2_recall: training: 0.782106 validation: 0.798852
09/16 08:53:53 AM: edges-spr2_f1: training: 0.805623 validation: 0.814173
09/16 08:53:53 AM: Global learning rate: 0.0001
09/16 08:53:53 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:53:58 AM: Update 2661: task edges-spr2, batch 61 (2661): mcc: 0.7019, acc: 0.0753, precision: 0.8307, recall: 0.7850, f1: 0.8072, edges-spr2_loss: 0.2953
09/16 08:54:01 AM: ***** Step 2700 / Validation 27 *****
09/16 08:54:01 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:54:01 AM: Validating...
09/16 08:54:02 AM: Updating LR scheduler:
09/16 08:54:02 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:54:02 AM: 	# validation passes without improvement: 0
09/16 08:54:02 AM: edges-spr2_loss: training: 0.295163 validation: 0.322271
09/16 08:54:02 AM: macro_avg: validation: 0.809572
09/16 08:54:02 AM: micro_avg: validation: 0.000000
09/16 08:54:02 AM: edges-spr2_mcc: training: 0.701061 validation: 0.706282
09/16 08:54:02 AM: edges-spr2_acc: training: 0.075345 validation: 0.061905
09/16 08:54:02 AM: edges-spr2_precision: training: 0.832620 validation: 0.849369
09/16 08:54:02 AM: edges-spr2_recall: training: 0.781502 validation: 0.773336
09/16 08:54:02 AM: edges-spr2_f1: training: 0.806251 validation: 0.809572
09/16 08:54:02 AM: Global learning rate: 5e-05
09/16 08:54:02 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:54:08 AM: Update 2775: task edges-spr2, batch 75 (2775): mcc: 0.6994, acc: 0.0766, precision: 0.8304, recall: 0.7828, f1: 0.8059, edges-spr2_loss: 0.2947
09/16 08:54:09 AM: ***** Step 2800 / Validation 28 *****
09/16 08:54:09 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:54:09 AM: Validating...
09/16 08:54:10 AM: Updating LR scheduler:
09/16 08:54:10 AM: 	Best result seen so far for macro_avg: 0.814
09/16 08:54:10 AM: 	# validation passes without improvement: 1
09/16 08:54:10 AM: edges-spr2_loss: training: 0.293395 validation: 0.319712
09/16 08:54:10 AM: macro_avg: validation: 0.811242
09/16 08:54:10 AM: micro_avg: validation: 0.000000
09/16 08:54:10 AM: edges-spr2_mcc: training: 0.701940 validation: 0.707382
09/16 08:54:10 AM: edges-spr2_acc: training: 0.076770 validation: 0.074603
09/16 08:54:10 AM: edges-spr2_precision: training: 0.832443 validation: 0.845675
09/16 08:54:10 AM: edges-spr2_recall: training: 0.783153 validation: 0.779502
09/16 08:54:10 AM: edges-spr2_f1: training: 0.807046 validation: 0.811242
09/16 08:54:10 AM: Global learning rate: 5e-05
09/16 08:54:10 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:54:18 AM: Update 2884: task edges-spr2, batch 84 (2884): mcc: 0.7010, acc: 0.0730, precision: 0.8321, recall: 0.7819, f1: 0.8062, edges-spr2_loss: 0.2931
09/16 08:54:19 AM: ***** Step 2900 / Validation 29 *****
09/16 08:54:19 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:54:19 AM: Validating...
09/16 08:54:20 AM: Best result seen so far for edges-spr2.
09/16 08:54:20 AM: Best result seen so far for macro.
09/16 08:54:20 AM: Updating LR scheduler:
09/16 08:54:20 AM: 	Best result seen so far for macro_avg: 0.816
09/16 08:54:20 AM: 	# validation passes without improvement: 0
09/16 08:54:20 AM: edges-spr2_loss: training: 0.293821 validation: 0.319648
09/16 08:54:20 AM: macro_avg: validation: 0.816123
09/16 08:54:20 AM: micro_avg: validation: 0.000000
09/16 08:54:20 AM: edges-spr2_mcc: training: 0.700968 validation: 0.710255
09/16 08:54:20 AM: edges-spr2_acc: training: 0.072305 validation: 0.066667
09/16 08:54:20 AM: edges-spr2_precision: training: 0.832031 validation: 0.832081
09/16 08:54:20 AM: edges-spr2_recall: training: 0.782018 validation: 0.800765
09/16 08:54:20 AM: edges-spr2_f1: training: 0.806250 validation: 0.816123
09/16 08:54:20 AM: Global learning rate: 5e-05
09/16 08:54:20 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:54:28 AM: Update 2995: task edges-spr2, batch 95 (2995): mcc: 0.7042, acc: 0.0792, precision: 0.8336, recall: 0.7860, f1: 0.8091, edges-spr2_loss: 0.2907
09/16 08:54:28 AM: ***** Step 3000 / Validation 30 *****
09/16 08:54:28 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:54:28 AM: Validating...
09/16 08:54:29 AM: Best result seen so far for edges-spr2.
09/16 08:54:29 AM: Best result seen so far for macro.
09/16 08:54:29 AM: Updating LR scheduler:
09/16 08:54:29 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:54:29 AM: 	# validation passes without improvement: 0
09/16 08:54:29 AM: edges-spr2_loss: training: 0.291014 validation: 0.320100
09/16 08:54:29 AM: macro_avg: validation: 0.816746
09/16 08:54:29 AM: micro_avg: validation: 0.000000
09/16 08:54:29 AM: edges-spr2_mcc: training: 0.703982 validation: 0.710903
09/16 08:54:29 AM: edges-spr2_acc: training: 0.078914 validation: 0.066667
09/16 08:54:29 AM: edges-spr2_precision: training: 0.832493 validation: 0.831315
09/16 08:54:29 AM: edges-spr2_recall: training: 0.786939 validation: 0.802679
09/16 08:54:29 AM: edges-spr2_f1: training: 0.809075 validation: 0.816746
09/16 08:54:29 AM: Global learning rate: 5e-05
09/16 08:54:29 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:54:38 AM: Update 3099: task edges-spr2, batch 99 (3099): mcc: 0.7011, acc: 0.0684, precision: 0.8306, recall: 0.7838, f1: 0.8065, edges-spr2_loss: 0.2921
09/16 08:54:38 AM: ***** Step 3100 / Validation 31 *****
09/16 08:54:38 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:54:38 AM: Validating...
09/16 08:54:39 AM: Updating LR scheduler:
09/16 08:54:39 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:54:39 AM: 	# validation passes without improvement: 1
09/16 08:54:39 AM: edges-spr2_loss: training: 0.292535 validation: 0.319496
09/16 08:54:39 AM: macro_avg: validation: 0.813051
09/16 08:54:39 AM: micro_avg: validation: 0.000000
09/16 08:54:39 AM: edges-spr2_mcc: training: 0.700833 validation: 0.709288
09/16 08:54:39 AM: edges-spr2_acc: training: 0.068260 validation: 0.080952
09/16 08:54:39 AM: edges-spr2_precision: training: 0.830627 validation: 0.844129
09/16 08:54:39 AM: edges-spr2_recall: training: 0.783377 validation: 0.784180
09/16 08:54:39 AM: edges-spr2_f1: training: 0.806310 validation: 0.813051
09/16 08:54:39 AM: Global learning rate: 5e-05
09/16 08:54:39 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:54:47 AM: ***** Step 3200 / Validation 32 *****
09/16 08:54:47 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:54:47 AM: Validating...
09/16 08:54:48 AM: Evaluate: task edges-spr2, batch 8 (10): mcc: 0.7179, acc: 0.0731, precision: 0.8372, recall: 0.8097, f1: 0.8232, edges-spr2_loss: 0.2959
09/16 08:54:48 AM: Updating LR scheduler:
09/16 08:54:48 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:54:48 AM: 	# validation passes without improvement: 2
09/16 08:54:48 AM: edges-spr2_loss: training: 0.291800 validation: 0.319251
09/16 08:54:48 AM: macro_avg: validation: 0.815633
09/16 08:54:48 AM: micro_avg: validation: 0.000000
09/16 08:54:48 AM: edges-spr2_mcc: training: 0.700939 validation: 0.709301
09/16 08:54:48 AM: edges-spr2_acc: training: 0.072573 validation: 0.071429
09/16 08:54:48 AM: edges-spr2_precision: training: 0.830982 validation: 0.830834
09/16 08:54:48 AM: edges-spr2_recall: training: 0.783766 validation: 0.800978
09/16 08:54:48 AM: edges-spr2_f1: training: 0.806683 validation: 0.815633
09/16 08:54:48 AM: Global learning rate: 5e-05
09/16 08:54:48 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:54:57 AM: ***** Step 3300 / Validation 33 *****
09/16 08:54:57 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:54:57 AM: Validating...
09/16 08:54:58 AM: Updating LR scheduler:
09/16 08:54:58 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:54:58 AM: 	# validation passes without improvement: 3
09/16 08:54:58 AM: edges-spr2_loss: training: 0.294140 validation: 0.319319
09/16 08:54:58 AM: macro_avg: validation: 0.813489
09/16 08:54:58 AM: micro_avg: validation: 0.000000
09/16 08:54:58 AM: edges-spr2_mcc: training: 0.699272 validation: 0.709134
09/16 08:54:58 AM: edges-spr2_acc: training: 0.071196 validation: 0.082540
09/16 08:54:58 AM: edges-spr2_precision: training: 0.830016 validation: 0.841400
09/16 08:54:58 AM: edges-spr2_recall: training: 0.782155 validation: 0.787370
09/16 08:54:58 AM: edges-spr2_f1: training: 0.805375 validation: 0.813489
09/16 08:54:58 AM: Global learning rate: 5e-05
09/16 08:54:58 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:54:58 AM: Update 3302: task edges-spr2, batch 2 (3302): mcc: 0.6750, acc: 0.0571, precision: 0.8160, recall: 0.7625, f1: 0.7883, edges-spr2_loss: 0.3066
09/16 08:55:06 AM: ***** Step 3400 / Validation 34 *****
09/16 08:55:06 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:55:06 AM: Validating...
09/16 08:55:07 AM: Updating LR scheduler:
09/16 08:55:07 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:55:07 AM: 	# validation passes without improvement: 0
09/16 08:55:07 AM: edges-spr2_loss: training: 0.292762 validation: 0.319071
09/16 08:55:07 AM: macro_avg: validation: 0.814434
09/16 08:55:07 AM: micro_avg: validation: 0.000000
09/16 08:55:07 AM: edges-spr2_mcc: training: 0.701685 validation: 0.709582
09/16 08:55:07 AM: edges-spr2_acc: training: 0.071850 validation: 0.074603
09/16 08:55:07 AM: edges-spr2_precision: training: 0.831927 validation: 0.838361
09/16 08:55:07 AM: edges-spr2_recall: training: 0.782554 validation: 0.791835
09/16 08:55:07 AM: edges-spr2_f1: training: 0.806486 validation: 0.814434
09/16 08:55:07 AM: Global learning rate: 2.5e-05
09/16 08:55:07 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:55:08 AM: Update 3415: task edges-spr2, batch 15 (3415): mcc: 0.7165, acc: 0.0895, precision: 0.8422, recall: 0.7967, f1: 0.8188, edges-spr2_loss: 0.2858
09/16 08:55:15 AM: ***** Step 3500 / Validation 35 *****
09/16 08:55:15 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:55:15 AM: Validating...
09/16 08:55:16 AM: Updating LR scheduler:
09/16 08:55:16 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:55:16 AM: 	# validation passes without improvement: 1
09/16 08:55:16 AM: edges-spr2_loss: training: 0.289504 validation: 0.317962
09/16 08:55:16 AM: macro_avg: validation: 0.816153
09/16 08:55:16 AM: micro_avg: validation: 0.000000
09/16 08:55:16 AM: edges-spr2_mcc: training: 0.706324 validation: 0.711271
09/16 08:55:16 AM: edges-spr2_acc: training: 0.079844 validation: 0.069841
09/16 08:55:16 AM: edges-spr2_precision: training: 0.834588 validation: 0.836084
09/16 08:55:16 AM: edges-spr2_recall: training: 0.787926 validation: 0.797151
09/16 08:55:16 AM: edges-spr2_f1: training: 0.810586 validation: 0.816153
09/16 08:55:16 AM: Global learning rate: 2.5e-05
09/16 08:55:16 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:55:18 AM: Update 3524: task edges-spr2, batch 24 (3524): mcc: 0.6914, acc: 0.0810, precision: 0.8249, recall: 0.7773, f1: 0.8004, edges-spr2_loss: 0.3012
09/16 08:55:24 AM: ***** Step 3600 / Validation 36 *****
09/16 08:55:24 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:55:24 AM: Validating...
09/16 08:55:25 AM: Updating LR scheduler:
09/16 08:55:25 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:55:25 AM: 	# validation passes without improvement: 2
09/16 08:55:25 AM: edges-spr2_loss: training: 0.289829 validation: 0.317864
09/16 08:55:25 AM: macro_avg: validation: 0.813905
09/16 08:55:25 AM: micro_avg: validation: 0.000000
09/16 08:55:25 AM: edges-spr2_mcc: training: 0.704588 validation: 0.709403
09/16 08:55:25 AM: edges-spr2_acc: training: 0.079338 validation: 0.076190
09/16 08:55:25 AM: edges-spr2_precision: training: 0.832958 validation: 0.840353
09/16 08:55:25 AM: edges-spr2_recall: training: 0.786370 validation: 0.789071
09/16 08:55:25 AM: edges-spr2_f1: training: 0.808994 validation: 0.813905
09/16 08:55:25 AM: Global learning rate: 2.5e-05
09/16 08:55:25 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:55:28 AM: Update 3640: task edges-spr2, batch 40 (3640): mcc: 0.7060, acc: 0.0778, precision: 0.8332, recall: 0.7884, f1: 0.8102, edges-spr2_loss: 0.2937
09/16 08:55:33 AM: ***** Step 3700 / Validation 37 *****
09/16 08:55:33 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:55:33 AM: Validating...
09/16 08:55:34 AM: Updating LR scheduler:
09/16 08:55:34 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:55:34 AM: 	# validation passes without improvement: 3
09/16 08:55:34 AM: edges-spr2_loss: training: 0.291697 validation: 0.317382
09/16 08:55:34 AM: macro_avg: validation: 0.812919
09/16 08:55:34 AM: micro_avg: validation: 0.000000
09/16 08:55:34 AM: edges-spr2_mcc: training: 0.705360 validation: 0.708267
09/16 08:55:34 AM: edges-spr2_acc: training: 0.076052 validation: 0.076190
09/16 08:55:34 AM: edges-spr2_precision: training: 0.834693 validation: 0.840909
09/16 08:55:34 AM: edges-spr2_recall: training: 0.785213 validation: 0.786732
09/16 08:55:34 AM: edges-spr2_f1: training: 0.809197 validation: 0.812919
09/16 08:55:34 AM: Global learning rate: 2.5e-05
09/16 08:55:34 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:55:38 AM: Update 3746: task edges-spr2, batch 46 (3746): mcc: 0.7064, acc: 0.0712, precision: 0.8320, recall: 0.7904, f1: 0.8107, edges-spr2_loss: 0.2886
09/16 08:55:43 AM: ***** Step 3800 / Validation 38 *****
09/16 08:55:43 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:55:43 AM: Validating...
09/16 08:55:44 AM: Updating LR scheduler:
09/16 08:55:44 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:55:44 AM: 	# validation passes without improvement: 0
09/16 08:55:44 AM: edges-spr2_loss: training: 0.291980 validation: 0.317024
09/16 08:55:44 AM: macro_avg: validation: 0.815551
09/16 08:55:44 AM: micro_avg: validation: 0.000000
09/16 08:55:44 AM: edges-spr2_mcc: training: 0.703805 validation: 0.711042
09/16 08:55:44 AM: edges-spr2_acc: training: 0.075439 validation: 0.074603
09/16 08:55:44 AM: edges-spr2_precision: training: 0.833009 validation: 0.838348
09/16 08:55:44 AM: edges-spr2_recall: training: 0.785721 validation: 0.793961
09/16 08:55:44 AM: edges-spr2_f1: training: 0.808675 validation: 0.815551
09/16 08:55:44 AM: Global learning rate: 1.25e-05
09/16 08:55:44 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:55:48 AM: Update 3854: task edges-spr2, batch 54 (3854): mcc: 0.7043, acc: 0.0730, precision: 0.8328, recall: 0.7858, f1: 0.8086, edges-spr2_loss: 0.2897
09/16 08:55:52 AM: ***** Step 3900 / Validation 39 *****
09/16 08:55:52 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:55:52 AM: Validating...
09/16 08:55:53 AM: Updating LR scheduler:
09/16 08:55:53 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:55:53 AM: 	# validation passes without improvement: 1
09/16 08:55:53 AM: edges-spr2_loss: training: 0.289500 validation: 0.316681
09/16 08:55:53 AM: macro_avg: validation: 0.813656
09/16 08:55:53 AM: micro_avg: validation: 0.000000
09/16 08:55:53 AM: edges-spr2_mcc: training: 0.705387 validation: 0.710023
09/16 08:55:53 AM: edges-spr2_acc: training: 0.073767 validation: 0.079365
09/16 08:55:53 AM: edges-spr2_precision: training: 0.833940 validation: 0.843957
09/16 08:55:53 AM: edges-spr2_recall: training: 0.785927 validation: 0.785456
09/16 08:55:53 AM: edges-spr2_f1: training: 0.809222 validation: 0.813656
09/16 08:55:53 AM: Global learning rate: 1.25e-05
09/16 08:55:53 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:55:58 AM: Update 3972: task edges-spr2, batch 72 (3972): mcc: 0.7086, acc: 0.0790, precision: 0.8365, recall: 0.7884, f1: 0.8118, edges-spr2_loss: 0.2867
09/16 08:56:01 AM: ***** Step 4000 / Validation 40 *****
09/16 08:56:01 AM: edges-spr2: trained on 100 batches, 1.429 epochs
09/16 08:56:01 AM: Validating...
09/16 08:56:02 AM: Updating LR scheduler:
09/16 08:56:02 AM: 	Best result seen so far for macro_avg: 0.817
09/16 08:56:02 AM: 	# validation passes without improvement: 2
09/16 08:56:02 AM: Ran out of early stopping patience. Stopping training.
09/16 08:56:02 AM: edges-spr2_loss: training: 0.289962 validation: 0.317011
09/16 08:56:02 AM: macro_avg: validation: 0.813938
09/16 08:56:02 AM: micro_avg: validation: 0.000000
09/16 08:56:02 AM: edges-spr2_mcc: training: 0.704287 validation: 0.709278
09/16 08:56:02 AM: edges-spr2_acc: training: 0.077194 validation: 0.074603
09/16 08:56:02 AM: edges-spr2_precision: training: 0.832091 validation: 0.839702
09/16 08:56:02 AM: edges-spr2_recall: training: 0.787071 validation: 0.789709
09/16 08:56:02 AM: edges-spr2_f1: training: 0.808955 validation: 0.813938
09/16 08:56:02 AM: Global learning rate: 1.25e-05
09/16 08:56:02 AM: Saving checkpoints to: ./experiments/spr2-squad-top/run
09/16 08:56:02 AM: Stopped training after 40 validation checks
09/16 08:56:02 AM: Trained edges-spr2 for 4000 batches or 57.143 epochs
09/16 08:56:02 AM: ***** VALIDATION RESULTS *****
09/16 08:56:02 AM: edges-spr2_f1 (for best val pass 30): edges-spr2_loss: 0.32010, macro_avg: 0.81675, micro_avg: 0.00000, edges-spr2_mcc: 0.71090, edges-spr2_acc: 0.06667, edges-spr2_precision: 0.83131, edges-spr2_recall: 0.80268, edges-spr2_f1: 0.81675
09/16 08:56:02 AM: micro_avg (for best val pass 1): edges-spr2_loss: 0.36858, macro_avg: 0.78762, micro_avg: 0.00000, edges-spr2_mcc: 0.65992, edges-spr2_acc: 0.05238, edges-spr2_precision: 0.78282, edges-spr2_recall: 0.79247, edges-spr2_f1: 0.78762
09/16 08:56:02 AM: macro_avg (for best val pass 30): edges-spr2_loss: 0.32010, macro_avg: 0.81675, micro_avg: 0.00000, edges-spr2_mcc: 0.71090, edges-spr2_acc: 0.06667, edges-spr2_precision: 0.83131, edges-spr2_recall: 0.80268, edges-spr2_f1: 0.81675
09/16 08:56:02 AM: Evaluating...
09/16 08:56:02 AM: Loaded model state from ./experiments/spr2-squad-top/run/edges-spr2/model_state_target_train_val_30.best.th
09/16 08:56:02 AM: Evaluating on: edges-spr2, split: val
09/16 08:56:03 AM: Task 'edges-spr2': sorting predictions by 'idx'
09/16 08:56:03 AM: Finished evaluating on: edges-spr2
09/16 08:56:03 AM: Task 'edges-spr2': joining predictions with input split 'val'
09/16 08:56:03 AM: Task 'edges-spr2': Wrote predictions to ./experiments/spr2-squad-top/run
09/16 08:56:03 AM: Wrote all preds for split 'val' to ./experiments/spr2-squad-top/run
09/16 08:56:03 AM: Evaluating on: edges-spr2, split: test
09/16 08:56:04 AM: Task 'edges-spr2': sorting predictions by 'idx'
09/16 08:56:04 AM: Finished evaluating on: edges-spr2
09/16 08:56:04 AM: Task 'edges-spr2': joining predictions with input split 'test'
09/16 08:56:04 AM: Task 'edges-spr2': Wrote predictions to ./experiments/spr2-squad-top/run
09/16 08:56:04 AM: Wrote all preds for split 'test' to ./experiments/spr2-squad-top/run
09/16 08:56:04 AM: Writing results for split 'val' to ./experiments/spr2-squad-top/results.tsv
09/16 08:56:04 AM: micro_avg: 0.000, macro_avg: 0.817, edges-spr2_mcc: 0.711, edges-spr2_acc: 0.067, edges-spr2_precision: 0.831, edges-spr2_recall: 0.803, edges-spr2_f1: 0.817
09/16 08:56:04 AM: Done!
09/16 08:56:04 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
