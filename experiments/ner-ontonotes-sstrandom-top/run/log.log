10/01 03:01:38 AM: Git branch: master
10/01 03:01:38 AM: Git SHA: 62183b2d03f2fae12b41eef8779808b6d354875e
10/01 03:01:38 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-sstrandom-top/",
  "exp_name": "experiments/ner-ontonotes-sstrandom-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-sstrandom-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/sstrandom",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-sstrandom-top__run",
  "run_dir": "./experiments/ner-ontonotes-sstrandom-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 03:01:38 AM: Saved config to ./experiments/ner-ontonotes-sstrandom-top/run/params.conf
10/01 03:01:38 AM: Using random seed 1234
10/01 03:01:42 AM: Using GPU 0
10/01 03:01:42 AM: Loading tasks...
10/01 03:01:42 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-sstrandom-top/
10/01 03:01:42 AM: 	Creating task edges-ner-ontonotes from scratch.
10/01 03:01:43 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
10/01 03:01:44 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
10/01 03:01:44 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
10/01 03:01:44 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
10/01 03:01:44 AM: 	Finished loading tasks: edges-ner-ontonotes.
10/01 03:01:44 AM: 	Building vocab from scratch.
10/01 03:01:44 AM: 	Counting units for task edges-ner-ontonotes.
10/01 03:01:46 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
10/01 03:01:47 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:01:47 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 03:01:47 AM: 	Saved vocab to ./experiments/ner-ontonotes-sstrandom-top/vocab
10/01 03:01:47 AM: Loading token dictionary from ./experiments/ner-ontonotes-sstrandom-top/vocab.
10/01 03:01:47 AM: 	Loaded vocab from ./experiments/ner-ontonotes-sstrandom-top/vocab
10/01 03:01:47 AM: 	Vocab namespace chars: size 77
10/01 03:01:47 AM: 	Vocab namespace tokens: size 22840
10/01 03:01:47 AM: 	Vocab namespace bert_uncased: size 30524
10/01 03:01:47 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
10/01 03:01:47 AM: 	Finished building vocab.
10/01 03:01:47 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
10/01 03:01:56 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-sstrandom-top/preproc/edges-ner-ontonotes__train_data
10/01 03:01:56 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
10/01 03:01:57 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-sstrandom-top/preproc/edges-ner-ontonotes__val_data
10/01 03:01:57 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
10/01 03:01:58 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-sstrandom-top/preproc/edges-ner-ontonotes__test_data
10/01 03:01:58 AM: 	Finished indexing tasks
10/01 03:01:58 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
10/01 03:01:58 AM: 	  Training on 
10/01 03:01:58 AM: 	  Evaluating on edges-ner-ontonotes
10/01 03:01:58 AM: 	Finished loading tasks in 15.747s
10/01 03:01:58 AM: 	 Tasks: ['edges-ner-ontonotes']
10/01 03:01:58 AM: Building model...
10/01 03:01:58 AM: Using BERT model (bert-base-uncased).
10/01 03:01:58 AM: LOADING A FUNETUNED MODEL from: 
10/01 03:01:58 AM: models/sstrandom
10/01 03:01:58 AM: loading configuration file models/sstrandom/config.json
10/01 03:01:58 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "random-sst-2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 03:01:58 AM: loading weights file models/sstrandom/pytorch_model.bin
10/01 03:02:02 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpy2uj9ku3
10/01 03:02:04 AM: copying /tmp/tmpy2uj9ku3 to cache at ./experiments/ner-ontonotes-sstrandom-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:02:04 AM: creating metadata file for ./experiments/ner-ontonotes-sstrandom-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:02:04 AM: removing temp file /tmp/tmpy2uj9ku3
10/01 03:02:04 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-sstrandom-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:02:04 AM: Initializing parameters
10/01 03:02:04 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 03:02:04 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 03:02:04 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 03:02:04 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 03:02:04 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 03:02:04 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 03:02:04 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
10/01 03:02:09 AM: Model specification:
10/01 03:02:09 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
10/01 03:02:09 AM: Model parameters:
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:02:09 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:02:09 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 03:02:09 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:02:09 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
10/01 03:02:09 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
10/01 03:02:09 AM: Total number of parameters: 109688338 (1.09688e+08)
10/01 03:02:09 AM: Number of trainable parameters: 206098 (206098)
10/01 03:02:09 AM: Finished building model in 10.798s
10/01 03:02:09 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

10/01 03:02:12 AM: patience = 9
10/01 03:02:12 AM: val_interval = 1000
10/01 03:02:12 AM: max_vals = 250
10/01 03:02:12 AM: cuda_device = 0
10/01 03:02:12 AM: grad_norm = 5.0
10/01 03:02:12 AM: grad_clipping = None
10/01 03:02:12 AM: lr_decay = 0.99
10/01 03:02:12 AM: min_lr = 1e-06
10/01 03:02:12 AM: keep_all_checkpoints = 0
10/01 03:02:12 AM: val_data_limit = 5000
10/01 03:02:12 AM: max_epochs = -1
10/01 03:02:12 AM: dec_val_scale = 250
10/01 03:02:12 AM: training_data_fraction = 1
10/01 03:02:12 AM: type = adam
10/01 03:02:12 AM: parameter_groups = None
10/01 03:02:12 AM: Number of trainable parameters: 206098
10/01 03:02:12 AM: infer_type_and_cast = True
10/01 03:02:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:02:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:02:12 AM: lr = 0.0001
10/01 03:02:12 AM: amsgrad = True
10/01 03:02:12 AM: type = reduce_on_plateau
10/01 03:02:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:02:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:02:12 AM: mode = max
10/01 03:02:12 AM: factor = 0.5
10/01 03:02:12 AM: patience = 3
10/01 03:02:12 AM: threshold = 0.0001
10/01 03:02:12 AM: threshold_mode = abs
10/01 03:02:12 AM: verbose = True
10/01 03:02:12 AM: type = adam
10/01 03:02:12 AM: parameter_groups = None
10/01 03:02:12 AM: Number of trainable parameters: 206098
10/01 03:02:12 AM: infer_type_and_cast = True
10/01 03:02:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:02:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:02:12 AM: lr = 0.0001
10/01 03:02:12 AM: amsgrad = True
10/01 03:02:12 AM: type = reduce_on_plateau
10/01 03:02:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:02:12 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:02:12 AM: mode = max
10/01 03:02:12 AM: factor = 0.5
10/01 03:02:12 AM: patience = 3
10/01 03:02:12 AM: threshold = 0.0001
10/01 03:02:12 AM: threshold_mode = abs
10/01 03:02:12 AM: verbose = True
10/01 03:02:12 AM: Starting training without restoring from a checkpoint.
10/01 03:02:12 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
10/01 03:02:12 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
10/01 03:02:22 AM: Update 148: task edges-ner-ontonotes, batch 148 (148): mcc: 0.0171, acc: 0.0150, precision: 0.0786, recall: 0.0399, f1: 0.0529, edges-ner-ontonotes_loss: 0.2451
10/01 03:02:34 AM: Update 314: task edges-ner-ontonotes, batch 314 (314): mcc: 0.2560, acc: 0.1662, precision: 0.4410, recall: 0.1796, f1: 0.2553, edges-ner-ontonotes_loss: 0.1786
10/01 03:02:44 AM: Update 454: task edges-ner-ontonotes, batch 454 (454): mcc: 0.4307, acc: 0.2991, precision: 0.6546, recall: 0.3111, f1: 0.4218, edges-ner-ontonotes_loss: 0.1514
10/01 03:02:54 AM: Update 593: task edges-ner-ontonotes, batch 593 (593): mcc: 0.5314, acc: 0.3915, precision: 0.7483, recall: 0.4034, f1: 0.5242, edges-ner-ontonotes_loss: 0.1329
10/01 03:03:04 AM: Update 710: task edges-ner-ontonotes, batch 710 (710): mcc: 0.5900, acc: 0.4508, precision: 0.7943, recall: 0.4631, f1: 0.5851, edges-ner-ontonotes_loss: 0.1214
10/01 03:03:14 AM: Update 852: task edges-ner-ontonotes, batch 852 (852): mcc: 0.6383, acc: 0.5039, precision: 0.8255, recall: 0.5172, f1: 0.6359, edges-ner-ontonotes_loss: 0.1105
10/01 03:03:24 AM: Update 967: task edges-ner-ontonotes, batch 967 (967): mcc: 0.6694, acc: 0.5405, precision: 0.8430, recall: 0.5543, f1: 0.6688, edges-ner-ontonotes_loss: 0.1033
10/01 03:03:27 AM: ***** Step 1000 / Validation 1 *****
10/01 03:03:27 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:03:27 AM: Validating...
10/01 03:03:34 AM: Evaluate: task edges-ner-ontonotes, batch 72 (157): mcc: 0.8248, acc: 0.7561, precision: 0.9038, recall: 0.7690, f1: 0.8310, edges-ner-ontonotes_loss: 0.0620
10/01 03:03:44 AM: Evaluate: task edges-ner-ontonotes, batch 152 (157): mcc: 0.8573, acc: 0.7960, precision: 0.9217, recall: 0.8112, f1: 0.8629, edges-ner-ontonotes_loss: 0.0517
10/01 03:03:45 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:03:45 AM: Best result seen so far for micro.
10/01 03:03:45 AM: Best result seen so far for macro.
10/01 03:03:45 AM: Updating LR scheduler:
10/01 03:03:45 AM: 	Best result seen so far for macro_avg: 0.863
10/01 03:03:45 AM: 	# validation passes without improvement: 0
10/01 03:03:45 AM: edges-ner-ontonotes_loss: training: 0.101581 validation: 0.051188
10/01 03:03:45 AM: macro_avg: validation: 0.863422
10/01 03:03:45 AM: micro_avg: validation: 0.000000
10/01 03:03:45 AM: edges-ner-ontonotes_mcc: training: 0.677045 validation: 0.857859
10/01 03:03:45 AM: edges-ner-ontonotes_acc: training: 0.549712 validation: 0.796254
10/01 03:03:45 AM: edges-ner-ontonotes_precision: training: 0.847189 validation: 0.922057
10/01 03:03:45 AM: edges-ner-ontonotes_recall: training: 0.563622 validation: 0.811799
10/01 03:03:45 AM: edges-ner-ontonotes_f1: training: 0.676907 validation: 0.863422
10/01 03:03:45 AM: Global learning rate: 0.0001
10/01 03:03:45 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:03:54 AM: Update 1133: task edges-ner-ontonotes, batch 133 (1133): mcc: 0.8609, acc: 0.7942, precision: 0.9270, recall: 0.8128, f1: 0.8662, edges-ner-ontonotes_loss: 0.0473
10/01 03:04:05 AM: Update 1253: task edges-ner-ontonotes, batch 253 (1253): mcc: 0.8625, acc: 0.7948, precision: 0.9297, recall: 0.8132, f1: 0.8676, edges-ner-ontonotes_loss: 0.0463
10/01 03:04:15 AM: Update 1391: task edges-ner-ontonotes, batch 391 (1391): mcc: 0.8480, acc: 0.7742, precision: 0.9233, recall: 0.7930, f1: 0.8532, edges-ner-ontonotes_loss: 0.0516
10/01 03:04:25 AM: Update 1537: task edges-ner-ontonotes, batch 537 (1537): mcc: 0.8436, acc: 0.7695, precision: 0.9209, recall: 0.7873, f1: 0.8489, edges-ner-ontonotes_loss: 0.0530
10/01 03:04:35 AM: Update 1686: task edges-ner-ontonotes, batch 686 (1686): mcc: 0.8406, acc: 0.7655, precision: 0.9192, recall: 0.7835, f1: 0.8460, edges-ner-ontonotes_loss: 0.0531
10/01 03:04:45 AM: Update 1850: task edges-ner-ontonotes, batch 850 (1850): mcc: 0.8405, acc: 0.7654, precision: 0.9188, recall: 0.7836, f1: 0.8458, edges-ner-ontonotes_loss: 0.0525
10/01 03:04:55 AM: Update 1972: task edges-ner-ontonotes, batch 972 (1972): mcc: 0.8434, acc: 0.7695, precision: 0.9194, recall: 0.7882, f1: 0.8488, edges-ner-ontonotes_loss: 0.0513
10/01 03:04:58 AM: ***** Step 2000 / Validation 2 *****
10/01 03:04:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:04:58 AM: Validating...
10/01 03:05:06 AM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.8723, acc: 0.8185, precision: 0.9322, recall: 0.8287, f1: 0.8774, edges-ner-ontonotes_loss: 0.0419
10/01 03:05:15 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:05:15 AM: Best result seen so far for macro.
10/01 03:05:15 AM: Updating LR scheduler:
10/01 03:05:15 AM: 	Best result seen so far for macro_avg: 0.895
10/01 03:05:15 AM: 	# validation passes without improvement: 0
10/01 03:05:15 AM: edges-ner-ontonotes_loss: training: 0.051040 validation: 0.036334
10/01 03:05:15 AM: macro_avg: validation: 0.894659
10/01 03:05:15 AM: micro_avg: validation: 0.000000
10/01 03:05:15 AM: edges-ner-ontonotes_mcc: training: 0.844110 validation: 0.889968
10/01 03:05:15 AM: edges-ner-ontonotes_acc: training: 0.770547 validation: 0.839248
10/01 03:05:15 AM: edges-ner-ontonotes_precision: training: 0.919655 validation: 0.940630
10/01 03:05:15 AM: edges-ner-ontonotes_recall: training: 0.789320 validation: 0.852972
10/01 03:05:15 AM: edges-ner-ontonotes_f1: training: 0.849518 validation: 0.894659
10/01 03:05:15 AM: Global learning rate: 0.0001
10/01 03:05:15 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:05:16 AM: Update 2002: task edges-ner-ontonotes, batch 2 (2002): mcc: 0.9044, acc: 0.8608, precision: 0.9494, recall: 0.8711, f1: 0.9086, edges-ner-ontonotes_loss: 0.0327
10/01 03:05:26 AM: Update 2144: task edges-ner-ontonotes, batch 144 (2144): mcc: 0.8801, acc: 0.8254, precision: 0.9331, recall: 0.8419, f1: 0.8852, edges-ner-ontonotes_loss: 0.0382
10/01 03:05:36 AM: Update 2258: task edges-ner-ontonotes, batch 258 (2258): mcc: 0.8807, acc: 0.8250, precision: 0.9337, recall: 0.8425, f1: 0.8858, edges-ner-ontonotes_loss: 0.0377
10/01 03:05:46 AM: Update 2393: task edges-ner-ontonotes, batch 393 (2393): mcc: 0.8843, acc: 0.8292, precision: 0.9350, recall: 0.8478, f1: 0.8893, edges-ner-ontonotes_loss: 0.0363
10/01 03:05:56 AM: Update 2503: task edges-ner-ontonotes, batch 503 (2503): mcc: 0.8873, acc: 0.8325, precision: 0.9365, recall: 0.8520, f1: 0.8922, edges-ner-ontonotes_loss: 0.0354
10/01 03:06:06 AM: Update 2638: task edges-ner-ontonotes, batch 638 (2638): mcc: 0.8888, acc: 0.8341, precision: 0.9368, recall: 0.8545, f1: 0.8937, edges-ner-ontonotes_loss: 0.0349
10/01 03:06:16 AM: Update 2776: task edges-ner-ontonotes, batch 776 (2776): mcc: 0.8905, acc: 0.8362, precision: 0.9373, recall: 0.8570, f1: 0.8953, edges-ner-ontonotes_loss: 0.0343
10/01 03:06:26 AM: Update 2887: task edges-ner-ontonotes, batch 887 (2887): mcc: 0.8877, acc: 0.8321, precision: 0.9360, recall: 0.8530, f1: 0.8926, edges-ner-ontonotes_loss: 0.0355
10/01 03:06:34 AM: ***** Step 3000 / Validation 3 *****
10/01 03:06:34 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:06:34 AM: Validating...
10/01 03:06:36 AM: Evaluate: task edges-ner-ontonotes, batch 21 (157): mcc: 0.8383, acc: 0.7821, precision: 0.8997, recall: 0.7968, f1: 0.8451, edges-ner-ontonotes_loss: 0.0448
10/01 03:06:46 AM: Evaluate: task edges-ner-ontonotes, batch 111 (157): mcc: 0.8919, acc: 0.8440, precision: 0.9393, recall: 0.8577, f1: 0.8967, edges-ner-ontonotes_loss: 0.0354
10/01 03:06:52 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:06:52 AM: Best result seen so far for macro.
10/01 03:06:52 AM: Updating LR scheduler:
10/01 03:06:52 AM: 	Best result seen so far for macro_avg: 0.906
10/01 03:06:52 AM: 	# validation passes without improvement: 0
10/01 03:06:52 AM: edges-ner-ontonotes_loss: training: 0.036889 validation: 0.032180
10/01 03:06:52 AM: macro_avg: validation: 0.905608
10/01 03:06:52 AM: micro_avg: validation: 0.000000
10/01 03:06:52 AM: edges-ner-ontonotes_mcc: training: 0.884286 validation: 0.901175
10/01 03:06:52 AM: edges-ner-ontonotes_acc: training: 0.827668 validation: 0.854716
10/01 03:06:52 AM: edges-ner-ontonotes_precision: training: 0.934042 validation: 0.945021
10/01 03:06:52 AM: edges-ner-ontonotes_recall: training: 0.848716 validation: 0.869351
10/01 03:06:52 AM: edges-ner-ontonotes_f1: training: 0.889337 validation: 0.905608
10/01 03:06:52 AM: Global learning rate: 0.0001
10/01 03:06:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:06:56 AM: Update 3065: task edges-ner-ontonotes, batch 65 (3065): mcc: 0.8487, acc: 0.7823, precision: 0.9076, recall: 0.8083, f1: 0.8551, edges-ner-ontonotes_loss: 0.0488
10/01 03:07:06 AM: Update 3193: task edges-ner-ontonotes, batch 193 (3193): mcc: 0.8537, acc: 0.7880, precision: 0.9138, recall: 0.8119, f1: 0.8598, edges-ner-ontonotes_loss: 0.0471
10/01 03:07:16 AM: Update 3363: task edges-ner-ontonotes, batch 363 (3363): mcc: 0.8587, acc: 0.7932, precision: 0.9173, recall: 0.8176, f1: 0.8646, edges-ner-ontonotes_loss: 0.0446
10/01 03:07:26 AM: Update 3493: task edges-ner-ontonotes, batch 493 (3493): mcc: 0.8634, acc: 0.8003, precision: 0.9194, recall: 0.8242, f1: 0.8692, edges-ner-ontonotes_loss: 0.0431
10/01 03:07:36 AM: Update 3633: task edges-ner-ontonotes, batch 633 (3633): mcc: 0.8702, acc: 0.8098, precision: 0.9228, recall: 0.8335, f1: 0.8758, edges-ner-ontonotes_loss: 0.0411
10/01 03:07:46 AM: Update 3748: task edges-ner-ontonotes, batch 748 (3748): mcc: 0.8744, acc: 0.8156, precision: 0.9248, recall: 0.8392, f1: 0.8799, edges-ner-ontonotes_loss: 0.0398
10/01 03:07:56 AM: Update 3882: task edges-ner-ontonotes, batch 882 (3882): mcc: 0.8797, acc: 0.8229, precision: 0.9272, recall: 0.8466, f1: 0.8851, edges-ner-ontonotes_loss: 0.0383
10/01 03:08:04 AM: ***** Step 4000 / Validation 4 *****
10/01 03:08:04 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:08:04 AM: Validating...
10/01 03:08:06 AM: Evaluate: task edges-ner-ontonotes, batch 18 (157): mcc: 0.8362, acc: 0.7752, precision: 0.8926, recall: 0.7995, f1: 0.8435, edges-ner-ontonotes_loss: 0.0454
10/01 03:08:16 AM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.9006, acc: 0.8567, precision: 0.9383, recall: 0.8745, f1: 0.9053, edges-ner-ontonotes_loss: 0.0328
10/01 03:08:22 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:08:22 AM: Best result seen so far for macro.
10/01 03:08:22 AM: Updating LR scheduler:
10/01 03:08:22 AM: 	Best result seen so far for macro_avg: 0.917
10/01 03:08:22 AM: 	# validation passes without improvement: 0
10/01 03:08:22 AM: edges-ner-ontonotes_loss: training: 0.037087 validation: 0.028912
10/01 03:08:22 AM: macro_avg: validation: 0.916719
10/01 03:08:22 AM: micro_avg: validation: 0.000000
10/01 03:08:22 AM: edges-ner-ontonotes_mcc: training: 0.883635 validation: 0.912445
10/01 03:08:22 AM: edges-ner-ontonotes_acc: training: 0.828230 validation: 0.871171
10/01 03:08:22 AM: edges-ner-ontonotes_precision: training: 0.929294 validation: 0.945375
10/01 03:08:22 AM: edges-ner-ontonotes_recall: training: 0.851942 validation: 0.889748
10/01 03:08:22 AM: edges-ner-ontonotes_f1: training: 0.888939 validation: 0.916719
10/01 03:08:22 AM: Global learning rate: 0.0001
10/01 03:08:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:08:26 AM: Update 4050: task edges-ner-ontonotes, batch 50 (4050): mcc: 0.9150, acc: 0.8704, precision: 0.9440, recall: 0.8957, f1: 0.9192, edges-ner-ontonotes_loss: 0.0270
10/01 03:08:36 AM: Update 4163: task edges-ner-ontonotes, batch 163 (4163): mcc: 0.9090, acc: 0.8627, precision: 0.9419, recall: 0.8868, f1: 0.9135, edges-ner-ontonotes_loss: 0.0283
10/01 03:08:46 AM: Update 4298: task edges-ner-ontonotes, batch 298 (4298): mcc: 0.9089, acc: 0.8626, precision: 0.9407, recall: 0.8877, f1: 0.9134, edges-ner-ontonotes_loss: 0.0285
10/01 03:08:56 AM: Update 4423: task edges-ner-ontonotes, batch 423 (4423): mcc: 0.9028, acc: 0.8543, precision: 0.9375, recall: 0.8794, f1: 0.9075, edges-ner-ontonotes_loss: 0.0306
10/01 03:09:06 AM: Update 4563: task edges-ner-ontonotes, batch 563 (4563): mcc: 0.8943, acc: 0.8430, precision: 0.9327, recall: 0.8684, f1: 0.8994, edges-ner-ontonotes_loss: 0.0345
10/01 03:09:16 AM: Update 4688: task edges-ner-ontonotes, batch 688 (4688): mcc: 0.8897, acc: 0.8369, precision: 0.9301, recall: 0.8624, f1: 0.8949, edges-ner-ontonotes_loss: 0.0363
10/01 03:09:27 AM: Update 4855: task edges-ner-ontonotes, batch 855 (4855): mcc: 0.8866, acc: 0.8326, precision: 0.9285, recall: 0.8582, f1: 0.8920, edges-ner-ontonotes_loss: 0.0370
10/01 03:09:37 AM: Update 4986: task edges-ner-ontonotes, batch 986 (4986): mcc: 0.8854, acc: 0.8306, precision: 0.9278, recall: 0.8565, f1: 0.8908, edges-ner-ontonotes_loss: 0.0371
10/01 03:09:38 AM: ***** Step 5000 / Validation 5 *****
10/01 03:09:38 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:09:38 AM: Validating...
10/01 03:09:47 AM: Evaluate: task edges-ner-ontonotes, batch 86 (157): mcc: 0.9072, acc: 0.8650, precision: 0.9435, recall: 0.8818, f1: 0.9116, edges-ner-ontonotes_loss: 0.0309
10/01 03:09:55 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:09:55 AM: Best result seen so far for macro.
10/01 03:09:55 AM: Updating LR scheduler:
10/01 03:09:55 AM: 	Best result seen so far for macro_avg: 0.921
10/01 03:09:55 AM: 	# validation passes without improvement: 0
10/01 03:09:55 AM: edges-ner-ontonotes_loss: training: 0.037104 validation: 0.027594
10/01 03:09:55 AM: macro_avg: validation: 0.920724
10/01 03:09:55 AM: micro_avg: validation: 0.000000
10/01 03:09:55 AM: edges-ner-ontonotes_mcc: training: 0.885434 validation: 0.916635
10/01 03:09:55 AM: edges-ner-ontonotes_acc: training: 0.830796 validation: 0.877161
10/01 03:09:55 AM: edges-ner-ontonotes_precision: training: 0.927846 validation: 0.948248
10/01 03:09:55 AM: edges-ner-ontonotes_recall: training: 0.856596 validation: 0.894753
10/01 03:09:55 AM: edges-ner-ontonotes_f1: training: 0.890799 validation: 0.920724
10/01 03:09:55 AM: Global learning rate: 0.0001
10/01 03:09:55 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:09:57 AM: Update 5017: task edges-ner-ontonotes, batch 17 (5017): mcc: 0.8803, acc: 0.8341, precision: 0.9081, recall: 0.8660, f1: 0.8865, edges-ner-ontonotes_loss: 0.0381
10/01 03:10:07 AM: Update 5153: task edges-ner-ontonotes, batch 153 (5153): mcc: 0.8936, acc: 0.8458, precision: 0.9279, recall: 0.8717, f1: 0.8989, edges-ner-ontonotes_loss: 0.0334
10/01 03:10:18 AM: Update 5295: task edges-ner-ontonotes, batch 295 (5295): mcc: 0.8978, acc: 0.8494, precision: 0.9330, recall: 0.8745, f1: 0.9028, edges-ner-ontonotes_loss: 0.0319
10/01 03:10:28 AM: Update 5431: task edges-ner-ontonotes, batch 431 (5431): mcc: 0.9021, acc: 0.8554, precision: 0.9349, recall: 0.8806, f1: 0.9069, edges-ner-ontonotes_loss: 0.0306
10/01 03:10:38 AM: Update 5571: task edges-ner-ontonotes, batch 571 (5571): mcc: 0.9051, acc: 0.8594, precision: 0.9367, recall: 0.8844, f1: 0.9098, edges-ner-ontonotes_loss: 0.0297
10/01 03:10:48 AM: Update 5672: task edges-ner-ontonotes, batch 672 (5672): mcc: 0.9067, acc: 0.8615, precision: 0.9377, recall: 0.8864, f1: 0.9114, edges-ner-ontonotes_loss: 0.0293
10/01 03:10:58 AM: Update 5812: task edges-ner-ontonotes, batch 812 (5812): mcc: 0.9076, acc: 0.8624, precision: 0.9383, recall: 0.8876, f1: 0.9123, edges-ner-ontonotes_loss: 0.0289
10/01 03:11:08 AM: Update 5932: task edges-ner-ontonotes, batch 932 (5932): mcc: 0.9074, acc: 0.8619, precision: 0.9383, recall: 0.8873, f1: 0.9121, edges-ner-ontonotes_loss: 0.0290
10/01 03:11:13 AM: ***** Step 6000 / Validation 6 *****
10/01 03:11:13 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:11:13 AM: Validating...
10/01 03:11:18 AM: Evaluate: task edges-ner-ontonotes, batch 52 (157): mcc: 0.8867, acc: 0.8418, precision: 0.9195, recall: 0.8669, f1: 0.8924, edges-ner-ontonotes_loss: 0.0352
10/01 03:11:28 AM: Evaluate: task edges-ner-ontonotes, batch 133 (157): mcc: 0.9175, acc: 0.8793, precision: 0.9477, recall: 0.8969, f1: 0.9216, edges-ner-ontonotes_loss: 0.0279
10/01 03:11:31 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:11:31 AM: Best result seen so far for macro.
10/01 03:11:31 AM: Updating LR scheduler:
10/01 03:11:31 AM: 	Best result seen so far for macro_avg: 0.924
10/01 03:11:31 AM: 	# validation passes without improvement: 0
10/01 03:11:31 AM: edges-ner-ontonotes_loss: training: 0.029964 validation: 0.026860
10/01 03:11:31 AM: macro_avg: validation: 0.923562
10/01 03:11:31 AM: micro_avg: validation: 0.000000
10/01 03:11:31 AM: edges-ner-ontonotes_mcc: training: 0.905293 validation: 0.919537
10/01 03:11:31 AM: edges-ner-ontonotes_acc: training: 0.858880 validation: 0.881635
10/01 03:11:31 AM: edges-ner-ontonotes_precision: training: 0.937064 validation: 0.948239
10/01 03:11:31 AM: edges-ner-ontonotes_recall: training: 0.884461 validation: 0.900136
10/01 03:11:31 AM: edges-ner-ontonotes_f1: training: 0.910003 validation: 0.923562
10/01 03:11:31 AM: Global learning rate: 0.0001
10/01 03:11:31 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:11:38 AM: Update 6105: task edges-ner-ontonotes, batch 105 (6105): mcc: 0.8714, acc: 0.8139, precision: 0.9170, recall: 0.8410, f1: 0.8774, edges-ner-ontonotes_loss: 0.0419
10/01 03:11:48 AM: Update 6228: task edges-ner-ontonotes, batch 228 (6228): mcc: 0.8703, acc: 0.8142, precision: 0.9146, recall: 0.8412, f1: 0.8764, edges-ner-ontonotes_loss: 0.0440
10/01 03:11:58 AM: Update 6395: task edges-ner-ontonotes, batch 395 (6395): mcc: 0.8709, acc: 0.8136, precision: 0.9156, recall: 0.8414, f1: 0.8769, edges-ner-ontonotes_loss: 0.0420
10/01 03:12:08 AM: Update 6538: task edges-ner-ontonotes, batch 538 (6538): mcc: 0.8736, acc: 0.8166, precision: 0.9175, recall: 0.8447, f1: 0.8796, edges-ner-ontonotes_loss: 0.0407
10/01 03:12:18 AM: Update 6678: task edges-ner-ontonotes, batch 678 (6678): mcc: 0.8795, acc: 0.8245, precision: 0.9207, recall: 0.8524, f1: 0.8853, edges-ner-ontonotes_loss: 0.0387
10/01 03:12:28 AM: Update 6820: task edges-ner-ontonotes, batch 820 (6820): mcc: 0.8846, acc: 0.8314, precision: 0.9239, recall: 0.8588, f1: 0.8902, edges-ner-ontonotes_loss: 0.0373
10/01 03:12:38 AM: Update 6938: task edges-ner-ontonotes, batch 938 (6938): mcc: 0.8883, acc: 0.8361, precision: 0.9260, recall: 0.8637, f1: 0.8938, edges-ner-ontonotes_loss: 0.0361
10/01 03:12:43 AM: ***** Step 7000 / Validation 7 *****
10/01 03:12:43 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:12:43 AM: Validating...
10/01 03:12:48 AM: Evaluate: task edges-ner-ontonotes, batch 55 (157): mcc: 0.8971, acc: 0.8549, precision: 0.9280, recall: 0.8780, f1: 0.9023, edges-ner-ontonotes_loss: 0.0339
10/01 03:12:59 AM: Evaluate: task edges-ner-ontonotes, batch 135 (157): mcc: 0.9210, acc: 0.8861, precision: 0.9476, recall: 0.9034, f1: 0.9250, edges-ner-ontonotes_loss: 0.0272
10/01 03:13:01 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:13:01 AM: Best result seen so far for macro.
10/01 03:13:01 AM: Updating LR scheduler:
10/01 03:13:01 AM: 	Best result seen so far for macro_avg: 0.926
10/01 03:13:01 AM: 	# validation passes without improvement: 0
10/01 03:13:01 AM: edges-ner-ontonotes_loss: training: 0.035479 validation: 0.026286
10/01 03:13:01 AM: macro_avg: validation: 0.926226
10/01 03:13:01 AM: micro_avg: validation: 0.000000
10/01 03:13:01 AM: edges-ner-ontonotes_mcc: training: 0.890162 validation: 0.922251
10/01 03:13:01 AM: edges-ner-ontonotes_acc: training: 0.838424 validation: 0.886715
10/01 03:13:01 AM: edges-ner-ontonotes_precision: training: 0.926923 validation: 0.947569
10/01 03:13:01 AM: edges-ner-ontonotes_recall: training: 0.866170 validation: 0.905823
10/01 03:13:01 AM: edges-ner-ontonotes_f1: training: 0.895518 validation: 0.926226
10/01 03:13:01 AM: Global learning rate: 0.0001
10/01 03:13:01 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:13:09 AM: Update 7107: task edges-ner-ontonotes, batch 107 (7107): mcc: 0.9163, acc: 0.8751, precision: 0.9422, recall: 0.9000, f1: 0.9206, edges-ner-ontonotes_loss: 0.0259
10/01 03:13:19 AM: Update 7225: task edges-ner-ontonotes, batch 225 (7225): mcc: 0.9172, acc: 0.8757, precision: 0.9430, recall: 0.9007, f1: 0.9214, edges-ner-ontonotes_loss: 0.0258
10/01 03:13:29 AM: Update 7359: task edges-ner-ontonotes, batch 359 (7359): mcc: 0.9173, acc: 0.8749, precision: 0.9434, recall: 0.9007, f1: 0.9216, edges-ner-ontonotes_loss: 0.0258
10/01 03:13:39 AM: Update 7478: task edges-ner-ontonotes, batch 478 (7478): mcc: 0.9148, acc: 0.8715, precision: 0.9412, recall: 0.8982, f1: 0.9192, edges-ner-ontonotes_loss: 0.0265
10/01 03:13:49 AM: Update 7619: task edges-ner-ontonotes, batch 619 (7619): mcc: 0.9071, acc: 0.8616, precision: 0.9363, recall: 0.8885, f1: 0.9118, edges-ner-ontonotes_loss: 0.0299
10/01 03:13:59 AM: Update 7763: task edges-ner-ontonotes, batch 763 (7763): mcc: 0.9016, acc: 0.8539, precision: 0.9331, recall: 0.8814, f1: 0.9065, edges-ner-ontonotes_loss: 0.0325
10/01 03:14:09 AM: Update 7903: task edges-ner-ontonotes, batch 903 (7903): mcc: 0.8988, acc: 0.8503, precision: 0.9314, recall: 0.8780, f1: 0.9039, edges-ner-ontonotes_loss: 0.0334
10/01 03:14:15 AM: ***** Step 8000 / Validation 8 *****
10/01 03:14:15 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:14:15 AM: Validating...
10/01 03:14:19 AM: Evaluate: task edges-ner-ontonotes, batch 45 (157): mcc: 0.8966, acc: 0.8536, precision: 0.9325, recall: 0.8727, f1: 0.9016, edges-ner-ontonotes_loss: 0.0329
10/01 03:14:29 AM: Evaluate: task edges-ner-ontonotes, batch 125 (157): mcc: 0.9197, acc: 0.8815, precision: 0.9524, recall: 0.8964, f1: 0.9235, edges-ner-ontonotes_loss: 0.0273
10/01 03:14:32 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:14:32 AM: Best result seen so far for macro.
10/01 03:14:32 AM: Updating LR scheduler:
10/01 03:14:32 AM: 	Best result seen so far for macro_avg: 0.927
10/01 03:14:32 AM: 	# validation passes without improvement: 0
10/01 03:14:32 AM: edges-ner-ontonotes_loss: training: 0.033714 validation: 0.025741
10/01 03:14:32 AM: macro_avg: validation: 0.926765
10/01 03:14:32 AM: micro_avg: validation: 0.000000
10/01 03:14:32 AM: edges-ner-ontonotes_mcc: training: 0.897817 validation: 0.922989
10/01 03:14:32 AM: edges-ner-ontonotes_acc: training: 0.848667 validation: 0.885957
10/01 03:14:32 AM: edges-ner-ontonotes_precision: training: 0.930836 validation: 0.953485
10/01 03:14:32 AM: edges-ner-ontonotes_recall: training: 0.876585 validation: 0.901501
10/01 03:14:32 AM: edges-ner-ontonotes_f1: training: 0.902896 validation: 0.926765
10/01 03:14:32 AM: Global learning rate: 0.0001
10/01 03:14:32 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:14:40 AM: Update 8094: task edges-ner-ontonotes, batch 94 (8094): mcc: 0.8825, acc: 0.8286, precision: 0.9210, recall: 0.8577, f1: 0.8882, edges-ner-ontonotes_loss: 0.0366
10/01 03:14:50 AM: Update 8236: task edges-ner-ontonotes, batch 236 (8236): mcc: 0.8970, acc: 0.8471, precision: 0.9305, recall: 0.8754, f1: 0.9021, edges-ner-ontonotes_loss: 0.0325
10/01 03:15:00 AM: Update 8376: task edges-ner-ontonotes, batch 376 (8376): mcc: 0.9003, acc: 0.8522, precision: 0.9323, recall: 0.8799, f1: 0.9053, edges-ner-ontonotes_loss: 0.0314
10/01 03:15:10 AM: Update 8494: task edges-ner-ontonotes, batch 494 (8494): mcc: 0.9028, acc: 0.8556, precision: 0.9334, recall: 0.8834, f1: 0.9077, edges-ner-ontonotes_loss: 0.0308
10/01 03:15:20 AM: Update 8625: task edges-ner-ontonotes, batch 625 (8625): mcc: 0.9064, acc: 0.8604, precision: 0.9357, recall: 0.8880, f1: 0.9112, edges-ner-ontonotes_loss: 0.0296
10/01 03:15:30 AM: Update 8745: task edges-ner-ontonotes, batch 745 (8745): mcc: 0.9084, acc: 0.8626, precision: 0.9370, recall: 0.8904, f1: 0.9131, edges-ner-ontonotes_loss: 0.0289
10/01 03:15:40 AM: Update 8886: task edges-ner-ontonotes, batch 886 (8886): mcc: 0.9101, acc: 0.8648, precision: 0.9379, recall: 0.8926, f1: 0.9147, edges-ner-ontonotes_loss: 0.0284
10/01 03:15:48 AM: ***** Step 9000 / Validation 9 *****
10/01 03:15:48 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:15:48 AM: Validating...
10/01 03:15:50 AM: Evaluate: task edges-ner-ontonotes, batch 17 (157): mcc: 0.8456, acc: 0.7908, precision: 0.8904, recall: 0.8186, f1: 0.8530, edges-ner-ontonotes_loss: 0.0417
10/01 03:16:00 AM: Evaluate: task edges-ner-ontonotes, batch 102 (157): mcc: 0.9089, acc: 0.8723, precision: 0.9377, recall: 0.8906, f1: 0.9136, edges-ner-ontonotes_loss: 0.0309
10/01 03:16:07 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:16:07 AM: Best result seen so far for macro.
10/01 03:16:07 AM: Updating LR scheduler:
10/01 03:16:07 AM: 	Best result seen so far for macro_avg: 0.929
10/01 03:16:07 AM: 	# validation passes without improvement: 0
10/01 03:16:07 AM: edges-ner-ontonotes_loss: training: 0.028045 validation: 0.025604
10/01 03:16:07 AM: macro_avg: validation: 0.928961
10/01 03:16:07 AM: micro_avg: validation: 0.000000
10/01 03:16:07 AM: edges-ner-ontonotes_mcc: training: 0.910952 validation: 0.925077
10/01 03:16:07 AM: edges-ner-ontonotes_acc: training: 0.865957 validation: 0.892554
10/01 03:16:07 AM: edges-ner-ontonotes_precision: training: 0.938438 validation: 0.947913
10/01 03:16:07 AM: edges-ner-ontonotes_recall: training: 0.893639 validation: 0.910752
10/01 03:16:07 AM: edges-ner-ontonotes_f1: training: 0.915491 validation: 0.928961
10/01 03:16:07 AM: Global learning rate: 0.0001
10/01 03:16:07 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:16:11 AM: Update 9033: task edges-ner-ontonotes, batch 33 (9033): mcc: 0.9096, acc: 0.8646, precision: 0.9359, recall: 0.8937, f1: 0.9143, edges-ner-ontonotes_loss: 0.0279
10/01 03:16:21 AM: Update 9171: task edges-ner-ontonotes, batch 171 (9171): mcc: 0.8812, acc: 0.8290, precision: 0.9196, recall: 0.8565, f1: 0.8870, edges-ner-ontonotes_loss: 0.0395
10/01 03:16:31 AM: Update 9321: task edges-ner-ontonotes, batch 321 (9321): mcc: 0.8785, acc: 0.8255, precision: 0.9187, recall: 0.8524, f1: 0.8843, edges-ner-ontonotes_loss: 0.0410
10/01 03:16:41 AM: Update 9466: task edges-ner-ontonotes, batch 466 (9466): mcc: 0.8786, acc: 0.8249, precision: 0.9180, recall: 0.8533, f1: 0.8845, edges-ner-ontonotes_loss: 0.0399
10/01 03:16:51 AM: Update 9636: task edges-ner-ontonotes, batch 636 (9636): mcc: 0.8800, acc: 0.8262, precision: 0.9195, recall: 0.8545, f1: 0.8858, edges-ner-ontonotes_loss: 0.0388
10/01 03:17:01 AM: Update 9755: task edges-ner-ontonotes, batch 755 (9755): mcc: 0.8841, acc: 0.8315, precision: 0.9218, recall: 0.8598, f1: 0.8897, edges-ner-ontonotes_loss: 0.0374
10/01 03:17:11 AM: Update 9900: task edges-ner-ontonotes, batch 900 (9900): mcc: 0.8883, acc: 0.8371, precision: 0.9244, recall: 0.8650, f1: 0.8937, edges-ner-ontonotes_loss: 0.0361
10/01 03:17:20 AM: ***** Step 10000 / Validation 10 *****
10/01 03:17:20 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:17:20 AM: Validating...
10/01 03:17:21 AM: Evaluate: task edges-ner-ontonotes, batch 15 (157): mcc: 0.8438, acc: 0.7876, precision: 0.8961, recall: 0.8100, f1: 0.8509, edges-ner-ontonotes_loss: 0.0421
10/01 03:17:31 AM: Evaluate: task edges-ner-ontonotes, batch 107 (157): mcc: 0.9137, acc: 0.8765, precision: 0.9432, recall: 0.8942, f1: 0.9180, edges-ner-ontonotes_loss: 0.0287
10/01 03:17:38 AM: Updating LR scheduler:
10/01 03:17:38 AM: 	Best result seen so far for macro_avg: 0.929
10/01 03:17:38 AM: 	# validation passes without improvement: 1
10/01 03:17:38 AM: edges-ner-ontonotes_loss: training: 0.035229 validation: 0.024955
10/01 03:17:38 AM: macro_avg: validation: 0.928950
10/01 03:17:38 AM: micro_avg: validation: 0.000000
10/01 03:17:38 AM: edges-ner-ontonotes_mcc: training: 0.890471 validation: 0.925099
10/01 03:17:38 AM: edges-ner-ontonotes_acc: training: 0.839718 validation: 0.891189
10/01 03:17:38 AM: edges-ner-ontonotes_precision: training: 0.925913 validation: 0.949126
10/01 03:17:38 AM: edges-ner-ontonotes_recall: training: 0.867701 validation: 0.909615
10/01 03:17:38 AM: edges-ner-ontonotes_f1: training: 0.895863 validation: 0.928950
10/01 03:17:38 AM: Global learning rate: 0.0001
10/01 03:17:38 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:17:41 AM: Update 10051: task edges-ner-ontonotes, batch 51 (10051): mcc: 0.9247, acc: 0.8852, precision: 0.9447, recall: 0.9131, f1: 0.9287, edges-ner-ontonotes_loss: 0.0238
10/01 03:17:51 AM: Update 10187: task edges-ner-ontonotes, batch 187 (10187): mcc: 0.9207, acc: 0.8792, precision: 0.9427, recall: 0.9076, f1: 0.9248, edges-ner-ontonotes_loss: 0.0248
10/01 03:18:02 AM: Update 10297: task edges-ner-ontonotes, batch 297 (10297): mcc: 0.9203, acc: 0.8793, precision: 0.9435, recall: 0.9061, f1: 0.9244, edges-ner-ontonotes_loss: 0.0248
10/01 03:18:12 AM: Update 10436: task edges-ner-ontonotes, batch 436 (10436): mcc: 0.9197, acc: 0.8782, precision: 0.9427, recall: 0.9058, f1: 0.9239, edges-ner-ontonotes_loss: 0.0247
10/01 03:18:22 AM: Update 10574: task edges-ner-ontonotes, batch 574 (10574): mcc: 0.9192, acc: 0.8779, precision: 0.9423, recall: 0.9052, f1: 0.9234, edges-ner-ontonotes_loss: 0.0251
10/01 03:18:32 AM: Update 10693: task edges-ner-ontonotes, batch 693 (10693): mcc: 0.9123, acc: 0.8690, precision: 0.9381, recall: 0.8965, f1: 0.9168, edges-ner-ontonotes_loss: 0.0278
10/01 03:18:42 AM: Update 10836: task edges-ner-ontonotes, batch 836 (10836): mcc: 0.9071, acc: 0.8625, precision: 0.9349, recall: 0.8899, f1: 0.9118, edges-ner-ontonotes_loss: 0.0301
10/01 03:18:52 AM: Update 10965: task edges-ner-ontonotes, batch 965 (10965): mcc: 0.9041, acc: 0.8584, precision: 0.9330, recall: 0.8861, f1: 0.9090, edges-ner-ontonotes_loss: 0.0314
10/01 03:18:54 AM: ***** Step 11000 / Validation 11 *****
10/01 03:18:54 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:18:54 AM: Validating...
10/01 03:19:02 AM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.9083, acc: 0.8662, precision: 0.9374, recall: 0.8897, f1: 0.9129, edges-ner-ontonotes_loss: 0.0306
10/01 03:19:12 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:19:12 AM: Best result seen so far for macro.
10/01 03:19:12 AM: Updating LR scheduler:
10/01 03:19:12 AM: 	Best result seen so far for macro_avg: 0.929
10/01 03:19:12 AM: 	# validation passes without improvement: 0
10/01 03:19:12 AM: edges-ner-ontonotes_loss: training: 0.031601 validation: 0.025004
10/01 03:19:12 AM: macro_avg: validation: 0.929133
10/01 03:19:12 AM: micro_avg: validation: 0.000000
10/01 03:19:12 AM: edges-ner-ontonotes_mcc: training: 0.903652 validation: 0.925311
10/01 03:19:12 AM: edges-ner-ontonotes_acc: training: 0.857739 validation: 0.888838
10/01 03:19:12 AM: edges-ner-ontonotes_precision: training: 0.932867 validation: 0.950004
10/01 03:19:12 AM: edges-ner-ontonotes_recall: training: 0.885451 validation: 0.909160
10/01 03:19:12 AM: edges-ner-ontonotes_f1: training: 0.908541 validation: 0.929133
10/01 03:19:12 AM: Global learning rate: 0.0001
10/01 03:19:12 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:19:12 AM: Update 11004: task edges-ner-ontonotes, batch 4 (11004): mcc: 0.8988, acc: 0.8465, precision: 0.9186, recall: 0.8904, f1: 0.9042, edges-ner-ontonotes_loss: 0.0282
10/01 03:19:22 AM: Update 11177: task edges-ner-ontonotes, batch 177 (11177): mcc: 0.8882, acc: 0.8344, precision: 0.9230, recall: 0.8663, f1: 0.8938, edges-ner-ontonotes_loss: 0.0353
10/01 03:19:32 AM: Update 11298: task edges-ner-ontonotes, batch 298 (11298): mcc: 0.8949, acc: 0.8447, precision: 0.9285, recall: 0.8734, f1: 0.9001, edges-ner-ontonotes_loss: 0.0332
10/01 03:19:42 AM: Update 11435: task edges-ner-ontonotes, batch 435 (11435): mcc: 0.8988, acc: 0.8510, precision: 0.9304, recall: 0.8788, f1: 0.9039, edges-ner-ontonotes_loss: 0.0321
10/01 03:19:52 AM: Update 11557: task edges-ner-ontonotes, batch 557 (11557): mcc: 0.9025, acc: 0.8558, precision: 0.9324, recall: 0.8838, f1: 0.9074, edges-ner-ontonotes_loss: 0.0309
10/01 03:20:02 AM: Update 11695: task edges-ner-ontonotes, batch 695 (11695): mcc: 0.9065, acc: 0.8611, precision: 0.9353, recall: 0.8885, f1: 0.9113, edges-ner-ontonotes_loss: 0.0297
10/01 03:20:12 AM: Update 11831: task edges-ner-ontonotes, batch 831 (11831): mcc: 0.9094, acc: 0.8647, precision: 0.9368, recall: 0.8923, f1: 0.9140, edges-ner-ontonotes_loss: 0.0287
10/01 03:20:22 AM: Update 11954: task edges-ner-ontonotes, batch 954 (11954): mcc: 0.9108, acc: 0.8667, precision: 0.9378, recall: 0.8941, f1: 0.9154, edges-ner-ontonotes_loss: 0.0282
10/01 03:20:26 AM: ***** Step 12000 / Validation 12 *****
10/01 03:20:26 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:20:26 AM: Validating...
10/01 03:20:32 AM: Evaluate: task edges-ner-ontonotes, batch 66 (157): mcc: 0.9007, acc: 0.8603, precision: 0.9289, recall: 0.8838, f1: 0.9058, edges-ner-ontonotes_loss: 0.0333
10/01 03:20:42 AM: Evaluate: task edges-ner-ontonotes, batch 150 (157): mcc: 0.9275, acc: 0.8959, precision: 0.9488, recall: 0.9145, f1: 0.9313, edges-ner-ontonotes_loss: 0.0251
10/01 03:20:43 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:20:43 AM: Best result seen so far for macro.
10/01 03:20:43 AM: Updating LR scheduler:
10/01 03:20:43 AM: 	Best result seen so far for macro_avg: 0.932
10/01 03:20:43 AM: 	# validation passes without improvement: 0
10/01 03:20:43 AM: edges-ner-ontonotes_loss: training: 0.028126 validation: 0.024698
10/01 03:20:43 AM: macro_avg: validation: 0.932212
10/01 03:20:43 AM: micro_avg: validation: 0.000000
10/01 03:20:43 AM: edges-ner-ontonotes_mcc: training: 0.911217 validation: 0.928460
10/01 03:20:43 AM: edges-ner-ontonotes_acc: training: 0.867154 validation: 0.897028
10/01 03:20:43 AM: edges-ner-ontonotes_precision: training: 0.937907 validation: 0.948943
10/01 03:20:43 AM: edges-ner-ontonotes_recall: training: 0.894643 validation: 0.916060
10/01 03:20:43 AM: edges-ner-ontonotes_f1: training: 0.915764 validation: 0.932212
10/01 03:20:43 AM: Global learning rate: 0.0001
10/01 03:20:43 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:20:52 AM: Update 12124: task edges-ner-ontonotes, batch 124 (12124): mcc: 0.9228, acc: 0.8828, precision: 0.9458, recall: 0.9086, f1: 0.9269, edges-ner-ontonotes_loss: 0.0242
10/01 03:21:02 AM: Update 12246: task edges-ner-ontonotes, batch 246 (12246): mcc: 0.9038, acc: 0.8570, precision: 0.9332, recall: 0.8854, f1: 0.9087, edges-ner-ontonotes_loss: 0.0319
10/01 03:21:12 AM: Update 12387: task edges-ner-ontonotes, batch 387 (12387): mcc: 0.8950, acc: 0.8453, precision: 0.9280, recall: 0.8742, f1: 0.9003, edges-ner-ontonotes_loss: 0.0355
10/01 03:21:22 AM: Update 12513: task edges-ner-ontonotes, batch 513 (12513): mcc: 0.8919, acc: 0.8417, precision: 0.9257, recall: 0.8706, f1: 0.8973, edges-ner-ontonotes_loss: 0.0362
10/01 03:21:33 AM: Update 12683: task edges-ner-ontonotes, batch 683 (12683): mcc: 0.8905, acc: 0.8396, precision: 0.9246, recall: 0.8690, f1: 0.8959, edges-ner-ontonotes_loss: 0.0359
10/01 03:21:43 AM: Update 12813: task edges-ner-ontonotes, batch 813 (12813): mcc: 0.8906, acc: 0.8397, precision: 0.9244, recall: 0.8693, f1: 0.8960, edges-ner-ontonotes_loss: 0.0357
10/01 03:21:53 AM: Update 12951: task edges-ner-ontonotes, batch 951 (12951): mcc: 0.8936, acc: 0.8438, precision: 0.9264, recall: 0.8730, f1: 0.8989, edges-ner-ontonotes_loss: 0.0346
10/01 03:21:56 AM: ***** Step 13000 / Validation 13 *****
10/01 03:21:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:21:56 AM: Validating...
10/01 03:22:03 AM: Evaluate: task edges-ner-ontonotes, batch 63 (157): mcc: 0.9106, acc: 0.8748, precision: 0.9357, recall: 0.8956, f1: 0.9152, edges-ner-ontonotes_loss: 0.0289
10/01 03:22:13 AM: Evaluate: task edges-ner-ontonotes, batch 146 (157): mcc: 0.9273, acc: 0.8953, precision: 0.9514, recall: 0.9114, f1: 0.9310, edges-ner-ontonotes_loss: 0.0248
10/01 03:22:14 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:22:14 AM: Best result seen so far for macro.
10/01 03:22:14 AM: Updating LR scheduler:
10/01 03:22:14 AM: 	Best result seen so far for macro_avg: 0.932
10/01 03:22:14 AM: 	# validation passes without improvement: 1
10/01 03:22:14 AM: edges-ner-ontonotes_loss: training: 0.034286 validation: 0.024197
10/01 03:22:14 AM: macro_avg: validation: 0.932245
10/01 03:22:14 AM: micro_avg: validation: 0.000000
10/01 03:22:14 AM: edges-ner-ontonotes_mcc: training: 0.894472 validation: 0.928553
10/01 03:22:14 AM: edges-ner-ontonotes_acc: training: 0.844946 validation: 0.896800
10/01 03:22:14 AM: edges-ner-ontonotes_precision: training: 0.926998 validation: 0.951302
10/01 03:22:14 AM: edges-ner-ontonotes_recall: training: 0.874064 validation: 0.913937
10/01 03:22:14 AM: edges-ner-ontonotes_f1: training: 0.899754 validation: 0.932245
10/01 03:22:14 AM: Global learning rate: 0.0001
10/01 03:22:14 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:22:23 AM: Update 13106: task edges-ner-ontonotes, batch 106 (13106): mcc: 0.9118, acc: 0.8687, precision: 0.9361, recall: 0.8974, f1: 0.9164, edges-ner-ontonotes_loss: 0.0279
10/01 03:22:33 AM: Update 13242: task edges-ner-ontonotes, batch 242 (13242): mcc: 0.9168, acc: 0.8749, precision: 0.9399, recall: 0.9031, f1: 0.9211, edges-ner-ontonotes_loss: 0.0260
10/01 03:22:43 AM: Update 13378: task edges-ner-ontonotes, batch 378 (13378): mcc: 0.9192, acc: 0.8782, precision: 0.9411, recall: 0.9064, f1: 0.9234, edges-ner-ontonotes_loss: 0.0252
10/01 03:22:53 AM: Update 13497: task edges-ner-ontonotes, batch 497 (13497): mcc: 0.9191, acc: 0.8777, precision: 0.9414, recall: 0.9059, f1: 0.9233, edges-ner-ontonotes_loss: 0.0251
10/01 03:23:03 AM: Update 13635: task edges-ner-ontonotes, batch 635 (13635): mcc: 0.9196, acc: 0.8783, precision: 0.9414, recall: 0.9070, f1: 0.9238, edges-ner-ontonotes_loss: 0.0250
10/01 03:23:13 AM: Update 13758: task edges-ner-ontonotes, batch 758 (13758): mcc: 0.9166, acc: 0.8743, precision: 0.9397, recall: 0.9030, f1: 0.9210, edges-ner-ontonotes_loss: 0.0263
10/01 03:23:23 AM: Update 13900: task edges-ner-ontonotes, batch 900 (13900): mcc: 0.9117, acc: 0.8678, precision: 0.9369, recall: 0.8966, f1: 0.9163, edges-ner-ontonotes_loss: 0.0286
10/01 03:23:30 AM: ***** Step 14000 / Validation 14 *****
10/01 03:23:30 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:23:30 AM: Validating...
10/01 03:23:33 AM: Evaluate: task edges-ner-ontonotes, batch 32 (157): mcc: 0.8815, acc: 0.8405, precision: 0.9074, recall: 0.8689, f1: 0.8877, edges-ner-ontonotes_loss: 0.0371
10/01 03:23:43 AM: Evaluate: task edges-ner-ontonotes, batch 114 (157): mcc: 0.9167, acc: 0.8810, precision: 0.9414, recall: 0.9014, f1: 0.9210, edges-ner-ontonotes_loss: 0.0280
10/01 03:23:48 AM: Updating LR scheduler:
10/01 03:23:48 AM: 	Best result seen so far for macro_avg: 0.932
10/01 03:23:48 AM: 	# validation passes without improvement: 2
10/01 03:23:48 AM: edges-ner-ontonotes_loss: training: 0.029852 validation: 0.025053
10/01 03:23:48 AM: macro_avg: validation: 0.928789
10/01 03:23:48 AM: micro_avg: validation: 0.000000
10/01 03:23:48 AM: edges-ner-ontonotes_mcc: training: 0.908925 validation: 0.924875
10/01 03:23:48 AM: edges-ner-ontonotes_acc: training: 0.864172 validation: 0.891796
10/01 03:23:48 AM: edges-ner-ontonotes_precision: training: 0.935252 validation: 0.946900
10/01 03:23:48 AM: edges-ner-ontonotes_recall: training: 0.892947 validation: 0.911359
10/01 03:23:48 AM: edges-ner-ontonotes_f1: training: 0.913610 validation: 0.928789
10/01 03:23:48 AM: Global learning rate: 0.0001
10/01 03:23:48 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:23:53 AM: Update 14064: task edges-ner-ontonotes, batch 64 (14064): mcc: 0.8858, acc: 0.8336, precision: 0.9235, recall: 0.8613, f1: 0.8913, edges-ner-ontonotes_loss: 0.0373
10/01 03:24:03 AM: Update 14230: task edges-ner-ontonotes, batch 230 (14230): mcc: 0.8872, acc: 0.8340, precision: 0.9233, recall: 0.8641, f1: 0.8927, edges-ner-ontonotes_loss: 0.0359
10/01 03:24:13 AM: Update 14360: task edges-ner-ontonotes, batch 360 (14360): mcc: 0.8874, acc: 0.8351, precision: 0.9225, recall: 0.8653, f1: 0.8930, edges-ner-ontonotes_loss: 0.0355
10/01 03:24:23 AM: Update 14506: task edges-ner-ontonotes, batch 506 (14506): mcc: 0.8937, acc: 0.8438, precision: 0.9259, recall: 0.8738, f1: 0.8991, edges-ner-ontonotes_loss: 0.0335
10/01 03:24:34 AM: Update 14631: task edges-ner-ontonotes, batch 631 (14631): mcc: 0.8984, acc: 0.8503, precision: 0.9289, recall: 0.8795, f1: 0.9036, edges-ner-ontonotes_loss: 0.0322
10/01 03:24:44 AM: Update 14763: task edges-ner-ontonotes, batch 763 (14763): mcc: 0.9030, acc: 0.8566, precision: 0.9317, recall: 0.8854, f1: 0.9079, edges-ner-ontonotes_loss: 0.0308
10/01 03:24:54 AM: Update 14903: task edges-ner-ontonotes, batch 903 (14903): mcc: 0.9068, acc: 0.8620, precision: 0.9342, recall: 0.8901, f1: 0.9116, edges-ner-ontonotes_loss: 0.0296
10/01 03:25:02 AM: ***** Step 15000 / Validation 15 *****
10/01 03:25:02 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:25:02 AM: Validating...
10/01 03:25:04 AM: Evaluate: task edges-ner-ontonotes, batch 14 (157): mcc: 0.8484, acc: 0.7980, precision: 0.8937, recall: 0.8207, f1: 0.8556, edges-ner-ontonotes_loss: 0.0424
10/01 03:25:14 AM: Evaluate: task edges-ner-ontonotes, batch 105 (157): mcc: 0.9154, acc: 0.8815, precision: 0.9406, recall: 0.8999, f1: 0.9198, edges-ner-ontonotes_loss: 0.0288
10/01 03:25:21 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:25:21 AM: Best result seen so far for macro.
10/01 03:25:21 AM: Updating LR scheduler:
10/01 03:25:21 AM: 	Best result seen so far for macro_avg: 0.933
10/01 03:25:21 AM: 	# validation passes without improvement: 0
10/01 03:25:21 AM: edges-ner-ontonotes_loss: training: 0.029213 validation: 0.024323
10/01 03:25:21 AM: macro_avg: validation: 0.932511
10/01 03:25:21 AM: micro_avg: validation: 0.000000
10/01 03:25:21 AM: edges-ner-ontonotes_mcc: training: 0.907795 validation: 0.928767
10/01 03:25:21 AM: edges-ner-ontonotes_acc: training: 0.863029 validation: 0.898392
10/01 03:25:21 AM: edges-ner-ontonotes_precision: training: 0.934718 validation: 0.948831
10/01 03:25:21 AM: edges-ner-ontonotes_recall: training: 0.891362 validation: 0.916743
10/01 03:25:21 AM: edges-ner-ontonotes_f1: training: 0.912525 validation: 0.932511
10/01 03:25:21 AM: Global learning rate: 0.0001
10/01 03:25:21 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:25:24 AM: Update 15047: task edges-ner-ontonotes, batch 47 (15047): mcc: 0.9210, acc: 0.8841, precision: 0.9426, recall: 0.9084, f1: 0.9252, edges-ner-ontonotes_loss: 0.0262
10/01 03:25:34 AM: Update 15183: task edges-ner-ontonotes, batch 183 (15183): mcc: 0.9220, acc: 0.8820, precision: 0.9441, recall: 0.9087, f1: 0.9261, edges-ner-ontonotes_loss: 0.0249
10/01 03:25:44 AM: Update 15287: task edges-ner-ontonotes, batch 287 (15287): mcc: 0.9193, acc: 0.8776, precision: 0.9433, recall: 0.9045, f1: 0.9235, edges-ner-ontonotes_loss: 0.0260
10/01 03:25:54 AM: Update 15430: task edges-ner-ontonotes, batch 430 (15430): mcc: 0.9081, acc: 0.8632, precision: 0.9355, recall: 0.8912, f1: 0.9128, edges-ner-ontonotes_loss: 0.0309
10/01 03:26:04 AM: Update 15562: task edges-ner-ontonotes, batch 562 (15562): mcc: 0.9015, acc: 0.8552, precision: 0.9311, recall: 0.8831, f1: 0.9065, edges-ner-ontonotes_loss: 0.0336
10/01 03:26:14 AM: Update 15728: task edges-ner-ontonotes, batch 728 (15728): mcc: 0.8982, acc: 0.8504, precision: 0.9289, recall: 0.8791, f1: 0.9033, edges-ner-ontonotes_loss: 0.0342
10/01 03:26:25 AM: Update 15874: task edges-ner-ontonotes, batch 874 (15874): mcc: 0.8969, acc: 0.8484, precision: 0.9281, recall: 0.8775, f1: 0.9021, edges-ner-ontonotes_loss: 0.0342
10/01 03:26:34 AM: ***** Step 16000 / Validation 16 *****
10/01 03:26:34 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:26:34 AM: Validating...
10/01 03:26:35 AM: Evaluate: task edges-ner-ontonotes, batch 12 (157): mcc: 0.8516, acc: 0.7892, precision: 0.9026, recall: 0.8182, f1: 0.8583, edges-ner-ontonotes_loss: 0.0410
10/01 03:26:45 AM: Evaluate: task edges-ner-ontonotes, batch 105 (157): mcc: 0.9189, acc: 0.8819, precision: 0.9473, recall: 0.8999, f1: 0.9230, edges-ner-ontonotes_loss: 0.0276
10/01 03:26:52 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:26:52 AM: Best result seen so far for macro.
10/01 03:26:52 AM: Updating LR scheduler:
10/01 03:26:52 AM: 	Best result seen so far for macro_avg: 0.934
10/01 03:26:52 AM: 	# validation passes without improvement: 0
10/01 03:26:52 AM: edges-ner-ontonotes_loss: training: 0.033412 validation: 0.023815
10/01 03:26:52 AM: macro_avg: validation: 0.934015
10/01 03:26:52 AM: micro_avg: validation: 0.000000
10/01 03:26:52 AM: edges-ner-ontonotes_mcc: training: 0.898309 validation: 0.930459
10/01 03:26:52 AM: edges-ner-ontonotes_acc: training: 0.850136 validation: 0.897407
10/01 03:26:52 AM: edges-ner-ontonotes_precision: training: 0.928970 validation: 0.954416
10/01 03:26:52 AM: edges-ner-ontonotes_recall: training: 0.879285 validation: 0.914468
10/01 03:26:52 AM: edges-ner-ontonotes_f1: training: 0.903445 validation: 0.934015
10/01 03:26:52 AM: Global learning rate: 0.0001
10/01 03:26:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:26:55 AM: Update 16047: task edges-ner-ontonotes, batch 47 (16047): mcc: 0.9079, acc: 0.8645, precision: 0.9356, recall: 0.8908, f1: 0.9126, edges-ner-ontonotes_loss: 0.0278
10/01 03:27:05 AM: Update 16184: task edges-ner-ontonotes, batch 184 (16184): mcc: 0.9070, acc: 0.8652, precision: 0.9335, recall: 0.8911, f1: 0.9118, edges-ner-ontonotes_loss: 0.0287
10/01 03:27:15 AM: Update 16296: task edges-ner-ontonotes, batch 296 (16296): mcc: 0.9108, acc: 0.8698, precision: 0.9357, recall: 0.8960, f1: 0.9155, edges-ner-ontonotes_loss: 0.0273
10/01 03:27:25 AM: Update 16433: task edges-ner-ontonotes, batch 433 (16433): mcc: 0.9162, acc: 0.8758, precision: 0.9399, recall: 0.9021, f1: 0.9206, edges-ner-ontonotes_loss: 0.0257
10/01 03:27:35 AM: Update 16526: task edges-ner-ontonotes, batch 526 (16526): mcc: 0.9176, acc: 0.8770, precision: 0.9413, recall: 0.9032, f1: 0.9218, edges-ner-ontonotes_loss: 0.0256
10/01 03:27:45 AM: Update 16664: task edges-ner-ontonotes, batch 664 (16664): mcc: 0.9189, acc: 0.8784, precision: 0.9421, recall: 0.9049, f1: 0.9231, edges-ner-ontonotes_loss: 0.0253
10/01 03:27:55 AM: Update 16797: task edges-ner-ontonotes, batch 797 (16797): mcc: 0.9195, acc: 0.8790, precision: 0.9424, recall: 0.9057, f1: 0.9237, edges-ner-ontonotes_loss: 0.0250
10/01 03:28:05 AM: Update 16920: task edges-ner-ontonotes, batch 920 (16920): mcc: 0.9153, acc: 0.8737, precision: 0.9398, recall: 0.9005, f1: 0.9197, edges-ner-ontonotes_loss: 0.0268
10/01 03:28:11 AM: ***** Step 17000 / Validation 17 *****
10/01 03:28:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:28:11 AM: Validating...
10/01 03:28:15 AM: Evaluate: task edges-ner-ontonotes, batch 45 (157): mcc: 0.8936, acc: 0.8549, precision: 0.9184, recall: 0.8808, f1: 0.8992, edges-ner-ontonotes_loss: 0.0342
10/01 03:28:25 AM: Evaluate: task edges-ner-ontonotes, batch 125 (157): mcc: 0.9217, acc: 0.8875, precision: 0.9459, recall: 0.9065, f1: 0.9258, edges-ner-ontonotes_loss: 0.0269
10/01 03:28:29 AM: Updating LR scheduler:
10/01 03:28:29 AM: 	Best result seen so far for macro_avg: 0.934
10/01 03:28:29 AM: 	# validation passes without improvement: 1
10/01 03:28:29 AM: edges-ner-ontonotes_loss: training: 0.027903 validation: 0.024890
10/01 03:28:29 AM: macro_avg: validation: 0.930504
10/01 03:28:29 AM: micro_avg: validation: 0.000000
10/01 03:28:29 AM: edges-ner-ontonotes_mcc: training: 0.912779 validation: 0.926697
10/01 03:28:29 AM: edges-ner-ontonotes_acc: training: 0.870374 validation: 0.893843
10/01 03:28:29 AM: edges-ner-ontonotes_precision: training: 0.938075 validation: 0.948991
10/01 03:28:29 AM: edges-ner-ontonotes_recall: training: 0.897387 validation: 0.912724
10/01 03:28:29 AM: edges-ner-ontonotes_f1: training: 0.917280 validation: 0.930504
10/01 03:28:29 AM: Global learning rate: 0.0001
10/01 03:28:29 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:28:35 AM: Update 17096: task edges-ner-ontonotes, batch 96 (17096): mcc: 0.8845, acc: 0.8336, precision: 0.9205, recall: 0.8618, f1: 0.8902, edges-ner-ontonotes_loss: 0.0405
10/01 03:28:45 AM: Update 17225: task edges-ner-ontonotes, batch 225 (17225): mcc: 0.8844, acc: 0.8311, precision: 0.9204, recall: 0.8617, f1: 0.8901, edges-ner-ontonotes_loss: 0.0382
10/01 03:28:56 AM: Update 17390: task edges-ner-ontonotes, batch 390 (17390): mcc: 0.8864, acc: 0.8335, precision: 0.9224, recall: 0.8636, f1: 0.8920, edges-ner-ontonotes_loss: 0.0366
10/01 03:29:06 AM: Update 17513: task edges-ner-ontonotes, batch 513 (17513): mcc: 0.8909, acc: 0.8398, precision: 0.9249, recall: 0.8695, f1: 0.8964, edges-ner-ontonotes_loss: 0.0353
10/01 03:29:16 AM: Update 17647: task edges-ner-ontonotes, batch 647 (17647): mcc: 0.8955, acc: 0.8464, precision: 0.9273, recall: 0.8756, f1: 0.9007, edges-ner-ontonotes_loss: 0.0337
10/01 03:29:26 AM: Update 17777: task edges-ner-ontonotes, batch 777 (17777): mcc: 0.8996, acc: 0.8518, precision: 0.9300, recall: 0.8807, f1: 0.9047, edges-ner-ontonotes_loss: 0.0324
10/01 03:29:36 AM: Update 17911: task edges-ner-ontonotes, batch 911 (17911): mcc: 0.9037, acc: 0.8572, precision: 0.9323, recall: 0.8861, f1: 0.9086, edges-ner-ontonotes_loss: 0.0312
10/01 03:29:42 AM: ***** Step 18000 / Validation 18 *****
10/01 03:29:42 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:29:42 AM: Validating...
10/01 03:29:46 AM: Evaluate: task edges-ner-ontonotes, batch 39 (157): mcc: 0.8878, acc: 0.8464, precision: 0.9187, recall: 0.8698, f1: 0.8936, edges-ner-ontonotes_loss: 0.0361
10/01 03:29:56 AM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.9224, acc: 0.8895, precision: 0.9465, recall: 0.9072, f1: 0.9264, edges-ner-ontonotes_loss: 0.0274
10/01 03:30:00 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:30:00 AM: Best result seen so far for macro.
10/01 03:30:00 AM: Updating LR scheduler:
10/01 03:30:00 AM: 	Best result seen so far for macro_avg: 0.934
10/01 03:30:00 AM: 	# validation passes without improvement: 0
10/01 03:30:00 AM: edges-ner-ontonotes_loss: training: 0.030397 validation: 0.024104
10/01 03:30:00 AM: macro_avg: validation: 0.934162
10/01 03:30:00 AM: micro_avg: validation: 0.000000
10/01 03:30:00 AM: edges-ner-ontonotes_mcc: training: 0.906137 validation: 0.930529
10/01 03:30:00 AM: edges-ner-ontonotes_acc: training: 0.860472 validation: 0.900136
10/01 03:30:00 AM: edges-ner-ontonotes_precision: training: 0.933908 validation: 0.951195
10/01 03:30:00 AM: edges-ner-ontonotes_recall: training: 0.889062 validation: 0.917728
10/01 03:30:00 AM: edges-ner-ontonotes_f1: training: 0.910933 validation: 0.934162
10/01 03:30:00 AM: Global learning rate: 0.0001
10/01 03:30:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:30:06 AM: Update 18056: task edges-ner-ontonotes, batch 56 (18056): mcc: 0.9185, acc: 0.8764, precision: 0.9400, recall: 0.9062, f1: 0.9228, edges-ner-ontonotes_loss: 0.0250
10/01 03:30:16 AM: Update 18193: task edges-ner-ontonotes, batch 193 (18193): mcc: 0.9214, acc: 0.8803, precision: 0.9426, recall: 0.9089, f1: 0.9255, edges-ner-ontonotes_loss: 0.0244
10/01 03:30:26 AM: Update 18329: task edges-ner-ontonotes, batch 329 (18329): mcc: 0.9213, acc: 0.8808, precision: 0.9424, recall: 0.9090, f1: 0.9254, edges-ner-ontonotes_loss: 0.0244
10/01 03:30:36 AM: Update 18447: task edges-ner-ontonotes, batch 447 (18447): mcc: 0.9137, acc: 0.8715, precision: 0.9373, recall: 0.8998, f1: 0.9182, edges-ner-ontonotes_loss: 0.0277
10/01 03:30:46 AM: Update 18592: task edges-ner-ontonotes, batch 592 (18592): mcc: 0.9074, acc: 0.8632, precision: 0.9337, recall: 0.8915, f1: 0.9122, edges-ner-ontonotes_loss: 0.0306
10/01 03:30:56 AM: Update 18713: task edges-ner-ontonotes, batch 713 (18713): mcc: 0.9041, acc: 0.8589, precision: 0.9313, recall: 0.8878, f1: 0.9091, edges-ner-ontonotes_loss: 0.0320
10/01 03:31:06 AM: Update 18881: task edges-ner-ontonotes, batch 881 (18881): mcc: 0.9024, acc: 0.8561, precision: 0.9310, recall: 0.8850, f1: 0.9074, edges-ner-ontonotes_loss: 0.0323
10/01 03:31:16 AM: ***** Step 19000 / Validation 19 *****
10/01 03:31:16 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:31:16 AM: Validating...
10/01 03:31:16 AM: Evaluate: task edges-ner-ontonotes, batch 8 (157): mcc: 0.8407, acc: 0.7776, precision: 0.8861, recall: 0.8137, f1: 0.8484, edges-ner-ontonotes_loss: 0.0431
10/01 03:31:26 AM: Evaluate: task edges-ner-ontonotes, batch 101 (157): mcc: 0.9190, acc: 0.8814, precision: 0.9455, recall: 0.9019, f1: 0.9232, edges-ner-ontonotes_loss: 0.0276
10/01 03:31:33 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:31:33 AM: Best result seen so far for macro.
10/01 03:31:33 AM: Updating LR scheduler:
10/01 03:31:33 AM: 	Best result seen so far for macro_avg: 0.935
10/01 03:31:33 AM: 	# validation passes without improvement: 0
10/01 03:31:33 AM: edges-ner-ontonotes_loss: training: 0.032447 validation: 0.023629
10/01 03:31:33 AM: macro_avg: validation: 0.935193
10/01 03:31:33 AM: micro_avg: validation: 0.000000
10/01 03:31:33 AM: edges-ner-ontonotes_mcc: training: 0.901094 validation: 0.931653
10/01 03:31:33 AM: edges-ner-ontonotes_acc: training: 0.854050 validation: 0.899151
10/01 03:31:33 AM: edges-ner-ontonotes_precision: training: 0.930175 validation: 0.953582
10/01 03:31:33 AM: edges-ner-ontonotes_recall: training: 0.883294 validation: 0.917501
10/01 03:31:33 AM: edges-ner-ontonotes_f1: training: 0.906129 validation: 0.935193
10/01 03:31:33 AM: Global learning rate: 0.0001
10/01 03:31:33 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:31:37 AM: Update 19043: task edges-ner-ontonotes, batch 43 (19043): mcc: 0.9092, acc: 0.8633, precision: 0.9339, recall: 0.8947, f1: 0.9139, edges-ner-ontonotes_loss: 0.0272
10/01 03:31:47 AM: Update 19179: task edges-ner-ontonotes, batch 179 (19179): mcc: 0.9073, acc: 0.8626, precision: 0.9344, recall: 0.8908, f1: 0.9121, edges-ner-ontonotes_loss: 0.0282
10/01 03:31:57 AM: Update 19302: task edges-ner-ontonotes, batch 302 (19302): mcc: 0.9104, acc: 0.8672, precision: 0.9356, recall: 0.8953, f1: 0.9150, edges-ner-ontonotes_loss: 0.0277
10/01 03:32:07 AM: Update 19436: task edges-ner-ontonotes, batch 436 (19436): mcc: 0.9153, acc: 0.8733, precision: 0.9388, recall: 0.9013, f1: 0.9197, edges-ner-ontonotes_loss: 0.0264
10/01 03:32:17 AM: Update 19573: task edges-ner-ontonotes, batch 573 (19573): mcc: 0.9179, acc: 0.8766, precision: 0.9408, recall: 0.9041, f1: 0.9221, edges-ner-ontonotes_loss: 0.0255
10/01 03:32:27 AM: Update 19697: task edges-ner-ontonotes, batch 697 (19697): mcc: 0.9190, acc: 0.8781, precision: 0.9413, recall: 0.9059, f1: 0.9233, edges-ner-ontonotes_loss: 0.0253
10/01 03:32:37 AM: Update 19832: task edges-ner-ontonotes, batch 832 (19832): mcc: 0.9193, acc: 0.8782, precision: 0.9418, recall: 0.9059, f1: 0.9235, edges-ner-ontonotes_loss: 0.0252
10/01 03:32:47 AM: Update 19945: task edges-ner-ontonotes, batch 945 (19945): mcc: 0.9187, acc: 0.8773, precision: 0.9415, recall: 0.9051, f1: 0.9229, edges-ner-ontonotes_loss: 0.0254
10/01 03:32:50 AM: ***** Step 20000 / Validation 20 *****
10/01 03:32:50 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:32:50 AM: Validating...
10/01 03:32:57 AM: Evaluate: task edges-ner-ontonotes, batch 63 (157): mcc: 0.9087, acc: 0.8692, precision: 0.9329, recall: 0.8948, f1: 0.9134, edges-ner-ontonotes_loss: 0.0298
10/01 03:33:07 AM: Evaluate: task edges-ner-ontonotes, batch 146 (157): mcc: 0.9257, acc: 0.8916, precision: 0.9510, recall: 0.9090, f1: 0.9295, edges-ner-ontonotes_loss: 0.0252
10/01 03:33:08 AM: Updating LR scheduler:
10/01 03:33:08 AM: 	Best result seen so far for macro_avg: 0.935
10/01 03:33:08 AM: 	# validation passes without improvement: 1
10/01 03:33:08 AM: edges-ner-ontonotes_loss: training: 0.026549 validation: 0.024565
10/01 03:33:08 AM: macro_avg: validation: 0.930973
10/01 03:33:08 AM: micro_avg: validation: 0.000000
10/01 03:33:08 AM: edges-ner-ontonotes_mcc: training: 0.916189 validation: 0.927250
10/01 03:33:08 AM: edges-ner-ontonotes_acc: training: 0.874195 validation: 0.893388
10/01 03:33:08 AM: edges-ner-ontonotes_precision: training: 0.939594 validation: 0.951615
10/01 03:33:08 AM: edges-ner-ontonotes_recall: training: 0.902266 validation: 0.911207
10/01 03:33:08 AM: edges-ner-ontonotes_f1: training: 0.920552 validation: 0.930973
10/01 03:33:08 AM: Global learning rate: 0.0001
10/01 03:33:08 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:33:17 AM: Update 20122: task edges-ner-ontonotes, batch 122 (20122): mcc: 0.8845, acc: 0.8335, precision: 0.9209, recall: 0.8614, f1: 0.8902, edges-ner-ontonotes_loss: 0.0384
10/01 03:33:27 AM: Update 20239: task edges-ner-ontonotes, batch 239 (20239): mcc: 0.8817, acc: 0.8313, precision: 0.9180, recall: 0.8590, f1: 0.8875, edges-ner-ontonotes_loss: 0.0394
10/01 03:33:37 AM: Update 20408: task edges-ner-ontonotes, batch 408 (20408): mcc: 0.8852, acc: 0.8340, precision: 0.9204, recall: 0.8632, f1: 0.8909, edges-ner-ontonotes_loss: 0.0373
10/01 03:33:47 AM: Update 20545: task edges-ner-ontonotes, batch 545 (20545): mcc: 0.8864, acc: 0.8351, precision: 0.9213, recall: 0.8645, f1: 0.8920, edges-ner-ontonotes_loss: 0.0365
10/01 03:33:57 AM: Update 20688: task edges-ner-ontonotes, batch 688 (20688): mcc: 0.8919, acc: 0.8431, precision: 0.9246, recall: 0.8717, f1: 0.8974, edges-ner-ontonotes_loss: 0.0346
10/01 03:34:07 AM: Update 20830: task edges-ner-ontonotes, batch 830 (20830): mcc: 0.8951, acc: 0.8466, precision: 0.9263, recall: 0.8758, f1: 0.9004, edges-ner-ontonotes_loss: 0.0335
10/01 03:34:17 AM: Update 20939: task edges-ner-ontonotes, batch 939 (20939): mcc: 0.8984, acc: 0.8509, precision: 0.9283, recall: 0.8802, f1: 0.9036, edges-ner-ontonotes_loss: 0.0324
10/01 03:34:22 AM: ***** Step 21000 / Validation 21 *****
10/01 03:34:22 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:34:22 AM: Validating...
10/01 03:34:27 AM: Evaluate: task edges-ner-ontonotes, batch 55 (157): mcc: 0.9065, acc: 0.8711, precision: 0.9296, recall: 0.8939, f1: 0.9114, edges-ner-ontonotes_loss: 0.0315
10/01 03:34:37 AM: Evaluate: task edges-ner-ontonotes, batch 135 (157): mcc: 0.9295, acc: 0.9002, precision: 0.9496, recall: 0.9173, f1: 0.9332, edges-ner-ontonotes_loss: 0.0251
10/01 03:34:40 AM: Updating LR scheduler:
10/01 03:34:40 AM: 	Best result seen so far for macro_avg: 0.935
10/01 03:34:40 AM: 	# validation passes without improvement: 2
10/01 03:34:40 AM: edges-ner-ontonotes_loss: training: 0.031846 validation: 0.024042
10/01 03:34:40 AM: macro_avg: validation: 0.934299
10/01 03:34:40 AM: micro_avg: validation: 0.000000
10/01 03:34:40 AM: edges-ner-ontonotes_mcc: training: 0.899999 validation: 0.930641
10/01 03:34:40 AM: edges-ner-ontonotes_acc: training: 0.853078 validation: 0.901198
10/01 03:34:40 AM: edges-ner-ontonotes_precision: training: 0.929306 validation: 0.949855
10/01 03:34:40 AM: edges-ner-ontonotes_recall: training: 0.882097 validation: 0.919245
10/01 03:34:40 AM: edges-ner-ontonotes_f1: training: 0.905086 validation: 0.934299
10/01 03:34:40 AM: Global learning rate: 0.0001
10/01 03:34:40 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:34:47 AM: Update 21106: task edges-ner-ontonotes, batch 106 (21106): mcc: 0.9241, acc: 0.8845, precision: 0.9445, recall: 0.9122, f1: 0.9281, edges-ner-ontonotes_loss: 0.0230
10/01 03:34:57 AM: Update 21220: task edges-ner-ontonotes, batch 220 (21220): mcc: 0.9224, acc: 0.8816, precision: 0.9433, recall: 0.9101, f1: 0.9264, edges-ner-ontonotes_loss: 0.0237
10/01 03:35:08 AM: Update 21359: task edges-ner-ontonotes, batch 359 (21359): mcc: 0.9223, acc: 0.8815, precision: 0.9432, recall: 0.9102, f1: 0.9264, edges-ner-ontonotes_loss: 0.0237
10/01 03:35:18 AM: Update 21481: task edges-ner-ontonotes, batch 481 (21481): mcc: 0.9225, acc: 0.8822, precision: 0.9434, recall: 0.9104, f1: 0.9266, edges-ner-ontonotes_loss: 0.0239
10/01 03:35:28 AM: Update 21624: task edges-ner-ontonotes, batch 624 (21624): mcc: 0.9138, acc: 0.8710, precision: 0.9381, recall: 0.8993, f1: 0.9183, edges-ner-ontonotes_loss: 0.0278
10/01 03:35:38 AM: Update 21764: task edges-ner-ontonotes, batch 764 (21764): mcc: 0.9092, acc: 0.8650, precision: 0.9352, recall: 0.8936, f1: 0.9139, edges-ner-ontonotes_loss: 0.0299
10/01 03:35:48 AM: Update 21908: task edges-ner-ontonotes, batch 908 (21908): mcc: 0.9057, acc: 0.8602, precision: 0.9329, recall: 0.8893, f1: 0.9106, edges-ner-ontonotes_loss: 0.0311
10/01 03:35:53 AM: ***** Step 22000 / Validation 22 *****
10/01 03:35:53 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:35:53 AM: Validating...
10/01 03:35:58 AM: Evaluate: task edges-ner-ontonotes, batch 47 (157): mcc: 0.9028, acc: 0.8648, precision: 0.9285, recall: 0.8881, f1: 0.9079, edges-ner-ontonotes_loss: 0.0314
10/01 03:36:08 AM: Evaluate: task edges-ner-ontonotes, batch 128 (157): mcc: 0.9286, acc: 0.8961, precision: 0.9514, recall: 0.9139, f1: 0.9323, edges-ner-ontonotes_loss: 0.0251
10/01 03:36:11 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:36:11 AM: Best result seen so far for macro.
10/01 03:36:11 AM: Updating LR scheduler:
10/01 03:36:11 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:36:11 AM: 	# validation passes without improvement: 0
10/01 03:36:11 AM: edges-ner-ontonotes_loss: training: 0.031311 validation: 0.023555
10/01 03:36:11 AM: macro_avg: validation: 0.935623
10/01 03:36:11 AM: micro_avg: validation: 0.000000
10/01 03:36:11 AM: edges-ner-ontonotes_mcc: training: 0.904779 validation: 0.932077
10/01 03:36:11 AM: edges-ner-ontonotes_acc: training: 0.858958 validation: 0.900971
10/01 03:36:11 AM: edges-ner-ontonotes_precision: training: 0.932221 validation: 0.952759
10/01 03:36:11 AM: edges-ner-ontonotes_recall: training: 0.888167 validation: 0.919093
10/01 03:36:11 AM: edges-ner-ontonotes_f1: training: 0.909661 validation: 0.935623
10/01 03:36:11 AM: Global learning rate: 0.0001
10/01 03:36:11 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:36:19 AM: Update 22098: task edges-ner-ontonotes, batch 98 (22098): mcc: 0.8922, acc: 0.8413, precision: 0.9245, recall: 0.8722, f1: 0.8976, edges-ner-ontonotes_loss: 0.0331
10/01 03:36:29 AM: Update 22242: task edges-ner-ontonotes, batch 242 (22242): mcc: 0.9030, acc: 0.8566, precision: 0.9315, recall: 0.8855, f1: 0.9079, edges-ner-ontonotes_loss: 0.0303
10/01 03:36:39 AM: Update 22382: task edges-ner-ontonotes, batch 382 (22382): mcc: 0.9067, acc: 0.8629, precision: 0.9338, recall: 0.8903, f1: 0.9115, edges-ner-ontonotes_loss: 0.0291
10/01 03:36:49 AM: Update 22501: task edges-ner-ontonotes, batch 501 (22501): mcc: 0.9106, acc: 0.8674, precision: 0.9359, recall: 0.8954, f1: 0.9152, edges-ner-ontonotes_loss: 0.0282
10/01 03:36:59 AM: Update 22642: task edges-ner-ontonotes, batch 642 (22642): mcc: 0.9146, acc: 0.8725, precision: 0.9385, recall: 0.9003, f1: 0.9190, edges-ner-ontonotes_loss: 0.0269
10/01 03:37:09 AM: Update 22763: task edges-ner-ontonotes, batch 763 (22763): mcc: 0.9170, acc: 0.8759, precision: 0.9400, recall: 0.9033, f1: 0.9213, edges-ner-ontonotes_loss: 0.0261
10/01 03:37:19 AM: Update 22900: task edges-ner-ontonotes, batch 900 (22900): mcc: 0.9181, acc: 0.8772, precision: 0.9409, recall: 0.9046, f1: 0.9224, edges-ner-ontonotes_loss: 0.0256
10/01 03:37:26 AM: ***** Step 23000 / Validation 23 *****
10/01 03:37:26 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:37:26 AM: Validating...
10/01 03:37:29 AM: Evaluate: task edges-ner-ontonotes, batch 29 (157): mcc: 0.8693, acc: 0.8266, precision: 0.9049, recall: 0.8487, f1: 0.8759, edges-ner-ontonotes_loss: 0.0410
10/01 03:37:39 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9206, acc: 0.8879, precision: 0.9447, recall: 0.9056, f1: 0.9247, edges-ner-ontonotes_loss: 0.0277
10/01 03:37:44 AM: Updating LR scheduler:
10/01 03:37:44 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:37:44 AM: 	# validation passes without improvement: 1
10/01 03:37:44 AM: edges-ner-ontonotes_loss: training: 0.025492 validation: 0.024046
10/01 03:37:44 AM: macro_avg: validation: 0.934259
10/01 03:37:44 AM: micro_avg: validation: 0.000000
10/01 03:37:44 AM: edges-ner-ontonotes_mcc: training: 0.918628 validation: 0.930625
10/01 03:37:44 AM: edges-ner-ontonotes_acc: training: 0.877538 validation: 0.901046
10/01 03:37:44 AM: edges-ner-ontonotes_precision: training: 0.941271 validation: 0.950990
10/01 03:37:44 AM: edges-ner-ontonotes_recall: training: 0.905183 validation: 0.918107
10/01 03:37:44 AM: edges-ner-ontonotes_f1: training: 0.922875 validation: 0.934259
10/01 03:37:44 AM: Global learning rate: 0.0001
10/01 03:37:44 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:37:49 AM: Update 23049: task edges-ner-ontonotes, batch 49 (23049): mcc: 0.9055, acc: 0.8582, precision: 0.9323, recall: 0.8895, f1: 0.9104, edges-ner-ontonotes_loss: 0.0279
10/01 03:37:59 AM: Update 23195: task edges-ner-ontonotes, batch 195 (23195): mcc: 0.8881, acc: 0.8384, precision: 0.9218, recall: 0.8673, f1: 0.8937, edges-ner-ontonotes_loss: 0.0380
10/01 03:38:11 AM: Update 23341: task edges-ner-ontonotes, batch 341 (23341): mcc: 0.8868, acc: 0.8370, precision: 0.9204, recall: 0.8661, f1: 0.8924, edges-ner-ontonotes_loss: 0.0385
10/01 03:38:21 AM: Update 23507: task edges-ner-ontonotes, batch 507 (23507): mcc: 0.8877, acc: 0.8372, precision: 0.9207, recall: 0.8676, f1: 0.8934, edges-ner-ontonotes_loss: 0.0372
10/01 03:38:31 AM: Update 23655: task edges-ner-ontonotes, batch 655 (23655): mcc: 0.8881, acc: 0.8371, precision: 0.9213, recall: 0.8677, f1: 0.8937, edges-ner-ontonotes_loss: 0.0364
10/01 03:38:41 AM: Update 23799: task edges-ner-ontonotes, batch 799 (23799): mcc: 0.8932, acc: 0.8435, precision: 0.9247, recall: 0.8739, f1: 0.8986, edges-ner-ontonotes_loss: 0.0347
10/01 03:38:51 AM: Update 23944: task edges-ner-ontonotes, batch 944 (23944): mcc: 0.8959, acc: 0.8472, precision: 0.9263, recall: 0.8774, f1: 0.9012, edges-ner-ontonotes_loss: 0.0336
10/01 03:38:57 AM: ***** Step 24000 / Validation 24 *****
10/01 03:38:57 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:38:57 AM: Validating...
10/01 03:39:01 AM: Evaluate: task edges-ner-ontonotes, batch 46 (157): mcc: 0.8984, acc: 0.8606, precision: 0.9208, recall: 0.8872, f1: 0.9037, edges-ner-ontonotes_loss: 0.0334
10/01 03:39:11 AM: Evaluate: task edges-ner-ontonotes, batch 127 (157): mcc: 0.9269, acc: 0.8965, precision: 0.9453, recall: 0.9168, f1: 0.9308, edges-ner-ontonotes_loss: 0.0255
10/01 03:39:14 AM: Updating LR scheduler:
10/01 03:39:14 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:39:14 AM: 	# validation passes without improvement: 2
10/01 03:39:14 AM: edges-ner-ontonotes_loss: training: 0.033235 validation: 0.023746
10/01 03:39:14 AM: macro_avg: validation: 0.934782
10/01 03:39:14 AM: micro_avg: validation: 0.000000
10/01 03:39:14 AM: edges-ner-ontonotes_mcc: training: 0.897172 validation: 0.931077
10/01 03:39:14 AM: edges-ner-ontonotes_acc: training: 0.848922 validation: 0.901577
10/01 03:39:14 AM: edges-ner-ontonotes_precision: training: 0.927087 validation: 0.946591
10/01 03:39:14 AM: edges-ner-ontonotes_recall: training: 0.878985 validation: 0.923264
10/01 03:39:14 AM: edges-ner-ontonotes_f1: training: 0.902396 validation: 0.934782
10/01 03:39:14 AM: Global learning rate: 0.0001
10/01 03:39:14 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:39:21 AM: Update 24092: task edges-ner-ontonotes, batch 92 (24092): mcc: 0.9227, acc: 0.8833, precision: 0.9400, recall: 0.9139, f1: 0.9268, edges-ner-ontonotes_loss: 0.0233
10/01 03:39:31 AM: Update 24236: task edges-ner-ontonotes, batch 236 (24236): mcc: 0.9234, acc: 0.8838, precision: 0.9415, recall: 0.9139, f1: 0.9275, edges-ner-ontonotes_loss: 0.0233
10/01 03:39:41 AM: Update 24350: task edges-ner-ontonotes, batch 350 (24350): mcc: 0.9229, acc: 0.8830, precision: 0.9419, recall: 0.9126, f1: 0.9270, edges-ner-ontonotes_loss: 0.0236
10/01 03:39:51 AM: Update 24490: task edges-ner-ontonotes, batch 490 (24490): mcc: 0.9239, acc: 0.8842, precision: 0.9430, recall: 0.9133, f1: 0.9279, edges-ner-ontonotes_loss: 0.0234
10/01 03:40:01 AM: Update 24621: task edges-ner-ontonotes, batch 621 (24621): mcc: 0.9218, acc: 0.8817, precision: 0.9419, recall: 0.9105, f1: 0.9260, edges-ner-ontonotes_loss: 0.0243
10/01 03:40:12 AM: Update 24767: task edges-ner-ontonotes, batch 767 (24767): mcc: 0.9152, acc: 0.8729, precision: 0.9378, recall: 0.9023, f1: 0.9197, edges-ner-ontonotes_loss: 0.0275
10/01 03:40:22 AM: Update 24897: task edges-ner-ontonotes, batch 897 (24897): mcc: 0.9119, acc: 0.8687, precision: 0.9361, recall: 0.8976, f1: 0.9164, edges-ner-ontonotes_loss: 0.0291
10/01 03:40:28 AM: ***** Step 25000 / Validation 25 *****
10/01 03:40:28 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:40:28 AM: Validating...
10/01 03:40:32 AM: Evaluate: task edges-ner-ontonotes, batch 39 (157): mcc: 0.8972, acc: 0.8571, precision: 0.9271, recall: 0.8789, f1: 0.9024, edges-ner-ontonotes_loss: 0.0332
10/01 03:40:42 AM: Evaluate: task edges-ner-ontonotes, batch 121 (157): mcc: 0.9271, acc: 0.8929, precision: 0.9541, recall: 0.9085, f1: 0.9307, edges-ner-ontonotes_loss: 0.0258
10/01 03:40:46 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:40:46 AM: Best result seen so far for macro.
10/01 03:40:46 AM: Updating LR scheduler:
10/01 03:40:46 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:40:46 AM: 	# validation passes without improvement: 3
10/01 03:40:46 AM: edges-ner-ontonotes_loss: training: 0.029800 validation: 0.023568
10/01 03:40:46 AM: macro_avg: validation: 0.935663
10/01 03:40:46 AM: micro_avg: validation: 0.000000
10/01 03:40:46 AM: edges-ner-ontonotes_mcc: training: 0.909839 validation: 0.932209
10/01 03:40:46 AM: edges-ner-ontonotes_acc: training: 0.865831 validation: 0.899909
10/01 03:40:46 AM: edges-ner-ontonotes_precision: training: 0.934714 validation: 0.956370
10/01 03:40:46 AM: edges-ner-ontonotes_recall: training: 0.895171 validation: 0.915833
10/01 03:40:46 AM: edges-ner-ontonotes_f1: training: 0.914515 validation: 0.935663
10/01 03:40:46 AM: Global learning rate: 0.0001
10/01 03:40:46 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:40:52 AM: Update 25103: task edges-ner-ontonotes, batch 103 (25103): mcc: 0.8945, acc: 0.8413, precision: 0.9290, recall: 0.8722, f1: 0.8997, edges-ner-ontonotes_loss: 0.0328
10/01 03:41:02 AM: Update 25240: task edges-ner-ontonotes, batch 240 (25240): mcc: 0.8953, acc: 0.8445, precision: 0.9267, recall: 0.8758, f1: 0.9005, edges-ner-ontonotes_loss: 0.0326
10/01 03:41:12 AM: Update 25383: task edges-ner-ontonotes, batch 383 (25383): mcc: 0.9015, acc: 0.8535, precision: 0.9292, recall: 0.8850, f1: 0.9066, edges-ner-ontonotes_loss: 0.0307
10/01 03:41:23 AM: Update 25523: task edges-ner-ontonotes, batch 523 (25523): mcc: 0.9053, acc: 0.8591, precision: 0.9317, recall: 0.8897, f1: 0.9102, edges-ner-ontonotes_loss: 0.0297
10/01 03:41:33 AM: Update 25660: task edges-ner-ontonotes, batch 660 (25660): mcc: 0.9097, acc: 0.8652, precision: 0.9344, recall: 0.8953, f1: 0.9144, edges-ner-ontonotes_loss: 0.0285
10/01 03:41:43 AM: Update 25801: task edges-ner-ontonotes, batch 801 (25801): mcc: 0.9129, acc: 0.8693, precision: 0.9363, recall: 0.8993, f1: 0.9174, edges-ner-ontonotes_loss: 0.0274
10/01 03:41:53 AM: Update 25926: task edges-ner-ontonotes, batch 926 (25926): mcc: 0.9148, acc: 0.8719, precision: 0.9378, recall: 0.9015, f1: 0.9193, edges-ner-ontonotes_loss: 0.0269
10/01 03:41:58 AM: ***** Step 26000 / Validation 26 *****
10/01 03:41:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:41:58 AM: Validating...
10/01 03:42:03 AM: Evaluate: task edges-ner-ontonotes, batch 50 (157): mcc: 0.8953, acc: 0.8580, precision: 0.9201, recall: 0.8823, f1: 0.9008, edges-ner-ontonotes_loss: 0.0344
10/01 03:42:13 AM: Evaluate: task edges-ner-ontonotes, batch 132 (157): mcc: 0.9287, acc: 0.8991, precision: 0.9474, recall: 0.9180, f1: 0.9325, edges-ner-ontonotes_loss: 0.0256
10/01 03:42:16 AM: Updating LR scheduler:
10/01 03:42:16 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:42:16 AM: 	# validation passes without improvement: 0
10/01 03:42:16 AM: edges-ner-ontonotes_loss: training: 0.026687 validation: 0.024081
10/01 03:42:16 AM: macro_avg: validation: 0.935630
10/01 03:42:16 AM: micro_avg: validation: 0.000000
10/01 03:42:16 AM: edges-ner-ontonotes_mcc: training: 0.915359 validation: 0.932006
10/01 03:42:16 AM: edges-ner-ontonotes_acc: training: 0.872690 validation: 0.903094
10/01 03:42:16 AM: edges-ner-ontonotes_precision: training: 0.937938 validation: 0.949134
10/01 03:42:16 AM: edges-ner-ontonotes_recall: training: 0.902329 validation: 0.922505
10/01 03:42:16 AM: edges-ner-ontonotes_f1: training: 0.919789 validation: 0.935630
10/01 03:42:16 AM: Global learning rate: 5e-05
10/01 03:42:16 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:42:23 AM: Update 26102: task edges-ner-ontonotes, batch 102 (26102): mcc: 0.9203, acc: 0.8792, precision: 0.9407, recall: 0.9089, f1: 0.9245, edges-ner-ontonotes_loss: 0.0241
10/01 03:42:33 AM: Update 26219: task edges-ner-ontonotes, batch 219 (26219): mcc: 0.9072, acc: 0.8618, precision: 0.9338, recall: 0.8910, f1: 0.9119, edges-ner-ontonotes_loss: 0.0301
10/01 03:42:43 AM: Update 26361: task edges-ner-ontonotes, batch 361 (26361): mcc: 0.8995, acc: 0.8533, precision: 0.9285, recall: 0.8819, f1: 0.9046, edges-ner-ontonotes_loss: 0.0335
10/01 03:42:53 AM: Update 26493: task edges-ner-ontonotes, batch 493 (26493): mcc: 0.8967, acc: 0.8495, precision: 0.9271, recall: 0.8780, f1: 0.9019, edges-ner-ontonotes_loss: 0.0348
10/01 03:43:03 AM: Update 26659: task edges-ner-ontonotes, batch 659 (26659): mcc: 0.8948, acc: 0.8465, precision: 0.9256, recall: 0.8761, f1: 0.9002, edges-ner-ontonotes_loss: 0.0348
10/01 03:43:13 AM: Update 26798: task edges-ner-ontonotes, batch 798 (26798): mcc: 0.8956, acc: 0.8475, precision: 0.9265, recall: 0.8766, f1: 0.9009, edges-ner-ontonotes_loss: 0.0341
10/01 03:43:24 AM: Update 26945: task edges-ner-ontonotes, batch 945 (26945): mcc: 0.8976, acc: 0.8502, precision: 0.9273, recall: 0.8796, f1: 0.9028, edges-ner-ontonotes_loss: 0.0332
10/01 03:43:27 AM: ***** Step 27000 / Validation 27 *****
10/01 03:43:27 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:43:27 AM: Validating...
10/01 03:43:34 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9156, acc: 0.8825, precision: 0.9387, recall: 0.9020, f1: 0.9200, edges-ner-ontonotes_loss: 0.0280
10/01 03:43:44 AM: Evaluate: task edges-ner-ontonotes, batch 145 (157): mcc: 0.9318, acc: 0.9014, precision: 0.9545, recall: 0.9169, f1: 0.9353, edges-ner-ontonotes_loss: 0.0236
10/01 03:43:45 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:43:45 AM: Best result seen so far for macro.
10/01 03:43:45 AM: Updating LR scheduler:
10/01 03:43:45 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:43:45 AM: 	# validation passes without improvement: 0
10/01 03:43:45 AM: edges-ner-ontonotes_loss: training: 0.032653 validation: 0.023015
10/01 03:43:45 AM: macro_avg: validation: 0.936394
10/01 03:43:45 AM: micro_avg: validation: 0.000000
10/01 03:43:45 AM: edges-ner-ontonotes_mcc: training: 0.898814 validation: 0.932922
10/01 03:43:45 AM: edges-ner-ontonotes_acc: training: 0.851698 validation: 0.902563
10/01 03:43:45 AM: edges-ner-ontonotes_precision: training: 0.928200 validation: 0.954768
10/01 03:43:45 AM: edges-ner-ontonotes_recall: training: 0.880963 validation: 0.918714
10/01 03:43:45 AM: edges-ner-ontonotes_f1: training: 0.903965 validation: 0.936394
10/01 03:43:45 AM: Global learning rate: 5e-05
10/01 03:43:45 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:43:54 AM: Update 27100: task edges-ner-ontonotes, batch 100 (27100): mcc: 0.9142, acc: 0.8740, precision: 0.9371, recall: 0.9009, f1: 0.9186, edges-ner-ontonotes_loss: 0.0268
10/01 03:44:04 AM: Update 27239: task edges-ner-ontonotes, batch 239 (27239): mcc: 0.9221, acc: 0.8837, precision: 0.9428, recall: 0.9102, f1: 0.9262, edges-ner-ontonotes_loss: 0.0241
10/01 03:44:14 AM: Update 27378: task edges-ner-ontonotes, batch 378 (27378): mcc: 0.9245, acc: 0.8863, precision: 0.9452, recall: 0.9123, f1: 0.9285, edges-ner-ontonotes_loss: 0.0233
10/01 03:44:24 AM: Update 27502: task edges-ner-ontonotes, batch 502 (27502): mcc: 0.9250, acc: 0.8866, precision: 0.9458, recall: 0.9127, f1: 0.9290, edges-ner-ontonotes_loss: 0.0232
10/01 03:44:34 AM: Update 27640: task edges-ner-ontonotes, batch 640 (27640): mcc: 0.9246, acc: 0.8859, precision: 0.9456, recall: 0.9122, f1: 0.9286, edges-ner-ontonotes_loss: 0.0232
10/01 03:44:44 AM: Update 27754: task edges-ner-ontonotes, batch 754 (27754): mcc: 0.9223, acc: 0.8826, precision: 0.9440, recall: 0.9094, f1: 0.9264, edges-ner-ontonotes_loss: 0.0241
10/01 03:44:54 AM: Update 27902: task edges-ner-ontonotes, batch 902 (27902): mcc: 0.9164, acc: 0.8746, precision: 0.9404, recall: 0.9018, f1: 0.9207, edges-ner-ontonotes_loss: 0.0269
10/01 03:45:01 AM: ***** Step 28000 / Validation 28 *****
10/01 03:45:01 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:45:01 AM: Validating...
10/01 03:45:04 AM: Evaluate: task edges-ner-ontonotes, batch 30 (157): mcc: 0.8878, acc: 0.8496, precision: 0.9116, recall: 0.8765, f1: 0.8937, edges-ner-ontonotes_loss: 0.0367
10/01 03:45:14 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9230, acc: 0.8901, precision: 0.9440, recall: 0.9106, f1: 0.9270, edges-ner-ontonotes_loss: 0.0268
10/01 03:45:20 AM: Updating LR scheduler:
10/01 03:45:20 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:45:20 AM: 	# validation passes without improvement: 1
10/01 03:45:20 AM: edges-ner-ontonotes_loss: training: 0.028173 validation: 0.023728
10/01 03:45:20 AM: macro_avg: validation: 0.934710
10/01 03:45:20 AM: micro_avg: validation: 0.000000
10/01 03:45:20 AM: edges-ner-ontonotes_mcc: training: 0.913904 validation: 0.931067
10/01 03:45:20 AM: edges-ner-ontonotes_acc: training: 0.871360 validation: 0.900591
10/01 03:45:20 AM: edges-ner-ontonotes_precision: training: 0.938778 validation: 0.949894
10/01 03:45:20 AM: edges-ner-ontonotes_recall: training: 0.898799 validation: 0.920003
10/01 03:45:20 AM: edges-ner-ontonotes_f1: training: 0.918354 validation: 0.934710
10/01 03:45:20 AM: Global learning rate: 5e-05
10/01 03:45:20 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:45:24 AM: Update 28037: task edges-ner-ontonotes, batch 37 (28037): mcc: 0.8857, acc: 0.8366, precision: 0.9225, recall: 0.8622, f1: 0.8913, edges-ner-ontonotes_loss: 0.0365
10/01 03:45:34 AM: Update 28188: task edges-ner-ontonotes, batch 188 (28188): mcc: 0.8871, acc: 0.8353, precision: 0.9190, recall: 0.8682, f1: 0.8928, edges-ner-ontonotes_loss: 0.0357
10/01 03:45:45 AM: Update 28322: task edges-ner-ontonotes, batch 322 (28322): mcc: 0.8918, acc: 0.8409, precision: 0.9240, recall: 0.8720, f1: 0.8973, edges-ner-ontonotes_loss: 0.0342
10/01 03:45:55 AM: Update 28450: task edges-ner-ontonotes, batch 450 (28450): mcc: 0.8979, acc: 0.8488, precision: 0.9282, recall: 0.8791, f1: 0.9030, edges-ner-ontonotes_loss: 0.0323
10/01 03:46:05 AM: Update 28580: task edges-ner-ontonotes, batch 580 (28580): mcc: 0.9012, acc: 0.8537, precision: 0.9302, recall: 0.8834, f1: 0.9062, edges-ner-ontonotes_loss: 0.0313
10/01 03:46:15 AM: Update 28686: task edges-ner-ontonotes, batch 686 (28686): mcc: 0.9049, acc: 0.8588, precision: 0.9328, recall: 0.8878, f1: 0.9097, edges-ner-ontonotes_loss: 0.0302
10/01 03:46:25 AM: Update 28810: task edges-ner-ontonotes, batch 810 (28810): mcc: 0.9084, acc: 0.8634, precision: 0.9345, recall: 0.8927, f1: 0.9131, edges-ner-ontonotes_loss: 0.0291
10/01 03:46:35 AM: Update 28942: task edges-ner-ontonotes, batch 942 (28942): mcc: 0.9114, acc: 0.8674, precision: 0.9365, recall: 0.8964, f1: 0.9160, edges-ner-ontonotes_loss: 0.0281
10/01 03:46:43 AM: ***** Step 29000 / Validation 29 *****
10/01 03:46:43 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:46:43 AM: Validating...
10/01 03:46:45 AM: Evaluate: task edges-ner-ontonotes, batch 29 (157): mcc: 0.8718, acc: 0.8305, precision: 0.9102, recall: 0.8481, f1: 0.8781, edges-ner-ontonotes_loss: 0.0403
10/01 03:46:56 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9229, acc: 0.8911, precision: 0.9458, recall: 0.9087, f1: 0.9269, edges-ner-ontonotes_loss: 0.0274
10/01 03:47:01 AM: Updating LR scheduler:
10/01 03:47:01 AM: 	Best result seen so far for macro_avg: 0.936
10/01 03:47:01 AM: 	# validation passes without improvement: 2
10/01 03:47:01 AM: edges-ner-ontonotes_loss: training: 0.027756 validation: 0.023839
10/01 03:47:01 AM: macro_avg: validation: 0.935342
10/01 03:47:01 AM: micro_avg: validation: 0.000000
10/01 03:47:01 AM: edges-ner-ontonotes_mcc: training: 0.912314 validation: 0.931744
10/01 03:47:01 AM: edges-ner-ontonotes_acc: training: 0.868759 validation: 0.902411
10/01 03:47:01 AM: edges-ner-ontonotes_precision: training: 0.937047 validation: 0.950877
10/01 03:47:01 AM: edges-ner-ontonotes_recall: training: 0.897518 validation: 0.920306
10/01 03:47:01 AM: edges-ner-ontonotes_f1: training: 0.916857 validation: 0.935342
10/01 03:47:01 AM: Global learning rate: 5e-05
10/01 03:47:01 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:47:06 AM: Update 29063: task edges-ner-ontonotes, batch 63 (29063): mcc: 0.9190, acc: 0.8769, precision: 0.9417, recall: 0.9054, f1: 0.9232, edges-ner-ontonotes_loss: 0.0259
10/01 03:47:16 AM: Update 29190: task edges-ner-ontonotes, batch 190 (29190): mcc: 0.9224, acc: 0.8819, precision: 0.9436, recall: 0.9100, f1: 0.9265, edges-ner-ontonotes_loss: 0.0246
10/01 03:47:26 AM: Update 29296: task edges-ner-ontonotes, batch 296 (29296): mcc: 0.9196, acc: 0.8783, precision: 0.9426, recall: 0.9056, f1: 0.9237, edges-ner-ontonotes_loss: 0.0254
10/01 03:47:36 AM: Update 29433: task edges-ner-ontonotes, batch 433 (29433): mcc: 0.9097, acc: 0.8661, precision: 0.9367, recall: 0.8931, f1: 0.9144, edges-ner-ontonotes_loss: 0.0303
10/01 03:47:46 AM: Update 29564: task edges-ner-ontonotes, batch 564 (29564): mcc: 0.9052, acc: 0.8604, precision: 0.9339, recall: 0.8872, f1: 0.9100, edges-ner-ontonotes_loss: 0.0322
10/01 03:47:57 AM: Update 29688: task edges-ner-ontonotes, batch 688 (29688): mcc: 0.9031, acc: 0.8573, precision: 0.9325, recall: 0.8847, f1: 0.9080, edges-ner-ontonotes_loss: 0.0327
10/01 03:48:07 AM: Update 29841: task edges-ner-ontonotes, batch 841 (29841): mcc: 0.9015, acc: 0.8549, precision: 0.9316, recall: 0.8828, f1: 0.9065, edges-ner-ontonotes_loss: 0.0329
10/01 03:48:17 AM: Update 29947: task edges-ner-ontonotes, batch 947 (29947): mcc: 0.9018, acc: 0.8553, precision: 0.9315, recall: 0.8833, f1: 0.9068, edges-ner-ontonotes_loss: 0.0325
10/01 03:48:21 AM: ***** Step 30000 / Validation 30 *****
10/01 03:48:21 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:48:21 AM: Validating...
10/01 03:48:27 AM: Evaluate: task edges-ner-ontonotes, batch 59 (157): mcc: 0.9157, acc: 0.8834, precision: 0.9378, recall: 0.9032, f1: 0.9201, edges-ner-ontonotes_loss: 0.0280
10/01 03:48:37 AM: Evaluate: task edges-ner-ontonotes, batch 136 (157): mcc: 0.9325, acc: 0.9036, precision: 0.9542, recall: 0.9185, f1: 0.9360, edges-ner-ontonotes_loss: 0.0237
10/01 03:48:39 AM: Best result seen so far for edges-ner-ontonotes.
10/01 03:48:39 AM: Best result seen so far for macro.
10/01 03:48:39 AM: Updating LR scheduler:
10/01 03:48:39 AM: 	Best result seen so far for macro_avg: 0.937
10/01 03:48:39 AM: 	# validation passes without improvement: 0
10/01 03:48:39 AM: edges-ner-ontonotes_loss: training: 0.032191 validation: 0.022943
10/01 03:48:39 AM: macro_avg: validation: 0.936740
10/01 03:48:39 AM: micro_avg: validation: 0.000000
10/01 03:48:39 AM: edges-ner-ontonotes_mcc: training: 0.902516 validation: 0.933260
10/01 03:48:39 AM: edges-ner-ontonotes_acc: training: 0.856402 validation: 0.904079
10/01 03:48:39 AM: edges-ner-ontonotes_precision: training: 0.931828 validation: 0.953934
10/01 03:48:39 AM: edges-ner-ontonotes_recall: training: 0.884342 validation: 0.920155
10/01 03:48:39 AM: edges-ner-ontonotes_f1: training: 0.907464 validation: 0.936740
10/01 03:48:39 AM: Global learning rate: 5e-05
10/01 03:48:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:48:47 AM: Update 30105: task edges-ner-ontonotes, batch 105 (30105): mcc: 0.9153, acc: 0.8739, precision: 0.9401, recall: 0.9002, f1: 0.9197, edges-ner-ontonotes_loss: 0.0267
10/01 03:48:57 AM: Update 30204: task edges-ner-ontonotes, batch 204 (30204): mcc: 0.9128, acc: 0.8695, precision: 0.9382, recall: 0.8974, f1: 0.9173, edges-ner-ontonotes_loss: 0.0266
10/01 03:49:07 AM: Update 30331: task edges-ner-ontonotes, batch 331 (30331): mcc: 0.9177, acc: 0.8758, precision: 0.9410, recall: 0.9037, f1: 0.9220, edges-ner-ontonotes_loss: 0.0252
10/01 03:49:17 AM: Update 30457: task edges-ner-ontonotes, batch 457 (30457): mcc: 0.9205, acc: 0.8792, precision: 0.9423, recall: 0.9077, f1: 0.9247, edges-ner-ontonotes_loss: 0.0244
10/01 03:49:27 AM: Update 30566: task edges-ner-ontonotes, batch 566 (30566): mcc: 0.9213, acc: 0.8801, precision: 0.9429, recall: 0.9086, f1: 0.9255, edges-ner-ontonotes_loss: 0.0243
10/01 03:49:37 AM: Update 30691: task edges-ner-ontonotes, batch 691 (30691): mcc: 0.9217, acc: 0.8808, precision: 0.9431, recall: 0.9092, f1: 0.9258, edges-ner-ontonotes_loss: 0.0242
10/01 03:49:49 AM: Update 30817: task edges-ner-ontonotes, batch 817 (30817): mcc: 0.9221, acc: 0.8815, precision: 0.9431, recall: 0.9098, f1: 0.9262, edges-ner-ontonotes_loss: 0.0242
10/01 03:49:59 AM: Update 30941: task edges-ner-ontonotes, batch 941 (30941): mcc: 0.9170, acc: 0.8751, precision: 0.9399, recall: 0.9036, f1: 0.9214, edges-ner-ontonotes_loss: 0.0264
10/01 03:50:03 AM: ***** Step 31000 / Validation 31 *****
10/01 03:50:03 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:50:03 AM: Validating...
10/01 03:50:09 AM: Evaluate: task edges-ner-ontonotes, batch 55 (157): mcc: 0.9097, acc: 0.8744, precision: 0.9303, recall: 0.8993, f1: 0.9145, edges-ner-ontonotes_loss: 0.0304
10/01 03:50:19 AM: Evaluate: task edges-ner-ontonotes, batch 132 (157): mcc: 0.9290, acc: 0.8981, precision: 0.9489, recall: 0.9171, f1: 0.9327, edges-ner-ontonotes_loss: 0.0249
10/01 03:50:21 AM: Updating LR scheduler:
10/01 03:50:21 AM: 	Best result seen so far for macro_avg: 0.937
10/01 03:50:21 AM: 	# validation passes without improvement: 1
10/01 03:50:21 AM: edges-ner-ontonotes_loss: training: 0.026930 validation: 0.023598
10/01 03:50:21 AM: macro_avg: validation: 0.935074
10/01 03:50:21 AM: micro_avg: validation: 0.000000
10/01 03:50:21 AM: edges-ner-ontonotes_mcc: training: 0.915806 validation: 0.931447
10/01 03:50:21 AM: edges-ner-ontonotes_acc: training: 0.873417 validation: 0.901198
10/01 03:50:21 AM: edges-ner-ontonotes_precision: training: 0.939366 validation: 0.950000
10/01 03:50:22 AM: edges-ner-ontonotes_recall: training: 0.901773 validation: 0.920610
10/01 03:50:22 AM: edges-ner-ontonotes_f1: training: 0.920186 validation: 0.935074
10/01 03:50:22 AM: Global learning rate: 5e-05
10/01 03:50:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:50:29 AM: Update 31099: task edges-ner-ontonotes, batch 99 (31099): mcc: 0.8833, acc: 0.8307, precision: 0.9178, recall: 0.8622, f1: 0.8891, edges-ner-ontonotes_loss: 0.0400
10/01 03:50:39 AM: Update 31217: task edges-ner-ontonotes, batch 217 (31217): mcc: 0.8843, acc: 0.8330, precision: 0.9189, recall: 0.8631, f1: 0.8901, edges-ner-ontonotes_loss: 0.0381
10/01 03:50:49 AM: Update 31370: task edges-ner-ontonotes, batch 370 (31370): mcc: 0.8894, acc: 0.8378, precision: 0.9221, recall: 0.8694, f1: 0.8949, edges-ner-ontonotes_loss: 0.0359
10/01 03:50:59 AM: Update 31484: task edges-ner-ontonotes, batch 484 (31484): mcc: 0.8933, acc: 0.8429, precision: 0.9250, recall: 0.8737, f1: 0.8986, edges-ner-ontonotes_loss: 0.0344
10/01 03:51:09 AM: Update 31617: task edges-ner-ontonotes, batch 617 (31617): mcc: 0.8980, acc: 0.8498, precision: 0.9281, recall: 0.8796, f1: 0.9032, edges-ner-ontonotes_loss: 0.0327
10/01 03:51:21 AM: Update 31747: task edges-ner-ontonotes, batch 747 (31747): mcc: 0.9003, acc: 0.8529, precision: 0.9288, recall: 0.8831, f1: 0.9054, edges-ner-ontonotes_loss: 0.0319
10/01 03:51:31 AM: Update 31873: task edges-ner-ontonotes, batch 873 (31873): mcc: 0.9047, acc: 0.8589, precision: 0.9319, recall: 0.8884, f1: 0.9096, edges-ner-ontonotes_loss: 0.0306
10/01 03:51:41 AM: Update 32000: task edges-ner-ontonotes, batch 1000 (32000): mcc: 0.9082, acc: 0.8634, precision: 0.9343, recall: 0.8924, f1: 0.9129, edges-ner-ontonotes_loss: 0.0295
10/01 03:51:41 AM: ***** Step 32000 / Validation 32 *****
10/01 03:51:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:51:41 AM: Validating...
10/01 03:51:51 AM: Evaluate: task edges-ner-ontonotes, batch 91 (157): mcc: 0.9174, acc: 0.8822, precision: 0.9450, recall: 0.8992, f1: 0.9216, edges-ner-ontonotes_loss: 0.0289
10/01 03:51:59 AM: Updating LR scheduler:
10/01 03:51:59 AM: 	Best result seen so far for macro_avg: 0.937
10/01 03:51:59 AM: 	# validation passes without improvement: 2
10/01 03:51:59 AM: edges-ner-ontonotes_loss: training: 0.029480 validation: 0.023592
10/01 03:51:59 AM: macro_avg: validation: 0.934682
10/01 03:51:59 AM: micro_avg: validation: 0.000000
10/01 03:51:59 AM: edges-ner-ontonotes_mcc: training: 0.908162 validation: 0.931087
10/01 03:51:59 AM: edges-ner-ontonotes_acc: training: 0.863447 validation: 0.900971
10/01 03:51:59 AM: edges-ner-ontonotes_precision: training: 0.934313 validation: 0.952029
10/01 03:51:59 AM: edges-ner-ontonotes_recall: training: 0.892437 validation: 0.917956
10/01 03:51:59 AM: edges-ner-ontonotes_f1: training: 0.912895 validation: 0.934682
10/01 03:51:59 AM: Global learning rate: 5e-05
10/01 03:51:59 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:52:01 AM: Update 32017: task edges-ner-ontonotes, batch 17 (32017): mcc: 0.9375, acc: 0.9018, precision: 0.9577, recall: 0.9243, f1: 0.9407, edges-ner-ontonotes_loss: 0.0205
10/01 03:52:11 AM: Update 32121: task edges-ner-ontonotes, batch 121 (32121): mcc: 0.9231, acc: 0.8854, precision: 0.9443, recall: 0.9106, f1: 0.9272, edges-ner-ontonotes_loss: 0.0242
10/01 03:52:21 AM: Update 32248: task edges-ner-ontonotes, batch 248 (32248): mcc: 0.9248, acc: 0.8864, precision: 0.9456, recall: 0.9126, f1: 0.9288, edges-ner-ontonotes_loss: 0.0234
10/01 03:52:32 AM: Update 32373: task edges-ner-ontonotes, batch 373 (32373): mcc: 0.9245, acc: 0.8853, precision: 0.9455, recall: 0.9120, f1: 0.9285, edges-ner-ontonotes_loss: 0.0236
10/01 03:52:42 AM: Update 32501: task edges-ner-ontonotes, batch 501 (32501): mcc: 0.9150, acc: 0.8731, precision: 0.9389, recall: 0.9008, f1: 0.9195, edges-ner-ontonotes_loss: 0.0277
10/01 03:52:52 AM: Update 32635: task edges-ner-ontonotes, batch 635 (32635): mcc: 0.9100, acc: 0.8669, precision: 0.9358, recall: 0.8945, f1: 0.9146, edges-ner-ontonotes_loss: 0.0302
10/01 03:53:02 AM: Update 32760: task edges-ner-ontonotes, batch 760 (32760): mcc: 0.9070, acc: 0.8630, precision: 0.9336, recall: 0.8909, f1: 0.9118, edges-ner-ontonotes_loss: 0.0310
10/01 03:53:12 AM: Update 32914: task edges-ner-ontonotes, batch 914 (32914): mcc: 0.9053, acc: 0.8606, precision: 0.9325, recall: 0.8889, f1: 0.9102, edges-ner-ontonotes_loss: 0.0313
10/01 03:53:20 AM: ***** Step 33000 / Validation 33 *****
10/01 03:53:20 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:53:20 AM: Validating...
10/01 03:53:22 AM: Evaluate: task edges-ner-ontonotes, batch 22 (157): mcc: 0.8886, acc: 0.8430, precision: 0.9234, recall: 0.8667, f1: 0.8942, edges-ner-ontonotes_loss: 0.0330
10/01 03:53:32 AM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.9230, acc: 0.8892, precision: 0.9485, recall: 0.9063, f1: 0.9269, edges-ner-ontonotes_loss: 0.0265
10/01 03:53:39 AM: Updating LR scheduler:
10/01 03:53:39 AM: 	Best result seen so far for macro_avg: 0.937
10/01 03:53:39 AM: 	# validation passes without improvement: 3
10/01 03:53:39 AM: edges-ner-ontonotes_loss: training: 0.031416 validation: 0.023134
10/01 03:53:39 AM: macro_avg: validation: 0.935998
10/01 03:53:39 AM: micro_avg: validation: 0.000000
10/01 03:53:39 AM: edges-ner-ontonotes_mcc: training: 0.904834 validation: 0.932506
10/01 03:53:39 AM: edges-ner-ontonotes_acc: training: 0.859724 validation: 0.901729
10/01 03:53:39 AM: edges-ner-ontonotes_precision: training: 0.932395 validation: 0.954517
10/01 03:53:39 AM: edges-ner-ontonotes_recall: training: 0.888102 validation: 0.918183
10/01 03:53:39 AM: edges-ner-ontonotes_f1: training: 0.909710 validation: 0.935998
10/01 03:53:39 AM: Global learning rate: 5e-05
10/01 03:53:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:53:42 AM: Update 33047: task edges-ner-ontonotes, batch 47 (33047): mcc: 0.9105, acc: 0.8644, precision: 0.9331, recall: 0.8980, f1: 0.9152, edges-ner-ontonotes_loss: 0.0287
10/01 03:53:53 AM: Update 33175: task edges-ner-ontonotes, batch 175 (33175): mcc: 0.9133, acc: 0.8703, precision: 0.9373, recall: 0.8991, f1: 0.9178, edges-ner-ontonotes_loss: 0.0271
10/01 03:54:05 AM: Update 33303: task edges-ner-ontonotes, batch 303 (33303): mcc: 0.9130, acc: 0.8708, precision: 0.9372, recall: 0.8987, f1: 0.9176, edges-ner-ontonotes_loss: 0.0269
10/01 03:54:15 AM: Update 33431: task edges-ner-ontonotes, batch 431 (33431): mcc: 0.9167, acc: 0.8757, precision: 0.9400, recall: 0.9029, f1: 0.9210, edges-ner-ontonotes_loss: 0.0257
10/01 03:54:25 AM: Update 33558: task edges-ner-ontonotes, batch 558 (33558): mcc: 0.9198, acc: 0.8795, precision: 0.9419, recall: 0.9068, f1: 0.9240, edges-ner-ontonotes_loss: 0.0249
10/01 03:54:35 AM: Update 33668: task edges-ner-ontonotes, batch 668 (33668): mcc: 0.9206, acc: 0.8803, precision: 0.9424, recall: 0.9079, f1: 0.9248, edges-ner-ontonotes_loss: 0.0247
10/01 03:54:45 AM: Update 33791: task edges-ner-ontonotes, batch 791 (33791): mcc: 0.9214, acc: 0.8814, precision: 0.9430, recall: 0.9087, f1: 0.9255, edges-ner-ontonotes_loss: 0.0244
10/01 03:54:55 AM: Update 33917: task edges-ner-ontonotes, batch 917 (33917): mcc: 0.9217, acc: 0.8815, precision: 0.9434, recall: 0.9089, f1: 0.9258, edges-ner-ontonotes_loss: 0.0243
10/01 03:55:03 AM: ***** Step 34000 / Validation 34 *****
10/01 03:55:03 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:55:03 AM: Validating...
10/01 03:55:05 AM: Evaluate: task edges-ner-ontonotes, batch 19 (157): mcc: 0.8793, acc: 0.8288, precision: 0.9093, recall: 0.8629, f1: 0.8855, edges-ner-ontonotes_loss: 0.0342
10/01 03:55:15 AM: Evaluate: task edges-ner-ontonotes, batch 106 (157): mcc: 0.9192, acc: 0.8842, precision: 0.9445, recall: 0.9031, f1: 0.9233, edges-ner-ontonotes_loss: 0.0277
10/01 03:55:22 AM: Updating LR scheduler:
10/01 03:55:22 AM: 	Best result seen so far for macro_avg: 0.937
10/01 03:55:22 AM: 	# validation passes without improvement: 0
10/01 03:55:22 AM: edges-ner-ontonotes_loss: training: 0.025466 validation: 0.023718
10/01 03:55:22 AM: macro_avg: validation: 0.933787
10/01 03:55:22 AM: micro_avg: validation: 0.000000
10/01 03:55:22 AM: edges-ner-ontonotes_mcc: training: 0.918766 validation: 0.930171
10/01 03:55:22 AM: edges-ner-ontonotes_acc: training: 0.877611 validation: 0.898923
10/01 03:55:22 AM: edges-ner-ontonotes_precision: training: 0.941408 validation: 0.952377
10/01 03:55:22 AM: edges-ner-ontonotes_recall: training: 0.905308 validation: 0.915908
10/01 03:55:22 AM: edges-ner-ontonotes_f1: training: 0.923005 validation: 0.933787
10/01 03:55:22 AM: Global learning rate: 2.5e-05
10/01 03:55:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:55:25 AM: Update 34038: task edges-ner-ontonotes, batch 38 (34038): mcc: 0.8752, acc: 0.8190, precision: 0.9154, recall: 0.8495, f1: 0.8812, edges-ner-ontonotes_loss: 0.0415
10/01 03:55:35 AM: Update 34177: task edges-ner-ontonotes, batch 177 (34177): mcc: 0.8831, acc: 0.8333, precision: 0.9180, recall: 0.8616, f1: 0.8889, edges-ner-ontonotes_loss: 0.0398
10/01 03:55:45 AM: Update 34289: task edges-ner-ontonotes, batch 289 (34289): mcc: 0.8859, acc: 0.8354, precision: 0.9199, recall: 0.8649, f1: 0.8916, edges-ner-ontonotes_loss: 0.0381
10/01 03:55:55 AM: Update 34443: task edges-ner-ontonotes, batch 443 (34443): mcc: 0.8877, acc: 0.8371, precision: 0.9208, recall: 0.8674, f1: 0.8933, edges-ner-ontonotes_loss: 0.0367
10/01 03:56:05 AM: Update 34562: task edges-ner-ontonotes, batch 562 (34562): mcc: 0.8896, acc: 0.8391, precision: 0.9222, recall: 0.8696, f1: 0.8951, edges-ner-ontonotes_loss: 0.0359
10/01 03:56:15 AM: Update 34689: task edges-ner-ontonotes, batch 689 (34689): mcc: 0.8942, acc: 0.8454, precision: 0.9253, recall: 0.8752, f1: 0.8996, edges-ner-ontonotes_loss: 0.0344
10/01 03:56:25 AM: Update 34819: task edges-ner-ontonotes, batch 819 (34819): mcc: 0.8978, acc: 0.8502, precision: 0.9276, recall: 0.8795, f1: 0.9029, edges-ner-ontonotes_loss: 0.0331
10/01 03:56:35 AM: Update 34931: task edges-ner-ontonotes, batch 931 (34931): mcc: 0.9010, acc: 0.8547, precision: 0.9295, recall: 0.8838, f1: 0.9061, edges-ner-ontonotes_loss: 0.0320
10/01 03:56:41 AM: ***** Step 35000 / Validation 35 *****
10/01 03:56:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:56:41 AM: Validating...
10/01 03:56:46 AM: Evaluate: task edges-ner-ontonotes, batch 46 (157): mcc: 0.8998, acc: 0.8639, precision: 0.9240, recall: 0.8869, f1: 0.9051, edges-ner-ontonotes_loss: 0.0328
10/01 03:56:56 AM: Evaluate: task edges-ner-ontonotes, batch 122 (157): mcc: 0.9273, acc: 0.8970, precision: 0.9489, recall: 0.9140, f1: 0.9311, edges-ner-ontonotes_loss: 0.0255
10/01 03:57:00 AM: Updating LR scheduler:
10/01 03:57:00 AM: 	Best result seen so far for macro_avg: 0.937
10/01 03:57:00 AM: 	# validation passes without improvement: 1
10/01 03:57:00 AM: edges-ner-ontonotes_loss: training: 0.031308 validation: 0.023187
10/01 03:57:00 AM: macro_avg: validation: 0.936257
10/01 03:57:00 AM: micro_avg: validation: 0.000000
10/01 03:57:00 AM: edges-ner-ontonotes_mcc: training: 0.903107 validation: 0.932715
10/01 03:57:00 AM: edges-ner-ontonotes_acc: training: 0.857553 validation: 0.903776
10/01 03:57:00 AM: edges-ner-ontonotes_precision: training: 0.930867 validation: 0.951959
10/01 03:57:00 AM: edges-ner-ontonotes_recall: training: 0.886366 validation: 0.921065
10/01 03:57:00 AM: edges-ner-ontonotes_f1: training: 0.908072 validation: 0.936257
10/01 03:57:00 AM: Global learning rate: 2.5e-05
10/01 03:57:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:57:06 AM: Update 35076: task edges-ner-ontonotes, batch 76 (35076): mcc: 0.9278, acc: 0.8905, precision: 0.9469, recall: 0.9168, f1: 0.9316, edges-ner-ontonotes_loss: 0.0221
10/01 03:57:16 AM: Update 35173: task edges-ner-ontonotes, batch 173 (35173): mcc: 0.9293, acc: 0.8918, precision: 0.9479, recall: 0.9187, f1: 0.9331, edges-ner-ontonotes_loss: 0.0213
10/01 03:57:26 AM: Update 35294: task edges-ner-ontonotes, batch 294 (35294): mcc: 0.9278, acc: 0.8896, precision: 0.9470, recall: 0.9168, f1: 0.9316, edges-ner-ontonotes_loss: 0.0221
10/01 03:57:36 AM: Update 35424: task edges-ner-ontonotes, batch 424 (35424): mcc: 0.9278, acc: 0.8895, precision: 0.9471, recall: 0.9167, f1: 0.9317, edges-ner-ontonotes_loss: 0.0222
10/01 03:57:46 AM: Update 35535: task edges-ner-ontonotes, batch 535 (35535): mcc: 0.9234, acc: 0.8837, precision: 0.9444, recall: 0.9110, f1: 0.9274, edges-ner-ontonotes_loss: 0.0241
10/01 03:57:56 AM: Update 35660: task edges-ner-ontonotes, batch 660 (35660): mcc: 0.9165, acc: 0.8751, precision: 0.9400, recall: 0.9025, f1: 0.9209, edges-ner-ontonotes_loss: 0.0270
10/01 03:58:07 AM: Update 35789: task edges-ner-ontonotes, batch 789 (35789): mcc: 0.9121, acc: 0.8695, precision: 0.9375, recall: 0.8967, f1: 0.9166, edges-ner-ontonotes_loss: 0.0291
10/01 03:58:17 AM: Update 35940: task edges-ner-ontonotes, batch 940 (35940): mcc: 0.9094, acc: 0.8658, precision: 0.9353, recall: 0.8937, f1: 0.9140, edges-ner-ontonotes_loss: 0.0299
10/01 03:58:20 AM: ***** Step 36000 / Validation 36 *****
10/01 03:58:20 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 03:58:20 AM: Validating...
10/01 03:58:27 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9168, acc: 0.8836, precision: 0.9381, recall: 0.9050, f1: 0.9212, edges-ner-ontonotes_loss: 0.0277
10/01 03:58:37 AM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.9319, acc: 0.9018, precision: 0.9537, recall: 0.9179, f1: 0.9355, edges-ner-ontonotes_loss: 0.0236
10/01 03:58:39 AM: Updating LR scheduler:
10/01 03:58:39 AM: 	Best result seen so far for macro_avg: 0.937
10/01 03:58:39 AM: 	# validation passes without improvement: 2
10/01 03:58:39 AM: edges-ner-ontonotes_loss: training: 0.030109 validation: 0.022912
10/01 03:58:39 AM: macro_avg: validation: 0.936170
10/01 03:58:39 AM: micro_avg: validation: 0.000000
10/01 03:58:39 AM: edges-ner-ontonotes_mcc: training: 0.908870 validation: 0.932667
10/01 03:58:39 AM: edges-ner-ontonotes_acc: training: 0.865044 validation: 0.902108
10/01 03:58:39 AM: edges-ner-ontonotes_precision: training: 0.935074 validation: 0.953812
10/01 03:58:39 AM: edges-ner-ontonotes_recall: training: 0.893017 validation: 0.919169
10/01 03:58:39 AM: edges-ner-ontonotes_f1: training: 0.913562 validation: 0.936170
10/01 03:58:39 AM: Global learning rate: 2.5e-05
10/01 03:58:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 03:58:48 AM: Update 36102: task edges-ner-ontonotes, batch 102 (36102): mcc: 0.8952, acc: 0.8438, precision: 0.9272, recall: 0.8753, f1: 0.9005, edges-ner-ontonotes_loss: 0.0330
10/01 03:58:58 AM: Update 36232: task edges-ner-ontonotes, batch 232 (36232): mcc: 0.9065, acc: 0.8602, precision: 0.9336, recall: 0.8900, f1: 0.9113, edges-ner-ontonotes_loss: 0.0294
10/01 03:59:08 AM: Update 36361: task edges-ner-ontonotes, batch 361 (36361): mcc: 0.9094, acc: 0.8655, precision: 0.9353, recall: 0.8938, f1: 0.9141, edges-ner-ontonotes_loss: 0.0285
10/01 03:59:18 AM: Update 36471: task edges-ner-ontonotes, batch 471 (36471): mcc: 0.9109, acc: 0.8679, precision: 0.9360, recall: 0.8959, f1: 0.9155, edges-ner-ontonotes_loss: 0.0279
10/01 03:59:28 AM: Update 36592: task edges-ner-ontonotes, batch 592 (36592): mcc: 0.9155, acc: 0.8739, precision: 0.9392, recall: 0.9013, f1: 0.9199, edges-ner-ontonotes_loss: 0.0266
10/01 03:59:38 AM: Update 36723: task edges-ner-ontonotes, batch 723 (36723): mcc: 0.9177, acc: 0.8769, precision: 0.9404, recall: 0.9044, f1: 0.9220, edges-ner-ontonotes_loss: 0.0258
10/01 03:59:49 AM: Update 36828: task edges-ner-ontonotes, batch 828 (36828): mcc: 0.9184, acc: 0.8776, precision: 0.9409, recall: 0.9052, f1: 0.9227, edges-ner-ontonotes_loss: 0.0255
10/01 03:59:59 AM: Update 36951: task edges-ner-ontonotes, batch 951 (36951): mcc: 0.9187, acc: 0.8776, precision: 0.9409, recall: 0.9057, f1: 0.9230, edges-ner-ontonotes_loss: 0.0253
10/01 04:00:03 AM: ***** Step 37000 / Validation 37 *****
10/01 04:00:03 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:00:03 AM: Validating...
10/01 04:00:09 AM: Evaluate: task edges-ner-ontonotes, batch 59 (157): mcc: 0.9119, acc: 0.8792, precision: 0.9339, recall: 0.8999, f1: 0.9166, edges-ner-ontonotes_loss: 0.0300
10/01 04:00:19 AM: Evaluate: task edges-ner-ontonotes, batch 136 (157): mcc: 0.9311, acc: 0.9026, precision: 0.9507, recall: 0.9193, f1: 0.9348, edges-ner-ontonotes_loss: 0.0244
10/01 04:00:21 AM: Updating LR scheduler:
10/01 04:00:21 AM: 	Best result seen so far for macro_avg: 0.937
10/01 04:00:21 AM: 	# validation passes without improvement: 3
10/01 04:00:21 AM: edges-ner-ontonotes_loss: training: 0.025317 validation: 0.023437
10/01 04:00:21 AM: macro_avg: validation: 0.935865
10/01 04:00:21 AM: micro_avg: validation: 0.000000
10/01 04:00:21 AM: edges-ner-ontonotes_mcc: training: 0.918959 validation: 0.932290
10/01 04:00:21 AM: edges-ner-ontonotes_acc: training: 0.877865 validation: 0.903625
10/01 04:00:21 AM: edges-ner-ontonotes_precision: training: 0.941139 validation: 0.951069
10/01 04:00:21 AM: edges-ner-ontonotes_recall: training: 0.905930 validation: 0.921140
10/01 04:00:21 AM: edges-ner-ontonotes_f1: training: 0.923199 validation: 0.935865
10/01 04:00:21 AM: Global learning rate: 2.5e-05
10/01 04:00:21 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:00:29 AM: Update 37076: task edges-ner-ontonotes, batch 76 (37076): mcc: 0.9066, acc: 0.8638, precision: 0.9352, recall: 0.8886, f1: 0.9113, edges-ner-ontonotes_loss: 0.0319
10/01 04:00:39 AM: Update 37204: task edges-ner-ontonotes, batch 204 (37204): mcc: 0.8944, acc: 0.8476, precision: 0.9278, recall: 0.8731, f1: 0.8996, edges-ner-ontonotes_loss: 0.0358
10/01 04:00:49 AM: Update 37336: task edges-ner-ontonotes, batch 336 (37336): mcc: 0.8916, acc: 0.8436, precision: 0.9247, recall: 0.8710, f1: 0.8970, edges-ner-ontonotes_loss: 0.0366
10/01 04:00:59 AM: Update 37471: task edges-ner-ontonotes, batch 471 (37471): mcc: 0.8908, acc: 0.8419, precision: 0.9239, recall: 0.8702, f1: 0.8963, edges-ner-ontonotes_loss: 0.0362
10/01 04:01:09 AM: Update 37614: task edges-ner-ontonotes, batch 614 (37614): mcc: 0.8922, acc: 0.8436, precision: 0.9247, recall: 0.8720, f1: 0.8976, edges-ner-ontonotes_loss: 0.0354
10/01 04:01:19 AM: Update 37726: task edges-ner-ontonotes, batch 726 (37726): mcc: 0.8942, acc: 0.8458, precision: 0.9262, recall: 0.8744, f1: 0.8996, edges-ner-ontonotes_loss: 0.0346
10/01 04:01:29 AM: Update 37858: task edges-ner-ontonotes, batch 858 (37858): mcc: 0.8979, acc: 0.8511, precision: 0.9283, recall: 0.8791, f1: 0.9030, edges-ner-ontonotes_loss: 0.0334
10/01 04:01:39 AM: Update 37971: task edges-ner-ontonotes, batch 971 (37971): mcc: 0.9000, acc: 0.8539, precision: 0.9295, recall: 0.8819, f1: 0.9051, edges-ner-ontonotes_loss: 0.0326
10/01 04:01:42 AM: ***** Step 38000 / Validation 38 *****
10/01 04:01:42 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:01:42 AM: Validating...
10/01 04:01:50 AM: Evaluate: task edges-ner-ontonotes, batch 70 (157): mcc: 0.9152, acc: 0.8799, precision: 0.9388, recall: 0.9013, f1: 0.9196, edges-ner-ontonotes_loss: 0.0293
10/01 04:02:00 AM: Evaluate: task edges-ner-ontonotes, batch 149 (157): mcc: 0.9335, acc: 0.9048, precision: 0.9526, recall: 0.9219, f1: 0.9370, edges-ner-ontonotes_loss: 0.0232
10/01 04:02:00 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:02:00 AM: Best result seen so far for macro.
10/01 04:02:00 AM: Updating LR scheduler:
10/01 04:02:00 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:02:00 AM: 	# validation passes without improvement: 0
10/01 04:02:00 AM: edges-ner-ontonotes_loss: training: 0.032287 validation: 0.022789
10/01 04:02:00 AM: macro_avg: validation: 0.937806
10/01 04:02:00 AM: micro_avg: validation: 0.000000
10/01 04:02:00 AM: edges-ner-ontonotes_mcc: training: 0.900732 validation: 0.934338
10/01 04:02:00 AM: edges-ner-ontonotes_acc: training: 0.854851 validation: 0.905672
10/01 04:02:00 AM: edges-ner-ontonotes_precision: training: 0.930022 validation: 0.952813
10/01 04:02:00 AM: edges-ner-ontonotes_recall: training: 0.882768 validation: 0.923264
10/01 04:02:00 AM: edges-ner-ontonotes_f1: training: 0.905779 validation: 0.937806
10/01 04:02:00 AM: Global learning rate: 2.5e-05
10/01 04:02:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:02:10 AM: Update 38116: task edges-ner-ontonotes, batch 116 (38116): mcc: 0.9276, acc: 0.8908, precision: 0.9470, recall: 0.9164, f1: 0.9314, edges-ner-ontonotes_loss: 0.0223
10/01 04:02:20 AM: Update 38237: task edges-ner-ontonotes, batch 237 (38237): mcc: 0.9286, acc: 0.8910, precision: 0.9473, recall: 0.9178, f1: 0.9323, edges-ner-ontonotes_loss: 0.0220
10/01 04:02:30 AM: Update 38341: task edges-ner-ontonotes, batch 341 (38341): mcc: 0.9276, acc: 0.8892, precision: 0.9471, recall: 0.9162, f1: 0.9314, edges-ner-ontonotes_loss: 0.0221
10/01 04:02:40 AM: Update 38470: task edges-ner-ontonotes, batch 470 (38470): mcc: 0.9262, acc: 0.8872, precision: 0.9458, recall: 0.9149, f1: 0.9301, edges-ner-ontonotes_loss: 0.0225
10/01 04:02:50 AM: Update 38593: task edges-ner-ontonotes, batch 593 (38593): mcc: 0.9262, acc: 0.8871, precision: 0.9462, recall: 0.9145, f1: 0.9301, edges-ner-ontonotes_loss: 0.0227
10/01 04:03:00 AM: Update 38701: task edges-ner-ontonotes, batch 701 (38701): mcc: 0.9201, acc: 0.8794, precision: 0.9427, recall: 0.9066, f1: 0.9243, edges-ner-ontonotes_loss: 0.0254
10/01 04:03:10 AM: Update 38836: task edges-ner-ontonotes, batch 836 (38836): mcc: 0.9153, acc: 0.8736, precision: 0.9397, recall: 0.9005, f1: 0.9197, edges-ner-ontonotes_loss: 0.0275
10/01 04:03:20 AM: Update 38956: task edges-ner-ontonotes, batch 956 (38956): mcc: 0.9128, acc: 0.8701, precision: 0.9379, recall: 0.8975, f1: 0.9173, edges-ner-ontonotes_loss: 0.0286
10/01 04:03:23 AM: ***** Step 39000 / Validation 39 *****
10/01 04:03:23 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:03:23 AM: Validating...
10/01 04:03:30 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9178, acc: 0.8859, precision: 0.9377, recall: 0.9070, f1: 0.9221, edges-ner-ontonotes_loss: 0.0280
10/01 04:03:40 AM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.9321, acc: 0.9026, precision: 0.9523, recall: 0.9195, f1: 0.9356, edges-ner-ontonotes_loss: 0.0237
10/01 04:03:42 AM: Updating LR scheduler:
10/01 04:03:42 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:03:42 AM: 	# validation passes without improvement: 1
10/01 04:03:42 AM: edges-ner-ontonotes_loss: training: 0.028916 validation: 0.023005
10/01 04:03:42 AM: macro_avg: validation: 0.936403
10/01 04:03:42 AM: micro_avg: validation: 0.000000
10/01 04:03:42 AM: edges-ner-ontonotes_mcc: training: 0.911476 validation: 0.932886
10/01 04:03:42 AM: edges-ner-ontonotes_acc: training: 0.868353 validation: 0.903018
10/01 04:03:42 AM: edges-ner-ontonotes_precision: training: 0.937117 validation: 0.952829
10/01 04:03:42 AM: edges-ner-ontonotes_recall: training: 0.895891 validation: 0.920534
10/01 04:03:42 AM: edges-ner-ontonotes_f1: training: 0.916040 validation: 0.936403
10/01 04:03:42 AM: Global learning rate: 2.5e-05
10/01 04:03:42 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:03:50 AM: Update 39127: task edges-ner-ontonotes, batch 127 (39127): mcc: 0.8953, acc: 0.8453, precision: 0.9262, recall: 0.8765, f1: 0.9006, edges-ner-ontonotes_loss: 0.0330
10/01 04:04:00 AM: Update 39247: task edges-ner-ontonotes, batch 247 (39247): mcc: 0.8973, acc: 0.8485, precision: 0.9284, recall: 0.8781, f1: 0.9025, edges-ner-ontonotes_loss: 0.0320
10/01 04:04:10 AM: Update 39376: task edges-ner-ontonotes, batch 376 (39376): mcc: 0.9019, acc: 0.8553, precision: 0.9305, recall: 0.8845, f1: 0.9069, edges-ner-ontonotes_loss: 0.0305
10/01 04:04:20 AM: Update 39504: task edges-ner-ontonotes, batch 504 (39504): mcc: 0.9064, acc: 0.8619, precision: 0.9330, recall: 0.8905, f1: 0.9112, edges-ner-ontonotes_loss: 0.0292
10/01 04:04:30 AM: Update 39609: task edges-ner-ontonotes, batch 609 (39609): mcc: 0.9095, acc: 0.8662, precision: 0.9352, recall: 0.8941, f1: 0.9142, edges-ner-ontonotes_loss: 0.0282
10/01 04:04:41 AM: Update 39733: task edges-ner-ontonotes, batch 733 (39733): mcc: 0.9135, acc: 0.8713, precision: 0.9375, recall: 0.8993, f1: 0.9180, edges-ner-ontonotes_loss: 0.0271
10/01 04:04:51 AM: Update 39842: task edges-ner-ontonotes, batch 842 (39842): mcc: 0.9154, acc: 0.8736, precision: 0.9388, recall: 0.9015, f1: 0.9198, edges-ner-ontonotes_loss: 0.0265
10/01 04:05:01 AM: Update 39974: task edges-ner-ontonotes, batch 974 (39974): mcc: 0.9168, acc: 0.8756, precision: 0.9397, recall: 0.9033, f1: 0.9211, edges-ner-ontonotes_loss: 0.0261
10/01 04:05:03 AM: ***** Step 40000 / Validation 40 *****
10/01 04:05:03 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:05:03 AM: Validating...
10/01 04:05:11 AM: Evaluate: task edges-ner-ontonotes, batch 72 (157): mcc: 0.9112, acc: 0.8740, precision: 0.9356, recall: 0.8969, f1: 0.9158, edges-ner-ontonotes_loss: 0.0308
10/01 04:05:21 AM: Evaluate: task edges-ner-ontonotes, batch 152 (157): mcc: 0.9316, acc: 0.9022, precision: 0.9502, recall: 0.9208, f1: 0.9353, edges-ner-ontonotes_loss: 0.0237
10/01 04:05:21 AM: Updating LR scheduler:
10/01 04:05:21 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:05:21 AM: 	# validation passes without improvement: 2
10/01 04:05:21 AM: edges-ner-ontonotes_loss: training: 0.026034 validation: 0.023384
10/01 04:05:21 AM: macro_avg: validation: 0.935951
10/01 04:05:21 AM: micro_avg: validation: 0.000000
10/01 04:05:21 AM: edges-ner-ontonotes_mcc: training: 0.916962 validation: 0.932365
10/01 04:05:21 AM: edges-ner-ontonotes_acc: training: 0.875786 validation: 0.903018
10/01 04:05:21 AM: edges-ner-ontonotes_precision: training: 0.939920 validation: 0.950438
10/01 04:05:21 AM: edges-ner-ontonotes_recall: training: 0.903392 validation: 0.921899
10/01 04:05:21 AM: edges-ner-ontonotes_f1: training: 0.921294 validation: 0.935951
10/01 04:05:21 AM: Global learning rate: 2.5e-05
10/01 04:05:21 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:05:31 AM: Update 40122: task edges-ner-ontonotes, batch 122 (40122): mcc: 0.9283, acc: 0.8877, precision: 0.9490, recall: 0.9158, f1: 0.9321, edges-ner-ontonotes_loss: 0.0225
10/01 04:05:41 AM: Update 40227: task edges-ner-ontonotes, batch 227 (40227): mcc: 0.9123, acc: 0.8689, precision: 0.9376, recall: 0.8970, f1: 0.9169, edges-ner-ontonotes_loss: 0.0285
10/01 04:05:51 AM: Update 40359: task edges-ner-ontonotes, batch 359 (40359): mcc: 0.9037, acc: 0.8587, precision: 0.9316, recall: 0.8867, f1: 0.9086, edges-ner-ontonotes_loss: 0.0324
10/01 04:06:01 AM: Update 40465: task edges-ner-ontonotes, batch 465 (40465): mcc: 0.8993, acc: 0.8530, precision: 0.9286, recall: 0.8816, f1: 0.9045, edges-ner-ontonotes_loss: 0.0336
10/01 04:06:11 AM: Update 40615: task edges-ner-ontonotes, batch 615 (40615): mcc: 0.8985, acc: 0.8517, precision: 0.9282, recall: 0.8803, f1: 0.9036, edges-ner-ontonotes_loss: 0.0336
10/01 04:06:23 AM: Update 40770: task edges-ner-ontonotes, batch 770 (40770): mcc: 0.8974, acc: 0.8495, precision: 0.9272, recall: 0.8793, f1: 0.9026, edges-ner-ontonotes_loss: 0.0336
10/01 04:06:33 AM: Update 40901: task edges-ner-ontonotes, batch 901 (40901): mcc: 0.9000, acc: 0.8531, precision: 0.9289, recall: 0.8825, f1: 0.9051, edges-ner-ontonotes_loss: 0.0325
10/01 04:06:41 AM: ***** Step 41000 / Validation 41 *****
10/01 04:06:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:06:41 AM: Validating...
10/01 04:06:43 AM: Evaluate: task edges-ner-ontonotes, batch 22 (157): mcc: 0.8839, acc: 0.8378, precision: 0.9174, recall: 0.8637, f1: 0.8897, edges-ner-ontonotes_loss: 0.0334
10/01 04:06:53 AM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.9228, acc: 0.8897, precision: 0.9466, recall: 0.9078, f1: 0.9268, edges-ner-ontonotes_loss: 0.0262
10/01 04:06:59 AM: Updating LR scheduler:
10/01 04:06:59 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:06:59 AM: 	# validation passes without improvement: 3
10/01 04:06:59 AM: edges-ner-ontonotes_loss: training: 0.032005 validation: 0.022677
10/01 04:06:59 AM: macro_avg: validation: 0.937117
10/01 04:06:59 AM: micro_avg: validation: 0.000000
10/01 04:06:59 AM: edges-ner-ontonotes_mcc: training: 0.901181 validation: 0.933634
10/01 04:06:59 AM: edges-ner-ontonotes_acc: training: 0.854691 validation: 0.904079
10/01 04:06:59 AM: edges-ner-ontonotes_precision: training: 0.929419 validation: 0.953251
10/01 04:06:59 AM: edges-ner-ontonotes_recall: training: 0.884185 validation: 0.921520
10/01 04:06:59 AM: edges-ner-ontonotes_f1: training: 0.906238 validation: 0.937117
10/01 04:06:59 AM: Global learning rate: 2.5e-05
10/01 04:06:59 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:07:03 AM: Update 41050: task edges-ner-ontonotes, batch 50 (41050): mcc: 0.9113, acc: 0.8695, precision: 0.9378, recall: 0.8949, f1: 0.9158, edges-ner-ontonotes_loss: 0.0266
10/01 04:07:13 AM: Update 41150: task edges-ner-ontonotes, batch 150 (41150): mcc: 0.9191, acc: 0.8789, precision: 0.9413, recall: 0.9059, f1: 0.9233, edges-ner-ontonotes_loss: 0.0255
10/01 04:07:23 AM: Update 41274: task edges-ner-ontonotes, batch 274 (41274): mcc: 0.9242, acc: 0.8855, precision: 0.9435, recall: 0.9134, f1: 0.9282, edges-ner-ontonotes_loss: 0.0236
10/01 04:07:34 AM: Update 41396: task edges-ner-ontonotes, batch 396 (41396): mcc: 0.9251, acc: 0.8865, precision: 0.9448, recall: 0.9138, f1: 0.9290, edges-ner-ontonotes_loss: 0.0233
10/01 04:07:44 AM: Update 41520: task edges-ner-ontonotes, batch 520 (41520): mcc: 0.9240, acc: 0.8851, precision: 0.9445, recall: 0.9121, f1: 0.9280, edges-ner-ontonotes_loss: 0.0237
10/01 04:07:54 AM: Update 41650: task edges-ner-ontonotes, batch 650 (41650): mcc: 0.9243, acc: 0.8853, precision: 0.9450, recall: 0.9121, f1: 0.9283, edges-ner-ontonotes_loss: 0.0236
10/01 04:08:04 AM: Update 41747: task edges-ner-ontonotes, batch 747 (41747): mcc: 0.9219, acc: 0.8819, precision: 0.9439, recall: 0.9087, f1: 0.9259, edges-ner-ontonotes_loss: 0.0244
10/01 04:08:14 AM: Update 41875: task edges-ner-ontonotes, batch 875 (41875): mcc: 0.9175, acc: 0.8761, precision: 0.9411, recall: 0.9032, f1: 0.9218, edges-ner-ontonotes_loss: 0.0265
10/01 04:08:23 AM: ***** Step 42000 / Validation 42 *****
10/01 04:08:23 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:08:23 AM: Validating...
10/01 04:08:24 AM: Evaluate: task edges-ner-ontonotes, batch 11 (157): mcc: 0.8630, acc: 0.8077, precision: 0.8965, recall: 0.8451, f1: 0.8700, edges-ner-ontonotes_loss: 0.0399
10/01 04:08:34 AM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.9185, acc: 0.8837, precision: 0.9427, recall: 0.9036, f1: 0.9227, edges-ner-ontonotes_loss: 0.0279
10/01 04:08:42 AM: Updating LR scheduler:
10/01 04:08:42 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:08:42 AM: 	# validation passes without improvement: 0
10/01 04:08:42 AM: edges-ner-ontonotes_loss: training: 0.028048 validation: 0.023432
10/01 04:08:42 AM: macro_avg: validation: 0.934877
10/01 04:08:42 AM: micro_avg: validation: 0.000000
10/01 04:08:42 AM: edges-ner-ontonotes_mcc: training: 0.914252 validation: 0.931293
10/01 04:08:42 AM: edges-ner-ontonotes_acc: training: 0.872018 validation: 0.900971
10/01 04:08:42 AM: edges-ner-ontonotes_precision: training: 0.938664 validation: 0.952190
10/01 04:08:42 AM: edges-ner-ontonotes_recall: training: 0.899559 validation: 0.918183
10/01 04:08:42 AM: edges-ner-ontonotes_f1: training: 0.918696 validation: 0.934877
10/01 04:08:42 AM: Global learning rate: 1.25e-05
10/01 04:08:42 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:08:44 AM: Update 42013: task edges-ner-ontonotes, batch 13 (42013): mcc: 0.8801, acc: 0.8332, precision: 0.9167, recall: 0.8573, f1: 0.8860, edges-ner-ontonotes_loss: 0.0440
10/01 04:08:54 AM: Update 42165: task edges-ner-ontonotes, batch 165 (42165): mcc: 0.8933, acc: 0.8439, precision: 0.9241, recall: 0.8748, f1: 0.8988, edges-ner-ontonotes_loss: 0.0347
10/01 04:09:05 AM: Update 42319: task edges-ner-ontonotes, batch 319 (42319): mcc: 0.8928, acc: 0.8433, precision: 0.9240, recall: 0.8739, f1: 0.8982, edges-ner-ontonotes_loss: 0.0338
10/01 04:09:15 AM: Update 42426: task edges-ner-ontonotes, batch 426 (42426): mcc: 0.8985, acc: 0.8510, precision: 0.9281, recall: 0.8805, f1: 0.9036, edges-ner-ontonotes_loss: 0.0320
10/01 04:09:25 AM: Update 42561: task edges-ner-ontonotes, batch 561 (42561): mcc: 0.9022, acc: 0.8560, precision: 0.9305, recall: 0.8851, f1: 0.9072, edges-ner-ontonotes_loss: 0.0309
10/01 04:09:35 AM: Update 42667: task edges-ner-ontonotes, batch 667 (42667): mcc: 0.9037, acc: 0.8585, precision: 0.9316, recall: 0.8868, f1: 0.9087, edges-ner-ontonotes_loss: 0.0303
10/01 04:09:45 AM: Update 42796: task edges-ner-ontonotes, batch 796 (42796): mcc: 0.9084, acc: 0.8646, precision: 0.9342, recall: 0.8929, f1: 0.9131, edges-ner-ontonotes_loss: 0.0290
10/01 04:09:55 AM: Update 42922: task edges-ner-ontonotes, batch 922 (42922): mcc: 0.9116, acc: 0.8687, precision: 0.9366, recall: 0.8967, f1: 0.9162, edges-ner-ontonotes_loss: 0.0281
10/01 04:10:04 AM: ***** Step 43000 / Validation 43 *****
10/01 04:10:04 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:10:04 AM: Validating...
10/01 04:10:05 AM: Evaluate: task edges-ner-ontonotes, batch 9 (157): mcc: 0.8466, acc: 0.7925, precision: 0.8985, recall: 0.8129, f1: 0.8536, edges-ner-ontonotes_loss: 0.0422
10/01 04:10:15 AM: Evaluate: task edges-ner-ontonotes, batch 98 (157): mcc: 0.9234, acc: 0.8914, precision: 0.9470, recall: 0.9086, f1: 0.9274, edges-ner-ontonotes_loss: 0.0272
10/01 04:10:23 AM: Updating LR scheduler:
10/01 04:10:23 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:10:23 AM: 	# validation passes without improvement: 1
10/01 04:10:23 AM: edges-ner-ontonotes_loss: training: 0.027590 validation: 0.023092
10/01 04:10:23 AM: macro_avg: validation: 0.936488
10/01 04:10:23 AM: micro_avg: validation: 0.000000
10/01 04:10:23 AM: edges-ner-ontonotes_mcc: training: 0.913210 validation: 0.932960
10/01 04:10:23 AM: edges-ner-ontonotes_acc: training: 0.870725 validation: 0.904231
10/01 04:10:23 AM: edges-ner-ontonotes_precision: training: 0.937551 validation: 0.952194
10/01 04:10:23 AM: edges-ner-ontonotes_recall: training: 0.898699 validation: 0.921292
10/01 04:10:23 AM: edges-ner-ontonotes_f1: training: 0.917714 validation: 0.936488
10/01 04:10:23 AM: Global learning rate: 1.25e-05
10/01 04:10:23 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:10:25 AM: Update 43029: task edges-ner-ontonotes, batch 29 (43029): mcc: 0.9278, acc: 0.8880, precision: 0.9497, recall: 0.9140, f1: 0.9315, edges-ner-ontonotes_loss: 0.0228
10/01 04:10:35 AM: Update 43158: task edges-ner-ontonotes, batch 158 (43158): mcc: 0.9235, acc: 0.8829, precision: 0.9442, recall: 0.9114, f1: 0.9275, edges-ner-ontonotes_loss: 0.0242
10/01 04:10:45 AM: Update 43266: task edges-ner-ontonotes, batch 266 (43266): mcc: 0.9235, acc: 0.8826, precision: 0.9451, recall: 0.9105, f1: 0.9275, edges-ner-ontonotes_loss: 0.0235
10/01 04:10:55 AM: Update 43393: task edges-ner-ontonotes, batch 393 (43393): mcc: 0.9115, acc: 0.8682, precision: 0.9373, recall: 0.8957, f1: 0.9161, edges-ner-ontonotes_loss: 0.0285
10/01 04:11:05 AM: Update 43531: task edges-ner-ontonotes, batch 531 (43531): mcc: 0.9062, acc: 0.8615, precision: 0.9341, recall: 0.8891, f1: 0.9110, edges-ner-ontonotes_loss: 0.0309
10/01 04:11:15 AM: Update 43643: task edges-ner-ontonotes, batch 643 (43643): mcc: 0.9041, acc: 0.8591, precision: 0.9326, recall: 0.8866, f1: 0.9090, edges-ner-ontonotes_loss: 0.0318
10/01 04:11:25 AM: Update 43796: task edges-ner-ontonotes, batch 796 (43796): mcc: 0.9021, acc: 0.8561, precision: 0.9310, recall: 0.8844, f1: 0.9071, edges-ner-ontonotes_loss: 0.0322
10/01 04:11:35 AM: Update 43913: task edges-ner-ontonotes, batch 913 (43913): mcc: 0.9018, acc: 0.8554, precision: 0.9311, recall: 0.8838, f1: 0.9068, edges-ner-ontonotes_loss: 0.0321
10/01 04:11:42 AM: ***** Step 44000 / Validation 44 *****
10/01 04:11:42 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:11:42 AM: Validating...
10/01 04:11:45 AM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.8974, acc: 0.8590, precision: 0.9227, recall: 0.8836, f1: 0.9027, edges-ner-ontonotes_loss: 0.0329
10/01 04:11:55 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9260, acc: 0.8938, precision: 0.9489, recall: 0.9116, f1: 0.9298, edges-ner-ontonotes_loss: 0.0257
10/01 04:12:01 AM: Updating LR scheduler:
10/01 04:12:01 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:12:01 AM: 	# validation passes without improvement: 2
10/01 04:12:01 AM: edges-ner-ontonotes_loss: training: 0.031805 validation: 0.022707
10/01 04:12:01 AM: macro_avg: validation: 0.937539
10/01 04:12:01 AM: micro_avg: validation: 0.000000
10/01 04:12:01 AM: edges-ner-ontonotes_mcc: training: 0.902177 validation: 0.934081
10/01 04:12:01 AM: edges-ner-ontonotes_acc: training: 0.855773 validation: 0.904762
10/01 04:12:01 AM: edges-ner-ontonotes_precision: training: 0.931123 validation: 0.953718
10/01 04:12:01 AM: edges-ner-ontonotes_recall: training: 0.884391 validation: 0.921899
10/01 04:12:01 AM: edges-ner-ontonotes_f1: training: 0.907156 validation: 0.937539
10/01 04:12:01 AM: Global learning rate: 1.25e-05
10/01 04:12:01 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:12:06 AM: Update 44066: task edges-ner-ontonotes, batch 66 (44066): mcc: 0.9183, acc: 0.8797, precision: 0.9400, recall: 0.9057, f1: 0.9226, edges-ner-ontonotes_loss: 0.0250
10/01 04:12:18 AM: Update 44195: task edges-ner-ontonotes, batch 195 (44195): mcc: 0.9161, acc: 0.8754, precision: 0.9397, recall: 0.9021, f1: 0.9205, edges-ner-ontonotes_loss: 0.0259
10/01 04:12:28 AM: Update 44325: task edges-ner-ontonotes, batch 325 (44325): mcc: 0.9198, acc: 0.8796, precision: 0.9418, recall: 0.9068, f1: 0.9240, edges-ner-ontonotes_loss: 0.0248
10/01 04:12:38 AM: Update 44449: task edges-ner-ontonotes, batch 449 (44449): mcc: 0.9225, acc: 0.8831, precision: 0.9431, recall: 0.9106, f1: 0.9266, edges-ner-ontonotes_loss: 0.0241
10/01 04:12:48 AM: Update 44556: task edges-ner-ontonotes, batch 556 (44556): mcc: 0.9239, acc: 0.8849, precision: 0.9447, recall: 0.9118, f1: 0.9279, edges-ner-ontonotes_loss: 0.0238
10/01 04:12:58 AM: Update 44683: task edges-ner-ontonotes, batch 683 (44683): mcc: 0.9235, acc: 0.8845, precision: 0.9443, recall: 0.9113, f1: 0.9275, edges-ner-ontonotes_loss: 0.0238
10/01 04:13:08 AM: Update 44809: task edges-ner-ontonotes, batch 809 (44809): mcc: 0.9240, acc: 0.8851, precision: 0.9446, recall: 0.9121, f1: 0.9280, edges-ner-ontonotes_loss: 0.0236
10/01 04:13:18 AM: Update 44907: task edges-ner-ontonotes, batch 907 (44907): mcc: 0.9206, acc: 0.8806, precision: 0.9424, recall: 0.9077, f1: 0.9247, edges-ner-ontonotes_loss: 0.0250
10/01 04:13:25 AM: ***** Step 45000 / Validation 45 *****
10/01 04:13:25 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:13:25 AM: Validating...
10/01 04:13:28 AM: Evaluate: task edges-ner-ontonotes, batch 30 (157): mcc: 0.8876, acc: 0.8485, precision: 0.9139, recall: 0.8739, f1: 0.8934, edges-ner-ontonotes_loss: 0.0362
10/01 04:13:38 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9249, acc: 0.8923, precision: 0.9469, recall: 0.9114, f1: 0.9289, edges-ner-ontonotes_loss: 0.0262
10/01 04:13:44 AM: Updating LR scheduler:
10/01 04:13:44 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:13:44 AM: 	# validation passes without improvement: 3
10/01 04:13:44 AM: edges-ner-ontonotes_loss: training: 0.026197 validation: 0.023067
10/01 04:13:44 AM: macro_avg: validation: 0.936187
10/01 04:13:44 AM: micro_avg: validation: 0.000000
10/01 04:13:44 AM: edges-ner-ontonotes_mcc: training: 0.917833 validation: 0.932651
10/01 04:13:44 AM: edges-ner-ontonotes_acc: training: 0.877072 validation: 0.902639
10/01 04:13:44 AM: edges-ner-ontonotes_precision: training: 0.940637 validation: 0.952381
10/01 04:13:44 AM: edges-ner-ontonotes_recall: training: 0.904316 validation: 0.920534
10/01 04:13:44 AM: edges-ner-ontonotes_f1: training: 0.922119 validation: 0.936187
10/01 04:13:44 AM: Global learning rate: 1.25e-05
10/01 04:13:44 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:13:48 AM: Update 45064: task edges-ner-ontonotes, batch 64 (45064): mcc: 0.8948, acc: 0.8464, precision: 0.9303, recall: 0.8715, f1: 0.8999, edges-ner-ontonotes_loss: 0.0382
10/01 04:13:59 AM: Update 45184: task edges-ner-ontonotes, batch 184 (45184): mcc: 0.8921, acc: 0.8443, precision: 0.9244, recall: 0.8722, f1: 0.8975, edges-ner-ontonotes_loss: 0.0374
10/01 04:14:09 AM: Update 45344: task edges-ner-ontonotes, batch 344 (45344): mcc: 0.8923, acc: 0.8428, precision: 0.9250, recall: 0.8719, f1: 0.8977, edges-ner-ontonotes_loss: 0.0355
10/01 04:14:19 AM: Update 45458: task edges-ner-ontonotes, batch 458 (45458): mcc: 0.8933, acc: 0.8441, precision: 0.9249, recall: 0.8739, f1: 0.8987, edges-ner-ontonotes_loss: 0.0347
10/01 04:14:29 AM: Update 45584: task edges-ner-ontonotes, batch 584 (45584): mcc: 0.8976, acc: 0.8498, precision: 0.9280, recall: 0.8789, f1: 0.9028, edges-ner-ontonotes_loss: 0.0333
10/01 04:14:39 AM: Update 45721: task edges-ner-ontonotes, batch 721 (45721): mcc: 0.9010, acc: 0.8543, precision: 0.9298, recall: 0.8835, f1: 0.9061, edges-ner-ontonotes_loss: 0.0319
10/01 04:14:49 AM: Update 45816: task edges-ner-ontonotes, batch 816 (45816): mcc: 0.9041, acc: 0.8581, precision: 0.9320, recall: 0.8871, f1: 0.9090, edges-ner-ontonotes_loss: 0.0309
10/01 04:14:59 AM: Update 45942: task edges-ner-ontonotes, batch 942 (45942): mcc: 0.9076, acc: 0.8628, precision: 0.9336, recall: 0.8921, f1: 0.9124, edges-ner-ontonotes_loss: 0.0297
10/01 04:15:04 AM: ***** Step 46000 / Validation 46 *****
10/01 04:15:04 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:15:04 AM: Validating...
10/01 04:15:09 AM: Evaluate: task edges-ner-ontonotes, batch 51 (157): mcc: 0.9058, acc: 0.8699, precision: 0.9310, recall: 0.8913, f1: 0.9107, edges-ner-ontonotes_loss: 0.0313
10/01 04:15:19 AM: Evaluate: task edges-ner-ontonotes, batch 127 (157): mcc: 0.9295, acc: 0.8990, precision: 0.9514, recall: 0.9155, f1: 0.9331, edges-ner-ontonotes_loss: 0.0249
10/01 04:15:22 AM: Updating LR scheduler:
10/01 04:15:22 AM: 	Best result seen so far for macro_avg: 0.938
10/01 04:15:22 AM: 	# validation passes without improvement: 0
10/01 04:15:22 AM: edges-ner-ontonotes_loss: training: 0.029285 validation: 0.022982
10/01 04:15:22 AM: macro_avg: validation: 0.937257
10/01 04:15:22 AM: micro_avg: validation: 0.000000
10/01 04:15:22 AM: edges-ner-ontonotes_mcc: training: 0.909047 validation: 0.933787
10/01 04:15:22 AM: edges-ner-ontonotes_acc: training: 0.864687 validation: 0.904762
10/01 04:15:22 AM: edges-ner-ontonotes_precision: training: 0.934429 validation: 0.953622
10/01 04:15:22 AM: edges-ner-ontonotes_recall: training: 0.893972 validation: 0.921444
10/01 04:15:22 AM: edges-ner-ontonotes_f1: training: 0.913753 validation: 0.937257
10/01 04:15:22 AM: Global learning rate: 6.25e-06
10/01 04:15:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-sstrandom-top/run
10/01 04:15:29 AM: Update 46064: task edges-ner-ontonotes, batch 64 (46064): mcc: 0.9260, acc: 0.8877, precision: 0.9464, recall: 0.9139, f1: 0.9299, edges-ner-ontonotes_loss: 0.0220
10/01 04:15:39 AM: Update 46189: task edges-ner-ontonotes, batch 189 (46189): mcc: 0.9266, acc: 0.8878, precision: 0.9474, recall: 0.9142, f1: 0.9305, edges-ner-ontonotes_loss: 0.0230
10/01 04:15:49 AM: Update 46317: task edges-ner-ontonotes, batch 317 (46317): mcc: 0.9251, acc: 0.8856, precision: 0.9462, recall: 0.9124, f1: 0.9290, edges-ner-ontonotes_loss: 0.0234
10/01 04:15:59 AM: Update 46420: task edges-ner-ontonotes, batch 420 (46420): mcc: 0.9203, acc: 0.8802, precision: 0.9429, recall: 0.9066, f1: 0.9244, edges-ner-ontonotes_loss: 0.0250
10/01 04:16:09 AM: Update 46546: task edges-ner-ontonotes, batch 546 (46546): mcc: 0.9139, acc: 0.8718, precision: 0.9393, recall: 0.8983, f1: 0.9183, edges-ner-ontonotes_loss: 0.0279
10/01 04:16:20 AM: Update 46681: task edges-ner-ontonotes, batch 681 (46681): mcc: 0.9081, acc: 0.8645, precision: 0.9356, recall: 0.8910, f1: 0.9127, edges-ner-ontonotes_loss: 0.0304
10/01 04:16:30 AM: Update 46831: task edges-ner-ontonotes, batch 831 (46831): mcc: 0.9053, acc: 0.8604, precision: 0.9335, recall: 0.8880, f1: 0.9101, edges-ner-ontonotes_loss: 0.0311
10/01 04:16:41 AM: Update 46987: task edges-ner-ontonotes, batch 987 (46987): mcc: 0.9040, acc: 0.8585, precision: 0.9323, recall: 0.8867, f1: 0.9089, edges-ner-ontonotes_loss: 0.0313
10/01 04:16:44 AM: ***** Step 47000 / Validation 47 *****
10/01 04:16:44 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:16:44 AM: Validating...
10/01 04:16:51 AM: Evaluate: task edges-ner-ontonotes, batch 64 (157): mcc: 0.9156, acc: 0.8811, precision: 0.9364, recall: 0.9041, f1: 0.9200, edges-ner-ontonotes_loss: 0.0284
10/01 04:17:01 AM: Evaluate: task edges-ner-ontonotes, batch 143 (157): mcc: 0.9326, acc: 0.9026, precision: 0.9540, recall: 0.9188, f1: 0.9361, edges-ner-ontonotes_loss: 0.0234
