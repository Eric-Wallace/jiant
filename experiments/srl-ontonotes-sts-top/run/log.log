09/16 09:10:58 AM: Git branch: master
09/16 09:10:58 AM: Git SHA: 1a42459c6cbb693793b9c0d01bca567d99b0baac
09/16 09:10:59 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/srl-ontonotes-sts-top/",
  "exp_name": "experiments/srl-ontonotes-sts-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/srl-ontonotes-sts-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/sts",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/srl-ontonotes-sts-top__run",
  "run_dir": "./experiments/srl-ontonotes-sts-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-srl-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:10:59 AM: Saved config to ./experiments/srl-ontonotes-sts-top/run/params.conf
09/16 09:10:59 AM: Using random seed 1234
09/16 09:10:59 AM: Using GPU 0
09/16 09:10:59 AM: Loading tasks...
09/16 09:10:59 AM: Writing pre-preprocessed tasks to ./experiments/srl-ontonotes-sts-top/
09/16 09:10:59 AM: 	Creating task edges-srl-ontonotes from scratch.
09/16 09:11:05 AM: Read=231480, Skip=21590, Total=253070 from ./probing_data/edges/ontonotes/srl/train.json.retokenized.bert-base-uncased
09/16 09:11:05 AM: Read=32486, Skip=2811, Total=35297 from ./probing_data/edges/ontonotes/srl/development.json.retokenized.bert-base-uncased
09/16 09:11:06 AM: Read=23800, Skip=2915, Total=26715 from ./probing_data/edges/ontonotes/srl/test.json.retokenized.bert-base-uncased
09/16 09:11:10 AM: 	Task 'edges-srl-ontonotes': |train|=231480 |val|=32486 |test|=23800
09/16 09:11:10 AM: 	Finished loading tasks: edges-srl-ontonotes.
09/16 09:11:10 AM: 	Building vocab from scratch.
09/16 09:11:10 AM: 	Counting units for task edges-srl-ontonotes.
09/16 09:11:18 AM: 	Task 'edges-srl-ontonotes': adding vocab namespace 'edges-srl-ontonotes_labels'
09/16 09:11:19 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:11:19 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:11:19 AM: 	Saved vocab to ./experiments/srl-ontonotes-sts-top/vocab
09/16 09:11:19 AM: Loading token dictionary from ./experiments/srl-ontonotes-sts-top/vocab.
09/16 09:11:20 AM: 	Loaded vocab from ./experiments/srl-ontonotes-sts-top/vocab
09/16 09:11:20 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:11:20 AM: 	Vocab namespace tokens: size 23662
09/16 09:11:20 AM: 	Vocab namespace edges-srl-ontonotes_labels: size 66
09/16 09:11:20 AM: 	Vocab namespace chars: size 76
09/16 09:11:20 AM: 	Finished building vocab.
09/16 09:11:20 AM: 	Task edges-srl-ontonotes (train): Indexing from scratch.
09/16 09:11:55 AM: 	Task edges-srl-ontonotes (train): Saved 231480 instances to ./experiments/srl-ontonotes-sts-top/preproc/edges-srl-ontonotes__train_data
09/16 09:11:55 AM: 	Task edges-srl-ontonotes (val): Indexing from scratch.
09/16 09:11:59 AM: 	Task edges-srl-ontonotes (val): Saved 32486 instances to ./experiments/srl-ontonotes-sts-top/preproc/edges-srl-ontonotes__val_data
09/16 09:11:59 AM: 	Task edges-srl-ontonotes (test): Indexing from scratch.
09/16 09:12:03 AM: 	Task edges-srl-ontonotes (test): Saved 23800 instances to ./experiments/srl-ontonotes-sts-top/preproc/edges-srl-ontonotes__test_data
09/16 09:12:03 AM: 	Finished indexing tasks
09/16 09:12:03 AM: 	Creating trimmed target-only version of edges-srl-ontonotes train.
09/16 09:12:03 AM: 	  Training on 
09/16 09:12:03 AM: 	  Evaluating on edges-srl-ontonotes
09/16 09:12:03 AM: 	Finished loading tasks in 63.540s
09/16 09:12:03 AM: 	 Tasks: ['edges-srl-ontonotes']
09/16 09:12:03 AM: Building model...
09/16 09:12:03 AM: Using BERT model (bert-base-uncased).
09/16 09:12:03 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:12:03 AM: models/sts
09/16 09:12:03 AM: loading configuration file models/sts/config.json
09/16 09:12:03 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sts-b",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 1,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:12:03 AM: loading weights file models/sts/pytorch_model.bin
09/16 09:12:06 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp46nqliv4
09/16 09:12:08 AM: copying /tmp/tmp46nqliv4 to cache at ./experiments/srl-ontonotes-sts-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:08 AM: creating metadata file for ./experiments/srl-ontonotes-sts-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:08 AM: removing temp file /tmp/tmp46nqliv4
09/16 09:12:08 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/srl-ontonotes-sts-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:08 AM: Initializing parameters
09/16 09:12:08 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:12:08 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:12:08 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:12:08 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:12:08 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:12:08 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:12:08 AM: 	Task 'edges-srl-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-srl-ontonotes"
}
09/16 09:12:12 AM: Model specification:
09/16 09:12:12 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-srl-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=66, bias=True)
      )
    )
  )
)
09/16 09:12:12 AM: Model parameters:
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:12 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 16896 with torch.Size([66, 256])
09/16 09:12:12 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 66 with torch.Size([66])
09/16 09:12:12 AM: Total number of parameters: 110155842 (1.10156e+08)
09/16 09:12:12 AM: Number of trainable parameters: 673602 (673602)
09/16 09:12:12 AM: Finished building model in 9.013s
09/16 09:12:12 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-srl-ontonotes 

09/16 09:12:26 AM: patience = 9
09/16 09:12:26 AM: val_interval = 1000
09/16 09:12:26 AM: max_vals = 250
09/16 09:12:26 AM: cuda_device = 0
09/16 09:12:26 AM: grad_norm = 5.0
09/16 09:12:26 AM: grad_clipping = None
09/16 09:12:26 AM: lr_decay = 0.99
09/16 09:12:26 AM: min_lr = 1e-06
09/16 09:12:26 AM: keep_all_checkpoints = 0
09/16 09:12:26 AM: val_data_limit = 5000
09/16 09:12:26 AM: max_epochs = -1
09/16 09:12:26 AM: dec_val_scale = 250
09/16 09:12:26 AM: training_data_fraction = 1
09/16 09:12:26 AM: type = adam
09/16 09:12:26 AM: parameter_groups = None
09/16 09:12:26 AM: Number of trainable parameters: 673602
09/16 09:12:26 AM: infer_type_and_cast = True
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: lr = 0.0001
09/16 09:12:26 AM: amsgrad = True
09/16 09:12:26 AM: type = reduce_on_plateau
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: mode = max
09/16 09:12:26 AM: factor = 0.5
09/16 09:12:26 AM: patience = 3
09/16 09:12:26 AM: threshold = 0.0001
09/16 09:12:26 AM: threshold_mode = abs
09/16 09:12:26 AM: verbose = True
09/16 09:12:26 AM: type = adam
09/16 09:12:26 AM: parameter_groups = None
09/16 09:12:26 AM: Number of trainable parameters: 673602
09/16 09:12:26 AM: infer_type_and_cast = True
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: lr = 0.0001
09/16 09:12:26 AM: amsgrad = True
09/16 09:12:26 AM: type = reduce_on_plateau
09/16 09:12:26 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:26 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:26 AM: mode = max
09/16 09:12:26 AM: factor = 0.5
09/16 09:12:26 AM: patience = 3
09/16 09:12:26 AM: threshold = 0.0001
09/16 09:12:26 AM: threshold_mode = abs
09/16 09:12:26 AM: verbose = True
09/16 09:12:26 AM: Starting training without restoring from a checkpoint.
09/16 09:12:26 AM: Training examples per task, before any subsampling: {'edges-srl-ontonotes': 231480}
09/16 09:12:26 AM: Beginning training with stopping criteria based on metric: edges-srl-ontonotes_f1
09/16 09:12:36 AM: Update 138: task edges-srl-ontonotes, batch 138 (138): mcc: 0.0534, acc: 0.0352, precision: 0.0661, recall: 0.0703, f1: 0.0681, edges-srl-ontonotes_loss: 0.1823
09/16 09:12:46 AM: Update 273: task edges-srl-ontonotes, batch 273 (273): mcc: 0.0470, acc: 0.0251, precision: 0.0779, recall: 0.0426, f1: 0.0551, edges-srl-ontonotes_loss: 0.1255
09/16 09:12:56 AM: Update 398: task edges-srl-ontonotes, batch 398 (398): mcc: 0.0634, acc: 0.0337, precision: 0.1143, recall: 0.0458, f1: 0.0654, edges-srl-ontonotes_loss: 0.1027
09/16 09:13:06 AM: Update 544: task edges-srl-ontonotes, batch 544 (544): mcc: 0.1550, acc: 0.0902, precision: 0.2630, recall: 0.1007, f1: 0.1457, edges-srl-ontonotes_loss: 0.0867
09/16 09:13:16 AM: Update 645: task edges-srl-ontonotes, batch 645 (645): mcc: 0.2213, acc: 0.1330, precision: 0.3609, recall: 0.1448, f1: 0.2066, edges-srl-ontonotes_loss: 0.0790
09/16 09:13:26 AM: Update 789: task edges-srl-ontonotes, batch 789 (789): mcc: 0.2941, acc: 0.1836, precision: 0.4587, recall: 0.1974, f1: 0.2760, edges-srl-ontonotes_loss: 0.0709
09/16 09:13:36 AM: Update 931: task edges-srl-ontonotes, batch 931 (931): mcc: 0.3517, acc: 0.2272, precision: 0.5279, recall: 0.2429, f1: 0.3327, edges-srl-ontonotes_loss: 0.0649
09/16 09:13:42 AM: ***** Step 1000 / Validation 1 *****
09/16 09:13:42 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:13:42 AM: Validating...
09/16 09:13:46 AM: Evaluate: task edges-srl-ontonotes, batch 56 (157): mcc: 0.6591, acc: 0.5070, precision: 0.8472, recall: 0.5188, f1: 0.6435, edges-srl-ontonotes_loss: 0.0284
09/16 09:13:53 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:13:53 AM: Best result seen so far for micro.
09/16 09:13:53 AM: Best result seen so far for macro.
09/16 09:13:53 AM: Updating LR scheduler:
09/16 09:13:53 AM: 	Best result seen so far for macro_avg: 0.672
09/16 09:13:53 AM: 	# validation passes without improvement: 0
09/16 09:13:53 AM: edges-srl-ontonotes_loss: training: 0.062488 validation: 0.026883
09/16 09:13:53 AM: macro_avg: validation: 0.672346
09/16 09:13:53 AM: micro_avg: validation: 0.000000
09/16 09:13:53 AM: edges-srl-ontonotes_mcc: training: 0.372667 validation: 0.684999
09/16 09:13:53 AM: edges-srl-ontonotes_acc: training: 0.243849 validation: 0.540220
09/16 09:13:53 AM: edges-srl-ontonotes_precision: training: 0.551514 validation: 0.857757
09/16 09:13:53 AM: edges-srl-ontonotes_recall: training: 0.260336 validation: 0.552844
09/16 09:13:53 AM: edges-srl-ontonotes_f1: training: 0.353708 validation: 0.672346
09/16 09:13:53 AM: Global learning rate: 0.0001
09/16 09:13:53 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:13:56 AM: Update 1034: task edges-srl-ontonotes, batch 34 (1034): mcc: 0.6265, acc: 0.4815, precision: 0.7789, recall: 0.5111, f1: 0.6172, edges-srl-ontonotes_loss: 0.0308
09/16 09:14:06 AM: Update 1174: task edges-srl-ontonotes, batch 174 (1174): mcc: 0.6472, acc: 0.5017, precision: 0.7962, recall: 0.5329, f1: 0.6385, edges-srl-ontonotes_loss: 0.0292
09/16 09:14:16 AM: Update 1295: task edges-srl-ontonotes, batch 295 (1295): mcc: 0.6548, acc: 0.5118, precision: 0.7959, recall: 0.5455, f1: 0.6473, edges-srl-ontonotes_loss: 0.0284
09/16 09:14:26 AM: Update 1429: task edges-srl-ontonotes, batch 429 (1429): mcc: 0.6608, acc: 0.5207, precision: 0.7963, recall: 0.5552, f1: 0.6543, edges-srl-ontonotes_loss: 0.0277
09/16 09:14:36 AM: Update 1561: task edges-srl-ontonotes, batch 561 (1561): mcc: 0.6693, acc: 0.5320, precision: 0.7987, recall: 0.5677, f1: 0.6637, edges-srl-ontonotes_loss: 0.0270
09/16 09:14:46 AM: Update 1660: task edges-srl-ontonotes, batch 660 (1660): mcc: 0.6684, acc: 0.5310, precision: 0.7962, recall: 0.5680, f1: 0.6630, edges-srl-ontonotes_loss: 0.0269
09/16 09:14:56 AM: Update 1796: task edges-srl-ontonotes, batch 796 (1796): mcc: 0.6691, acc: 0.5322, precision: 0.7953, recall: 0.5699, f1: 0.6640, edges-srl-ontonotes_loss: 0.0268
09/16 09:15:06 AM: Update 1880: task edges-srl-ontonotes, batch 880 (1880): mcc: 0.6697, acc: 0.5331, precision: 0.7938, recall: 0.5719, f1: 0.6648, edges-srl-ontonotes_loss: 0.0266
09/16 09:15:16 AM: Update 1999: task edges-srl-ontonotes, batch 999 (1999): mcc: 0.6718, acc: 0.5358, precision: 0.7939, recall: 0.5755, f1: 0.6673, edges-srl-ontonotes_loss: 0.0264
09/16 09:15:16 AM: ***** Step 2000 / Validation 2 *****
09/16 09:15:16 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:15:16 AM: Validating...
09/16 09:15:26 AM: Evaluate: task edges-srl-ontonotes, batch 130 (157): mcc: 0.7435, acc: 0.6217, precision: 0.8680, recall: 0.6424, f1: 0.7384, edges-srl-ontonotes_loss: 0.0207
09/16 09:15:28 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:15:28 AM: Best result seen so far for macro.
09/16 09:15:28 AM: Updating LR scheduler:
09/16 09:15:28 AM: 	Best result seen so far for macro_avg: 0.737
09/16 09:15:28 AM: 	# validation passes without improvement: 0
09/16 09:15:28 AM: edges-srl-ontonotes_loss: training: 0.026400 validation: 0.020825
09/16 09:15:28 AM: macro_avg: validation: 0.736595
09/16 09:15:28 AM: micro_avg: validation: 0.000000
09/16 09:15:28 AM: edges-srl-ontonotes_mcc: training: 0.671825 validation: 0.741822
09/16 09:15:28 AM: edges-srl-ontonotes_acc: training: 0.535799 validation: 0.619891
09/16 09:15:28 AM: edges-srl-ontonotes_precision: training: 0.793905 validation: 0.867000
09/16 09:15:28 AM: edges-srl-ontonotes_recall: training: 0.575511 validation: 0.640289
09/16 09:15:28 AM: edges-srl-ontonotes_f1: training: 0.667294 validation: 0.736595
09/16 09:15:28 AM: Global learning rate: 0.0001
09/16 09:15:28 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:15:36 AM: Update 2103: task edges-srl-ontonotes, batch 103 (2103): mcc: 0.6864, acc: 0.5553, precision: 0.7966, recall: 0.5985, f1: 0.6834, edges-srl-ontonotes_loss: 0.0248
09/16 09:15:47 AM: Update 2216: task edges-srl-ontonotes, batch 216 (2216): mcc: 0.6884, acc: 0.5580, precision: 0.7937, recall: 0.6041, f1: 0.6860, edges-srl-ontonotes_loss: 0.0246
09/16 09:15:57 AM: Update 2329: task edges-srl-ontonotes, batch 329 (2329): mcc: 0.6984, acc: 0.5710, precision: 0.7992, recall: 0.6173, f1: 0.6966, edges-srl-ontonotes_loss: 0.0239
09/16 09:16:07 AM: Update 2444: task edges-srl-ontonotes, batch 444 (2444): mcc: 0.7055, acc: 0.5810, precision: 0.8031, recall: 0.6266, f1: 0.7040, edges-srl-ontonotes_loss: 0.0234
09/16 09:16:17 AM: Update 2558: task edges-srl-ontonotes, batch 558 (2558): mcc: 0.7086, acc: 0.5854, precision: 0.8048, recall: 0.6306, f1: 0.7071, edges-srl-ontonotes_loss: 0.0231
09/16 09:16:27 AM: Update 2681: task edges-srl-ontonotes, batch 681 (2681): mcc: 0.7117, acc: 0.5904, precision: 0.8060, recall: 0.6351, f1: 0.7104, edges-srl-ontonotes_loss: 0.0229
09/16 09:16:37 AM: Update 2796: task edges-srl-ontonotes, batch 796 (2796): mcc: 0.7135, acc: 0.5934, precision: 0.8062, recall: 0.6382, f1: 0.7124, edges-srl-ontonotes_loss: 0.0227
09/16 09:16:47 AM: Update 2884: task edges-srl-ontonotes, batch 884 (2884): mcc: 0.7156, acc: 0.5961, precision: 0.8071, recall: 0.6412, f1: 0.7146, edges-srl-ontonotes_loss: 0.0225
09/16 09:16:56 AM: ***** Step 3000 / Validation 3 *****
09/16 09:16:56 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:16:56 AM: Validating...
09/16 09:16:57 AM: Evaluate: task edges-srl-ontonotes, batch 11 (157): mcc: 0.7644, acc: 0.6701, precision: 0.8269, recall: 0.7127, f1: 0.7655, edges-srl-ontonotes_loss: 0.0186
09/16 09:17:07 AM: Evaluate: task edges-srl-ontonotes, batch 144 (157): mcc: 0.7548, acc: 0.6495, precision: 0.8462, recall: 0.6791, f1: 0.7535, edges-srl-ontonotes_loss: 0.0196
09/16 09:17:08 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:17:08 AM: Best result seen so far for macro.
09/16 09:17:08 AM: Updating LR scheduler:
09/16 09:17:08 AM: 	Best result seen so far for macro_avg: 0.754
09/16 09:17:08 AM: 	# validation passes without improvement: 0
09/16 09:17:08 AM: edges-srl-ontonotes_loss: training: 0.022383 validation: 0.019686
09/16 09:17:08 AM: macro_avg: validation: 0.754013
09/16 09:17:08 AM: micro_avg: validation: 0.000000
09/16 09:17:08 AM: edges-srl-ontonotes_mcc: training: 0.717830 validation: 0.755289
09/16 09:17:08 AM: edges-srl-ontonotes_acc: training: 0.598625 validation: 0.650219
09/16 09:17:08 AM: edges-srl-ontonotes_precision: training: 0.808143 validation: 0.846449
09/16 09:17:08 AM: edges-srl-ontonotes_recall: training: 0.644295 validation: 0.679778
09/16 09:17:08 AM: edges-srl-ontonotes_f1: training: 0.716978 validation: 0.754013
09/16 09:17:08 AM: Global learning rate: 0.0001
09/16 09:17:08 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:17:17 AM: Update 3116: task edges-srl-ontonotes, batch 116 (3116): mcc: 0.7419, acc: 0.6322, precision: 0.8216, recall: 0.6762, f1: 0.7418, edges-srl-ontonotes_loss: 0.0206
09/16 09:17:27 AM: Update 3230: task edges-srl-ontonotes, batch 230 (3230): mcc: 0.7370, acc: 0.6247, precision: 0.8180, recall: 0.6705, f1: 0.7369, edges-srl-ontonotes_loss: 0.0210
09/16 09:17:37 AM: Update 3361: task edges-srl-ontonotes, batch 361 (3361): mcc: 0.7335, acc: 0.6213, precision: 0.8152, recall: 0.6665, f1: 0.7334, edges-srl-ontonotes_loss: 0.0211
09/16 09:17:47 AM: Update 3459: task edges-srl-ontonotes, batch 459 (3459): mcc: 0.7325, acc: 0.6195, precision: 0.8146, recall: 0.6651, f1: 0.7323, edges-srl-ontonotes_loss: 0.0212
09/16 09:17:57 AM: Update 3580: task edges-srl-ontonotes, batch 580 (3580): mcc: 0.7319, acc: 0.6186, precision: 0.8139, recall: 0.6647, f1: 0.7317, edges-srl-ontonotes_loss: 0.0212
09/16 09:18:07 AM: Update 3690: task edges-srl-ontonotes, batch 690 (3690): mcc: 0.7305, acc: 0.6167, precision: 0.8129, recall: 0.6630, f1: 0.7303, edges-srl-ontonotes_loss: 0.0212
09/16 09:18:17 AM: Update 3806: task edges-srl-ontonotes, batch 806 (3806): mcc: 0.7310, acc: 0.6171, precision: 0.8130, recall: 0.6638, f1: 0.7308, edges-srl-ontonotes_loss: 0.0211
09/16 09:18:27 AM: Update 3940: task edges-srl-ontonotes, batch 940 (3940): mcc: 0.7305, acc: 0.6169, precision: 0.8117, recall: 0.6640, f1: 0.7304, edges-srl-ontonotes_loss: 0.0211
09/16 09:18:32 AM: ***** Step 4000 / Validation 4 *****
09/16 09:18:32 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:18:32 AM: Validating...
09/16 09:18:37 AM: Evaluate: task edges-srl-ontonotes, batch 69 (157): mcc: 0.7411, acc: 0.6353, precision: 0.8465, recall: 0.6548, f1: 0.7384, edges-srl-ontonotes_loss: 0.0200
09/16 09:18:46 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:18:46 AM: Best result seen so far for macro.
09/16 09:18:46 AM: Updating LR scheduler:
09/16 09:18:46 AM: 	Best result seen so far for macro_avg: 0.754
09/16 09:18:46 AM: 	# validation passes without improvement: 1
09/16 09:18:46 AM: edges-srl-ontonotes_loss: training: 0.021126 validation: 0.018997
09/16 09:18:46 AM: macro_avg: validation: 0.754040
09/16 09:18:46 AM: micro_avg: validation: 0.000000
09/16 09:18:46 AM: edges-srl-ontonotes_mcc: training: 0.730006 validation: 0.756309
09/16 09:18:46 AM: edges-srl-ontonotes_acc: training: 0.616304 validation: 0.650989
09/16 09:18:46 AM: edges-srl-ontonotes_precision: training: 0.810964 validation: 0.856639
09/16 09:18:46 AM: edges-srl-ontonotes_recall: training: 0.663718 validation: 0.673389
09/16 09:18:46 AM: edges-srl-ontonotes_f1: training: 0.729990 validation: 0.754040
09/16 09:18:46 AM: Global learning rate: 0.0001
09/16 09:18:46 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:18:47 AM: Update 4018: task edges-srl-ontonotes, batch 18 (4018): mcc: 0.7319, acc: 0.6296, precision: 0.8140, recall: 0.6646, f1: 0.7317, edges-srl-ontonotes_loss: 0.0209
09/16 09:18:57 AM: Update 4131: task edges-srl-ontonotes, batch 131 (4131): mcc: 0.7282, acc: 0.6166, precision: 0.8085, recall: 0.6625, f1: 0.7283, edges-srl-ontonotes_loss: 0.0210
09/16 09:19:07 AM: Update 4248: task edges-srl-ontonotes, batch 248 (4248): mcc: 0.7348, acc: 0.6235, precision: 0.8115, recall: 0.6719, f1: 0.7351, edges-srl-ontonotes_loss: 0.0207
09/16 09:19:17 AM: Update 4376: task edges-srl-ontonotes, batch 376 (4376): mcc: 0.7412, acc: 0.6321, precision: 0.8162, recall: 0.6795, f1: 0.7416, edges-srl-ontonotes_loss: 0.0204
09/16 09:19:27 AM: Update 4491: task edges-srl-ontonotes, batch 491 (4491): mcc: 0.7413, acc: 0.6327, precision: 0.8158, recall: 0.6801, f1: 0.7418, edges-srl-ontonotes_loss: 0.0203
09/16 09:19:38 AM: Update 4603: task edges-srl-ontonotes, batch 603 (4603): mcc: 0.7419, acc: 0.6334, precision: 0.8158, recall: 0.6812, f1: 0.7424, edges-srl-ontonotes_loss: 0.0202
09/16 09:19:48 AM: Update 4721: task edges-srl-ontonotes, batch 721 (4721): mcc: 0.7407, acc: 0.6323, precision: 0.8143, recall: 0.6802, f1: 0.7413, edges-srl-ontonotes_loss: 0.0203
09/16 09:19:58 AM: Update 4820: task edges-srl-ontonotes, batch 820 (4820): mcc: 0.7375, acc: 0.6291, precision: 0.8121, recall: 0.6763, f1: 0.7380, edges-srl-ontonotes_loss: 0.0205
09/16 09:20:08 AM: Update 4919: task edges-srl-ontonotes, batch 919 (4919): mcc: 0.7364, acc: 0.6277, precision: 0.8117, recall: 0.6747, f1: 0.7369, edges-srl-ontonotes_loss: 0.0205
09/16 09:20:15 AM: ***** Step 5000 / Validation 5 *****
09/16 09:20:15 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:20:15 AM: Validating...
09/16 09:20:18 AM: Evaluate: task edges-srl-ontonotes, batch 32 (157): mcc: 0.7491, acc: 0.6426, precision: 0.8447, recall: 0.6702, f1: 0.7474, edges-srl-ontonotes_loss: 0.0191
09/16 09:20:28 AM: Updating LR scheduler:
09/16 09:20:28 AM: 	Best result seen so far for macro_avg: 0.754
09/16 09:20:28 AM: 	# validation passes without improvement: 2
09/16 09:20:28 AM: edges-srl-ontonotes_loss: training: 0.020525 validation: 0.019076
09/16 09:20:28 AM: macro_avg: validation: 0.747094
09/16 09:20:28 AM: micro_avg: validation: 0.000000
09/16 09:20:28 AM: edges-srl-ontonotes_mcc: training: 0.736434 validation: 0.749502
09/16 09:20:28 AM: edges-srl-ontonotes_acc: training: 0.627347 validation: 0.639058
09/16 09:20:28 AM: edges-srl-ontonotes_precision: training: 0.811994 validation: 0.851556
09/16 09:20:28 AM: edges-srl-ontonotes_recall: training: 0.674443 validation: 0.665461
09/16 09:20:28 AM: edges-srl-ontonotes_f1: training: 0.736854 validation: 0.747094
09/16 09:20:28 AM: Global learning rate: 0.0001
09/16 09:20:28 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:20:28 AM: Update 5002: task edges-srl-ontonotes, batch 2 (5002): mcc: 0.7234, acc: 0.6087, precision: 0.8030, recall: 0.6584, f1: 0.7235, edges-srl-ontonotes_loss: 0.0226
09/16 09:20:38 AM: Update 5099: task edges-srl-ontonotes, batch 99 (5099): mcc: 0.7373, acc: 0.6276, precision: 0.8141, recall: 0.6742, f1: 0.7376, edges-srl-ontonotes_loss: 0.0203
09/16 09:20:48 AM: Update 5236: task edges-srl-ontonotes, batch 236 (5236): mcc: 0.7476, acc: 0.6394, precision: 0.8209, recall: 0.6872, f1: 0.7481, edges-srl-ontonotes_loss: 0.0196
09/16 09:20:58 AM: Update 5358: task edges-srl-ontonotes, batch 358 (5358): mcc: 0.7535, acc: 0.6465, precision: 0.8245, recall: 0.6948, f1: 0.7541, edges-srl-ontonotes_loss: 0.0192
09/16 09:21:08 AM: Update 5498: task edges-srl-ontonotes, batch 498 (5498): mcc: 0.7623, acc: 0.6567, precision: 0.8302, recall: 0.7061, f1: 0.7631, edges-srl-ontonotes_loss: 0.0186
09/16 09:21:18 AM: Update 5640: task edges-srl-ontonotes, batch 640 (5640): mcc: 0.7705, acc: 0.6665, precision: 0.8352, recall: 0.7166, f1: 0.7714, edges-srl-ontonotes_loss: 0.0181
09/16 09:21:28 AM: Update 5787: task edges-srl-ontonotes, batch 787 (5787): mcc: 0.7729, acc: 0.6694, precision: 0.8358, recall: 0.7207, f1: 0.7740, edges-srl-ontonotes_loss: 0.0179
09/16 09:21:38 AM: Update 5940: task edges-srl-ontonotes, batch 940 (5940): mcc: 0.7764, acc: 0.6731, precision: 0.8376, recall: 0.7255, f1: 0.7775, edges-srl-ontonotes_loss: 0.0177
09/16 09:21:45 AM: ***** Step 6000 / Validation 6 *****
09/16 09:21:45 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:21:45 AM: Validating...
09/16 09:21:48 AM: Evaluate: task edges-srl-ontonotes, batch 43 (157): mcc: 0.7624, acc: 0.6623, precision: 0.8508, recall: 0.6889, f1: 0.7613, edges-srl-ontonotes_loss: 0.0184
09/16 09:21:57 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:21:57 AM: Best result seen so far for macro.
09/16 09:21:57 AM: Updating LR scheduler:
09/16 09:21:57 AM: 	Best result seen so far for macro_avg: 0.784
09/16 09:21:57 AM: 	# validation passes without improvement: 0
09/16 09:21:57 AM: edges-srl-ontonotes_loss: training: 0.017660 validation: 0.017231
09/16 09:21:57 AM: macro_avg: validation: 0.784360
09/16 09:21:57 AM: micro_avg: validation: 0.000000
09/16 09:21:57 AM: edges-srl-ontonotes_mcc: training: 0.777185 validation: 0.784803
09/16 09:21:57 AM: edges-srl-ontonotes_acc: training: 0.673658 validation: 0.689323
09/16 09:21:57 AM: edges-srl-ontonotes_precision: training: 0.838016 validation: 0.864184
09/16 09:21:57 AM: edges-srl-ontonotes_recall: training: 0.726559 validation: 0.718036
09/16 09:21:57 AM: edges-srl-ontonotes_f1: training: 0.778317 validation: 0.784360
09/16 09:21:57 AM: Global learning rate: 0.0001
09/16 09:21:57 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:21:58 AM: Update 6023: task edges-srl-ontonotes, batch 23 (6023): mcc: 0.7956, acc: 0.6875, precision: 0.8555, recall: 0.7452, f1: 0.7966, edges-srl-ontonotes_loss: 0.0166
09/16 09:22:08 AM: Update 6175: task edges-srl-ontonotes, batch 175 (6175): mcc: 0.7897, acc: 0.6855, precision: 0.8441, recall: 0.7443, f1: 0.7911, edges-srl-ontonotes_loss: 0.0169
09/16 09:22:18 AM: Update 6296: task edges-srl-ontonotes, batch 296 (6296): mcc: 0.7947, acc: 0.6935, precision: 0.8490, recall: 0.7493, f1: 0.7961, edges-srl-ontonotes_loss: 0.0166
09/16 09:22:28 AM: Update 6457: task edges-srl-ontonotes, batch 457 (6457): mcc: 0.7983, acc: 0.6999, precision: 0.8517, recall: 0.7536, f1: 0.7997, edges-srl-ontonotes_loss: 0.0162
09/16 09:22:38 AM: Update 6606: task edges-srl-ontonotes, batch 606 (6606): mcc: 0.7988, acc: 0.7016, precision: 0.8521, recall: 0.7542, f1: 0.8002, edges-srl-ontonotes_loss: 0.0162
09/16 09:22:48 AM: Update 6744: task edges-srl-ontonotes, batch 744 (6744): mcc: 0.7921, acc: 0.6931, precision: 0.8464, recall: 0.7468, f1: 0.7935, edges-srl-ontonotes_loss: 0.0167
09/16 09:22:58 AM: Update 6878: task edges-srl-ontonotes, batch 878 (6878): mcc: 0.7876, acc: 0.6876, precision: 0.8428, recall: 0.7417, f1: 0.7890, edges-srl-ontonotes_loss: 0.0170
09/16 09:23:08 AM: Update 6995: task edges-srl-ontonotes, batch 995 (6995): mcc: 0.7815, acc: 0.6799, precision: 0.8380, recall: 0.7345, f1: 0.7829, edges-srl-ontonotes_loss: 0.0174
09/16 09:23:09 AM: ***** Step 7000 / Validation 7 *****
09/16 09:23:09 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:23:09 AM: Validating...
09/16 09:23:19 AM: Evaluate: task edges-srl-ontonotes, batch 104 (157): mcc: 0.7966, acc: 0.7120, precision: 0.8584, recall: 0.7445, f1: 0.7974, edges-srl-ontonotes_loss: 0.0164
09/16 09:23:23 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:23:23 AM: Best result seen so far for macro.
09/16 09:23:23 AM: Updating LR scheduler:
09/16 09:23:23 AM: 	Best result seen so far for macro_avg: 0.799
09/16 09:23:23 AM: 	# validation passes without improvement: 0
09/16 09:23:23 AM: edges-srl-ontonotes_loss: training: 0.017376 validation: 0.016513
09/16 09:23:23 AM: macro_avg: validation: 0.799245
09/16 09:23:23 AM: micro_avg: validation: 0.000000
09/16 09:23:23 AM: edges-srl-ontonotes_mcc: training: 0.781280 validation: 0.798167
09/16 09:23:23 AM: edges-srl-ontonotes_acc: training: 0.679695 validation: 0.715341
09/16 09:23:23 AM: edges-srl-ontonotes_precision: training: 0.837815 validation: 0.855937
09/16 09:23:23 AM: edges-srl-ontonotes_recall: training: 0.734313 validation: 0.749596
09/16 09:23:23 AM: edges-srl-ontonotes_f1: training: 0.782657 validation: 0.799245
09/16 09:23:23 AM: Global learning rate: 0.0001
09/16 09:23:23 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:23:29 AM: Update 7067: task edges-srl-ontonotes, batch 67 (7067): mcc: 0.7494, acc: 0.6413, precision: 0.8083, recall: 0.7014, f1: 0.7510, edges-srl-ontonotes_loss: 0.0195
09/16 09:23:39 AM: Update 7200: task edges-srl-ontonotes, batch 200 (7200): mcc: 0.7485, acc: 0.6393, precision: 0.8095, recall: 0.6985, f1: 0.7499, edges-srl-ontonotes_loss: 0.0196
09/16 09:23:49 AM: Update 7332: task edges-srl-ontonotes, batch 332 (7332): mcc: 0.7551, acc: 0.6495, precision: 0.8179, recall: 0.7035, f1: 0.7564, edges-srl-ontonotes_loss: 0.0192
09/16 09:23:59 AM: Update 7466: task edges-srl-ontonotes, batch 466 (7466): mcc: 0.7581, acc: 0.6522, precision: 0.8206, recall: 0.7067, f1: 0.7594, edges-srl-ontonotes_loss: 0.0189
09/16 09:24:09 AM: Update 7594: task edges-srl-ontonotes, batch 594 (7594): mcc: 0.7614, acc: 0.6564, precision: 0.8237, recall: 0.7099, f1: 0.7626, edges-srl-ontonotes_loss: 0.0187
09/16 09:24:19 AM: Update 7741: task edges-srl-ontonotes, batch 741 (7741): mcc: 0.7635, acc: 0.6587, precision: 0.8256, recall: 0.7121, f1: 0.7647, edges-srl-ontonotes_loss: 0.0185
09/16 09:24:30 AM: Update 7873: task edges-srl-ontonotes, batch 873 (7873): mcc: 0.7666, acc: 0.6620, precision: 0.8281, recall: 0.7157, f1: 0.7678, edges-srl-ontonotes_loss: 0.0183
09/16 09:24:40 AM: ***** Step 8000 / Validation 8 *****
09/16 09:24:40 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:24:40 AM: Validating...
09/16 09:24:40 AM: Evaluate: task edges-srl-ontonotes, batch 1 (157): mcc: 0.7821, acc: 0.7079, precision: 0.8442, recall: 0.7303, f1: 0.7831, edges-srl-ontonotes_loss: 0.0164
09/16 09:24:50 AM: Evaluate: task edges-srl-ontonotes, batch 133 (157): mcc: 0.8111, acc: 0.7343, precision: 0.8639, recall: 0.7666, f1: 0.8123, edges-srl-ontonotes_loss: 0.0151
09/16 09:24:52 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:24:52 AM: Best result seen so far for macro.
09/16 09:24:52 AM: Updating LR scheduler:
09/16 09:24:52 AM: 	Best result seen so far for macro_avg: 0.808
09/16 09:24:52 AM: 	# validation passes without improvement: 0
09/16 09:24:52 AM: edges-srl-ontonotes_loss: training: 0.018324 validation: 0.015526
09/16 09:24:52 AM: macro_avg: validation: 0.807631
09/16 09:24:52 AM: micro_avg: validation: 0.000000
09/16 09:24:52 AM: edges-srl-ontonotes_mcc: training: 0.766432 validation: 0.806546
09/16 09:24:52 AM: edges-srl-ontonotes_acc: training: 0.662059 validation: 0.727812
09/16 09:24:52 AM: edges-srl-ontonotes_precision: training: 0.827602 validation: 0.862539
09/16 09:24:52 AM: edges-srl-ontonotes_recall: training: 0.715839 validation: 0.759295
09/16 09:24:52 AM: edges-srl-ontonotes_f1: training: 0.767674 validation: 0.807631
09/16 09:24:52 AM: Global learning rate: 0.0001
09/16 09:24:52 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:25:00 AM: Update 8114: task edges-srl-ontonotes, batch 114 (8114): mcc: 0.7645, acc: 0.6656, precision: 0.8253, recall: 0.7144, f1: 0.7658, edges-srl-ontonotes_loss: 0.0182
09/16 09:25:10 AM: Update 8206: task edges-srl-ontonotes, batch 206 (8206): mcc: 0.7580, acc: 0.6567, precision: 0.8187, recall: 0.7082, f1: 0.7594, edges-srl-ontonotes_loss: 0.0187
09/16 09:25:20 AM: Update 8341: task edges-srl-ontonotes, batch 341 (8341): mcc: 0.7540, acc: 0.6515, precision: 0.8151, recall: 0.7039, f1: 0.7554, edges-srl-ontonotes_loss: 0.0190
09/16 09:25:30 AM: Update 8471: task edges-srl-ontonotes, batch 471 (8471): mcc: 0.7518, acc: 0.6485, precision: 0.8142, recall: 0.7006, f1: 0.7531, edges-srl-ontonotes_loss: 0.0192
09/16 09:25:40 AM: Update 8591: task edges-srl-ontonotes, batch 591 (8591): mcc: 0.7508, acc: 0.6469, precision: 0.8135, recall: 0.6993, f1: 0.7521, edges-srl-ontonotes_loss: 0.0192
09/16 09:25:50 AM: Update 8721: task edges-srl-ontonotes, batch 721 (8721): mcc: 0.7512, acc: 0.6471, precision: 0.8142, recall: 0.6996, f1: 0.7525, edges-srl-ontonotes_loss: 0.0192
09/16 09:26:00 AM: Update 8854: task edges-srl-ontonotes, batch 854 (8854): mcc: 0.7499, acc: 0.6459, precision: 0.8130, recall: 0.6982, f1: 0.7512, edges-srl-ontonotes_loss: 0.0192
09/16 09:26:10 AM: Update 8980: task edges-srl-ontonotes, batch 980 (8980): mcc: 0.7462, acc: 0.6409, precision: 0.8101, recall: 0.6938, f1: 0.7474, edges-srl-ontonotes_loss: 0.0195
09/16 09:26:12 AM: ***** Step 9000 / Validation 9 *****
09/16 09:26:12 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:26:12 AM: Validating...
09/16 09:26:20 AM: Evaluate: task edges-srl-ontonotes, batch 106 (157): mcc: 0.8056, acc: 0.7273, precision: 0.8575, recall: 0.7621, f1: 0.8070, edges-srl-ontonotes_loss: 0.0154
09/16 09:26:24 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:26:24 AM: Best result seen so far for macro.
09/16 09:26:24 AM: Updating LR scheduler:
09/16 09:26:24 AM: 	Best result seen so far for macro_avg: 0.809
09/16 09:26:24 AM: 	# validation passes without improvement: 0
09/16 09:26:24 AM: edges-srl-ontonotes_loss: training: 0.019534 validation: 0.015443
09/16 09:26:24 AM: macro_avg: validation: 0.808557
09/16 09:26:24 AM: micro_avg: validation: 0.000000
09/16 09:26:24 AM: edges-srl-ontonotes_mcc: training: 0.745777 validation: 0.807120
09/16 09:26:24 AM: edges-srl-ontonotes_acc: training: 0.640552 validation: 0.729428
09/16 09:26:24 AM: edges-srl-ontonotes_precision: training: 0.809936 validation: 0.857192
09/16 09:26:24 AM: edges-srl-ontonotes_recall: training: 0.693219 validation: 0.765145
09/16 09:26:24 AM: edges-srl-ontonotes_f1: training: 0.747046 validation: 0.808557
09/16 09:26:24 AM: Global learning rate: 0.0001
09/16 09:26:24 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:26:30 AM: Update 9076: task edges-srl-ontonotes, batch 76 (9076): mcc: 0.7403, acc: 0.6352, precision: 0.8057, recall: 0.6868, f1: 0.7415, edges-srl-ontonotes_loss: 0.0202
09/16 09:26:40 AM: Update 9164: task edges-srl-ontonotes, batch 164 (9164): mcc: 0.7368, acc: 0.6289, precision: 0.8009, recall: 0.6845, f1: 0.7381, edges-srl-ontonotes_loss: 0.0203
09/16 09:26:50 AM: Update 9294: task edges-srl-ontonotes, batch 294 (9294): mcc: 0.7347, acc: 0.6243, precision: 0.8010, recall: 0.6807, f1: 0.7360, edges-srl-ontonotes_loss: 0.0204
09/16 09:27:00 AM: Update 9432: task edges-srl-ontonotes, batch 432 (9432): mcc: 0.7348, acc: 0.6247, precision: 0.8023, recall: 0.6797, f1: 0.7359, edges-srl-ontonotes_loss: 0.0203
09/16 09:27:10 AM: Update 9545: task edges-srl-ontonotes, batch 545 (9545): mcc: 0.7402, acc: 0.6319, precision: 0.8064, recall: 0.6861, f1: 0.7414, edges-srl-ontonotes_loss: 0.0200
09/16 09:27:20 AM: Update 9668: task edges-srl-ontonotes, batch 668 (9668): mcc: 0.7434, acc: 0.6368, precision: 0.8079, recall: 0.6906, f1: 0.7446, edges-srl-ontonotes_loss: 0.0197
09/16 09:27:30 AM: Update 9781: task edges-srl-ontonotes, batch 781 (9781): mcc: 0.7449, acc: 0.6394, precision: 0.8092, recall: 0.6923, f1: 0.7462, edges-srl-ontonotes_loss: 0.0196
09/16 09:27:40 AM: Update 9918: task edges-srl-ontonotes, batch 918 (9918): mcc: 0.7469, acc: 0.6424, precision: 0.8103, recall: 0.6949, f1: 0.7482, edges-srl-ontonotes_loss: 0.0195
09/16 09:27:47 AM: ***** Step 10000 / Validation 10 *****
09/16 09:27:47 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:27:47 AM: Validating...
09/16 09:27:50 AM: Evaluate: task edges-srl-ontonotes, batch 34 (157): mcc: 0.7925, acc: 0.7164, precision: 0.8532, recall: 0.7416, f1: 0.7935, edges-srl-ontonotes_loss: 0.0162
09/16 09:28:00 AM: Updating LR scheduler:
09/16 09:28:00 AM: 	Best result seen so far for macro_avg: 0.809
09/16 09:28:00 AM: 	# validation passes without improvement: 1
09/16 09:28:00 AM: edges-srl-ontonotes_loss: training: 0.019387 validation: 0.015764
09/16 09:28:00 AM: macro_avg: validation: 0.802832
09/16 09:28:00 AM: micro_avg: validation: 0.000000
09/16 09:28:00 AM: edges-srl-ontonotes_mcc: training: 0.747839 validation: 0.802007
09/16 09:28:00 AM: edges-srl-ontonotes_acc: training: 0.643419 validation: 0.725502
09/16 09:28:00 AM: edges-srl-ontonotes_precision: training: 0.811317 validation: 0.862780
09/16 09:28:00 AM: edges-srl-ontonotes_recall: training: 0.695808 validation: 0.750674
09/16 09:28:00 AM: edges-srl-ontonotes_f1: training: 0.749136 validation: 0.802832
09/16 09:28:00 AM: Global learning rate: 0.0001
09/16 09:28:00 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:28:00 AM: Update 10001: task edges-srl-ontonotes, batch 1 (10001): mcc: 0.7956, acc: 0.7229, precision: 0.8400, recall: 0.7590, f1: 0.7975, edges-srl-ontonotes_loss: 0.0166
09/16 09:28:11 AM: Update 10094: task edges-srl-ontonotes, batch 94 (10094): mcc: 0.7728, acc: 0.6799, precision: 0.8337, recall: 0.7223, f1: 0.7740, edges-srl-ontonotes_loss: 0.0180
09/16 09:28:21 AM: Update 10222: task edges-srl-ontonotes, batch 222 (10222): mcc: 0.7697, acc: 0.6738, precision: 0.8294, recall: 0.7203, f1: 0.7710, edges-srl-ontonotes_loss: 0.0182
09/16 09:28:31 AM: Update 10357: task edges-srl-ontonotes, batch 357 (10357): mcc: 0.7678, acc: 0.6711, precision: 0.8274, recall: 0.7184, f1: 0.7691, edges-srl-ontonotes_loss: 0.0182
09/16 09:28:41 AM: Update 10477: task edges-srl-ontonotes, batch 477 (10477): mcc: 0.7652, acc: 0.6678, precision: 0.8250, recall: 0.7159, f1: 0.7666, edges-srl-ontonotes_loss: 0.0184
09/16 09:28:51 AM: Update 10598: task edges-srl-ontonotes, batch 598 (10598): mcc: 0.7641, acc: 0.6661, precision: 0.8248, recall: 0.7141, f1: 0.7655, edges-srl-ontonotes_loss: 0.0184
09/16 09:29:01 AM: Update 10713: task edges-srl-ontonotes, batch 713 (10713): mcc: 0.7630, acc: 0.6648, precision: 0.8244, recall: 0.7124, f1: 0.7643, edges-srl-ontonotes_loss: 0.0185
09/16 09:29:11 AM: Update 10845: task edges-srl-ontonotes, batch 845 (10845): mcc: 0.7621, acc: 0.6630, precision: 0.8242, recall: 0.7109, f1: 0.7633, edges-srl-ontonotes_loss: 0.0185
09/16 09:29:21 AM: Update 10971: task edges-srl-ontonotes, batch 971 (10971): mcc: 0.7612, acc: 0.6618, precision: 0.8236, recall: 0.7096, f1: 0.7624, edges-srl-ontonotes_loss: 0.0186
09/16 09:29:23 AM: ***** Step 11000 / Validation 11 *****
09/16 09:29:23 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:29:23 AM: Validating...
09/16 09:29:31 AM: Evaluate: task edges-srl-ontonotes, batch 102 (157): mcc: 0.7900, acc: 0.7104, precision: 0.8525, recall: 0.7375, f1: 0.7908, edges-srl-ontonotes_loss: 0.0165
09/16 09:29:35 AM: Updating LR scheduler:
09/16 09:29:35 AM: 	Best result seen so far for macro_avg: 0.809
09/16 09:29:35 AM: 	# validation passes without improvement: 2
09/16 09:29:35 AM: edges-srl-ontonotes_loss: training: 0.018573 validation: 0.016150
09/16 09:29:35 AM: macro_avg: validation: 0.799442
09/16 09:29:35 AM: micro_avg: validation: 0.000000
09/16 09:29:35 AM: edges-srl-ontonotes_mcc: training: 0.761103 validation: 0.798345
09/16 09:29:35 AM: edges-srl-ontonotes_acc: training: 0.661738 validation: 0.721576
09/16 09:29:35 AM: edges-srl-ontonotes_precision: training: 0.823628 validation: 0.855788
09/16 09:29:35 AM: edges-srl-ontonotes_recall: training: 0.709491 validation: 0.750058
09/16 09:29:35 AM: edges-srl-ontonotes_f1: training: 0.762311 validation: 0.799442
09/16 09:29:35 AM: Global learning rate: 0.0001
09/16 09:29:35 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:29:41 AM: Update 11057: task edges-srl-ontonotes, batch 57 (11057): mcc: 0.7538, acc: 0.6493, precision: 0.8198, recall: 0.6995, f1: 0.7549, edges-srl-ontonotes_loss: 0.0190
09/16 09:29:51 AM: Update 11187: task edges-srl-ontonotes, batch 187 (11187): mcc: 0.7527, acc: 0.6536, precision: 0.8187, recall: 0.6984, f1: 0.7538, edges-srl-ontonotes_loss: 0.0192
09/16 09:30:01 AM: Update 11312: task edges-srl-ontonotes, batch 312 (11312): mcc: 0.7530, acc: 0.6537, precision: 0.8192, recall: 0.6985, f1: 0.7541, edges-srl-ontonotes_loss: 0.0190
09/16 09:30:11 AM: Update 11402: task edges-srl-ontonotes, batch 402 (11402): mcc: 0.7548, acc: 0.6566, precision: 0.8199, recall: 0.7012, f1: 0.7559, edges-srl-ontonotes_loss: 0.0189
09/16 09:30:21 AM: Update 11537: task edges-srl-ontonotes, batch 537 (11537): mcc: 0.7595, acc: 0.6626, precision: 0.8226, recall: 0.7073, f1: 0.7606, edges-srl-ontonotes_loss: 0.0187
09/16 09:30:31 AM: Update 11655: task edges-srl-ontonotes, batch 655 (11655): mcc: 0.7609, acc: 0.6646, precision: 0.8234, recall: 0.7094, f1: 0.7621, edges-srl-ontonotes_loss: 0.0186
09/16 09:30:41 AM: Update 11793: task edges-srl-ontonotes, batch 793 (11793): mcc: 0.7625, acc: 0.6667, precision: 0.8246, recall: 0.7112, f1: 0.7637, edges-srl-ontonotes_loss: 0.0185
09/16 09:30:51 AM: Update 11922: task edges-srl-ontonotes, batch 922 (11922): mcc: 0.7637, acc: 0.6682, precision: 0.8253, recall: 0.7128, f1: 0.7649, edges-srl-ontonotes_loss: 0.0184
09/16 09:30:59 AM: ***** Step 12000 / Validation 12 *****
09/16 09:30:59 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:30:59 AM: Validating...
09/16 09:31:01 AM: Evaluate: task edges-srl-ontonotes, batch 30 (157): mcc: 0.7966, acc: 0.7187, precision: 0.8584, recall: 0.7445, f1: 0.7974, edges-srl-ontonotes_loss: 0.0162
09/16 09:31:11 AM: Evaluate: task edges-srl-ontonotes, batch 154 (157): mcc: 0.7969, acc: 0.7169, precision: 0.8626, recall: 0.7415, f1: 0.7975, edges-srl-ontonotes_loss: 0.0162
09/16 09:31:11 AM: Updating LR scheduler:
09/16 09:31:11 AM: 	Best result seen so far for macro_avg: 0.809
09/16 09:31:11 AM: 	# validation passes without improvement: 3
09/16 09:31:11 AM: edges-srl-ontonotes_loss: training: 0.018577 validation: 0.016323
09/16 09:31:11 AM: macro_avg: validation: 0.797036
09/16 09:31:11 AM: micro_avg: validation: 0.000000
09/16 09:31:11 AM: edges-srl-ontonotes_mcc: training: 0.761320 validation: 0.796494
09/16 09:31:11 AM: edges-srl-ontonotes_acc: training: 0.665306 validation: 0.716265
09/16 09:31:11 AM: edges-srl-ontonotes_precision: training: 0.823524 validation: 0.862171
09/16 09:31:11 AM: edges-srl-ontonotes_recall: training: 0.709980 validation: 0.741051
09/16 09:31:11 AM: edges-srl-ontonotes_f1: training: 0.762549 validation: 0.797036
09/16 09:31:11 AM: Global learning rate: 0.0001
09/16 09:31:11 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:31:21 AM: Update 12105: task edges-srl-ontonotes, batch 105 (12105): mcc: 0.7442, acc: 0.6448, precision: 0.8169, recall: 0.6844, f1: 0.7448, edges-srl-ontonotes_loss: 0.0197
09/16 09:31:31 AM: Update 12222: task edges-srl-ontonotes, batch 222 (12222): mcc: 0.7481, acc: 0.6487, precision: 0.8193, recall: 0.6895, f1: 0.7488, edges-srl-ontonotes_loss: 0.0194
09/16 09:31:41 AM: Update 12320: task edges-srl-ontonotes, batch 320 (12320): mcc: 0.7533, acc: 0.6551, precision: 0.8223, recall: 0.6962, f1: 0.7540, edges-srl-ontonotes_loss: 0.0191
09/16 09:31:51 AM: Update 12459: task edges-srl-ontonotes, batch 459 (12459): mcc: 0.7596, acc: 0.6617, precision: 0.8264, recall: 0.7044, f1: 0.7605, edges-srl-ontonotes_loss: 0.0185
09/16 09:32:01 AM: Update 12596: task edges-srl-ontonotes, batch 596 (12596): mcc: 0.7670, acc: 0.6708, precision: 0.8302, recall: 0.7145, f1: 0.7681, edges-srl-ontonotes_loss: 0.0180
09/16 09:32:12 AM: Update 12753: task edges-srl-ontonotes, batch 753 (12753): mcc: 0.7771, acc: 0.6829, precision: 0.8371, recall: 0.7273, f1: 0.7783, edges-srl-ontonotes_loss: 0.0174
09/16 09:32:22 AM: Update 12897: task edges-srl-ontonotes, batch 897 (12897): mcc: 0.7833, acc: 0.6908, precision: 0.8410, recall: 0.7353, f1: 0.7846, edges-srl-ontonotes_loss: 0.0170
09/16 09:32:29 AM: ***** Step 13000 / Validation 13 *****
09/16 09:32:29 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:32:29 AM: Validating...
09/16 09:32:32 AM: Evaluate: task edges-srl-ontonotes, batch 38 (157): mcc: 0.7950, acc: 0.7146, precision: 0.8584, recall: 0.7416, f1: 0.7957, edges-srl-ontonotes_loss: 0.0164
09/16 09:32:40 AM: Updating LR scheduler:
09/16 09:32:40 AM: 	Best result seen so far for macro_avg: 0.809
09/16 09:32:40 AM: 	# validation passes without improvement: 0
09/16 09:32:40 AM: edges-srl-ontonotes_loss: training: 0.016767 validation: 0.015660
09/16 09:32:40 AM: macro_avg: validation: 0.807355
09/16 09:32:40 AM: micro_avg: validation: 0.000000
09/16 09:32:40 AM: edges-srl-ontonotes_mcc: training: 0.785920 validation: 0.806288
09/16 09:32:40 AM: edges-srl-ontonotes_acc: training: 0.693961 validation: 0.729197
09/16 09:32:40 AM: edges-srl-ontonotes_precision: training: 0.842525 validation: 0.862606
09/16 09:32:40 AM: edges-srl-ontonotes_recall: training: 0.738749 validation: 0.758756
09/16 09:32:40 AM: edges-srl-ontonotes_f1: training: 0.787232 validation: 0.807355
09/16 09:32:40 AM: Global learning rate: 5e-05
09/16 09:32:40 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:32:42 AM: Update 13021: task edges-srl-ontonotes, batch 21 (13021): mcc: 0.7945, acc: 0.7095, precision: 0.8394, recall: 0.7577, f1: 0.7964, edges-srl-ontonotes_loss: 0.0162
09/16 09:32:52 AM: Update 13182: task edges-srl-ontonotes, batch 182 (13182): mcc: 0.8133, acc: 0.7278, precision: 0.8586, recall: 0.7755, f1: 0.8149, edges-srl-ontonotes_loss: 0.0150
09/16 09:33:02 AM: Update 13340: task edges-srl-ontonotes, batch 340 (13340): mcc: 0.8127, acc: 0.7272, precision: 0.8580, recall: 0.7749, f1: 0.8143, edges-srl-ontonotes_loss: 0.0151
09/16 09:33:12 AM: Update 13506: task edges-srl-ontonotes, batch 506 (13506): mcc: 0.8156, acc: 0.7315, precision: 0.8600, recall: 0.7786, f1: 0.8172, edges-srl-ontonotes_loss: 0.0149
09/16 09:33:22 AM: Update 13631: task edges-srl-ontonotes, batch 631 (13631): mcc: 0.8169, acc: 0.7329, precision: 0.8617, recall: 0.7795, f1: 0.8185, edges-srl-ontonotes_loss: 0.0149
09/16 09:33:32 AM: Update 13803: task edges-srl-ontonotes, batch 803 (13803): mcc: 0.8197, acc: 0.7367, precision: 0.8640, recall: 0.7826, f1: 0.8213, edges-srl-ontonotes_loss: 0.0147
09/16 09:33:42 AM: Update 13922: task edges-srl-ontonotes, batch 922 (13922): mcc: 0.8152, acc: 0.7311, precision: 0.8607, recall: 0.7772, f1: 0.8168, edges-srl-ontonotes_loss: 0.0151
09/16 09:33:48 AM: ***** Step 14000 / Validation 14 *****
09/16 09:33:48 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:33:48 AM: Validating...
09/16 09:33:52 AM: Evaluate: task edges-srl-ontonotes, batch 54 (157): mcc: 0.8013, acc: 0.7260, precision: 0.8692, recall: 0.7439, f1: 0.8017, edges-srl-ontonotes_loss: 0.0160
09/16 09:34:00 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:34:00 AM: Best result seen so far for macro.
09/16 09:34:00 AM: Updating LR scheduler:
09/16 09:34:00 AM: 	Best result seen so far for macro_avg: 0.819
09/16 09:34:00 AM: 	# validation passes without improvement: 0
09/16 09:34:00 AM: edges-srl-ontonotes_loss: training: 0.015211 validation: 0.014882
09/16 09:34:00 AM: macro_avg: validation: 0.818638
09/16 09:34:00 AM: micro_avg: validation: 0.000000
09/16 09:34:00 AM: edges-srl-ontonotes_mcc: training: 0.813127 validation: 0.818079
09/16 09:34:00 AM: edges-srl-ontonotes_acc: training: 0.728503 validation: 0.746517
09/16 09:34:00 AM: edges-srl-ontonotes_precision: training: 0.859366 validation: 0.879766
09/16 09:34:00 AM: edges-srl-ontonotes_recall: training: 0.774441 validation: 0.765453
09/16 09:34:00 AM: edges-srl-ontonotes_f1: training: 0.814696 validation: 0.818638
09/16 09:34:00 AM: Global learning rate: 5e-05
09/16 09:34:00 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:34:02 AM: Update 14020: task edges-srl-ontonotes, batch 20 (14020): mcc: 0.7825, acc: 0.6910, precision: 0.8354, recall: 0.7387, f1: 0.7841, edges-srl-ontonotes_loss: 0.0175
09/16 09:34:12 AM: Update 14143: task edges-srl-ontonotes, batch 143 (14143): mcc: 0.7867, acc: 0.6961, precision: 0.8412, recall: 0.7413, f1: 0.7881, edges-srl-ontonotes_loss: 0.0171
09/16 09:34:22 AM: Update 14265: task edges-srl-ontonotes, batch 265 (14265): mcc: 0.7748, acc: 0.6814, precision: 0.8305, recall: 0.7288, f1: 0.7763, edges-srl-ontonotes_loss: 0.0178
09/16 09:34:32 AM: Update 14397: task edges-srl-ontonotes, batch 397 (14397): mcc: 0.7739, acc: 0.6807, precision: 0.8298, recall: 0.7277, f1: 0.7754, edges-srl-ontonotes_loss: 0.0179
09/16 09:34:42 AM: Update 14493: task edges-srl-ontonotes, batch 493 (14493): mcc: 0.7734, acc: 0.6803, precision: 0.8303, recall: 0.7264, f1: 0.7749, edges-srl-ontonotes_loss: 0.0179
09/16 09:34:52 AM: Update 14645: task edges-srl-ontonotes, batch 645 (14645): mcc: 0.7772, acc: 0.6843, precision: 0.8340, recall: 0.7301, f1: 0.7786, edges-srl-ontonotes_loss: 0.0176
09/16 09:35:02 AM: Update 14777: task edges-srl-ontonotes, batch 777 (14777): mcc: 0.7794, acc: 0.6866, precision: 0.8362, recall: 0.7322, f1: 0.7807, edges-srl-ontonotes_loss: 0.0175
09/16 09:35:12 AM: Update 14901: task edges-srl-ontonotes, batch 901 (14901): mcc: 0.7809, acc: 0.6885, precision: 0.8374, recall: 0.7340, f1: 0.7823, edges-srl-ontonotes_loss: 0.0173
09/16 09:35:21 AM: ***** Step 15000 / Validation 15 *****
09/16 09:35:21 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:35:21 AM: Validating...
09/16 09:35:23 AM: Evaluate: task edges-srl-ontonotes, batch 23 (157): mcc: 0.8259, acc: 0.7581, precision: 0.8771, recall: 0.7823, f1: 0.8270, edges-srl-ontonotes_loss: 0.0140
09/16 09:35:32 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:35:32 AM: Best result seen so far for macro.
09/16 09:35:32 AM: Updating LR scheduler:
09/16 09:35:32 AM: 	Best result seen so far for macro_avg: 0.828
09/16 09:35:32 AM: 	# validation passes without improvement: 0
09/16 09:35:32 AM: edges-srl-ontonotes_loss: training: 0.017273 validation: 0.014171
09/16 09:35:32 AM: macro_avg: validation: 0.828268
09/16 09:35:32 AM: micro_avg: validation: 0.000000
09/16 09:35:32 AM: edges-srl-ontonotes_mcc: training: 0.782145 validation: 0.826957
09/16 09:35:32 AM: edges-srl-ontonotes_acc: training: 0.689998 validation: 0.761142
09/16 09:35:32 AM: edges-srl-ontonotes_precision: training: 0.838571 validation: 0.874476
09/16 09:35:32 AM: edges-srl-ontonotes_recall: training: 0.735248 validation: 0.786698
09/16 09:35:32 AM: edges-srl-ontonotes_f1: training: 0.783518 validation: 0.828268
09/16 09:35:32 AM: Global learning rate: 5e-05
09/16 09:35:32 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sts-top/run
09/16 09:35:33 AM: Update 15002: task edges-srl-ontonotes, batch 2 (15002): mcc: 0.8197, acc: 0.7605, precision: 0.8428, recall: 0.8024, f1: 0.8221, edges-srl-ontonotes_loss: 0.0137
09/16 09:35:43 AM: Update 15120: task edges-srl-ontonotes, batch 120 (15120): mcc: 0.7946, acc: 0.7053, precision: 0.8465, recall: 0.7513, f1: 0.7961, edges-srl-ontonotes_loss: 0.0162
09/16 09:35:53 AM: Update 15251: task edges-srl-ontonotes, batch 251 (15251): mcc: 0.7864, acc: 0.6972, precision: 0.8405, recall: 0.7415, f1: 0.7879, edges-srl-ontonotes_loss: 0.0169
09/16 09:36:03 AM: Update 15393: task edges-srl-ontonotes, batch 393 (15393): mcc: 0.7851, acc: 0.6974, precision: 0.8391, recall: 0.7403, f1: 0.7866, edges-srl-ontonotes_loss: 0.0170
09/16 09:36:13 AM: Update 15497: task edges-srl-ontonotes, batch 497 (15497): mcc: 0.7833, acc: 0.6941, precision: 0.8377, recall: 0.7381, f1: 0.7847, edges-srl-ontonotes_loss: 0.0172
09/16 09:36:23 AM: Update 15633: task edges-srl-ontonotes, batch 633 (15633): mcc: 0.7792, acc: 0.6891, precision: 0.8355, recall: 0.7325, f1: 0.7806, edges-srl-ontonotes_loss: 0.0175
09/16 09:36:33 AM: Update 15756: task edges-srl-ontonotes, batch 756 (15756): mcc: 0.7777, acc: 0.6868, precision: 0.8349, recall: 0.7303, f1: 0.7791, edges-srl-ontonotes_loss: 0.0176
