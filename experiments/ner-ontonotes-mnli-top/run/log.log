09/16 09:13:23 AM: Git branch: master
09/16 09:13:23 AM: Git SHA: 3ca0f74688379229ab3eec908a215358ad18b3f4
09/16 09:13:23 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-mnli-top/",
  "exp_name": "experiments/ner-ontonotes-mnli-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-mnli-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/mnli",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-mnli-top__run",
  "run_dir": "./experiments/ner-ontonotes-mnli-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:13:23 AM: Saved config to ./experiments/ner-ontonotes-mnli-top/run/params.conf
09/16 09:13:23 AM: Using random seed 1234
09/16 09:13:58 AM: Using GPU 0
09/16 09:13:58 AM: Loading tasks...
09/16 09:13:58 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-mnli-top/
09/16 09:13:58 AM: 	Creating task edges-ner-ontonotes from scratch.
09/16 09:13:59 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 09:13:59 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 09:13:59 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 09:14:00 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 09:14:00 AM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 09:14:00 AM: 	Building vocab from scratch.
09/16 09:14:00 AM: 	Counting units for task edges-ner-ontonotes.
09/16 09:14:03 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 09:14:04 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:04 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:14:04 AM: 	Saved vocab to ./experiments/ner-ontonotes-mnli-top/vocab
09/16 09:14:04 AM: Loading token dictionary from ./experiments/ner-ontonotes-mnli-top/vocab.
09/16 09:14:04 AM: 	Loaded vocab from ./experiments/ner-ontonotes-mnli-top/vocab
09/16 09:14:04 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 09:14:04 AM: 	Vocab namespace tokens: size 22840
09/16 09:14:04 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:14:04 AM: 	Vocab namespace chars: size 77
09/16 09:14:04 AM: 	Finished building vocab.
09/16 09:14:04 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 09:14:18 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-mnli-top/preproc/edges-ner-ontonotes__train_data
09/16 09:14:18 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 09:14:20 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-mnli-top/preproc/edges-ner-ontonotes__val_data
09/16 09:14:20 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 09:14:21 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-mnli-top/preproc/edges-ner-ontonotes__test_data
09/16 09:14:21 AM: 	Finished indexing tasks
09/16 09:14:21 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 09:14:21 AM: 	  Training on 
09/16 09:14:21 AM: 	  Evaluating on edges-ner-ontonotes
09/16 09:14:21 AM: 	Finished loading tasks in 23.822s
09/16 09:14:21 AM: 	 Tasks: ['edges-ner-ontonotes']
09/16 09:14:21 AM: Building model...
09/16 09:14:21 AM: Using BERT model (bert-base-uncased).
09/16 09:14:21 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:14:21 AM: models/mnli
09/16 09:14:21 AM: loading configuration file models/mnli/config.json
09/16 09:14:21 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 3,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:14:21 AM: loading weights file models/mnli/pytorch_model.bin
09/16 09:14:26 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmps96i57h_
09/16 09:14:29 AM: copying /tmp/tmps96i57h_ to cache at ./experiments/ner-ontonotes-mnli-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: creating metadata file for ./experiments/ner-ontonotes-mnli-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:29 AM: removing temp file /tmp/tmps96i57h_
09/16 09:14:29 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-mnli-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:14:30 AM: Initializing parameters
09/16 09:14:30 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:14:30 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:14:30 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 09:15:04 AM: Model specification:
09/16 09:15:04 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 09:15:04 AM: Model parameters:
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:15:04 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:15:04 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:15:04 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:15:04 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 09:15:04 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 09:15:04 AM: Total number of parameters: 109688338 (1.09688e+08)
09/16 09:15:04 AM: Number of trainable parameters: 206098 (206098)
09/16 09:15:04 AM: Finished building model in 42.257s
09/16 09:15:04 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 09:15:07 AM: patience = 9
09/16 09:15:07 AM: val_interval = 1000
09/16 09:15:07 AM: max_vals = 250
09/16 09:15:07 AM: cuda_device = 0
09/16 09:15:07 AM: grad_norm = 5.0
09/16 09:15:07 AM: grad_clipping = None
09/16 09:15:07 AM: lr_decay = 0.99
09/16 09:15:07 AM: min_lr = 1e-06
09/16 09:15:07 AM: keep_all_checkpoints = 0
09/16 09:15:07 AM: val_data_limit = 5000
09/16 09:15:07 AM: max_epochs = -1
09/16 09:15:07 AM: dec_val_scale = 250
09/16 09:15:07 AM: training_data_fraction = 1
09/16 09:15:07 AM: type = adam
09/16 09:15:07 AM: parameter_groups = None
09/16 09:15:07 AM: Number of trainable parameters: 206098
09/16 09:15:07 AM: infer_type_and_cast = True
09/16 09:15:07 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:15:07 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:15:07 AM: lr = 0.0001
09/16 09:15:07 AM: amsgrad = True
09/16 09:15:07 AM: type = reduce_on_plateau
09/16 09:15:07 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:15:07 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:15:07 AM: mode = max
09/16 09:15:07 AM: factor = 0.5
09/16 09:15:07 AM: patience = 3
09/16 09:15:07 AM: threshold = 0.0001
09/16 09:15:07 AM: threshold_mode = abs
09/16 09:15:07 AM: verbose = True
09/16 09:15:07 AM: type = adam
09/16 09:15:07 AM: parameter_groups = None
09/16 09:15:07 AM: Number of trainable parameters: 206098
09/16 09:15:07 AM: infer_type_and_cast = True
09/16 09:15:07 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:15:07 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:15:07 AM: lr = 0.0001
09/16 09:15:07 AM: amsgrad = True
09/16 09:15:07 AM: type = reduce_on_plateau
09/16 09:15:07 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:15:07 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:15:07 AM: mode = max
09/16 09:15:07 AM: factor = 0.5
09/16 09:15:07 AM: patience = 3
09/16 09:15:07 AM: threshold = 0.0001
09/16 09:15:07 AM: threshold_mode = abs
09/16 09:15:07 AM: verbose = True
09/16 09:15:07 AM: Starting training without restoring from a checkpoint.
09/16 09:15:07 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 09:15:07 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 09:15:17 AM: Update 36: task edges-ner-ontonotes, batch 36 (36): mcc: 0.0135, acc: 0.0131, precision: 0.0643, recall: 0.1271, f1: 0.0854, edges-ner-ontonotes_loss: 0.4741
09/16 09:15:28 AM: Update 185: task edges-ner-ontonotes, batch 185 (185): mcc: 0.1123, acc: 0.0794, precision: 0.2084, recall: 0.1034, f1: 0.1382, edges-ner-ontonotes_loss: 0.2166
09/16 09:15:39 AM: Update 314: task edges-ner-ontonotes, batch 314 (314): mcc: 0.3132, acc: 0.2194, precision: 0.4888, recall: 0.2347, f1: 0.3172, edges-ner-ontonotes_loss: 0.1707
09/16 09:15:49 AM: Update 444: task edges-ner-ontonotes, batch 444 (444): mcc: 0.4691, acc: 0.3480, precision: 0.6640, recall: 0.3615, f1: 0.4682, edges-ner-ontonotes_loss: 0.1447
09/16 09:15:59 AM: Update 569: task edges-ner-ontonotes, batch 569 (569): mcc: 0.5550, acc: 0.4298, precision: 0.7419, recall: 0.4430, f1: 0.5547, edges-ner-ontonotes_loss: 0.1281
09/16 09:16:09 AM: Update 675: task edges-ner-ontonotes, batch 675 (675): mcc: 0.6040, acc: 0.4798, precision: 0.7819, recall: 0.4927, f1: 0.6045, edges-ner-ontonotes_loss: 0.1175
09/16 09:16:19 AM: Update 799: task edges-ner-ontonotes, batch 799 (799): mcc: 0.6480, acc: 0.5277, precision: 0.8139, recall: 0.5405, f1: 0.6496, edges-ner-ontonotes_loss: 0.1077
09/16 09:16:29 AM: Update 926: task edges-ner-ontonotes, batch 926 (926): mcc: 0.6824, acc: 0.5676, precision: 0.8359, recall: 0.5804, f1: 0.6851, edges-ner-ontonotes_loss: 0.0993
09/16 09:16:37 AM: ***** Step 1000 / Validation 1 *****
09/16 09:16:37 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:16:37 AM: Validating...
09/16 09:16:39 AM: Evaluate: task edges-ner-ontonotes, batch 23 (157): mcc: 0.7605, acc: 0.6932, precision: 0.8403, recall: 0.7102, f1: 0.7698, edges-ner-ontonotes_loss: 0.0739
09/16 09:16:49 AM: Evaluate: task edges-ner-ontonotes, batch 83 (157): mcc: 0.8421, acc: 0.7821, precision: 0.9105, recall: 0.7938, f1: 0.8481, edges-ner-ontonotes_loss: 0.0553
09/16 09:16:59 AM: Evaluate: task edges-ner-ontonotes, batch 134 (157): mcc: 0.8591, acc: 0.8038, precision: 0.9201, recall: 0.8157, f1: 0.8648, edges-ner-ontonotes_loss: 0.0493
09/16 09:17:02 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:17:03 AM: Best result seen so far for micro.
09/16 09:17:03 AM: Best result seen so far for macro.
09/16 09:17:03 AM: Updating LR scheduler:
09/16 09:17:03 AM: 	Best result seen so far for macro_avg: 0.863
09/16 09:17:03 AM: 	# validation passes without improvement: 0
09/16 09:17:03 AM: edges-ner-ontonotes_loss: training: 0.095525 validation: 0.048975
09/16 09:17:03 AM: macro_avg: validation: 0.862554
09/16 09:17:03 AM: micro_avg: validation: 0.000000
09/16 09:17:03 AM: edges-ner-ontonotes_mcc: training: 0.697271 validation: 0.856742
09/16 09:17:03 AM: edges-ner-ontonotes_acc: training: 0.585422 validation: 0.800425
09/16 09:17:03 AM: edges-ner-ontonotes_precision: training: 0.844722 validation: 0.918236
09/16 09:17:03 AM: edges-ner-ontonotes_recall: training: 0.598304 validation: 0.813239
09/16 09:17:03 AM: edges-ner-ontonotes_f1: training: 0.700473 validation: 0.862554
09/16 09:17:03 AM: Global learning rate: 0.0001
09/16 09:17:03 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:17:09 AM: Update 1079: task edges-ner-ontonotes, batch 79 (1079): mcc: 0.8615, acc: 0.7981, precision: 0.9279, recall: 0.8130, f1: 0.8667, edges-ner-ontonotes_loss: 0.0449
09/16 09:17:19 AM: Update 1201: task edges-ner-ontonotes, batch 201 (1201): mcc: 0.8649, acc: 0.8039, precision: 0.9287, recall: 0.8184, f1: 0.8701, edges-ner-ontonotes_loss: 0.0441
09/16 09:17:29 AM: Update 1296: task edges-ner-ontonotes, batch 296 (1296): mcc: 0.8587, acc: 0.7952, precision: 0.9263, recall: 0.8095, f1: 0.8639, edges-ner-ontonotes_loss: 0.0458
09/16 09:17:39 AM: Update 1423: task edges-ner-ontonotes, batch 423 (1423): mcc: 0.8505, acc: 0.7834, precision: 0.9226, recall: 0.7981, f1: 0.8559, edges-ner-ontonotes_loss: 0.0486
09/16 09:17:49 AM: Update 1552: task edges-ner-ontonotes, batch 552 (1552): mcc: 0.8466, acc: 0.7781, precision: 0.9203, recall: 0.7933, f1: 0.8521, edges-ner-ontonotes_loss: 0.0496
09/16 09:17:59 AM: Update 1687: task edges-ner-ontonotes, batch 687 (1687): mcc: 0.8457, acc: 0.7769, precision: 0.9196, recall: 0.7922, f1: 0.8511, edges-ner-ontonotes_loss: 0.0493
09/16 09:18:09 AM: Update 1834: task edges-ner-ontonotes, batch 834 (1834): mcc: 0.8461, acc: 0.7777, precision: 0.9196, recall: 0.7929, f1: 0.8516, edges-ner-ontonotes_loss: 0.0487
09/16 09:18:19 AM: Update 1946: task edges-ner-ontonotes, batch 946 (1946): mcc: 0.8478, acc: 0.7800, precision: 0.9201, recall: 0.7955, f1: 0.8533, edges-ner-ontonotes_loss: 0.0478
09/16 09:18:24 AM: ***** Step 2000 / Validation 2 *****
09/16 09:18:24 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:18:24 AM: Validating...
09/16 09:18:29 AM: Evaluate: task edges-ner-ontonotes, batch 57 (157): mcc: 0.8713, acc: 0.8266, precision: 0.9221, recall: 0.8361, f1: 0.8770, edges-ner-ontonotes_loss: 0.0400
09/16 09:18:40 AM: Evaluate: task edges-ner-ontonotes, batch 135 (157): mcc: 0.8893, acc: 0.8449, precision: 0.9355, recall: 0.8565, f1: 0.8943, edges-ner-ontonotes_loss: 0.0351
09/16 09:18:42 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:18:42 AM: Best result seen so far for macro.
09/16 09:18:42 AM: Updating LR scheduler:
09/16 09:18:42 AM: 	Best result seen so far for macro_avg: 0.894
09/16 09:18:42 AM: 	# validation passes without improvement: 0
09/16 09:18:42 AM: edges-ner-ontonotes_loss: training: 0.047422 validation: 0.034811
09/16 09:18:42 AM: macro_avg: validation: 0.894370
09/16 09:18:42 AM: micro_avg: validation: 0.000000
09/16 09:18:42 AM: edges-ner-ontonotes_mcc: training: 0.849140 validation: 0.889419
09/16 09:18:42 AM: edges-ner-ontonotes_acc: training: 0.781912 validation: 0.843267
09/16 09:18:42 AM: edges-ner-ontonotes_precision: training: 0.920672 validation: 0.935791
09/16 09:18:42 AM: edges-ner-ontonotes_recall: training: 0.797397 validation: 0.856460
09/16 09:18:42 AM: edges-ner-ontonotes_f1: training: 0.854612 validation: 0.894370
09/16 09:18:42 AM: Global learning rate: 0.0001
09/16 09:18:42 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:18:50 AM: Update 2098: task edges-ner-ontonotes, batch 98 (2098): mcc: 0.8843, acc: 0.8300, precision: 0.9365, recall: 0.8464, f1: 0.8892, edges-ner-ontonotes_loss: 0.0360
09/16 09:19:00 AM: Update 2201: task edges-ner-ontonotes, batch 201 (2201): mcc: 0.8810, acc: 0.8257, precision: 0.9337, recall: 0.8430, f1: 0.8860, edges-ner-ontonotes_loss: 0.0365
09/16 09:19:10 AM: Update 2325: task edges-ner-ontonotes, batch 325 (2325): mcc: 0.8884, acc: 0.8356, precision: 0.9363, recall: 0.8541, f1: 0.8933, edges-ner-ontonotes_loss: 0.0344
09/16 09:19:20 AM: Update 2450: task edges-ner-ontonotes, batch 450 (2450): mcc: 0.8929, acc: 0.8422, precision: 0.9383, recall: 0.8605, f1: 0.8977, edges-ner-ontonotes_loss: 0.0333
09/16 09:19:30 AM: Update 2549: task edges-ner-ontonotes, batch 549 (2549): mcc: 0.8935, acc: 0.8428, precision: 0.9385, recall: 0.8614, f1: 0.8983, edges-ner-ontonotes_loss: 0.0329
09/16 09:19:40 AM: Update 2676: task edges-ner-ontonotes, batch 676 (2676): mcc: 0.8944, acc: 0.8441, precision: 0.9383, recall: 0.8633, f1: 0.8992, edges-ner-ontonotes_loss: 0.0327
09/16 09:19:50 AM: Update 2800: task edges-ner-ontonotes, batch 800 (2800): mcc: 0.8963, acc: 0.8466, precision: 0.9392, recall: 0.8659, f1: 0.9011, edges-ner-ontonotes_loss: 0.0321
09/16 09:20:00 AM: Update 2895: task edges-ner-ontonotes, batch 895 (2895): mcc: 0.8931, acc: 0.8422, precision: 0.9374, recall: 0.8616, f1: 0.8979, edges-ner-ontonotes_loss: 0.0334
09/16 09:20:08 AM: ***** Step 3000 / Validation 3 *****
09/16 09:20:08 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:20:08 AM: Validating...
09/16 09:20:10 AM: Evaluate: task edges-ner-ontonotes, batch 22 (157): mcc: 0.8527, acc: 0.8030, precision: 0.8950, recall: 0.8274, f1: 0.8599, edges-ner-ontonotes_loss: 0.0422
09/16 09:20:20 AM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.8945, acc: 0.8535, precision: 0.9321, recall: 0.8694, f1: 0.8996, edges-ner-ontonotes_loss: 0.0335
09/16 09:20:27 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:20:27 AM: Best result seen so far for macro.
09/16 09:20:27 AM: Updating LR scheduler:
09/16 09:20:27 AM: 	Best result seen so far for macro_avg: 0.910
09/16 09:20:27 AM: 	# validation passes without improvement: 0
09/16 09:20:27 AM: edges-ner-ontonotes_loss: training: 0.034607 validation: 0.030751
09/16 09:20:27 AM: macro_avg: validation: 0.909796
09/16 09:20:27 AM: micro_avg: validation: 0.000000
09/16 09:20:27 AM: edges-ner-ontonotes_mcc: training: 0.890128 validation: 0.905247
09/16 09:20:27 AM: edges-ner-ontonotes_acc: training: 0.838481 validation: 0.864726
09/16 09:20:27 AM: edges-ner-ontonotes_precision: training: 0.935606 validation: 0.941372
09/16 09:20:27 AM: edges-ner-ontonotes_recall: training: 0.857929 validation: 0.880270
09/16 09:20:27 AM: edges-ner-ontonotes_f1: training: 0.895085 validation: 0.909796
09/16 09:20:27 AM: Global learning rate: 0.0001
09/16 09:20:27 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:20:30 AM: Update 3046: task edges-ner-ontonotes, batch 46 (3046): mcc: 0.8648, acc: 0.8057, precision: 0.9206, recall: 0.8257, f1: 0.8705, edges-ner-ontonotes_loss: 0.0451
09/16 09:20:40 AM: Update 3165: task edges-ner-ontonotes, batch 165 (3165): mcc: 0.8620, acc: 0.8027, precision: 0.9171, recall: 0.8237, f1: 0.8679, edges-ner-ontonotes_loss: 0.0441
09/16 09:20:50 AM: Update 3317: task edges-ner-ontonotes, batch 317 (3317): mcc: 0.8687, acc: 0.8113, precision: 0.9214, recall: 0.8321, f1: 0.8745, edges-ner-ontonotes_loss: 0.0412
09/16 09:21:00 AM: Update 3436: task edges-ner-ontonotes, batch 436 (3436): mcc: 0.8711, acc: 0.8150, precision: 0.9229, recall: 0.8350, f1: 0.8767, edges-ner-ontonotes_loss: 0.0401
09/16 09:21:10 AM: Update 3565: task edges-ner-ontonotes, batch 565 (3565): mcc: 0.8766, acc: 0.8230, precision: 0.9253, recall: 0.8429, f1: 0.8822, edges-ner-ontonotes_loss: 0.0387
09/16 09:21:21 AM: Update 3691: task edges-ner-ontonotes, batch 691 (3691): mcc: 0.8809, acc: 0.8288, precision: 0.9274, recall: 0.8487, f1: 0.8863, edges-ner-ontonotes_loss: 0.0375
09/16 09:21:31 AM: Update 3791: task edges-ner-ontonotes, batch 791 (3791): mcc: 0.8842, acc: 0.8333, precision: 0.9293, recall: 0.8530, f1: 0.8895, edges-ner-ontonotes_loss: 0.0365
09/16 09:21:41 AM: Update 3917: task edges-ner-ontonotes, batch 917 (3917): mcc: 0.8884, acc: 0.8384, precision: 0.9316, recall: 0.8584, f1: 0.8935, edges-ner-ontonotes_loss: 0.0352
09/16 09:21:47 AM: ***** Step 4000 / Validation 4 *****
09/16 09:21:47 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:21:47 AM: Validating...
09/16 09:21:51 AM: Evaluate: task edges-ner-ontonotes, batch 37 (157): mcc: 0.8695, acc: 0.8278, precision: 0.9099, recall: 0.8441, f1: 0.8758, edges-ner-ontonotes_loss: 0.0395
09/16 09:22:01 AM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.9058, acc: 0.8686, precision: 0.9377, recall: 0.8849, f1: 0.9105, edges-ner-ontonotes_loss: 0.0304
09/16 09:22:06 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:22:06 AM: Best result seen so far for macro.
09/16 09:22:06 AM: Updating LR scheduler:
09/16 09:22:06 AM: 	Best result seen so far for macro_avg: 0.917
09/16 09:22:06 AM: 	# validation passes without improvement: 0
09/16 09:22:06 AM: edges-ner-ontonotes_loss: training: 0.034351 validation: 0.027952
09/16 09:22:06 AM: macro_avg: validation: 0.917111
09/16 09:22:06 AM: micro_avg: validation: 0.000000
09/16 09:22:06 AM: edges-ner-ontonotes_mcc: training: 0.890998 validation: 0.912743
09/16 09:22:06 AM: edges-ner-ontonotes_acc: training: 0.841668 validation: 0.875341
09/16 09:22:06 AM: edges-ner-ontonotes_precision: training: 0.933241 validation: 0.942466
09/16 09:22:06 AM: edges-ner-ontonotes_recall: training: 0.861737 validation: 0.893085
09/16 09:22:06 AM: edges-ner-ontonotes_f1: training: 0.896065 validation: 0.917111
09/16 09:22:06 AM: Global learning rate: 0.0001
09/16 09:22:06 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:22:12 AM: Update 4052: task edges-ner-ontonotes, batch 52 (4052): mcc: 0.9170, acc: 0.8747, precision: 0.9482, recall: 0.8954, f1: 0.9210, edges-ner-ontonotes_loss: 0.0255
09/16 09:22:22 AM: Update 4172: task edges-ner-ontonotes, batch 172 (4172): mcc: 0.9134, acc: 0.8706, precision: 0.9447, recall: 0.8922, f1: 0.9177, edges-ner-ontonotes_loss: 0.0267
09/16 09:22:32 AM: Update 4293: task edges-ner-ontonotes, batch 293 (4293): mcc: 0.9131, acc: 0.8703, precision: 0.9438, recall: 0.8925, f1: 0.9174, edges-ner-ontonotes_loss: 0.0268
09/16 09:22:42 AM: Update 4403: task edges-ner-ontonotes, batch 403 (4403): mcc: 0.9092, acc: 0.8655, precision: 0.9416, recall: 0.8874, f1: 0.9137, edges-ner-ontonotes_loss: 0.0283
09/16 09:22:52 AM: Update 4534: task edges-ner-ontonotes, batch 534 (4534): mcc: 0.9021, acc: 0.8560, precision: 0.9375, recall: 0.8782, f1: 0.9069, edges-ner-ontonotes_loss: 0.0317
09/16 09:23:02 AM: Update 4657: task edges-ner-ontonotes, batch 657 (4657): mcc: 0.8969, acc: 0.8497, precision: 0.9343, recall: 0.8716, f1: 0.9019, edges-ner-ontonotes_loss: 0.0336
09/16 09:23:12 AM: Update 4782: task edges-ner-ontonotes, batch 782 (4782): mcc: 0.8950, acc: 0.8476, precision: 0.9330, recall: 0.8693, f1: 0.9000, edges-ner-ontonotes_loss: 0.0341
09/16 09:23:22 AM: Update 4928: task edges-ner-ontonotes, batch 928 (4928): mcc: 0.8938, acc: 0.8460, precision: 0.9323, recall: 0.8678, f1: 0.8989, edges-ner-ontonotes_loss: 0.0343
09/16 09:23:30 AM: ***** Step 5000 / Validation 5 *****
09/16 09:23:32 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:23:32 AM: Validating...
09/16 09:23:32 AM: Evaluate: task edges-ner-ontonotes, batch 8 (157): mcc: 0.8169, acc: 0.7605, precision: 0.8679, recall: 0.7871, f1: 0.8255, edges-ner-ontonotes_loss: 0.0475
09/16 09:23:42 AM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.9060, acc: 0.8711, precision: 0.9344, recall: 0.8883, f1: 0.9108, edges-ner-ontonotes_loss: 0.0295
09/16 09:23:50 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:23:50 AM: Best result seen so far for macro.
09/16 09:23:50 AM: Updating LR scheduler:
09/16 09:23:50 AM: 	Best result seen so far for macro_avg: 0.923
09/16 09:23:50 AM: 	# validation passes without improvement: 0
09/16 09:23:50 AM: edges-ner-ontonotes_loss: training: 0.034440 validation: 0.026368
09/16 09:23:50 AM: macro_avg: validation: 0.922796
09/16 09:23:50 AM: micro_avg: validation: 0.000000
09/16 09:23:50 AM: edges-ner-ontonotes_mcc: training: 0.893073 validation: 0.918706
09/16 09:23:50 AM: edges-ner-ontonotes_acc: training: 0.845026 validation: 0.884820
09/16 09:23:50 AM: edges-ner-ontonotes_precision: training: 0.931760 validation: 0.946793
09/16 09:23:50 AM: edges-ner-ontonotes_recall: training: 0.866948 validation: 0.899985
09/16 09:23:50 AM: edges-ner-ontonotes_f1: training: 0.898187 validation: 0.922796
09/16 09:23:50 AM: Global learning rate: 0.0001
09/16 09:23:50 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:23:53 AM: Update 5028: task edges-ner-ontonotes, batch 28 (5028): mcc: 0.8879, acc: 0.8450, precision: 0.9224, recall: 0.8662, f1: 0.8934, edges-ner-ontonotes_loss: 0.0340
09/16 09:24:03 AM: Update 5151: task edges-ner-ontonotes, batch 151 (5151): mcc: 0.8959, acc: 0.8504, precision: 0.9286, recall: 0.8751, f1: 0.9011, edges-ner-ontonotes_loss: 0.0320
09/16 09:24:13 AM: Update 5283: task edges-ner-ontonotes, batch 283 (5283): mcc: 0.9011, acc: 0.8564, precision: 0.9344, recall: 0.8794, f1: 0.9060, edges-ner-ontonotes_loss: 0.0305
09/16 09:24:23 AM: Update 5390: task edges-ner-ontonotes, batch 390 (5390): mcc: 0.9048, acc: 0.8610, precision: 0.9370, recall: 0.8836, f1: 0.9095, edges-ner-ontonotes_loss: 0.0292
09/16 09:24:33 AM: Update 5513: task edges-ner-ontonotes, batch 513 (5513): mcc: 0.9090, acc: 0.8668, precision: 0.9397, recall: 0.8889, f1: 0.9136, edges-ner-ontonotes_loss: 0.0282
09/16 09:24:43 AM: Update 5608: task edges-ner-ontonotes, batch 608 (5608): mcc: 0.9114, acc: 0.8700, precision: 0.9416, recall: 0.8915, f1: 0.9159, edges-ner-ontonotes_loss: 0.0275
09/16 09:24:53 AM: Update 5731: task edges-ner-ontonotes, batch 731 (5731): mcc: 0.9126, acc: 0.8711, precision: 0.9420, recall: 0.8932, f1: 0.9170, edges-ner-ontonotes_loss: 0.0271
09/16 09:25:03 AM: Update 5860: task edges-ner-ontonotes, batch 860 (5860): mcc: 0.9137, acc: 0.8724, precision: 0.9429, recall: 0.8944, f1: 0.9180, edges-ner-ontonotes_loss: 0.0269
09/16 09:25:13 AM: Update 5965: task edges-ner-ontonotes, batch 965 (5965): mcc: 0.9122, acc: 0.8702, precision: 0.9420, recall: 0.8926, f1: 0.9166, edges-ner-ontonotes_loss: 0.0276
09/16 09:25:16 AM: ***** Step 6000 / Validation 6 *****
09/16 09:25:16 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:25:16 AM: Validating...
09/16 09:25:24 AM: Evaluate: task edges-ner-ontonotes, batch 67 (157): mcc: 0.8986, acc: 0.8615, precision: 0.9307, recall: 0.8781, f1: 0.9036, edges-ner-ontonotes_loss: 0.0330
09/16 09:25:35 AM: Evaluate: task edges-ner-ontonotes, batch 147 (157): mcc: 0.9199, acc: 0.8864, precision: 0.9464, recall: 0.9025, f1: 0.9239, edges-ner-ontonotes_loss: 0.0265
09/16 09:25:36 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:25:36 AM: Best result seen so far for macro.
09/16 09:25:36 AM: Updating LR scheduler:
09/16 09:25:36 AM: 	Best result seen so far for macro_avg: 0.925
09/16 09:25:36 AM: 	# validation passes without improvement: 0
09/16 09:25:36 AM: edges-ner-ontonotes_loss: training: 0.027988 validation: 0.026053
09/16 09:25:36 AM: macro_avg: validation: 0.925059
09/16 09:25:36 AM: micro_avg: validation: 0.000000
09/16 09:25:36 AM: edges-ner-ontonotes_mcc: training: 0.911327 validation: 0.921049
09/16 09:25:36 AM: edges-ner-ontonotes_acc: training: 0.869031 validation: 0.887701
09/16 09:25:36 AM: edges-ner-ontonotes_precision: training: 0.941401 validation: 0.947452
09/16 09:25:36 AM: edges-ner-ontonotes_recall: training: 0.891481 validation: 0.903700
09/16 09:25:36 AM: edges-ner-ontonotes_f1: training: 0.915761 validation: 0.925059
09/16 09:25:36 AM: Global learning rate: 0.0001
09/16 09:25:36 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:25:45 AM: Update 6117: task edges-ner-ontonotes, batch 117 (6117): mcc: 0.8787, acc: 0.8274, precision: 0.9198, recall: 0.8518, f1: 0.8845, edges-ner-ontonotes_loss: 0.0401
09/16 09:25:56 AM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.8786, acc: 0.8275, precision: 0.9204, recall: 0.8510, f1: 0.8843, edges-ner-ontonotes_loss: 0.0405
09/16 09:26:06 AM: Update 6369: task edges-ner-ontonotes, batch 369 (6369): mcc: 0.8812, acc: 0.8305, precision: 0.9220, recall: 0.8544, f1: 0.8869, edges-ner-ontonotes_loss: 0.0383
09/16 09:26:16 AM: Update 6520: task edges-ner-ontonotes, batch 520 (6520): mcc: 0.8839, acc: 0.8341, precision: 0.9241, recall: 0.8572, f1: 0.8894, edges-ner-ontonotes_loss: 0.0370
09/16 09:26:26 AM: Update 6623: task edges-ner-ontonotes, batch 623 (6623): mcc: 0.8879, acc: 0.8393, precision: 0.9263, recall: 0.8625, f1: 0.8933, edges-ner-ontonotes_loss: 0.0357
09/16 09:26:36 AM: Update 6757: task edges-ner-ontonotes, batch 757 (6757): mcc: 0.8916, acc: 0.8440, precision: 0.9285, recall: 0.8672, f1: 0.8968, edges-ner-ontonotes_loss: 0.0346
09/16 09:26:46 AM: Update 6857: task edges-ner-ontonotes, batch 857 (6857): mcc: 0.8938, acc: 0.8468, precision: 0.9299, recall: 0.8701, f1: 0.8990, edges-ner-ontonotes_loss: 0.0339
09/16 09:26:56 AM: Update 6979: task edges-ner-ontonotes, batch 979 (6979): mcc: 0.8978, acc: 0.8522, precision: 0.9323, recall: 0.8753, f1: 0.9029, edges-ner-ontonotes_loss: 0.0327
09/16 09:26:58 AM: ***** Step 7000 / Validation 7 *****
09/16 09:26:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:26:58 AM: Validating...
09/16 09:27:06 AM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.9025, acc: 0.8692, precision: 0.9289, recall: 0.8872, f1: 0.9076, edges-ner-ontonotes_loss: 0.0319
09/16 09:27:16 AM: Evaluate: task edges-ner-ontonotes, batch 155 (157): mcc: 0.9218, acc: 0.8916, precision: 0.9422, recall: 0.9102, f1: 0.9259, edges-ner-ontonotes_loss: 0.0256
09/16 09:27:16 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:27:16 AM: Best result seen so far for macro.
09/16 09:27:16 AM: Updating LR scheduler:
09/16 09:27:16 AM: 	Best result seen so far for macro_avg: 0.926
09/16 09:27:16 AM: 	# validation passes without improvement: 0
09/16 09:27:16 AM: edges-ner-ontonotes_loss: training: 0.032492 validation: 0.025536
09/16 09:27:16 AM: macro_avg: validation: 0.926109
09/16 09:27:16 AM: micro_avg: validation: 0.000000
09/16 09:27:16 AM: edges-ner-ontonotes_mcc: training: 0.898461 validation: 0.921994
09/16 09:27:16 AM: edges-ner-ontonotes_acc: training: 0.853072 validation: 0.891796
09/16 09:27:16 AM: edges-ner-ontonotes_precision: training: 0.932691 validation: 0.942317
09/16 09:27:16 AM: edges-ner-ontonotes_recall: training: 0.876004 validation: 0.910449
09/16 09:27:16 AM: edges-ner-ontonotes_f1: training: 0.903459 validation: 0.926109
09/16 09:27:16 AM: Global learning rate: 0.0001
09/16 09:27:16 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:27:26 AM: Update 7123: task edges-ner-ontonotes, batch 123 (7123): mcc: 0.9247, acc: 0.8874, precision: 0.9479, recall: 0.9100, f1: 0.9286, edges-ner-ontonotes_loss: 0.0233
09/16 09:27:36 AM: Update 7221: task edges-ner-ontonotes, batch 221 (7221): mcc: 0.9251, acc: 0.8876, precision: 0.9484, recall: 0.9103, f1: 0.9289, edges-ner-ontonotes_loss: 0.0236
09/16 09:27:46 AM: Update 7344: task edges-ner-ontonotes, batch 344 (7344): mcc: 0.9249, acc: 0.8868, precision: 0.9474, recall: 0.9109, f1: 0.9288, edges-ner-ontonotes_loss: 0.0236
09/16 09:27:56 AM: Update 7469: task edges-ner-ontonotes, batch 469 (7469): mcc: 0.9229, acc: 0.8846, precision: 0.9457, recall: 0.9089, f1: 0.9269, edges-ner-ontonotes_loss: 0.0241
09/16 09:28:06 AM: Update 7570: task edges-ner-ontonotes, batch 570 (7570): mcc: 0.9164, acc: 0.8763, precision: 0.9413, recall: 0.9009, f1: 0.9207, edges-ner-ontonotes_loss: 0.0270
09/16 09:28:16 AM: Update 7697: task edges-ner-ontonotes, batch 697 (7697): mcc: 0.9103, acc: 0.8685, precision: 0.9378, recall: 0.8931, f1: 0.9149, edges-ner-ontonotes_loss: 0.0293
09/16 09:28:26 AM: Update 7813: task edges-ner-ontonotes, batch 813 (7813): mcc: 0.9070, acc: 0.8642, precision: 0.9359, recall: 0.8887, f1: 0.9117, edges-ner-ontonotes_loss: 0.0307
09/16 09:28:36 AM: Update 7956: task edges-ner-ontonotes, batch 956 (7956): mcc: 0.9054, acc: 0.8621, precision: 0.9350, recall: 0.8865, f1: 0.9101, edges-ner-ontonotes_loss: 0.0311
09/16 09:28:39 AM: ***** Step 8000 / Validation 8 *****
09/16 09:28:39 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:28:39 AM: Validating...
09/16 09:28:46 AM: Evaluate: task edges-ner-ontonotes, batch 64 (157): mcc: 0.9095, acc: 0.8762, precision: 0.9359, recall: 0.8934, f1: 0.9141, edges-ner-ontonotes_loss: 0.0295
09/16 09:28:56 AM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.9253, acc: 0.8945, precision: 0.9498, recall: 0.9094, f1: 0.9291, edges-ner-ontonotes_loss: 0.0249
09/16 09:28:58 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:28:58 AM: Best result seen so far for macro.
09/16 09:28:58 AM: Updating LR scheduler:
09/16 09:28:58 AM: 	Best result seen so far for macro_avg: 0.929
09/16 09:28:58 AM: 	# validation passes without improvement: 0
09/16 09:28:58 AM: edges-ner-ontonotes_loss: training: 0.031224 validation: 0.024741
09/16 09:28:58 AM: macro_avg: validation: 0.928807
09/16 09:28:58 AM: micro_avg: validation: 0.000000
09/16 09:28:58 AM: edges-ner-ontonotes_mcc: training: 0.905136 validation: 0.925004
09/16 09:28:58 AM: edges-ner-ontonotes_acc: training: 0.861738 validation: 0.893009
09/16 09:28:58 AM: edges-ner-ontonotes_precision: training: 0.934899 validation: 0.950981
09/16 09:28:58 AM: edges-ner-ontonotes_recall: training: 0.886248 validation: 0.907643
09/16 09:28:58 AM: edges-ner-ontonotes_f1: training: 0.909924 validation: 0.928807
09/16 09:28:58 AM: Global learning rate: 0.0001
09/16 09:28:58 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:29:06 AM: Update 8097: task edges-ner-ontonotes, batch 97 (8097): mcc: 0.8899, acc: 0.8402, precision: 0.9268, recall: 0.8659, f1: 0.8953, edges-ner-ontonotes_loss: 0.0330
09/16 09:29:16 AM: Update 8226: task edges-ner-ontonotes, batch 226 (8226): mcc: 0.9029, acc: 0.8583, precision: 0.9336, recall: 0.8833, f1: 0.9077, edges-ner-ontonotes_loss: 0.0304
09/16 09:29:26 AM: Update 8352: task edges-ner-ontonotes, batch 352 (8352): mcc: 0.9035, acc: 0.8597, precision: 0.9334, recall: 0.8848, f1: 0.9084, edges-ner-ontonotes_loss: 0.0300
09/16 09:29:37 AM: Update 8456: task edges-ner-ontonotes, batch 456 (8456): mcc: 0.9068, acc: 0.8642, precision: 0.9357, recall: 0.8885, f1: 0.9115, edges-ner-ontonotes_loss: 0.0292
09/16 09:29:47 AM: Update 8574: task edges-ner-ontonotes, batch 574 (8574): mcc: 0.9107, acc: 0.8695, precision: 0.9378, recall: 0.8938, f1: 0.9152, edges-ner-ontonotes_loss: 0.0280
09/16 09:29:57 AM: Update 8695: task edges-ner-ontonotes, batch 695 (8695): mcc: 0.9141, acc: 0.8742, precision: 0.9401, recall: 0.8980, f1: 0.9186, edges-ner-ontonotes_loss: 0.0270
09/16 09:30:07 AM: Update 8800: task edges-ner-ontonotes, batch 800 (8800): mcc: 0.9159, acc: 0.8763, precision: 0.9416, recall: 0.8997, f1: 0.9202, edges-ner-ontonotes_loss: 0.0265
09/16 09:30:17 AM: Update 8927: task edges-ner-ontonotes, batch 927 (8927): mcc: 0.9165, acc: 0.8774, precision: 0.9417, recall: 0.9009, f1: 0.9208, edges-ner-ontonotes_loss: 0.0262
09/16 09:30:23 AM: ***** Step 9000 / Validation 9 *****
09/16 09:30:23 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:30:23 AM: Validating...
09/16 09:30:27 AM: Evaluate: task edges-ner-ontonotes, batch 42 (157): mcc: 0.8894, acc: 0.8551, precision: 0.9153, recall: 0.8759, f1: 0.8952, edges-ner-ontonotes_loss: 0.0360
09/16 09:30:37 AM: Evaluate: task edges-ner-ontonotes, batch 114 (157): mcc: 0.9165, acc: 0.8847, precision: 0.9383, recall: 0.9041, f1: 0.9209, edges-ner-ontonotes_loss: 0.0279
09/16 09:30:42 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:30:42 AM: Best result seen so far for macro.
09/16 09:30:42 AM: Updating LR scheduler:
09/16 09:30:42 AM: 	Best result seen so far for macro_avg: 0.930
09/16 09:30:42 AM: 	# validation passes without improvement: 0
09/16 09:30:42 AM: edges-ner-ontonotes_loss: training: 0.025945 validation: 0.024800
09/16 09:30:42 AM: macro_avg: validation: 0.929648
09/16 09:30:42 AM: micro_avg: validation: 0.000000
09/16 09:30:42 AM: edges-ner-ontonotes_mcc: training: 0.917229 validation: 0.925729
09/16 09:30:42 AM: edges-ner-ontonotes_acc: training: 0.878272 validation: 0.895966
09/16 09:30:42 AM: edges-ner-ontonotes_precision: training: 0.941993 validation: 0.945503
09/16 09:30:42 AM: edges-ner-ontonotes_recall: training: 0.901875 validation: 0.914316
09/16 09:30:42 AM: edges-ner-ontonotes_f1: training: 0.921498 validation: 0.929648
09/16 09:30:42 AM: Global learning rate: 0.0001
09/16 09:30:42 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:30:47 AM: Update 9039: task edges-ner-ontonotes, batch 39 (9039): mcc: 0.9127, acc: 0.8695, precision: 0.9374, recall: 0.8979, f1: 0.9172, edges-ner-ontonotes_loss: 0.0286
09/16 09:30:57 AM: Update 9166: task edges-ner-ontonotes, batch 166 (9166): mcc: 0.8930, acc: 0.8467, precision: 0.9257, recall: 0.8726, f1: 0.8984, edges-ner-ontonotes_loss: 0.0359
09/16 09:31:07 AM: Update 9298: task edges-ner-ontonotes, batch 298 (9298): mcc: 0.8881, acc: 0.8402, precision: 0.9229, recall: 0.8662, f1: 0.8937, edges-ner-ontonotes_loss: 0.0376
09/16 09:31:17 AM: Update 9414: task edges-ner-ontonotes, batch 414 (9414): mcc: 0.8865, acc: 0.8387, precision: 0.9214, recall: 0.8646, f1: 0.8921, edges-ner-ontonotes_loss: 0.0374
09/16 09:31:27 AM: Update 9561: task edges-ner-ontonotes, batch 561 (9561): mcc: 0.8900, acc: 0.8428, precision: 0.9243, recall: 0.8684, f1: 0.8955, edges-ner-ontonotes_loss: 0.0357
09/16 09:31:37 AM: Update 9678: task edges-ner-ontonotes, batch 678 (9678): mcc: 0.8912, acc: 0.8442, precision: 0.9252, recall: 0.8697, f1: 0.8966, edges-ner-ontonotes_loss: 0.0350
09/16 09:31:47 AM: Update 9803: task edges-ner-ontonotes, batch 803 (9803): mcc: 0.8938, acc: 0.8475, precision: 0.9265, recall: 0.8733, f1: 0.8991, edges-ner-ontonotes_loss: 0.0340
09/16 09:31:57 AM: Update 9931: task edges-ner-ontonotes, batch 931 (9931): mcc: 0.8966, acc: 0.8512, precision: 0.9285, recall: 0.8767, f1: 0.9018, edges-ner-ontonotes_loss: 0.0331
09/16 09:32:05 AM: ***** Step 10000 / Validation 10 *****
09/16 09:32:05 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:32:05 AM: Validating...
09/16 09:32:07 AM: Evaluate: task edges-ner-ontonotes, batch 27 (157): mcc: 0.8664, acc: 0.8264, precision: 0.8968, recall: 0.8511, f1: 0.8733, edges-ner-ontonotes_loss: 0.0397
09/16 09:32:18 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.9176, acc: 0.8864, precision: 0.9389, recall: 0.9055, f1: 0.9219, edges-ner-ontonotes_loss: 0.0267
09/16 09:32:24 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:32:24 AM: Best result seen so far for macro.
09/16 09:32:24 AM: Updating LR scheduler:
09/16 09:32:24 AM: 	Best result seen so far for macro_avg: 0.931
09/16 09:32:24 AM: 	# validation passes without improvement: 0
09/16 09:32:24 AM: edges-ner-ontonotes_loss: training: 0.032520 validation: 0.024080
09/16 09:32:24 AM: macro_avg: validation: 0.930827
09/16 09:32:24 AM: micro_avg: validation: 0.000000
09/16 09:32:24 AM: edges-ner-ontonotes_mcc: training: 0.898417 validation: 0.926957
09/16 09:32:24 AM: edges-ner-ontonotes_acc: training: 0.853371 validation: 0.897558
09/16 09:32:24 AM: edges-ner-ontonotes_precision: training: 0.929706 validation: 0.945836
09/16 09:32:24 AM: edges-ner-ontonotes_recall: training: 0.878777 validation: 0.916288
09/16 09:32:24 AM: edges-ner-ontonotes_f1: training: 0.903525 validation: 0.930827
09/16 09:32:24 AM: Global learning rate: 0.0001
09/16 09:32:24 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:32:28 AM: Update 10054: task edges-ner-ontonotes, batch 54 (10054): mcc: 0.9302, acc: 0.8927, precision: 0.9516, recall: 0.9168, f1: 0.9338, edges-ner-ontonotes_loss: 0.0214
09/16 09:32:39 AM: Update 10183: task edges-ner-ontonotes, batch 183 (10183): mcc: 0.9274, acc: 0.8899, precision: 0.9503, recall: 0.9128, f1: 0.9312, edges-ner-ontonotes_loss: 0.0225
09/16 09:32:49 AM: Update 10281: task edges-ner-ontonotes, batch 281 (10281): mcc: 0.9265, acc: 0.8898, precision: 0.9489, recall: 0.9125, f1: 0.9303, edges-ner-ontonotes_loss: 0.0225
09/16 09:32:59 AM: Update 10403: task edges-ner-ontonotes, batch 403 (10403): mcc: 0.9263, acc: 0.8897, precision: 0.9484, recall: 0.9126, f1: 0.9301, edges-ner-ontonotes_loss: 0.0226
09/16 09:33:09 AM: Update 10531: task edges-ner-ontonotes, batch 531 (10531): mcc: 0.9266, acc: 0.8904, precision: 0.9485, recall: 0.9132, f1: 0.9305, edges-ner-ontonotes_loss: 0.0229
09/16 09:33:19 AM: Update 10634: task edges-ner-ontonotes, batch 634 (10634): mcc: 0.9237, acc: 0.8864, precision: 0.9468, recall: 0.9093, f1: 0.9277, edges-ner-ontonotes_loss: 0.0240
09/16 09:33:29 AM: Update 10762: task edges-ner-ontonotes, batch 762 (10762): mcc: 0.9180, acc: 0.8793, precision: 0.9430, recall: 0.9024, f1: 0.9222, edges-ner-ontonotes_loss: 0.0264
09/16 09:33:39 AM: Update 10888: task edges-ner-ontonotes, batch 888 (10888): mcc: 0.9138, acc: 0.8741, precision: 0.9402, recall: 0.8972, f1: 0.9182, edges-ner-ontonotes_loss: 0.0282
09/16 09:33:49 AM: ***** Step 11000 / Validation 11 *****
09/16 09:33:49 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:33:50 AM: Validating...
09/16 09:33:50 AM: Evaluate: task edges-ner-ontonotes, batch 1 (157): mcc: 0.7777, acc: 0.6721, precision: 0.8627, recall: 0.7213, f1: 0.7857, edges-ner-ontonotes_loss: 0.0506
09/16 09:34:00 AM: Evaluate: task edges-ner-ontonotes, batch 88 (157): mcc: 0.9184, acc: 0.8841, precision: 0.9436, recall: 0.9024, f1: 0.9226, edges-ner-ontonotes_loss: 0.0270
09/16 09:34:09 AM: Updating LR scheduler:
09/16 09:34:09 AM: 	Best result seen so far for macro_avg: 0.931
09/16 09:34:09 AM: 	# validation passes without improvement: 1
09/16 09:34:09 AM: edges-ner-ontonotes_loss: training: 0.028878 validation: 0.023949
09/16 09:34:09 AM: macro_avg: validation: 0.930761
09/16 09:34:09 AM: micro_avg: validation: 0.000000
09/16 09:34:09 AM: edges-ner-ontonotes_mcc: training: 0.911499 validation: 0.927032
09/16 09:34:09 AM: edges-ner-ontonotes_acc: training: 0.871266 validation: 0.895587
09/16 09:34:09 AM: edges-ner-ontonotes_precision: training: 0.938524 validation: 0.951668
09/16 09:34:09 AM: edges-ner-ontonotes_recall: training: 0.894570 validation: 0.910752
09/16 09:34:09 AM: edges-ner-ontonotes_f1: training: 0.916020 validation: 0.930761
09/16 09:34:09 AM: Global learning rate: 0.0001
09/16 09:34:09 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:34:10 AM: Update 11019: task edges-ner-ontonotes, batch 19 (11019): mcc: 0.9033, acc: 0.8625, precision: 0.9356, recall: 0.8821, f1: 0.9081, edges-ner-ontonotes_loss: 0.0304
09/16 09:34:20 AM: Update 11178: task edges-ner-ontonotes, batch 178 (11178): mcc: 0.8997, acc: 0.8554, precision: 0.9322, recall: 0.8789, f1: 0.9047, edges-ner-ontonotes_loss: 0.0316
09/16 09:34:30 AM: Update 11284: task edges-ner-ontonotes, batch 284 (11284): mcc: 0.9040, acc: 0.8610, precision: 0.9349, recall: 0.8841, f1: 0.9088, edges-ner-ontonotes_loss: 0.0307
09/16 09:34:40 AM: Update 11403: task edges-ner-ontonotes, batch 403 (11403): mcc: 0.9059, acc: 0.8642, precision: 0.9355, recall: 0.8870, f1: 0.9106, edges-ner-ontonotes_loss: 0.0299
09/16 09:34:51 AM: Update 11519: task edges-ner-ontonotes, batch 519 (11519): mcc: 0.9078, acc: 0.8671, precision: 0.9362, recall: 0.8898, f1: 0.9124, edges-ner-ontonotes_loss: 0.0294
09/16 09:35:01 AM: Update 11642: task edges-ner-ontonotes, batch 642 (11642): mcc: 0.9121, acc: 0.8724, precision: 0.9392, recall: 0.8950, f1: 0.9166, edges-ner-ontonotes_loss: 0.0280
09/16 09:35:11 AM: Update 11766: task edges-ner-ontonotes, batch 766 (11766): mcc: 0.9154, acc: 0.8768, precision: 0.9414, recall: 0.8992, f1: 0.9198, edges-ner-ontonotes_loss: 0.0269
09/16 09:35:21 AM: Update 11873: task edges-ner-ontonotes, batch 873 (11873): mcc: 0.9171, acc: 0.8790, precision: 0.9424, recall: 0.9013, f1: 0.9214, edges-ner-ontonotes_loss: 0.0263
09/16 09:35:31 AM: Update 11994: task edges-ner-ontonotes, batch 994 (11994): mcc: 0.9181, acc: 0.8803, precision: 0.9430, recall: 0.9026, f1: 0.9224, edges-ner-ontonotes_loss: 0.0260
09/16 09:35:31 AM: ***** Step 12000 / Validation 12 *****
09/16 09:35:31 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:35:31 AM: Validating...
09/16 09:35:41 AM: Evaluate: task edges-ner-ontonotes, batch 86 (157): mcc: 0.9159, acc: 0.8860, precision: 0.9363, recall: 0.9050, f1: 0.9204, edges-ner-ontonotes_loss: 0.0288
09/16 09:35:50 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:35:50 AM: Best result seen so far for macro.
09/16 09:35:50 AM: Updating LR scheduler:
09/16 09:35:50 AM: 	Best result seen so far for macro_avg: 0.932
09/16 09:35:50 AM: 	# validation passes without improvement: 0
09/16 09:35:50 AM: edges-ner-ontonotes_loss: training: 0.025954 validation: 0.023958
09/16 09:35:50 AM: macro_avg: validation: 0.932201
09/16 09:35:50 AM: micro_avg: validation: 0.000000
09/16 09:35:50 AM: edges-ner-ontonotes_mcc: training: 0.918178 validation: 0.928369
09/16 09:35:50 AM: edges-ner-ontonotes_acc: training: 0.880339 validation: 0.900819
09/16 09:35:50 AM: edges-ner-ontonotes_precision: training: 0.942935 validation: 0.945207
09/16 09:35:50 AM: edges-ner-ontonotes_recall: training: 0.902730 validation: 0.919548
09/16 09:35:50 AM: edges-ner-ontonotes_f1: training: 0.922395 validation: 0.932201
09/16 09:35:50 AM: Global learning rate: 0.0001
09/16 09:35:50 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:35:51 AM: Update 12010: task edges-ner-ontonotes, batch 10 (12010): mcc: 0.9344, acc: 0.9006, precision: 0.9565, recall: 0.9198, f1: 0.9378, edges-ner-ontonotes_loss: 0.0224
09/16 09:36:01 AM: Update 12132: task edges-ner-ontonotes, batch 132 (12132): mcc: 0.9296, acc: 0.8951, precision: 0.9505, recall: 0.9166, f1: 0.9332, edges-ner-ontonotes_loss: 0.0224
09/16 09:36:11 AM: Update 12243: task edges-ner-ontonotes, batch 243 (12243): mcc: 0.9127, acc: 0.8733, precision: 0.9391, recall: 0.8963, f1: 0.9172, edges-ner-ontonotes_loss: 0.0291
09/16 09:36:21 AM: Update 12373: task edges-ner-ontonotes, batch 373 (12373): mcc: 0.9045, acc: 0.8625, precision: 0.9338, recall: 0.8861, f1: 0.9093, edges-ner-ontonotes_loss: 0.0321
09/16 09:36:31 AM: Update 12477: task edges-ner-ontonotes, batch 477 (12477): mcc: 0.9012, acc: 0.8580, precision: 0.9314, recall: 0.8823, f1: 0.9062, edges-ner-ontonotes_loss: 0.0329
09/16 09:36:41 AM: Update 12630: task edges-ner-ontonotes, batch 630 (12630): mcc: 0.9008, acc: 0.8572, precision: 0.9312, recall: 0.8816, f1: 0.9058, edges-ner-ontonotes_loss: 0.0326
09/16 09:36:52 AM: Update 12762: task edges-ner-ontonotes, batch 762 (12762): mcc: 0.9005, acc: 0.8567, precision: 0.9312, recall: 0.8812, f1: 0.9055, edges-ner-ontonotes_loss: 0.0325
09/16 09:37:02 AM: Update 12887: task edges-ner-ontonotes, batch 887 (12887): mcc: 0.9023, acc: 0.8591, precision: 0.9320, recall: 0.8837, f1: 0.9072, edges-ner-ontonotes_loss: 0.0318
09/16 09:37:11 AM: ***** Step 13000 / Validation 13 *****
09/16 09:37:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:37:11 AM: Validating...
09/16 09:37:12 AM: Evaluate: task edges-ner-ontonotes, batch 9 (157): mcc: 0.8427, acc: 0.7908, precision: 0.8800, recall: 0.8231, f1: 0.8506, edges-ner-ontonotes_loss: 0.0429
09/16 09:37:22 AM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.9172, acc: 0.8864, precision: 0.9383, recall: 0.9055, f1: 0.9216, edges-ner-ontonotes_loss: 0.0267
09/16 09:37:30 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:37:30 AM: Best result seen so far for macro.
09/16 09:37:30 AM: Updating LR scheduler:
09/16 09:37:30 AM: 	Best result seen so far for macro_avg: 0.933
09/16 09:37:30 AM: 	# validation passes without improvement: 0
09/16 09:37:30 AM: edges-ner-ontonotes_loss: training: 0.031318 validation: 0.023241
09/16 09:37:30 AM: macro_avg: validation: 0.932979
09/16 09:37:30 AM: micro_avg: validation: 0.000000
09/16 09:37:30 AM: edges-ner-ontonotes_mcc: training: 0.903666 validation: 0.929247
09/16 09:37:30 AM: edges-ner-ontonotes_acc: training: 0.861200 validation: 0.900212
09/16 09:37:30 AM: edges-ner-ontonotes_precision: training: 0.932733 validation: 0.948664
09/16 09:37:30 AM: edges-ner-ontonotes_recall: training: 0.885606 validation: 0.917804
09/16 09:37:30 AM: edges-ner-ontonotes_f1: training: 0.908559 validation: 0.932979
09/16 09:37:30 AM: Global learning rate: 0.0001
09/16 09:37:30 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:37:32 AM: Update 13033: task edges-ner-ontonotes, batch 33 (13033): mcc: 0.9162, acc: 0.8825, precision: 0.9387, recall: 0.9032, f1: 0.9206, edges-ner-ontonotes_loss: 0.0266
09/16 09:37:42 AM: Update 13137: task edges-ner-ontonotes, batch 137 (13137): mcc: 0.9203, acc: 0.8820, precision: 0.9427, recall: 0.9069, f1: 0.9245, edges-ner-ontonotes_loss: 0.0249
09/16 09:37:52 AM: Update 13261: task edges-ner-ontonotes, batch 261 (13261): mcc: 0.9255, acc: 0.8887, precision: 0.9467, recall: 0.9128, f1: 0.9294, edges-ner-ontonotes_loss: 0.0232
09/16 09:38:02 AM: Update 13385: task edges-ner-ontonotes, batch 385 (13385): mcc: 0.9272, acc: 0.8911, precision: 0.9481, recall: 0.9145, f1: 0.9310, edges-ner-ontonotes_loss: 0.0228
09/16 09:38:12 AM: Update 13487: task edges-ner-ontonotes, batch 487 (13487): mcc: 0.9260, acc: 0.8895, precision: 0.9471, recall: 0.9133, f1: 0.9299, edges-ner-ontonotes_loss: 0.0231
09/16 09:38:22 AM: Update 13610: task edges-ner-ontonotes, batch 610 (13610): mcc: 0.9267, acc: 0.8904, precision: 0.9472, recall: 0.9145, f1: 0.9306, edges-ner-ontonotes_loss: 0.0229
09/16 09:38:32 AM: Update 13712: task edges-ner-ontonotes, batch 712 (13712): mcc: 0.9263, acc: 0.8898, precision: 0.9472, recall: 0.9137, f1: 0.9301, edges-ner-ontonotes_loss: 0.0231
09/16 09:38:42 AM: Update 13843: task edges-ner-ontonotes, batch 843 (13843): mcc: 0.9209, acc: 0.8830, precision: 0.9434, recall: 0.9073, f1: 0.9250, edges-ner-ontonotes_loss: 0.0254
09/16 09:38:53 AM: Update 13966: task edges-ner-ontonotes, batch 966 (13966): mcc: 0.9168, acc: 0.8777, precision: 0.9407, recall: 0.9023, f1: 0.9211, edges-ner-ontonotes_loss: 0.0271
09/16 09:38:55 AM: ***** Step 14000 / Validation 14 *****
09/16 09:38:56 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:38:56 AM: Validating...
09/16 09:39:03 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9099, acc: 0.8762, precision: 0.9317, recall: 0.8982, f1: 0.9147, edges-ner-ontonotes_loss: 0.0299
09/16 09:39:13 AM: Evaluate: task edges-ner-ontonotes, batch 139 (157): mcc: 0.9261, acc: 0.8959, precision: 0.9466, recall: 0.9139, f1: 0.9300, edges-ner-ontonotes_loss: 0.0246
09/16 09:39:15 AM: Updating LR scheduler:
09/16 09:39:15 AM: 	Best result seen so far for macro_avg: 0.933
09/16 09:39:17 AM: 	# validation passes without improvement: 1
09/16 09:39:17 AM: edges-ner-ontonotes_loss: training: 0.027446 validation: 0.024184
09/16 09:39:17 AM: macro_avg: validation: 0.930279
09/16 09:39:17 AM: micro_avg: validation: 0.000000
09/16 09:39:17 AM: edges-ner-ontonotes_mcc: training: 0.916227 validation: 0.926443
09/16 09:39:17 AM: edges-ner-ontonotes_acc: training: 0.877010 validation: 0.895511
09/16 09:39:17 AM: edges-ner-ontonotes_precision: training: 0.940307 validation: 0.948114
09/16 09:39:17 AM: edges-ner-ontonotes_recall: training: 0.901645 validation: 0.913103
09/16 09:39:17 AM: edges-ner-ontonotes_f1: training: 0.920570 validation: 0.930279
09/16 09:39:17 AM: Global learning rate: 0.0001
09/16 09:39:17 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:39:23 AM: Update 14061: task edges-ner-ontonotes, batch 61 (14061): mcc: 0.8940, acc: 0.8456, precision: 0.9265, recall: 0.8736, f1: 0.8993, edges-ner-ontonotes_loss: 0.0326
09/16 09:39:33 AM: Update 14212: task edges-ner-ontonotes, batch 212 (14212): mcc: 0.9005, acc: 0.8555, precision: 0.9316, recall: 0.8808, f1: 0.9055, edges-ner-ontonotes_loss: 0.0311
09/16 09:39:43 AM: Update 14329: task edges-ner-ontonotes, batch 329 (14329): mcc: 0.8967, acc: 0.8514, precision: 0.9281, recall: 0.8772, f1: 0.9019, edges-ner-ontonotes_loss: 0.0318
09/16 09:39:53 AM: Update 14456: task edges-ner-ontonotes, batch 456 (14456): mcc: 0.9025, acc: 0.8587, precision: 0.9319, recall: 0.8842, f1: 0.9074, edges-ner-ontonotes_loss: 0.0307
09/16 09:40:03 AM: Update 14585: task edges-ner-ontonotes, batch 585 (14585): mcc: 0.9056, acc: 0.8635, precision: 0.9333, recall: 0.8886, f1: 0.9104, edges-ner-ontonotes_loss: 0.0298
09/16 09:40:13 AM: Update 14681: task edges-ner-ontonotes, batch 681 (14681): mcc: 0.9081, acc: 0.8668, precision: 0.9352, recall: 0.8915, f1: 0.9128, edges-ner-ontonotes_loss: 0.0290
09/16 09:40:23 AM: Update 14802: task edges-ner-ontonotes, batch 802 (14802): mcc: 0.9120, acc: 0.8715, precision: 0.9378, recall: 0.8962, f1: 0.9166, edges-ner-ontonotes_loss: 0.0278
09/16 09:40:33 AM: Update 14924: task edges-ner-ontonotes, batch 924 (14924): mcc: 0.9154, acc: 0.8759, precision: 0.9400, recall: 0.9003, f1: 0.9198, edges-ner-ontonotes_loss: 0.0269
09/16 09:40:41 AM: ***** Step 15000 / Validation 15 *****
09/16 09:40:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:40:41 AM: Validating...
09/16 09:40:43 AM: Evaluate: task edges-ner-ontonotes, batch 19 (157): mcc: 0.8726, acc: 0.8305, precision: 0.9045, recall: 0.8552, f1: 0.8792, edges-ner-ontonotes_loss: 0.0370
09/16 09:40:53 AM: Evaluate: task edges-ner-ontonotes, batch 107 (157): mcc: 0.9191, acc: 0.8895, precision: 0.9402, recall: 0.9070, f1: 0.9233, edges-ner-ontonotes_loss: 0.0273
09/16 09:41:00 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:41:00 AM: Best result seen so far for macro.
09/16 09:41:00 AM: Updating LR scheduler:
09/16 09:41:00 AM: 	Best result seen so far for macro_avg: 0.934
09/16 09:41:00 AM: 	# validation passes without improvement: 0
09/16 09:41:00 AM: edges-ner-ontonotes_loss: training: 0.026602 validation: 0.023523
09/16 09:41:00 AM: macro_avg: validation: 0.934252
09/16 09:41:00 AM: micro_avg: validation: 0.000000
09/16 09:41:00 AM: edges-ner-ontonotes_mcc: training: 0.916031 validation: 0.930569
09/16 09:41:00 AM: edges-ner-ontonotes_acc: training: 0.876729 validation: 0.902866
09/16 09:41:00 AM: edges-ner-ontonotes_precision: training: 0.940362 validation: 0.948788
09/16 09:41:00 AM: edges-ner-ontonotes_recall: training: 0.901227 validation: 0.920155
09/16 09:41:00 AM: edges-ner-ontonotes_f1: training: 0.920379 validation: 0.934252
09/16 09:41:00 AM: Global learning rate: 0.0001
09/16 09:41:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:41:03 AM: Update 15035: task edges-ner-ontonotes, batch 35 (15035): mcc: 0.9277, acc: 0.8913, precision: 0.9475, recall: 0.9160, f1: 0.9315, edges-ner-ontonotes_loss: 0.0223
09/16 09:41:13 AM: Update 15158: task edges-ner-ontonotes, batch 158 (15158): mcc: 0.9260, acc: 0.8887, precision: 0.9466, recall: 0.9136, f1: 0.9298, edges-ner-ontonotes_loss: 0.0225
09/16 09:41:23 AM: Update 15258: task edges-ner-ontonotes, batch 258 (15258): mcc: 0.9258, acc: 0.8892, precision: 0.9463, recall: 0.9138, f1: 0.9297, edges-ner-ontonotes_loss: 0.0224
09/16 09:41:33 AM: Update 15386: task edges-ner-ontonotes, batch 386 (15386): mcc: 0.9155, acc: 0.8762, precision: 0.9400, recall: 0.9007, f1: 0.9199, edges-ner-ontonotes_loss: 0.0274
09/16 09:41:43 AM: Update 15522: task edges-ner-ontonotes, batch 522 (15522): mcc: 0.9093, acc: 0.8682, precision: 0.9359, recall: 0.8930, f1: 0.9139, edges-ner-ontonotes_loss: 0.0303
09/16 09:41:53 AM: Update 15646: task edges-ner-ontonotes, batch 646 (15646): mcc: 0.9065, acc: 0.8645, precision: 0.9339, recall: 0.8897, f1: 0.9113, edges-ner-ontonotes_loss: 0.0310
09/16 09:42:03 AM: Update 15794: task edges-ner-ontonotes, batch 794 (15794): mcc: 0.9054, acc: 0.8629, precision: 0.9333, recall: 0.8883, f1: 0.9103, edges-ner-ontonotes_loss: 0.0311
09/16 09:42:13 AM: Update 15908: task edges-ner-ontonotes, batch 908 (15908): mcc: 0.9048, acc: 0.8621, precision: 0.9329, recall: 0.8876, f1: 0.9097, edges-ner-ontonotes_loss: 0.0310
09/16 09:42:21 AM: ***** Step 16000 / Validation 16 *****
09/16 09:42:21 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:42:21 AM: Validating...
09/16 09:42:23 AM: Evaluate: task edges-ner-ontonotes, batch 30 (157): mcc: 0.8888, acc: 0.8549, precision: 0.9141, recall: 0.8760, f1: 0.8946, edges-ner-ontonotes_loss: 0.0360
09/16 09:42:34 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9217, acc: 0.8910, precision: 0.9429, recall: 0.9093, f1: 0.9258, edges-ner-ontonotes_loss: 0.0257
09/16 09:42:40 AM: Updating LR scheduler:
09/16 09:42:40 AM: 	Best result seen so far for macro_avg: 0.934
09/16 09:42:40 AM: 	# validation passes without improvement: 1
09/16 09:42:40 AM: edges-ner-ontonotes_loss: training: 0.030478 validation: 0.023084
09/16 09:42:40 AM: macro_avg: validation: 0.933894
09/16 09:42:40 AM: micro_avg: validation: 0.000000
09/16 09:42:40 AM: edges-ner-ontonotes_mcc: training: 0.906307 validation: 0.930232
09/16 09:42:40 AM: edges-ner-ontonotes_acc: training: 0.864110 validation: 0.901577
09/16 09:42:40 AM: edges-ner-ontonotes_precision: training: 0.933867 validation: 0.950314
09/16 09:42:40 AM: edges-ner-ontonotes_recall: training: 0.889418 validation: 0.918032
09/16 09:42:40 AM: edges-ner-ontonotes_f1: training: 0.911101 validation: 0.933894
09/16 09:42:40 AM: Global learning rate: 0.0001
09/16 09:42:40 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:42:44 AM: Update 16059: task edges-ner-ontonotes, batch 59 (16059): mcc: 0.9116, acc: 0.8737, precision: 0.9356, recall: 0.8976, f1: 0.9162, edges-ner-ontonotes_loss: 0.0270
09/16 09:42:54 AM: Update 16185: task edges-ner-ontonotes, batch 185 (16185): mcc: 0.9097, acc: 0.8707, precision: 0.9331, recall: 0.8965, f1: 0.9144, edges-ner-ontonotes_loss: 0.0276
09/16 09:43:04 AM: Update 16290: task edges-ner-ontonotes, batch 290 (16290): mcc: 0.9169, acc: 0.8795, precision: 0.9397, recall: 0.9036, f1: 0.9213, edges-ner-ontonotes_loss: 0.0255
09/16 09:43:15 AM: Update 16416: task edges-ner-ontonotes, batch 416 (16416): mcc: 0.9215, acc: 0.8849, precision: 0.9430, recall: 0.9089, f1: 0.9256, edges-ner-ontonotes_loss: 0.0241
09/16 09:43:25 AM: Update 16503: task edges-ner-ontonotes, batch 503 (16503): mcc: 0.9240, acc: 0.8881, precision: 0.9452, recall: 0.9114, f1: 0.9280, edges-ner-ontonotes_loss: 0.0235
09/16 09:43:35 AM: Update 16626: task edges-ner-ontonotes, batch 626 (16626): mcc: 0.9255, acc: 0.8898, precision: 0.9459, recall: 0.9135, f1: 0.9294, edges-ner-ontonotes_loss: 0.0232
09/16 09:43:45 AM: Update 16741: task edges-ner-ontonotes, batch 741 (16741): mcc: 0.9256, acc: 0.8899, precision: 0.9462, recall: 0.9135, f1: 0.9295, edges-ner-ontonotes_loss: 0.0232
09/16 09:43:55 AM: Update 16847: task edges-ner-ontonotes, batch 847 (16847): mcc: 0.9244, acc: 0.8880, precision: 0.9455, recall: 0.9119, f1: 0.9284, edges-ner-ontonotes_loss: 0.0236
09/16 09:44:05 AM: Update 16972: task edges-ner-ontonotes, batch 972 (16972): mcc: 0.9204, acc: 0.8832, precision: 0.9428, recall: 0.9070, f1: 0.9245, edges-ner-ontonotes_loss: 0.0254
09/16 09:44:07 AM: ***** Step 17000 / Validation 17 *****
09/16 09:44:07 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:44:07 AM: Validating...
09/16 09:44:15 AM: Evaluate: task edges-ner-ontonotes, batch 72 (157): mcc: 0.9078, acc: 0.8702, precision: 0.9350, recall: 0.8911, f1: 0.9125, edges-ner-ontonotes_loss: 0.0306
09/16 09:44:25 AM: Evaluate: task edges-ner-ontonotes, batch 152 (157): mcc: 0.9249, acc: 0.8929, precision: 0.9483, recall: 0.9101, f1: 0.9288, edges-ner-ontonotes_loss: 0.0245
09/16 09:44:25 AM: Updating LR scheduler:
09/16 09:44:25 AM: 	Best result seen so far for macro_avg: 0.934
09/16 09:44:25 AM: 	# validation passes without improvement: 2
09/16 09:44:25 AM: edges-ner-ontonotes_loss: training: 0.025856 validation: 0.024306
09/16 09:44:25 AM: macro_avg: validation: 0.929398
09/16 09:44:25 AM: micro_avg: validation: 0.000000
09/16 09:44:25 AM: edges-ner-ontonotes_mcc: training: 0.919546 validation: 0.925549
09/16 09:44:25 AM: edges-ner-ontonotes_acc: training: 0.882326 validation: 0.893691
09/16 09:44:25 AM: edges-ner-ontonotes_precision: training: 0.942130 validation: 0.948740
09/16 09:44:25 AM: edges-ner-ontonotes_recall: training: 0.906060 validation: 0.910828
09/16 09:44:25 AM: edges-ner-ontonotes_f1: training: 0.923743 validation: 0.929398
09/16 09:44:25 AM: Global learning rate: 0.0001
09/16 09:44:25 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:44:36 AM: Update 17117: task edges-ner-ontonotes, batch 117 (17117): mcc: 0.8893, acc: 0.8432, precision: 0.9228, recall: 0.8685, f1: 0.8948, edges-ner-ontonotes_loss: 0.0369
09/16 09:44:46 AM: Update 17269: task edges-ner-ontonotes, batch 269 (17269): mcc: 0.8955, acc: 0.8508, precision: 0.9262, recall: 0.8767, f1: 0.9008, edges-ner-ontonotes_loss: 0.0341
09/16 09:44:56 AM: Update 17424: task edges-ner-ontonotes, batch 424 (17424): mcc: 0.8981, acc: 0.8541, precision: 0.9285, recall: 0.8795, f1: 0.9033, edges-ner-ontonotes_loss: 0.0329
09/16 09:45:06 AM: Update 17528: task edges-ner-ontonotes, batch 528 (17528): mcc: 0.9006, acc: 0.8570, precision: 0.9294, recall: 0.8830, f1: 0.9056, edges-ner-ontonotes_loss: 0.0321
09/16 09:45:16 AM: Update 17652: task edges-ner-ontonotes, batch 652 (17652): mcc: 0.9036, acc: 0.8611, precision: 0.9314, recall: 0.8868, f1: 0.9086, edges-ner-ontonotes_loss: 0.0310
09/16 09:45:26 AM: Update 17767: task edges-ner-ontonotes, batch 767 (17767): mcc: 0.9065, acc: 0.8648, precision: 0.9337, recall: 0.8900, f1: 0.9113, edges-ner-ontonotes_loss: 0.0300
09/16 09:45:36 AM: Update 17885: task edges-ner-ontonotes, batch 885 (17885): mcc: 0.9103, acc: 0.8696, precision: 0.9360, recall: 0.8947, f1: 0.9149, edges-ner-ontonotes_loss: 0.0288
09/16 09:45:46 AM: ***** Step 18000 / Validation 18 *****
09/16 09:45:46 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:45:46 AM: Validating...
09/16 09:45:46 AM: Evaluate: task edges-ner-ontonotes, batch 8 (157): mcc: 0.8319, acc: 0.7795, precision: 0.8730, recall: 0.8099, f1: 0.8402, edges-ner-ontonotes_loss: 0.0468
09/16 09:45:59 AM: Evaluate: task edges-ner-ontonotes, batch 91 (157): mcc: 0.9193, acc: 0.8881, precision: 0.9430, recall: 0.9048, f1: 0.9235, edges-ner-ontonotes_loss: 0.0278
09/16 09:46:07 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:46:07 AM: Best result seen so far for macro.
09/16 09:46:07 AM: Updating LR scheduler:
09/16 09:46:07 AM: 	Best result seen so far for macro_avg: 0.935
09/16 09:46:07 AM: 	# validation passes without improvement: 0
09/16 09:46:07 AM: edges-ner-ontonotes_loss: training: 0.027845 validation: 0.023197
09/16 09:46:07 AM: macro_avg: validation: 0.934628
09/16 09:46:07 AM: micro_avg: validation: 0.000000
09/16 09:46:07 AM: edges-ner-ontonotes_mcc: training: 0.913381 validation: 0.930995
09/16 09:46:07 AM: edges-ner-ontonotes_acc: training: 0.873635 validation: 0.902715
09/16 09:46:07 AM: edges-ner-ontonotes_precision: training: 0.938058 validation: 0.950455
09/16 09:46:07 AM: edges-ner-ontonotes_recall: training: 0.898525 validation: 0.919321
09/16 09:46:07 AM: edges-ner-ontonotes_f1: training: 0.917866 validation: 0.934628
09/16 09:46:07 AM: Global learning rate: 0.0001
09/16 09:46:07 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:46:09 AM: Update 18019: task edges-ner-ontonotes, batch 19 (18019): mcc: 0.9263, acc: 0.8852, precision: 0.9447, recall: 0.9161, f1: 0.9302, edges-ner-ontonotes_loss: 0.0229
09/16 09:46:19 AM: Update 18117: task edges-ner-ontonotes, batch 117 (18117): mcc: 0.9282, acc: 0.8917, precision: 0.9460, recall: 0.9184, f1: 0.9320, edges-ner-ontonotes_loss: 0.0225
09/16 09:46:29 AM: Update 18241: task edges-ner-ontonotes, batch 241 (18241): mcc: 0.9284, acc: 0.8920, precision: 0.9481, recall: 0.9167, f1: 0.9321, edges-ner-ontonotes_loss: 0.0221
09/16 09:46:39 AM: Update 18364: task edges-ner-ontonotes, batch 364 (18364): mcc: 0.9284, acc: 0.8919, precision: 0.9481, recall: 0.9169, f1: 0.9322, edges-ner-ontonotes_loss: 0.0223
09/16 09:46:49 AM: Update 18468: task edges-ner-ontonotes, batch 468 (18468): mcc: 0.9195, acc: 0.8808, precision: 0.9417, recall: 0.9064, f1: 0.9237, edges-ner-ontonotes_loss: 0.0258
09/16 09:46:59 AM: Update 18600: task edges-ner-ontonotes, batch 600 (18600): mcc: 0.9140, acc: 0.8738, precision: 0.9387, recall: 0.8991, f1: 0.9185, edges-ner-ontonotes_loss: 0.0282
09/16 09:47:09 AM: Update 18707: task edges-ner-ontonotes, batch 707 (18707): mcc: 0.9111, acc: 0.8701, precision: 0.9368, recall: 0.8956, f1: 0.9157, edges-ner-ontonotes_loss: 0.0294
09/16 09:47:19 AM: Update 18854: task edges-ner-ontonotes, batch 854 (18854): mcc: 0.9102, acc: 0.8689, precision: 0.9363, recall: 0.8943, f1: 0.9148, edges-ner-ontonotes_loss: 0.0294
09/16 09:47:30 AM: Update 18986: task edges-ner-ontonotes, batch 986 (18986): mcc: 0.9092, acc: 0.8674, precision: 0.9357, recall: 0.8929, f1: 0.9138, edges-ner-ontonotes_loss: 0.0296
09/16 09:47:31 AM: ***** Step 19000 / Validation 19 *****
09/16 09:47:31 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:47:31 AM: Validating...
09/16 09:47:40 AM: Evaluate: task edges-ner-ontonotes, batch 80 (157): mcc: 0.9237, acc: 0.8929, precision: 0.9426, recall: 0.9133, f1: 0.9277, edges-ner-ontonotes_loss: 0.0266
09/16 09:47:50 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:47:50 AM: Best result seen so far for macro.
09/16 09:47:50 AM: Updating LR scheduler:
09/16 09:47:50 AM: 	Best result seen so far for macro_avg: 0.936
09/16 09:47:50 AM: 	# validation passes without improvement: 0
09/16 09:47:50 AM: edges-ner-ontonotes_loss: training: 0.029603 validation: 0.022621
09/16 09:47:50 AM: macro_avg: validation: 0.935936
09/16 09:47:50 AM: micro_avg: validation: 0.000000
09/16 09:47:50 AM: edges-ner-ontonotes_mcc: training: 0.909178 validation: 0.932319
09/16 09:47:50 AM: edges-ner-ontonotes_acc: training: 0.867338 validation: 0.903852
09/16 09:47:50 AM: edges-ner-ontonotes_precision: training: 0.935833 validation: 0.948882
09/16 09:47:50 AM: edges-ner-ontonotes_recall: training: 0.892856 validation: 0.923339
09/16 09:47:50 AM: edges-ner-ontonotes_f1: training: 0.913840 validation: 0.935936
09/16 09:47:50 AM: Global learning rate: 0.0001
09/16 09:47:50 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:47:50 AM: Update 19003: task edges-ner-ontonotes, batch 3 (19003): mcc: 0.9137, acc: 0.8843, precision: 0.9249, recall: 0.9120, f1: 0.9184, edges-ner-ontonotes_loss: 0.0267
09/16 09:48:00 AM: Update 19125: task edges-ner-ontonotes, batch 125 (19125): mcc: 0.9132, acc: 0.8740, precision: 0.9383, recall: 0.8979, f1: 0.9177, edges-ner-ontonotes_loss: 0.0273
09/16 09:48:10 AM: Update 19249: task edges-ner-ontonotes, batch 249 (19249): mcc: 0.9140, acc: 0.8738, precision: 0.9386, recall: 0.8991, f1: 0.9184, edges-ner-ontonotes_loss: 0.0267
09/16 09:48:20 AM: Update 19349: task edges-ner-ontonotes, batch 349 (19349): mcc: 0.9177, acc: 0.8787, precision: 0.9407, recall: 0.9040, f1: 0.9220, edges-ner-ontonotes_loss: 0.0259
09/16 09:48:31 AM: Update 19471: task edges-ner-ontonotes, batch 471 (19471): mcc: 0.9219, acc: 0.8848, precision: 0.9435, recall: 0.9092, f1: 0.9260, edges-ner-ontonotes_loss: 0.0245
09/16 09:48:41 AM: Update 19595: task edges-ner-ontonotes, batch 595 (19595): mcc: 0.9240, acc: 0.8874, precision: 0.9450, recall: 0.9115, f1: 0.9279, edges-ner-ontonotes_loss: 0.0237
09/16 09:48:51 AM: Update 19701: task edges-ner-ontonotes, batch 701 (19701): mcc: 0.9247, acc: 0.8884, precision: 0.9453, recall: 0.9126, f1: 0.9287, edges-ner-ontonotes_loss: 0.0235
09/16 09:49:01 AM: Update 19825: task edges-ner-ontonotes, batch 825 (19825): mcc: 0.9254, acc: 0.8892, precision: 0.9460, recall: 0.9133, f1: 0.9293, edges-ner-ontonotes_loss: 0.0232
09/16 09:49:11 AM: Update 19925: task edges-ner-ontonotes, batch 925 (19925): mcc: 0.9260, acc: 0.8899, precision: 0.9463, recall: 0.9141, f1: 0.9299, edges-ner-ontonotes_loss: 0.0231
09/16 09:49:17 AM: ***** Step 20000 / Validation 20 *****
09/16 09:49:17 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:49:17 AM: Validating...
09/16 09:49:21 AM: Evaluate: task edges-ner-ontonotes, batch 42 (157): mcc: 0.9059, acc: 0.8763, precision: 0.9222, recall: 0.9000, f1: 0.9110, edges-ner-ontonotes_loss: 0.0320
09/16 09:49:33 AM: Evaluate: task edges-ner-ontonotes, batch 118 (157): mcc: 0.9228, acc: 0.8933, precision: 0.9406, recall: 0.9136, f1: 0.9269, edges-ner-ontonotes_loss: 0.0254
09/16 09:49:37 AM: Updating LR scheduler:
09/16 09:49:37 AM: 	Best result seen so far for macro_avg: 0.936
09/16 09:49:37 AM: 	# validation passes without improvement: 1
09/16 09:49:37 AM: edges-ner-ontonotes_loss: training: 0.024332 validation: 0.023078
09/16 09:49:37 AM: macro_avg: validation: 0.933805
09/16 09:49:37 AM: micro_avg: validation: 0.000000
09/16 09:49:37 AM: edges-ner-ontonotes_mcc: training: 0.923222 validation: 0.930092
09/16 09:49:37 AM: edges-ner-ontonotes_acc: training: 0.886411 validation: 0.902411
09/16 09:49:37 AM: edges-ner-ontonotes_precision: training: 0.944359 validation: 0.948109
09/16 09:49:37 AM: edges-ner-ontonotes_recall: training: 0.910750 validation: 0.919927
09/16 09:49:37 AM: edges-ner-ontonotes_f1: training: 0.927250 validation: 0.933805
09/16 09:49:37 AM: Global learning rate: 0.0001
09/16 09:49:37 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:49:43 AM: Update 20072: task edges-ner-ontonotes, batch 72 (20072): mcc: 0.8950, acc: 0.8483, precision: 0.9276, recall: 0.8745, f1: 0.9003, edges-ner-ontonotes_loss: 0.0355
09/16 09:49:53 AM: Update 20190: task edges-ner-ontonotes, batch 190 (20190): mcc: 0.8962, acc: 0.8524, precision: 0.9275, recall: 0.8768, f1: 0.9014, edges-ner-ontonotes_loss: 0.0348
09/16 09:50:03 AM: Update 20304: task edges-ner-ontonotes, batch 304 (20304): mcc: 0.8941, acc: 0.8496, precision: 0.9253, recall: 0.8750, f1: 0.8994, edges-ner-ontonotes_loss: 0.0348
09/16 09:50:13 AM: Update 20456: task edges-ner-ontonotes, batch 456 (20456): mcc: 0.8968, acc: 0.8527, precision: 0.9274, recall: 0.8780, f1: 0.9020, edges-ner-ontonotes_loss: 0.0333
09/16 09:50:23 AM: Update 20568: task edges-ner-ontonotes, batch 568 (20568): mcc: 0.8982, acc: 0.8545, precision: 0.9280, recall: 0.8800, f1: 0.9034, edges-ner-ontonotes_loss: 0.0326
09/16 09:50:33 AM: Update 20696: task edges-ner-ontonotes, batch 696 (20696): mcc: 0.9019, acc: 0.8588, precision: 0.9306, recall: 0.8844, f1: 0.9069, edges-ner-ontonotes_loss: 0.0315
09/16 09:50:43 AM: Update 20823: task edges-ner-ontonotes, batch 823 (20823): mcc: 0.9042, acc: 0.8620, precision: 0.9317, recall: 0.8876, f1: 0.9091, edges-ner-ontonotes_loss: 0.0307
09/16 09:50:53 AM: Update 20922: task edges-ner-ontonotes, batch 922 (20922): mcc: 0.9073, acc: 0.8659, precision: 0.9338, recall: 0.8913, f1: 0.9121, edges-ner-ontonotes_loss: 0.0298
09/16 09:51:00 AM: ***** Step 21000 / Validation 21 *****
09/16 09:51:00 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:51:00 AM: Validating...
09/16 09:51:03 AM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.8932, acc: 0.8603, precision: 0.9170, recall: 0.8814, f1: 0.8989, edges-ner-ontonotes_loss: 0.0343
09/16 09:51:13 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9221, acc: 0.8930, precision: 0.9404, recall: 0.9126, f1: 0.9263, edges-ner-ontonotes_loss: 0.0258
09/16 09:51:19 AM: Updating LR scheduler:
09/16 09:51:19 AM: 	Best result seen so far for macro_avg: 0.936
09/16 09:51:19 AM: 	# validation passes without improvement: 2
09/16 09:51:19 AM: edges-ner-ontonotes_loss: training: 0.029041 validation: 0.022874
09/16 09:51:19 AM: macro_avg: validation: 0.935352
09/16 09:51:19 AM: micro_avg: validation: 0.000000
09/16 09:51:19 AM: edges-ner-ontonotes_mcc: training: 0.910102 validation: 0.931704
09/16 09:51:19 AM: edges-ner-ontonotes_acc: training: 0.869445 validation: 0.904686
09/16 09:51:19 AM: edges-ner-ontonotes_precision: training: 0.935822 validation: 0.948402
09/16 09:51:19 AM: edges-ner-ontonotes_recall: training: 0.894586 validation: 0.922657
09/16 09:51:19 AM: edges-ner-ontonotes_f1: training: 0.914740 validation: 0.935352
09/16 09:51:19 AM: Global learning rate: 0.0001
09/16 09:51:19 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:51:23 AM: Update 21059: task edges-ner-ontonotes, batch 59 (21059): mcc: 0.9312, acc: 0.8974, precision: 0.9482, recall: 0.9219, f1: 0.9349, edges-ner-ontonotes_loss: 0.0213
09/16 09:51:34 AM: Update 21168: task edges-ner-ontonotes, batch 168 (21168): mcc: 0.9293, acc: 0.8957, precision: 0.9472, recall: 0.9194, f1: 0.9331, edges-ner-ontonotes_loss: 0.0213
09/16 09:51:44 AM: Update 21292: task edges-ner-ontonotes, batch 292 (21292): mcc: 0.9291, acc: 0.8934, precision: 0.9476, recall: 0.9185, f1: 0.9328, edges-ner-ontonotes_loss: 0.0213
09/16 09:51:54 AM: Update 21418: task edges-ner-ontonotes, batch 418 (21418): mcc: 0.9293, acc: 0.8937, precision: 0.9473, recall: 0.9192, f1: 0.9330, edges-ner-ontonotes_loss: 0.0216
09/16 09:52:04 AM: Update 21526: task edges-ner-ontonotes, batch 526 (21526): mcc: 0.9259, acc: 0.8896, precision: 0.9453, recall: 0.9149, f1: 0.9298, edges-ner-ontonotes_loss: 0.0231
09/16 09:52:14 AM: Update 21653: task edges-ner-ontonotes, batch 653 (21653): mcc: 0.9198, acc: 0.8818, precision: 0.9416, recall: 0.9071, f1: 0.9240, edges-ner-ontonotes_loss: 0.0257
09/16 09:52:24 AM: Update 21778: task edges-ner-ontonotes, batch 778 (21778): mcc: 0.9157, acc: 0.8765, precision: 0.9390, recall: 0.9020, f1: 0.9201, edges-ner-ontonotes_loss: 0.0274
09/16 09:52:34 AM: Update 21902: task edges-ner-ontonotes, batch 902 (21902): mcc: 0.9138, acc: 0.8740, precision: 0.9377, recall: 0.8997, f1: 0.9183, edges-ner-ontonotes_loss: 0.0280
09/16 09:52:41 AM: ***** Step 22000 / Validation 22 *****
09/16 09:52:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:52:41 AM: Validating...
09/16 09:52:44 AM: Evaluate: task edges-ner-ontonotes, batch 33 (157): mcc: 0.8979, acc: 0.8618, precision: 0.9236, recall: 0.8837, f1: 0.9032, edges-ner-ontonotes_loss: 0.0324
09/16 09:52:55 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9230, acc: 0.8919, precision: 0.9456, recall: 0.9091, f1: 0.9270, edges-ner-ontonotes_loss: 0.0249
09/16 09:53:00 AM: Updating LR scheduler:
09/16 09:53:00 AM: 	Best result seen so far for macro_avg: 0.936
09/16 09:53:00 AM: 	# validation passes without improvement: 3
09/16 09:53:00 AM: edges-ner-ontonotes_loss: training: 0.028294 validation: 0.022630
09/16 09:53:00 AM: macro_avg: validation: 0.934132
09/16 09:53:00 AM: micro_avg: validation: 0.000000
09/16 09:53:00 AM: edges-ner-ontonotes_mcc: training: 0.912941 validation: 0.930522
09/16 09:53:00 AM: edges-ner-ontonotes_acc: training: 0.872779 validation: 0.901350
09/16 09:53:00 AM: edges-ner-ontonotes_precision: training: 0.937149 validation: 0.952193
09/16 09:53:00 AM: edges-ner-ontonotes_recall: training: 0.898588 validation: 0.916743
09/16 09:53:00 AM: edges-ner-ontonotes_f1: training: 0.917463 validation: 0.934132
09/16 09:53:00 AM: Global learning rate: 0.0001
09/16 09:53:00 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:53:05 AM: Update 22074: task edges-ner-ontonotes, batch 74 (22074): mcc: 0.9051, acc: 0.8598, precision: 0.9341, recall: 0.8869, f1: 0.9099, edges-ner-ontonotes_loss: 0.0286
09/16 09:53:17 AM: Update 22175: task edges-ner-ontonotes, batch 175 (22175): mcc: 0.9083, acc: 0.8666, precision: 0.9344, recall: 0.8925, f1: 0.9130, edges-ner-ontonotes_loss: 0.0279
09/16 09:53:27 AM: Update 22303: task edges-ner-ontonotes, batch 303 (22303): mcc: 0.9105, acc: 0.8700, precision: 0.9351, recall: 0.8961, f1: 0.9152, edges-ner-ontonotes_loss: 0.0274
09/16 09:53:37 AM: Update 22416: task edges-ner-ontonotes, batch 416 (22416): mcc: 0.9114, acc: 0.8716, precision: 0.9362, recall: 0.8967, f1: 0.9160, edges-ner-ontonotes_loss: 0.0272
09/16 09:53:47 AM: Update 22534: task edges-ner-ontonotes, batch 534 (22534): mcc: 0.9166, acc: 0.8780, precision: 0.9397, recall: 0.9029, f1: 0.9209, edges-ner-ontonotes_loss: 0.0259
09/16 09:53:57 AM: Update 22658: task edges-ner-ontonotes, batch 658 (22658): mcc: 0.9206, acc: 0.8832, precision: 0.9425, recall: 0.9076, f1: 0.9247, edges-ner-ontonotes_loss: 0.0247
09/16 09:54:07 AM: Update 22760: task edges-ner-ontonotes, batch 760 (22760): mcc: 0.9228, acc: 0.8862, precision: 0.9440, recall: 0.9103, f1: 0.9268, edges-ner-ontonotes_loss: 0.0241
09/16 09:54:18 AM: Update 22883: task edges-ner-ontonotes, batch 883 (22883): mcc: 0.9237, acc: 0.8873, precision: 0.9447, recall: 0.9113, f1: 0.9277, edges-ner-ontonotes_loss: 0.0238
09/16 09:54:27 AM: ***** Step 23000 / Validation 23 *****
09/16 09:54:27 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:54:27 AM: Validating...
09/16 09:54:28 AM: Evaluate: task edges-ner-ontonotes, batch 5 (157): mcc: 0.8138, acc: 0.7552, precision: 0.8576, recall: 0.7910, f1: 0.8230, edges-ner-ontonotes_loss: 0.0550
09/16 09:54:38 AM: Evaluate: task edges-ner-ontonotes, batch 96 (157): mcc: 0.9234, acc: 0.8955, precision: 0.9414, recall: 0.9141, f1: 0.9275, edges-ner-ontonotes_loss: 0.0272
09/16 09:54:46 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:54:46 AM: Best result seen so far for macro.
09/16 09:54:46 AM: Updating LR scheduler:
09/16 09:54:46 AM: 	Best result seen so far for macro_avg: 0.936
09/16 09:54:46 AM: 	# validation passes without improvement: 0
09/16 09:54:46 AM: edges-ner-ontonotes_loss: training: 0.023554 validation: 0.022897
09/16 09:54:46 AM: macro_avg: validation: 0.936496
09/16 09:54:46 AM: micro_avg: validation: 0.000000
09/16 09:54:46 AM: edges-ner-ontonotes_mcc: training: 0.924430 validation: 0.932865
09/16 09:54:46 AM: edges-ner-ontonotes_acc: training: 0.888347 validation: 0.907264
09/16 09:54:46 AM: edges-ner-ontonotes_precision: training: 0.945054 validation: 0.946765
09/16 09:54:46 AM: edges-ner-ontonotes_recall: training: 0.912328 validation: 0.926448
09/16 09:54:46 AM: edges-ner-ontonotes_f1: training: 0.928403 validation: 0.936496
09/16 09:54:46 AM: Global learning rate: 0.0001
09/16 09:54:46 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:54:48 AM: Update 23026: task edges-ner-ontonotes, batch 26 (23026): mcc: 0.9339, acc: 0.8995, precision: 0.9471, recall: 0.9280, f1: 0.9375, edges-ner-ontonotes_loss: 0.0203
09/16 09:54:58 AM: Update 23125: task edges-ner-ontonotes, batch 125 (23125): mcc: 0.9002, acc: 0.8575, precision: 0.9282, recall: 0.8835, f1: 0.9053, edges-ner-ontonotes_loss: 0.0332
09/16 09:55:08 AM: Update 23254: task edges-ner-ontonotes, batch 254 (23254): mcc: 0.8969, acc: 0.8531, precision: 0.9270, recall: 0.8785, f1: 0.9021, edges-ner-ontonotes_loss: 0.0346
09/16 09:55:18 AM: Update 23366: task edges-ner-ontonotes, batch 366 (23366): mcc: 0.8962, acc: 0.8526, precision: 0.9267, recall: 0.8776, f1: 0.9014, edges-ner-ontonotes_loss: 0.0346
09/16 09:55:28 AM: Update 23506: task edges-ner-ontonotes, batch 506 (23506): mcc: 0.8975, acc: 0.8545, precision: 0.9274, recall: 0.8793, f1: 0.9027, edges-ner-ontonotes_loss: 0.0334
09/16 09:55:40 AM: Update 23654: task edges-ner-ontonotes, batch 654 (23654): mcc: 0.8988, acc: 0.8556, precision: 0.9285, recall: 0.8806, f1: 0.9039, edges-ner-ontonotes_loss: 0.0326
09/16 09:55:50 AM: Update 23776: task edges-ner-ontonotes, batch 776 (23776): mcc: 0.9023, acc: 0.8598, precision: 0.9306, recall: 0.8852, f1: 0.9073, edges-ner-ontonotes_loss: 0.0315
09/16 09:56:00 AM: Update 23901: task edges-ner-ontonotes, batch 901 (23901): mcc: 0.9038, acc: 0.8620, precision: 0.9311, recall: 0.8875, f1: 0.9088, edges-ner-ontonotes_loss: 0.0309
09/16 09:56:10 AM: Update 23996: task edges-ner-ontonotes, batch 996 (23996): mcc: 0.9058, acc: 0.8642, precision: 0.9323, recall: 0.8899, f1: 0.9106, edges-ner-ontonotes_loss: 0.0303
09/16 09:56:10 AM: ***** Step 24000 / Validation 24 *****
09/16 09:56:10 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:56:10 AM: Validating...
09/16 09:56:20 AM: Evaluate: task edges-ner-ontonotes, batch 86 (157): mcc: 0.9235, acc: 0.8942, precision: 0.9414, recall: 0.9141, f1: 0.9275, edges-ner-ontonotes_loss: 0.0266
09/16 09:56:29 AM: Updating LR scheduler:
09/16 09:56:29 AM: 	Best result seen so far for macro_avg: 0.936
09/16 09:56:31 AM: 	# validation passes without improvement: 1
09/16 09:56:31 AM: edges-ner-ontonotes_loss: training: 0.030263 validation: 0.022808
09/16 09:56:31 AM: macro_avg: validation: 0.936087
09/16 09:56:31 AM: micro_avg: validation: 0.000000
09/16 09:56:31 AM: edges-ner-ontonotes_mcc: training: 0.905894 validation: 0.932429
09/16 09:56:31 AM: edges-ner-ontonotes_acc: training: 0.864399 validation: 0.905520
09/16 09:56:31 AM: edges-ner-ontonotes_precision: training: 0.932393 validation: 0.946166
09/16 09:56:31 AM: edges-ner-ontonotes_recall: training: 0.890074 validation: 0.926221
09/16 09:56:31 AM: edges-ner-ontonotes_f1: training: 0.910742 validation: 0.936087
09/16 09:56:31 AM: Global learning rate: 0.0001
09/16 09:56:31 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:56:31 AM: Update 24001: task edges-ner-ontonotes, batch 1 (24001): mcc: 0.9471, acc: 0.9333, precision: 0.9500, recall: 0.9500, f1: 0.9500, edges-ner-ontonotes_loss: 0.0178
09/16 09:56:41 AM: Update 24118: task edges-ner-ontonotes, batch 118 (24118): mcc: 0.9346, acc: 0.9002, precision: 0.9523, recall: 0.9243, f1: 0.9381, edges-ner-ontonotes_loss: 0.0199
09/16 09:56:53 AM: Update 24246: task edges-ner-ontonotes, batch 246 (24246): mcc: 0.9337, acc: 0.8998, precision: 0.9510, recall: 0.9238, f1: 0.9372, edges-ner-ontonotes_loss: 0.0203
09/16 09:57:03 AM: Update 24341: task edges-ner-ontonotes, batch 341 (24341): mcc: 0.9312, acc: 0.8962, precision: 0.9491, recall: 0.9211, f1: 0.9349, edges-ner-ontonotes_loss: 0.0210
09/16 09:57:13 AM: Update 24467: task edges-ner-ontonotes, batch 467 (24467): mcc: 0.9320, acc: 0.8975, precision: 0.9501, recall: 0.9216, f1: 0.9356, edges-ner-ontonotes_loss: 0.0210
09/16 09:57:23 AM: Update 24589: task edges-ner-ontonotes, batch 589 (24589): mcc: 0.9315, acc: 0.8965, precision: 0.9496, recall: 0.9212, f1: 0.9352, edges-ner-ontonotes_loss: 0.0212
09/16 09:57:33 AM: Update 24701: task edges-ner-ontonotes, batch 701 (24701): mcc: 0.9254, acc: 0.8888, precision: 0.9455, recall: 0.9138, f1: 0.9294, edges-ner-ontonotes_loss: 0.0239
09/16 09:57:43 AM: Update 24823: task edges-ner-ontonotes, batch 823 (24823): mcc: 0.9207, acc: 0.8829, precision: 0.9427, recall: 0.9076, f1: 0.9248, edges-ner-ontonotes_loss: 0.0260
09/16 09:57:53 AM: Update 24937: task edges-ner-ontonotes, batch 937 (24937): mcc: 0.9184, acc: 0.8799, precision: 0.9411, recall: 0.9049, f1: 0.9226, edges-ner-ontonotes_loss: 0.0269
09/16 09:57:57 AM: ***** Step 25000 / Validation 25 *****
09/16 09:57:57 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:57:57 AM: Validating...
09/16 09:58:03 AM: Evaluate: task edges-ner-ontonotes, batch 52 (157): mcc: 0.9113, acc: 0.8800, precision: 0.9331, recall: 0.8995, f1: 0.9160, edges-ner-ontonotes_loss: 0.0293
09/16 09:58:13 AM: Evaluate: task edges-ner-ontonotes, batch 129 (157): mcc: 0.9288, acc: 0.8997, precision: 0.9506, recall: 0.9151, f1: 0.9325, edges-ner-ontonotes_loss: 0.0237
09/16 09:58:17 AM: Updating LR scheduler:
09/16 09:58:17 AM: 	Best result seen so far for macro_avg: 0.936
09/16 09:58:17 AM: 	# validation passes without improvement: 2
09/16 09:58:17 AM: edges-ner-ontonotes_loss: training: 0.027145 validation: 0.022406
09/16 09:58:17 AM: macro_avg: validation: 0.935763
09/16 09:58:17 AM: micro_avg: validation: 0.000000
09/16 09:58:17 AM: edges-ner-ontonotes_mcc: training: 0.917513 validation: 0.932230
09/16 09:58:17 AM: edges-ner-ontonotes_acc: training: 0.878798 validation: 0.904079
09/16 09:58:17 AM: edges-ner-ontonotes_precision: training: 0.940439 validation: 0.953130
09/16 09:58:17 AM: edges-ner-ontonotes_recall: training: 0.903913 validation: 0.919017
09/16 09:58:17 AM: edges-ner-ontonotes_f1: training: 0.921815 validation: 0.935763
09/16 09:58:17 AM: Global learning rate: 0.0001
09/16 09:58:17 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:58:23 AM: Update 25099: task edges-ner-ontonotes, batch 99 (25099): mcc: 0.9025, acc: 0.8574, precision: 0.9307, recall: 0.8853, f1: 0.9074, edges-ner-ontonotes_loss: 0.0305
09/16 09:58:33 AM: Update 25212: task edges-ner-ontonotes, batch 212 (25212): mcc: 0.9024, acc: 0.8589, precision: 0.9302, recall: 0.8858, f1: 0.9074, edges-ner-ontonotes_loss: 0.0302
09/16 09:58:44 AM: Update 25333: task edges-ner-ontonotes, batch 333 (25333): mcc: 0.9072, acc: 0.8653, precision: 0.9323, recall: 0.8925, f1: 0.9120, edges-ner-ontonotes_loss: 0.0291
09/16 09:58:54 AM: Update 25468: task edges-ner-ontonotes, batch 468 (25468): mcc: 0.9115, acc: 0.8713, precision: 0.9354, recall: 0.8975, f1: 0.9161, edges-ner-ontonotes_loss: 0.0280
09/16 09:59:04 AM: Update 25563: task edges-ner-ontonotes, batch 563 (25563): mcc: 0.9132, acc: 0.8738, precision: 0.9367, recall: 0.8995, f1: 0.9177, edges-ner-ontonotes_loss: 0.0274
09/16 09:59:14 AM: Update 25689: task edges-ner-ontonotes, batch 689 (25689): mcc: 0.9171, acc: 0.8786, precision: 0.9397, recall: 0.9039, f1: 0.9214, edges-ner-ontonotes_loss: 0.0262
09/16 09:59:24 AM: Update 25809: task edges-ner-ontonotes, batch 809 (25809): mcc: 0.9202, acc: 0.8827, precision: 0.9418, recall: 0.9076, f1: 0.9244, edges-ner-ontonotes_loss: 0.0253
09/16 09:59:34 AM: Update 25909: task edges-ner-ontonotes, batch 909 (25909): mcc: 0.9218, acc: 0.8845, precision: 0.9430, recall: 0.9094, f1: 0.9259, edges-ner-ontonotes_loss: 0.0248
09/16 09:59:41 AM: ***** Step 26000 / Validation 26 *****
09/16 09:59:41 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:59:41 AM: Validating...
09/16 09:59:44 AM: Evaluate: task edges-ner-ontonotes, batch 27 (157): mcc: 0.8782, acc: 0.8414, precision: 0.9015, recall: 0.8685, f1: 0.8847, edges-ner-ontonotes_loss: 0.0402
09/16 09:59:55 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.9246, acc: 0.8976, precision: 0.9411, recall: 0.9165, f1: 0.9286, edges-ner-ontonotes_loss: 0.0259
09/16 10:00:01 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:00:01 AM: Best result seen so far for macro.
09/16 10:00:01 AM: Updating LR scheduler:
09/16 10:00:01 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:00:01 AM: 	# validation passes without improvement: 0
09/16 10:00:01 AM: edges-ner-ontonotes_loss: training: 0.024603 validation: 0.022650
09/16 10:00:01 AM: macro_avg: validation: 0.938035
09/16 10:00:01 AM: micro_avg: validation: 0.000000
09/16 10:00:01 AM: edges-ner-ontonotes_mcc: training: 0.922391 validation: 0.934512
09/16 10:00:01 AM: edges-ner-ontonotes_acc: training: 0.885187 validation: 0.909008
09/16 10:00:01 AM: edges-ner-ontonotes_precision: training: 0.943353 validation: 0.949437
09/16 10:00:01 AM: edges-ner-ontonotes_recall: training: 0.910179 validation: 0.926903
09/16 10:00:01 AM: edges-ner-ontonotes_f1: training: 0.926469 validation: 0.938035
09/16 10:00:01 AM: Global learning rate: 0.0001
09/16 10:00:01 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:00:05 AM: Update 26046: task edges-ner-ontonotes, batch 46 (26046): mcc: 0.9340, acc: 0.8992, precision: 0.9521, recall: 0.9234, f1: 0.9375, edges-ner-ontonotes_loss: 0.0196
09/16 10:00:15 AM: Update 26150: task edges-ner-ontonotes, batch 150 (26150): mcc: 0.9285, acc: 0.8934, precision: 0.9475, recall: 0.9176, f1: 0.9323, edges-ner-ontonotes_loss: 0.0217
09/16 10:00:25 AM: Update 26279: task edges-ner-ontonotes, batch 279 (26279): mcc: 0.9114, acc: 0.8711, precision: 0.9358, recall: 0.8971, f1: 0.9160, edges-ner-ontonotes_loss: 0.0291
09/16 10:00:35 AM: Update 26408: task edges-ner-ontonotes, batch 408 (26408): mcc: 0.9062, acc: 0.8641, precision: 0.9322, recall: 0.8908, f1: 0.9111, edges-ner-ontonotes_loss: 0.0311
09/16 10:00:45 AM: Update 26528: task edges-ner-ontonotes, batch 528 (26528): mcc: 0.9045, acc: 0.8622, precision: 0.9313, recall: 0.8886, f1: 0.9094, edges-ner-ontonotes_loss: 0.0317
09/16 10:00:55 AM: Update 26676: task edges-ner-ontonotes, batch 676 (26676): mcc: 0.9037, acc: 0.8609, precision: 0.9306, recall: 0.8877, f1: 0.9087, edges-ner-ontonotes_loss: 0.0316
09/16 10:01:05 AM: Update 26787: task edges-ner-ontonotes, batch 787 (26787): mcc: 0.9047, acc: 0.8621, precision: 0.9314, recall: 0.8888, f1: 0.9096, edges-ner-ontonotes_loss: 0.0310
09/16 10:01:15 AM: Update 26916: task edges-ner-ontonotes, batch 916 (26916): mcc: 0.9059, acc: 0.8638, precision: 0.9317, recall: 0.8908, f1: 0.9108, edges-ner-ontonotes_loss: 0.0304
09/16 10:01:22 AM: ***** Step 27000 / Validation 27 *****
09/16 10:01:22 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:01:22 AM: Validating...
09/16 10:01:25 AM: Evaluate: task edges-ner-ontonotes, batch 33 (157): mcc: 0.9009, acc: 0.8678, precision: 0.9240, recall: 0.8888, f1: 0.9061, edges-ner-ontonotes_loss: 0.0325
09/16 10:01:36 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9250, acc: 0.8960, precision: 0.9445, recall: 0.9139, f1: 0.9290, edges-ner-ontonotes_loss: 0.0247
09/16 10:01:41 AM: Updating LR scheduler:
09/16 10:01:41 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:01:41 AM: 	# validation passes without improvement: 1
09/16 10:01:41 AM: edges-ner-ontonotes_loss: training: 0.029872 validation: 0.022207
09/16 10:01:41 AM: macro_avg: validation: 0.936598
09/16 10:01:41 AM: micro_avg: validation: 0.000000
09/16 10:01:41 AM: edges-ner-ontonotes_mcc: training: 0.907270 validation: 0.933052
09/16 10:01:41 AM: edges-ner-ontonotes_acc: training: 0.865490 validation: 0.905823
09/16 10:01:41 AM: edges-ner-ontonotes_precision: training: 0.932559 validation: 0.951208
09/16 10:01:41 AM: edges-ner-ontonotes_recall: training: 0.892476 validation: 0.922430
09/16 10:01:41 AM: edges-ner-ontonotes_f1: training: 0.912077 validation: 0.936598
09/16 10:01:41 AM: Global learning rate: 0.0001
09/16 10:01:41 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:01:46 AM: Update 27057: task edges-ner-ontonotes, batch 57 (27057): mcc: 0.9112, acc: 0.8733, precision: 0.9350, recall: 0.8973, f1: 0.9158, edges-ner-ontonotes_loss: 0.0280
09/16 10:01:56 AM: Update 27157: task edges-ner-ontonotes, batch 157 (27157): mcc: 0.9223, acc: 0.8862, precision: 0.9426, recall: 0.9108, f1: 0.9264, edges-ner-ontonotes_loss: 0.0241
09/16 10:02:06 AM: Update 27278: task edges-ner-ontonotes, batch 278 (27278): mcc: 0.9275, acc: 0.8929, precision: 0.9466, recall: 0.9165, f1: 0.9313, edges-ner-ontonotes_loss: 0.0221
09/16 10:02:16 AM: Update 27393: task edges-ner-ontonotes, batch 393 (27393): mcc: 0.9306, acc: 0.8968, precision: 0.9487, recall: 0.9202, f1: 0.9342, edges-ner-ontonotes_loss: 0.0214
09/16 10:02:26 AM: Update 27519: task edges-ner-ontonotes, batch 519 (27519): mcc: 0.9320, acc: 0.8986, precision: 0.9500, recall: 0.9217, f1: 0.9356, edges-ner-ontonotes_loss: 0.0212
09/16 10:02:36 AM: Update 27641: task edges-ner-ontonotes, batch 641 (27641): mcc: 0.9314, acc: 0.8976, precision: 0.9494, recall: 0.9212, f1: 0.9351, edges-ner-ontonotes_loss: 0.0214
09/16 10:02:46 AM: Update 27741: task edges-ner-ontonotes, batch 741 (27741): mcc: 0.9291, acc: 0.8945, precision: 0.9478, recall: 0.9185, f1: 0.9329, edges-ner-ontonotes_loss: 0.0221
09/16 10:02:56 AM: Update 27866: task edges-ner-ontonotes, batch 866 (27866): mcc: 0.9241, acc: 0.8882, precision: 0.9445, recall: 0.9122, f1: 0.9281, edges-ner-ontonotes_loss: 0.0244
09/16 10:03:06 AM: Update 27997: task edges-ner-ontonotes, batch 997 (27997): mcc: 0.9203, acc: 0.8833, precision: 0.9420, recall: 0.9075, f1: 0.9245, edges-ner-ontonotes_loss: 0.0260
09/16 10:03:07 AM: ***** Step 28000 / Validation 28 *****
09/16 10:03:07 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:03:07 AM: Validating...
09/16 10:03:17 AM: Evaluate: task edges-ner-ontonotes, batch 91 (157): mcc: 0.9233, acc: 0.8924, precision: 0.9420, recall: 0.9133, f1: 0.9274, edges-ner-ontonotes_loss: 0.0266
09/16 10:03:25 AM: Updating LR scheduler:
09/16 10:03:25 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:03:25 AM: 	# validation passes without improvement: 2
09/16 10:03:25 AM: edges-ner-ontonotes_loss: training: 0.026041 validation: 0.022592
09/16 10:03:25 AM: macro_avg: validation: 0.936742
09/16 10:03:25 AM: micro_avg: validation: 0.000000
09/16 10:03:25 AM: edges-ner-ontonotes_mcc: training: 0.920196 validation: 0.933151
09/16 10:03:25 AM: edges-ner-ontonotes_acc: training: 0.883206 validation: 0.906278
09/16 10:03:25 AM: edges-ner-ontonotes_precision: training: 0.941973 validation: 0.948539
09/16 10:03:25 AM: edges-ner-ontonotes_recall: training: 0.907426 validation: 0.925235
09/16 10:03:25 AM: edges-ner-ontonotes_f1: training: 0.924377 validation: 0.936742
09/16 10:03:25 AM: Global learning rate: 0.0001
09/16 10:03:25 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:03:28 AM: Update 28009: task edges-ner-ontonotes, batch 9 (28009): mcc: 0.8902, acc: 0.8455, precision: 0.9163, recall: 0.8764, f1: 0.8959, edges-ner-ontonotes_loss: 0.0400
09/16 10:03:38 AM: Update 28154: task edges-ner-ontonotes, batch 154 (28154): mcc: 0.8988, acc: 0.8538, precision: 0.9274, recall: 0.8818, f1: 0.9040, edges-ner-ontonotes_loss: 0.0317
09/16 10:03:48 AM: Update 28310: task edges-ner-ontonotes, batch 310 (28310): mcc: 0.9021, acc: 0.8590, precision: 0.9286, recall: 0.8866, f1: 0.9072, edges-ner-ontonotes_loss: 0.0305
09/16 10:03:58 AM: Update 28417: task edges-ner-ontonotes, batch 417 (28417): mcc: 0.9050, acc: 0.8623, precision: 0.9309, recall: 0.8898, f1: 0.9099, edges-ner-ontonotes_loss: 0.0297
09/16 10:04:08 AM: Update 28545: task edges-ner-ontonotes, batch 545 (28545): mcc: 0.9078, acc: 0.8662, precision: 0.9330, recall: 0.8931, f1: 0.9126, edges-ner-ontonotes_loss: 0.0290
09/16 10:04:18 AM: Update 28656: task edges-ner-ontonotes, batch 656 (28656): mcc: 0.9112, acc: 0.8709, precision: 0.9354, recall: 0.8970, f1: 0.9158, edges-ner-ontonotes_loss: 0.0280
09/16 10:04:28 AM: Update 28780: task edges-ner-ontonotes, batch 780 (28780): mcc: 0.9156, acc: 0.8767, precision: 0.9385, recall: 0.9022, f1: 0.9200, edges-ner-ontonotes_loss: 0.0267
09/16 10:04:38 AM: Update 28903: task edges-ner-ontonotes, batch 903 (28903): mcc: 0.9191, acc: 0.8811, precision: 0.9409, recall: 0.9064, f1: 0.9234, edges-ner-ontonotes_loss: 0.0257
09/16 10:04:48 AM: Update 29000: task edges-ner-ontonotes, batch 1000 (29000): mcc: 0.9203, acc: 0.8827, precision: 0.9418, recall: 0.9078, f1: 0.9245, edges-ner-ontonotes_loss: 0.0253
09/16 10:04:48 AM: ***** Step 29000 / Validation 29 *****
09/16 10:04:48 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:04:48 AM: Validating...
09/16 10:04:58 AM: Evaluate: task edges-ner-ontonotes, batch 92 (157): mcc: 0.9223, acc: 0.8934, precision: 0.9415, recall: 0.9119, f1: 0.9264, edges-ner-ontonotes_loss: 0.0276
09/16 10:05:07 AM: Updating LR scheduler:
09/16 10:05:07 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:05:07 AM: 	# validation passes without improvement: 3
09/16 10:05:07 AM: edges-ner-ontonotes_loss: training: 0.025250 validation: 0.022897
09/16 10:05:07 AM: macro_avg: validation: 0.936802
09/16 10:05:07 AM: micro_avg: validation: 0.000000
09/16 10:05:07 AM: edges-ner-ontonotes_mcc: training: 0.920311 validation: 0.933208
09/16 10:05:07 AM: edges-ner-ontonotes_acc: training: 0.882686 validation: 0.906885
09/16 10:05:07 AM: edges-ner-ontonotes_precision: training: 0.941811 validation: 0.948264
09/16 10:05:07 AM: edges-ner-ontonotes_recall: training: 0.907798 validation: 0.925614
09/16 10:05:07 AM: edges-ner-ontonotes_f1: training: 0.924492 validation: 0.936802
09/16 10:05:07 AM: Global learning rate: 0.0001
09/16 10:05:07 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:05:08 AM: Update 29021: task edges-ner-ontonotes, batch 21 (29021): mcc: 0.9339, acc: 0.9031, precision: 0.9513, recall: 0.9238, f1: 0.9374, edges-ner-ontonotes_loss: 0.0213
09/16 10:05:18 AM: Update 29147: task edges-ner-ontonotes, batch 147 (29147): mcc: 0.9317, acc: 0.8990, precision: 0.9487, recall: 0.9224, f1: 0.9354, edges-ner-ontonotes_loss: 0.0221
09/16 10:05:29 AM: Update 29261: task edges-ner-ontonotes, batch 261 (29261): mcc: 0.9327, acc: 0.8998, precision: 0.9499, recall: 0.9232, f1: 0.9363, edges-ner-ontonotes_loss: 0.0215
09/16 10:05:39 AM: Update 29385: task edges-ner-ontonotes, batch 385 (29385): mcc: 0.9191, acc: 0.8827, precision: 0.9404, recall: 0.9068, f1: 0.9233, edges-ner-ontonotes_loss: 0.0264
09/16 10:05:49 AM: Update 29516: task edges-ner-ontonotes, batch 516 (29516): mcc: 0.9132, acc: 0.8748, precision: 0.9368, recall: 0.8994, f1: 0.9178, edges-ner-ontonotes_loss: 0.0289
09/16 10:05:59 AM: Update 29633: task edges-ner-ontonotes, batch 633 (29633): mcc: 0.9111, acc: 0.8721, precision: 0.9354, recall: 0.8969, f1: 0.9157, edges-ner-ontonotes_loss: 0.0296
09/16 10:06:09 AM: Update 29784: task edges-ner-ontonotes, batch 784 (29784): mcc: 0.9100, acc: 0.8701, precision: 0.9346, recall: 0.8955, f1: 0.9146, edges-ner-ontonotes_loss: 0.0298
09/16 10:06:19 AM: Update 29902: task edges-ner-ontonotes, batch 902 (29902): mcc: 0.9093, acc: 0.8692, precision: 0.9342, recall: 0.8946, f1: 0.9140, edges-ner-ontonotes_loss: 0.0297
09/16 10:06:27 AM: ***** Step 30000 / Validation 30 *****
09/16 10:06:27 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:06:27 AM: Validating...
09/16 10:06:29 AM: Evaluate: task edges-ner-ontonotes, batch 22 (157): mcc: 0.8914, acc: 0.8519, precision: 0.9165, recall: 0.8785, f1: 0.8971, edges-ner-ontonotes_loss: 0.0320
09/16 10:06:39 AM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.9239, acc: 0.8955, precision: 0.9429, recall: 0.9135, f1: 0.9279, edges-ner-ontonotes_loss: 0.0250
09/16 10:06:46 AM: Updating LR scheduler:
09/16 10:06:46 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:06:46 AM: 	# validation passes without improvement: 0
09/16 10:06:46 AM: edges-ner-ontonotes_loss: training: 0.029455 validation: 0.022103
09/16 10:06:46 AM: macro_avg: validation: 0.937012
09/16 10:06:46 AM: micro_avg: validation: 0.000000
09/16 10:06:46 AM: edges-ner-ontonotes_mcc: training: 0.909889 validation: 0.933481
09/16 10:06:46 AM: edges-ner-ontonotes_acc: training: 0.869888 validation: 0.907037
09/16 10:06:46 AM: edges-ner-ontonotes_precision: training: 0.934366 validation: 0.951176
09/16 10:06:46 AM: edges-ner-ontonotes_recall: training: 0.895601 validation: 0.923264
09/16 10:06:46 AM: edges-ner-ontonotes_f1: training: 0.914573 validation: 0.937012
09/16 10:06:46 AM: Global learning rate: 5e-05
09/16 10:06:46 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:06:49 AM: Update 30047: task edges-ner-ontonotes, batch 47 (30047): mcc: 0.9171, acc: 0.8824, precision: 0.9395, recall: 0.9041, f1: 0.9215, edges-ner-ontonotes_loss: 0.0258
09/16 10:06:59 AM: Update 30174: task edges-ner-ontonotes, batch 174 (30174): mcc: 0.9176, acc: 0.8816, precision: 0.9394, recall: 0.9051, f1: 0.9219, edges-ner-ontonotes_loss: 0.0261
09/16 10:07:09 AM: Update 30272: task edges-ner-ontonotes, batch 272 (30272): mcc: 0.9213, acc: 0.8857, precision: 0.9418, recall: 0.9096, f1: 0.9254, edges-ner-ontonotes_loss: 0.0247
09/16 10:07:19 AM: Update 30393: task edges-ner-ontonotes, batch 393 (30393): mcc: 0.9270, acc: 0.8926, precision: 0.9464, recall: 0.9157, f1: 0.9308, edges-ner-ontonotes_loss: 0.0232
09/16 10:07:29 AM: Update 30505: task edges-ner-ontonotes, batch 505 (30505): mcc: 0.9288, acc: 0.8942, precision: 0.9483, recall: 0.9173, f1: 0.9326, edges-ner-ontonotes_loss: 0.0226
09/16 10:07:39 AM: Update 30631: task edges-ner-ontonotes, batch 631 (30631): mcc: 0.9284, acc: 0.8938, precision: 0.9479, recall: 0.9171, f1: 0.9322, edges-ner-ontonotes_loss: 0.0226
09/16 10:07:49 AM: Update 30751: task edges-ner-ontonotes, batch 751 (30751): mcc: 0.9295, acc: 0.8949, precision: 0.9486, recall: 0.9183, f1: 0.9332, edges-ner-ontonotes_loss: 0.0223
09/16 10:07:59 AM: Update 30846: task edges-ner-ontonotes, batch 846 (30846): mcc: 0.9277, acc: 0.8925, precision: 0.9474, recall: 0.9163, f1: 0.9316, edges-ner-ontonotes_loss: 0.0228
09/16 10:08:09 AM: Update 30977: task edges-ner-ontonotes, batch 977 (30977): mcc: 0.9235, acc: 0.8871, precision: 0.9443, recall: 0.9114, f1: 0.9275, edges-ner-ontonotes_loss: 0.0247
09/16 10:08:11 AM: ***** Step 31000 / Validation 31 *****
09/16 10:08:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:08:11 AM: Validating...
09/16 10:08:19 AM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.9191, acc: 0.8877, precision: 0.9398, recall: 0.9075, f1: 0.9233, edges-ner-ontonotes_loss: 0.0281
09/16 10:08:29 AM: Evaluate: task edges-ner-ontonotes, batch 153 (157): mcc: 0.9325, acc: 0.9052, precision: 0.9498, recall: 0.9228, f1: 0.9361, edges-ner-ontonotes_loss: 0.0226
09/16 10:08:30 AM: Updating LR scheduler:
09/16 10:08:31 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:08:31 AM: 	# validation passes without improvement: 1
09/16 10:08:31 AM: edges-ner-ontonotes_loss: training: 0.024830 validation: 0.022444
09/16 10:08:31 AM: macro_avg: validation: 0.936430
09/16 10:08:31 AM: micro_avg: validation: 0.000000
09/16 10:08:31 AM: edges-ner-ontonotes_mcc: training: 0.923030 validation: 0.932855
09/16 10:08:31 AM: edges-ner-ontonotes_acc: training: 0.886619 validation: 0.905748
09/16 10:08:31 AM: edges-ner-ontonotes_precision: training: 0.943859 validation: 0.950059
09/16 10:08:31 AM: edges-ner-ontonotes_recall: training: 0.910879 validation: 0.923188
09/16 10:08:31 AM: edges-ner-ontonotes_f1: training: 0.927076 validation: 0.936430
09/16 10:08:31 AM: Global learning rate: 5e-05
09/16 10:08:31 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:08:40 AM: Update 31113: task edges-ner-ontonotes, batch 113 (31113): mcc: 0.8969, acc: 0.8545, precision: 0.9272, recall: 0.8784, f1: 0.9022, edges-ner-ontonotes_loss: 0.0361
09/16 10:08:50 AM: Update 31231: task edges-ner-ontonotes, batch 231 (31231): mcc: 0.8975, acc: 0.8559, precision: 0.9251, recall: 0.8815, f1: 0.9028, edges-ner-ontonotes_loss: 0.0335
09/16 10:09:00 AM: Update 31375: task edges-ner-ontonotes, batch 375 (31375): mcc: 0.9018, acc: 0.8605, precision: 0.9291, recall: 0.8858, f1: 0.9069, edges-ner-ontonotes_loss: 0.0315
09/16 10:09:10 AM: Update 31485: task edges-ner-ontonotes, batch 485 (31485): mcc: 0.9046, acc: 0.8632, precision: 0.9311, recall: 0.8890, f1: 0.9095, edges-ner-ontonotes_loss: 0.0305
09/16 10:09:20 AM: Update 31617: task edges-ner-ontonotes, batch 617 (31617): mcc: 0.9086, acc: 0.8681, precision: 0.9338, recall: 0.8938, f1: 0.9134, edges-ner-ontonotes_loss: 0.0293
09/16 10:09:30 AM: Update 31746: task edges-ner-ontonotes, batch 746 (31746): mcc: 0.9101, acc: 0.8701, precision: 0.9344, recall: 0.8959, f1: 0.9147, edges-ner-ontonotes_loss: 0.0287
09/16 10:09:40 AM: Update 31838: task edges-ner-ontonotes, batch 838 (31838): mcc: 0.9133, acc: 0.8741, precision: 0.9369, recall: 0.8995, f1: 0.9178, edges-ner-ontonotes_loss: 0.0279
09/16 10:09:50 AM: Update 31965: task edges-ner-ontonotes, batch 965 (31965): mcc: 0.9165, acc: 0.8780, precision: 0.9391, recall: 0.9033, f1: 0.9208, edges-ner-ontonotes_loss: 0.0268
09/16 10:09:53 AM: ***** Step 32000 / Validation 32 *****
09/16 10:09:53 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:09:53 AM: Validating...
09/16 10:10:00 AM: Evaluate: task edges-ner-ontonotes, batch 66 (157): mcc: 0.9129, acc: 0.8813, precision: 0.9352, recall: 0.9003, f1: 0.9174, edges-ner-ontonotes_loss: 0.0299
09/16 10:10:10 AM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.9329, acc: 0.9071, precision: 0.9500, recall: 0.9233, f1: 0.9365, edges-ner-ontonotes_loss: 0.0230
09/16 10:10:11 AM: Updating LR scheduler:
09/16 10:10:11 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:10:11 AM: 	# validation passes without improvement: 2
09/16 10:10:11 AM: edges-ner-ontonotes_loss: training: 0.026554 validation: 0.022692
09/16 10:10:11 AM: macro_avg: validation: 0.936836
09/16 10:10:11 AM: micro_avg: validation: 0.000000
09/16 10:10:11 AM: edges-ner-ontonotes_mcc: training: 0.917234 validation: 0.933290
09/16 10:10:11 AM: edges-ner-ontonotes_acc: training: 0.878926 validation: 0.907113
09/16 10:10:11 AM: edges-ner-ontonotes_precision: training: 0.939630 validation: 0.950734
09/16 10:10:11 AM: edges-ner-ontonotes_recall: training: 0.904181 validation: 0.923339
09/16 10:10:11 AM: edges-ner-ontonotes_f1: training: 0.921565 validation: 0.936836
09/16 10:10:11 AM: Global learning rate: 5e-05
09/16 10:10:11 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:10:20 AM: Update 32081: task edges-ner-ontonotes, batch 81 (32081): mcc: 0.9316, acc: 0.8977, precision: 0.9483, recall: 0.9226, f1: 0.9353, edges-ner-ontonotes_loss: 0.0204
09/16 10:10:30 AM: Update 32199: task edges-ner-ontonotes, batch 199 (32199): mcc: 0.9314, acc: 0.8962, precision: 0.9490, recall: 0.9215, f1: 0.9350, edges-ner-ontonotes_loss: 0.0215
09/16 10:10:40 AM: Update 32328: task edges-ner-ontonotes, batch 328 (32328): mcc: 0.9311, acc: 0.8963, precision: 0.9488, recall: 0.9210, f1: 0.9347, edges-ner-ontonotes_loss: 0.0216
09/16 10:10:50 AM: Update 32434: task edges-ner-ontonotes, batch 434 (32434): mcc: 0.9263, acc: 0.8898, precision: 0.9464, recall: 0.9144, f1: 0.9301, edges-ner-ontonotes_loss: 0.0236
09/16 10:11:00 AM: Update 32566: task edges-ner-ontonotes, batch 566 (32566): mcc: 0.9191, acc: 0.8812, precision: 0.9417, recall: 0.9056, f1: 0.9233, edges-ner-ontonotes_loss: 0.0266
09/16 10:11:10 AM: Update 32678: task edges-ner-ontonotes, batch 678 (32678): mcc: 0.9158, acc: 0.8770, precision: 0.9393, recall: 0.9018, f1: 0.9201, edges-ner-ontonotes_loss: 0.0281
09/16 10:11:20 AM: Update 32828: task edges-ner-ontonotes, batch 828 (32828): mcc: 0.9146, acc: 0.8753, precision: 0.9387, recall: 0.9001, f1: 0.9190, edges-ner-ontonotes_loss: 0.0282
09/16 10:11:30 AM: Update 32979: task edges-ner-ontonotes, batch 979 (32979): mcc: 0.9131, acc: 0.8736, precision: 0.9373, recall: 0.8987, f1: 0.9176, edges-ner-ontonotes_loss: 0.0285
09/16 10:11:34 AM: ***** Step 33000 / Validation 33 *****
09/16 10:11:34 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:11:34 AM: Validating...
09/16 10:11:40 AM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.9206, acc: 0.8922, precision: 0.9399, recall: 0.9101, f1: 0.9248, edges-ner-ontonotes_loss: 0.0268
09/16 10:11:50 AM: Evaluate: task edges-ner-ontonotes, batch 139 (157): mcc: 0.9331, acc: 0.9068, precision: 0.9523, recall: 0.9215, f1: 0.9366, edges-ner-ontonotes_loss: 0.0224
09/16 10:11:52 AM: Updating LR scheduler:
09/16 10:11:52 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:11:52 AM: 	# validation passes without improvement: 3
09/16 10:11:52 AM: edges-ner-ontonotes_loss: training: 0.028481 validation: 0.021982
09/16 10:11:52 AM: macro_avg: validation: 0.936975
09/16 10:11:52 AM: micro_avg: validation: 0.000000
09/16 10:11:52 AM: edges-ner-ontonotes_mcc: training: 0.913087 validation: 0.933492
09/16 10:11:52 AM: edges-ner-ontonotes_acc: training: 0.873559 validation: 0.906506
09/16 10:11:52 AM: edges-ner-ontonotes_precision: training: 0.937457 validation: 0.953525
09/16 10:11:52 AM: edges-ner-ontonotes_recall: training: 0.898559 validation: 0.920989
09/16 10:11:52 AM: edges-ner-ontonotes_f1: training: 0.917596 validation: 0.936975
09/16 10:11:52 AM: Global learning rate: 5e-05
09/16 10:11:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:12:00 AM: Update 33099: task edges-ner-ontonotes, batch 99 (33099): mcc: 0.9131, acc: 0.8736, precision: 0.9378, recall: 0.8983, f1: 0.9176, edges-ner-ontonotes_loss: 0.0271
09/16 10:12:11 AM: Update 33229: task edges-ner-ontonotes, batch 229 (33229): mcc: 0.9158, acc: 0.8782, precision: 0.9377, recall: 0.9033, f1: 0.9202, edges-ner-ontonotes_loss: 0.0263
09/16 10:12:21 AM: Update 33321: task edges-ner-ontonotes, batch 321 (33321): mcc: 0.9176, acc: 0.8804, precision: 0.9396, recall: 0.9048, f1: 0.9219, edges-ner-ontonotes_loss: 0.0256
09/16 10:12:31 AM: Update 33444: task edges-ner-ontonotes, batch 444 (33444): mcc: 0.9221, acc: 0.8863, precision: 0.9421, recall: 0.9108, f1: 0.9262, edges-ner-ontonotes_loss: 0.0242
09/16 10:12:41 AM: Update 33571: task edges-ner-ontonotes, batch 571 (33571): mcc: 0.9266, acc: 0.8923, precision: 0.9458, recall: 0.9157, f1: 0.9305, edges-ner-ontonotes_loss: 0.0230
09/16 10:12:51 AM: Update 33686: task edges-ner-ontonotes, batch 686 (33686): mcc: 0.9274, acc: 0.8927, precision: 0.9463, recall: 0.9167, f1: 0.9313, edges-ner-ontonotes_loss: 0.0226
09/16 10:13:01 AM: Update 33801: task edges-ner-ontonotes, batch 801 (33801): mcc: 0.9282, acc: 0.8936, precision: 0.9471, recall: 0.9175, f1: 0.9320, edges-ner-ontonotes_loss: 0.0225
09/16 10:13:11 AM: Update 33924: task edges-ner-ontonotes, batch 924 (33924): mcc: 0.9287, acc: 0.8941, precision: 0.9474, recall: 0.9181, f1: 0.9325, edges-ner-ontonotes_loss: 0.0223
09/16 10:13:19 AM: ***** Step 34000 / Validation 34 *****
09/16 10:13:19 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:13:19 AM: Validating...
09/16 10:13:21 AM: Evaluate: task edges-ner-ontonotes, batch 19 (157): mcc: 0.8914, acc: 0.8518, precision: 0.9195, recall: 0.8756, f1: 0.8970, edges-ner-ontonotes_loss: 0.0322
09/16 10:13:31 AM: Evaluate: task edges-ner-ontonotes, batch 106 (157): mcc: 0.9219, acc: 0.8923, precision: 0.9445, recall: 0.9081, f1: 0.9259, edges-ner-ontonotes_loss: 0.0261
09/16 10:13:37 AM: Updating LR scheduler:
09/16 10:13:37 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:13:37 AM: 	# validation passes without improvement: 0
09/16 10:13:37 AM: edges-ner-ontonotes_loss: training: 0.023376 validation: 0.022726
09/16 10:13:37 AM: macro_avg: validation: 0.935150
09/16 10:13:37 AM: micro_avg: validation: 0.000000
09/16 10:13:37 AM: edges-ner-ontonotes_mcc: training: 0.926016 validation: 0.931580
09/16 10:13:37 AM: edges-ner-ontonotes_acc: training: 0.890552 validation: 0.904155
09/16 10:13:37 AM: edges-ner-ontonotes_precision: training: 0.945579 validation: 0.952430
09/16 10:13:37 AM: edges-ner-ontonotes_recall: training: 0.914778 validation: 0.918486
09/16 10:13:37 AM: edges-ner-ontonotes_f1: training: 0.929924 validation: 0.935150
09/16 10:13:37 AM: Global learning rate: 2.5e-05
09/16 10:13:37 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:13:41 AM: Update 34038: task edges-ner-ontonotes, batch 38 (34038): mcc: 0.8856, acc: 0.8363, precision: 0.9240, recall: 0.8605, f1: 0.8911, edges-ner-ontonotes_loss: 0.0382
09/16 10:13:51 AM: Update 34174: task edges-ner-ontonotes, batch 174 (34174): mcc: 0.8926, acc: 0.8482, precision: 0.9237, recall: 0.8736, f1: 0.8980, edges-ner-ontonotes_loss: 0.0368
09/16 10:14:01 AM: Update 34283: task edges-ner-ontonotes, batch 283 (34283): mcc: 0.8953, acc: 0.8509, precision: 0.9249, recall: 0.8776, f1: 0.9006, edges-ner-ontonotes_loss: 0.0352
09/16 10:14:11 AM: Update 34433: task edges-ner-ontonotes, batch 433 (34433): mcc: 0.8987, acc: 0.8551, precision: 0.9271, recall: 0.8818, f1: 0.9039, edges-ner-ontonotes_loss: 0.0334
09/16 10:14:21 AM: Update 34551: task edges-ner-ontonotes, batch 551 (34551): mcc: 0.9007, acc: 0.8576, precision: 0.9285, recall: 0.8842, f1: 0.9058, edges-ner-ontonotes_loss: 0.0325
09/16 10:14:31 AM: Update 34679: task edges-ner-ontonotes, batch 679 (34679): mcc: 0.9036, acc: 0.8612, precision: 0.9304, recall: 0.8877, f1: 0.9086, edges-ner-ontonotes_loss: 0.0314
09/16 10:14:41 AM: Update 34811: task edges-ner-ontonotes, batch 811 (34811): mcc: 0.9062, acc: 0.8646, precision: 0.9325, recall: 0.8905, f1: 0.9110, edges-ner-ontonotes_loss: 0.0305
09/16 10:14:51 AM: Update 34919: task edges-ner-ontonotes, batch 919 (34919): mcc: 0.9087, acc: 0.8678, precision: 0.9342, recall: 0.8936, f1: 0.9135, edges-ner-ontonotes_loss: 0.0296
09/16 10:14:58 AM: ***** Step 35000 / Validation 35 *****
09/16 10:14:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:14:58 AM: Validating...
09/16 10:15:01 AM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.8993, acc: 0.8678, precision: 0.9191, recall: 0.8907, f1: 0.9047, edges-ner-ontonotes_loss: 0.0330
09/16 10:15:11 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9250, acc: 0.8971, precision: 0.9430, recall: 0.9153, f1: 0.9290, edges-ner-ontonotes_loss: 0.0250
09/16 10:15:17 AM: Updating LR scheduler:
09/16 10:15:17 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:15:17 AM: 	# validation passes without improvement: 1
09/16 10:15:17 AM: edges-ner-ontonotes_loss: training: 0.028762 validation: 0.022121
09/16 10:15:17 AM: macro_avg: validation: 0.937827
09/16 10:15:17 AM: micro_avg: validation: 0.000000
09/16 10:15:17 AM: edges-ner-ontonotes_mcc: training: 0.911504 validation: 0.934317
09/16 10:15:17 AM: edges-ner-ontonotes_acc: training: 0.871468 validation: 0.908477
09/16 10:15:17 AM: edges-ner-ontonotes_precision: training: 0.936009 validation: 0.950686
09/16 10:15:17 AM: edges-ner-ontonotes_recall: training: 0.897017 validation: 0.925311
09/16 10:15:17 AM: edges-ner-ontonotes_f1: training: 0.916098 validation: 0.937827
09/16 10:15:17 AM: Global learning rate: 2.5e-05
09/16 10:15:17 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:15:21 AM: Update 35058: task edges-ner-ontonotes, batch 58 (35058): mcc: 0.9318, acc: 0.8958, precision: 0.9487, recall: 0.9224, f1: 0.9354, edges-ner-ontonotes_loss: 0.0203
09/16 10:15:33 AM: Update 35172: task edges-ner-ontonotes, batch 172 (35172): mcc: 0.9380, acc: 0.9050, precision: 0.9538, recall: 0.9290, f1: 0.9413, edges-ner-ontonotes_loss: 0.0195
09/16 10:15:43 AM: Update 35291: task edges-ner-ontonotes, batch 291 (35291): mcc: 0.9352, acc: 0.9014, precision: 0.9526, recall: 0.9252, f1: 0.9387, edges-ner-ontonotes_loss: 0.0202
09/16 10:15:53 AM: Update 35415: task edges-ner-ontonotes, batch 415 (35415): mcc: 0.9352, acc: 0.9016, precision: 0.9528, recall: 0.9249, f1: 0.9387, edges-ner-ontonotes_loss: 0.0202
09/16 10:16:03 AM: Update 35526: task edges-ner-ontonotes, batch 526 (35526): mcc: 0.9309, acc: 0.8965, precision: 0.9497, recall: 0.9199, f1: 0.9346, edges-ner-ontonotes_loss: 0.0219
09/16 10:16:13 AM: Update 35651: task edges-ner-ontonotes, batch 651 (35651): mcc: 0.9245, acc: 0.8884, precision: 0.9453, recall: 0.9122, f1: 0.9285, edges-ner-ontonotes_loss: 0.0244
09/16 10:16:24 AM: Update 35789: task edges-ner-ontonotes, batch 789 (35789): mcc: 0.9196, acc: 0.8824, precision: 0.9418, recall: 0.9064, f1: 0.9238, edges-ner-ontonotes_loss: 0.0265
09/16 10:16:34 AM: Update 35934: task edges-ner-ontonotes, batch 934 (35934): mcc: 0.9175, acc: 0.8800, precision: 0.9402, recall: 0.9042, f1: 0.9218, edges-ner-ontonotes_loss: 0.0271
09/16 10:16:39 AM: ***** Step 36000 / Validation 36 *****
09/16 10:16:39 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:16:39 AM: Validating...
09/16 10:16:44 AM: Evaluate: task edges-ner-ontonotes, batch 56 (157): mcc: 0.9185, acc: 0.8897, precision: 0.9382, recall: 0.9079, f1: 0.9228, edges-ner-ontonotes_loss: 0.0275
09/16 10:16:55 AM: Evaluate: task edges-ner-ontonotes, batch 134 (157): mcc: 0.9322, acc: 0.9055, precision: 0.9513, recall: 0.9207, f1: 0.9358, edges-ner-ontonotes_loss: 0.0226
09/16 10:16:57 AM: Updating LR scheduler:
09/16 10:16:57 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:16:57 AM: 	# validation passes without improvement: 2
09/16 10:16:57 AM: Ran out of early stopping patience. Stopping training.
09/16 10:16:57 AM: edges-ner-ontonotes_loss: training: 0.027191 validation: 0.021801
09/16 10:16:57 AM: macro_avg: validation: 0.937616
09/16 10:16:57 AM: micro_avg: validation: 0.000000
09/16 10:16:57 AM: edges-ner-ontonotes_mcc: training: 0.917043 validation: 0.934162
09/16 10:16:57 AM: edges-ner-ontonotes_acc: training: 0.879169 validation: 0.907719
09/16 10:16:57 AM: edges-ner-ontonotes_precision: training: 0.939948 validation: 0.953797
09/16 10:16:57 AM: edges-ner-ontonotes_recall: training: 0.903515 validation: 0.921975
09/16 10:16:57 AM: edges-ner-ontonotes_f1: training: 0.921371 validation: 0.937616
09/16 10:16:57 AM: Global learning rate: 2.5e-05
09/16 10:16:57 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:16:57 AM: Stopped training after 36 validation checks
09/16 10:16:57 AM: Trained edges-ner-ontonotes for 36000 batches or 23.166 epochs
09/16 10:16:57 AM: ***** VALIDATION RESULTS *****
09/16 10:16:57 AM: edges-ner-ontonotes_f1 (for best val pass 26): edges-ner-ontonotes_loss: 0.02265, macro_avg: 0.93803, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.93451, edges-ner-ontonotes_acc: 0.90901, edges-ner-ontonotes_precision: 0.94944, edges-ner-ontonotes_recall: 0.92690, edges-ner-ontonotes_f1: 0.93803
09/16 10:16:57 AM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.04897, macro_avg: 0.86255, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.85674, edges-ner-ontonotes_acc: 0.80042, edges-ner-ontonotes_precision: 0.91824, edges-ner-ontonotes_recall: 0.81324, edges-ner-ontonotes_f1: 0.86255
09/16 10:16:57 AM: macro_avg (for best val pass 26): edges-ner-ontonotes_loss: 0.02265, macro_avg: 0.93803, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.93451, edges-ner-ontonotes_acc: 0.90901, edges-ner-ontonotes_precision: 0.94944, edges-ner-ontonotes_recall: 0.92690, edges-ner-ontonotes_f1: 0.93803
09/16 10:16:57 AM: Evaluating...
09/16 10:16:57 AM: Loaded model state from ./experiments/ner-ontonotes-mnli-top/run/edges-ner-ontonotes/model_state_target_train_val_26.best.th
09/16 10:16:57 AM: Evaluating on: edges-ner-ontonotes, split: val
09/16 10:17:27 AM: 	Task edges-ner-ontonotes: batch 235
09/16 10:17:28 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 10:17:28 AM: Finished evaluating on: edges-ner-ontonotes
09/16 10:17:28 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
09/16 10:17:28 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:17:28 AM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:17:28 AM: Evaluating on: edges-ner-ontonotes, split: test
09/16 10:17:48 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 10:17:48 AM: Finished evaluating on: edges-ner-ontonotes
09/16 10:17:48 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
09/16 10:17:48 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:17:48 AM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:17:48 AM: Writing results for split 'val' to ./experiments/ner-ontonotes-mnli-top/results.tsv
09/16 10:17:48 AM: micro_avg: 0.000, macro_avg: 0.934, edges-ner-ontonotes_mcc: 0.931, edges-ner-ontonotes_acc: 0.905, edges-ner-ontonotes_precision: 0.946, edges-ner-ontonotes_recall: 0.924, edges-ner-ontonotes_f1: 0.934
09/16 10:17:48 AM: Done!
