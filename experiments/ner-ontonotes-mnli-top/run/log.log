09/16 09:43:38 AM: Git branch: master
09/16 09:43:38 AM: Git SHA: 3df92c4ffc379a5a4d4b935b653f2ef463350c4f
09/16 09:43:38 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-mnli-top/",
  "exp_name": "experiments/ner-ontonotes-mnli-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-mnli-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/mnli",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-mnli-top__run",
  "run_dir": "./experiments/ner-ontonotes-mnli-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:43:38 AM: Saved config to ./experiments/ner-ontonotes-mnli-top/run/params.conf
09/16 09:43:38 AM: Using random seed 1234
09/16 09:43:39 AM: Using GPU 0
09/16 09:43:39 AM: Loading tasks...
09/16 09:43:39 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-mnli-top/
09/16 09:43:39 AM: 	Creating task edges-ner-ontonotes from scratch.
09/16 09:43:41 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 09:43:41 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 09:43:41 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 09:43:42 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 09:43:42 AM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 09:43:42 AM: 	Building vocab from scratch.
09/16 09:43:42 AM: 	Counting units for task edges-ner-ontonotes.
09/16 09:43:44 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 09:43:45 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:43:45 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:43:45 AM: 	Saved vocab to ./experiments/ner-ontonotes-mnli-top/vocab
09/16 09:43:45 AM: Loading token dictionary from ./experiments/ner-ontonotes-mnli-top/vocab.
09/16 09:43:46 AM: 	Loaded vocab from ./experiments/ner-ontonotes-mnli-top/vocab
09/16 09:43:46 AM: 	Vocab namespace chars: size 77
09/16 09:43:46 AM: 	Vocab namespace tokens: size 22840
09/16 09:43:46 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:43:46 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 09:43:46 AM: 	Finished building vocab.
09/16 09:43:46 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 09:43:58 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-mnli-top/preproc/edges-ner-ontonotes__train_data
09/16 09:43:58 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 09:44:00 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-mnli-top/preproc/edges-ner-ontonotes__val_data
09/16 09:44:00 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 09:44:01 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-mnli-top/preproc/edges-ner-ontonotes__test_data
09/16 09:44:01 AM: 	Finished indexing tasks
09/16 09:44:01 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 09:44:01 AM: 	  Training on 
09/16 09:44:01 AM: 	  Evaluating on edges-ner-ontonotes
09/16 09:44:01 AM: 	Finished loading tasks in 22.334s
09/16 09:44:01 AM: 	 Tasks: ['edges-ner-ontonotes']
09/16 09:44:01 AM: Building model...
09/16 09:44:01 AM: Using BERT model (bert-base-uncased).
09/16 09:44:01 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:44:01 AM: models/mnli
09/16 09:44:01 AM: loading configuration file models/mnli/config.json
09/16 09:44:01 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 3,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:44:01 AM: loading weights file models/mnli/pytorch_model.bin
09/16 09:44:07 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp4uf0uasq
09/16 09:44:10 AM: copying /tmp/tmp4uf0uasq to cache at ./experiments/ner-ontonotes-mnli-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:44:10 AM: creating metadata file for ./experiments/ner-ontonotes-mnli-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:44:10 AM: removing temp file /tmp/tmp4uf0uasq
09/16 09:44:10 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-mnli-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:44:11 AM: Initializing parameters
09/16 09:44:11 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:44:11 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:44:11 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:44:11 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:44:11 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:44:11 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:44:11 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 09:44:18 AM: Model specification:
09/16 09:44:18 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 09:44:18 AM: Model parameters:
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:44:18 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:44:18 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:44:18 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:44:18 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 09:44:18 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 09:44:18 AM: Total number of parameters: 109688338 (1.09688e+08)
09/16 09:44:18 AM: Number of trainable parameters: 206098 (206098)
09/16 09:44:18 AM: Finished building model in 16.497s
09/16 09:44:18 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 09:44:22 AM: patience = 9
09/16 09:44:22 AM: val_interval = 1000
09/16 09:44:22 AM: max_vals = 250
09/16 09:44:22 AM: cuda_device = 0
09/16 09:44:22 AM: grad_norm = 5.0
09/16 09:44:22 AM: grad_clipping = None
09/16 09:44:22 AM: lr_decay = 0.99
09/16 09:44:22 AM: min_lr = 1e-06
09/16 09:44:22 AM: keep_all_checkpoints = 0
09/16 09:44:22 AM: val_data_limit = 5000
09/16 09:44:22 AM: max_epochs = -1
09/16 09:44:22 AM: dec_val_scale = 250
09/16 09:44:22 AM: training_data_fraction = 1
09/16 09:44:22 AM: type = adam
09/16 09:44:22 AM: parameter_groups = None
09/16 09:44:22 AM: Number of trainable parameters: 206098
09/16 09:44:22 AM: infer_type_and_cast = True
09/16 09:44:22 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:44:22 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:44:22 AM: lr = 0.0001
09/16 09:44:22 AM: amsgrad = True
09/16 09:44:22 AM: type = reduce_on_plateau
09/16 09:44:22 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:44:22 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:44:22 AM: mode = max
09/16 09:44:22 AM: factor = 0.5
09/16 09:44:22 AM: patience = 3
09/16 09:44:22 AM: threshold = 0.0001
09/16 09:44:22 AM: threshold_mode = abs
09/16 09:44:22 AM: verbose = True
09/16 09:44:22 AM: type = adam
09/16 09:44:22 AM: parameter_groups = None
09/16 09:44:22 AM: Number of trainable parameters: 206098
09/16 09:44:22 AM: infer_type_and_cast = True
09/16 09:44:22 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:44:22 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:44:22 AM: lr = 0.0001
09/16 09:44:22 AM: amsgrad = True
09/16 09:44:22 AM: type = reduce_on_plateau
09/16 09:44:22 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:44:22 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:44:22 AM: mode = max
09/16 09:44:22 AM: factor = 0.5
09/16 09:44:22 AM: patience = 3
09/16 09:44:22 AM: threshold = 0.0001
09/16 09:44:22 AM: threshold_mode = abs
09/16 09:44:22 AM: verbose = True
09/16 09:44:22 AM: Starting training without restoring from a checkpoint.
09/16 09:44:22 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 09:44:22 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 09:44:32 AM: Update 116: task edges-ner-ontonotes, batch 116 (116): mcc: 0.0268, acc: 0.0205, precision: 0.0868, recall: 0.0582, f1: 0.0697, edges-ner-ontonotes_loss: 0.2674
09/16 09:44:42 AM: Update 259: task edges-ner-ontonotes, batch 259 (259): mcc: 0.2395, acc: 0.1663, precision: 0.3901, recall: 0.1839, f1: 0.2499, edges-ner-ontonotes_loss: 0.1862
09/16 09:44:52 AM: Update 349: task edges-ner-ontonotes, batch 349 (349): mcc: 0.3659, acc: 0.2598, precision: 0.5540, recall: 0.2742, f1: 0.3668, edges-ner-ontonotes_loss: 0.1627
09/16 09:45:02 AM: Update 470: task edges-ner-ontonotes, batch 470 (470): mcc: 0.4895, acc: 0.3668, precision: 0.6836, recall: 0.3801, f1: 0.4886, edges-ner-ontonotes_loss: 0.1407
09/16 09:45:12 AM: Update 587: task edges-ner-ontonotes, batch 587 (587): mcc: 0.5640, acc: 0.4390, precision: 0.7494, recall: 0.4519, f1: 0.5638, edges-ner-ontonotes_loss: 0.1260
09/16 09:45:22 AM: Update 687: task edges-ner-ontonotes, batch 687 (687): mcc: 0.6089, acc: 0.4851, precision: 0.7858, recall: 0.4979, f1: 0.6096, edges-ner-ontonotes_loss: 0.1165
09/16 09:45:32 AM: Update 806: task edges-ner-ontonotes, batch 806 (806): mcc: 0.6503, acc: 0.5304, precision: 0.8154, recall: 0.5432, f1: 0.6520, edges-ner-ontonotes_loss: 0.1071
09/16 09:45:42 AM: Update 923: task edges-ner-ontonotes, batch 923 (923): mcc: 0.6817, acc: 0.5668, precision: 0.8355, recall: 0.5796, f1: 0.6844, edges-ner-ontonotes_loss: 0.0994
09/16 09:45:51 AM: ***** Step 1000 / Validation 1 *****
09/16 09:45:51 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:45:51 AM: Validating...
09/16 09:45:52 AM: Evaluate: task edges-ner-ontonotes, batch 15 (157): mcc: 0.7468, acc: 0.6756, precision: 0.8340, recall: 0.6916, f1: 0.7561, edges-ner-ontonotes_loss: 0.0753
09/16 09:46:03 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.8466, acc: 0.7891, precision: 0.9122, recall: 0.8003, f1: 0.8526, edges-ner-ontonotes_loss: 0.0534
09/16 09:46:11 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:46:11 AM: Best result seen so far for micro.
09/16 09:46:11 AM: Best result seen so far for macro.
09/16 09:46:11 AM: Updating LR scheduler:
09/16 09:46:11 AM: 	Best result seen so far for macro_avg: 0.863
09/16 09:46:11 AM: 	# validation passes without improvement: 0
09/16 09:46:11 AM: edges-ner-ontonotes_loss: training: 0.095525 validation: 0.048975
09/16 09:46:11 AM: macro_avg: validation: 0.862554
09/16 09:46:11 AM: micro_avg: validation: 0.000000
09/16 09:46:11 AM: edges-ner-ontonotes_mcc: training: 0.697271 validation: 0.856742
09/16 09:46:11 AM: edges-ner-ontonotes_acc: training: 0.585422 validation: 0.800425
09/16 09:46:11 AM: edges-ner-ontonotes_precision: training: 0.844722 validation: 0.918236
09/16 09:46:11 AM: edges-ner-ontonotes_recall: training: 0.598304 validation: 0.813239
09/16 09:46:11 AM: edges-ner-ontonotes_f1: training: 0.700473 validation: 0.862554
09/16 09:46:11 AM: Global learning rate: 0.0001
09/16 09:46:11 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:46:13 AM: Update 1024: task edges-ner-ontonotes, batch 24 (1024): mcc: 0.8624, acc: 0.7958, precision: 0.9353, recall: 0.8082, f1: 0.8671, edges-ner-ontonotes_loss: 0.0452
09/16 09:46:23 AM: Update 1144: task edges-ner-ontonotes, batch 144 (1144): mcc: 0.8623, acc: 0.7996, precision: 0.9280, recall: 0.8144, f1: 0.8675, edges-ner-ontonotes_loss: 0.0447
09/16 09:46:34 AM: Update 1253: task edges-ner-ontonotes, batch 253 (1253): mcc: 0.8640, acc: 0.8022, precision: 0.9291, recall: 0.8165, f1: 0.8692, edges-ner-ontonotes_loss: 0.0438
09/16 09:46:44 AM: Update 1373: task edges-ner-ontonotes, batch 373 (1373): mcc: 0.8514, acc: 0.7847, precision: 0.9228, recall: 0.7996, f1: 0.8568, edges-ner-ontonotes_loss: 0.0483
09/16 09:46:54 AM: Update 1499: task edges-ner-ontonotes, batch 499 (1499): mcc: 0.8481, acc: 0.7801, precision: 0.9214, recall: 0.7950, f1: 0.8535, edges-ner-ontonotes_loss: 0.0491
09/16 09:47:04 AM: Update 1621: task edges-ner-ontonotes, batch 621 (1621): mcc: 0.8456, acc: 0.7768, precision: 0.9194, recall: 0.7922, f1: 0.8510, edges-ner-ontonotes_loss: 0.0496
09/16 09:47:14 AM: Update 1770: task edges-ner-ontonotes, batch 770 (1770): mcc: 0.8453, acc: 0.7768, precision: 0.9189, recall: 0.7921, f1: 0.8508, edges-ner-ontonotes_loss: 0.0492
09/16 09:47:24 AM: Update 1881: task edges-ner-ontonotes, batch 881 (1881): mcc: 0.8462, acc: 0.7778, precision: 0.9195, recall: 0.7932, f1: 0.8517, edges-ner-ontonotes_loss: 0.0484
09/16 09:47:33 AM: ***** Step 2000 / Validation 2 *****
09/16 09:47:33 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:47:33 AM: Validating...
09/16 09:47:34 AM: Evaluate: task edges-ner-ontonotes, batch 4 (157): mcc: 0.6877, acc: 0.6196, precision: 0.7926, recall: 0.6232, f1: 0.6978, edges-ner-ontonotes_loss: 0.0772
09/16 09:47:44 AM: Evaluate: task edges-ner-ontonotes, batch 94 (157): mcc: 0.8820, acc: 0.8369, precision: 0.9319, recall: 0.8466, f1: 0.8872, edges-ner-ontonotes_loss: 0.0371
09/16 09:47:52 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:47:52 AM: Best result seen so far for macro.
09/16 09:47:52 AM: Updating LR scheduler:
09/16 09:47:52 AM: 	Best result seen so far for macro_avg: 0.894
09/16 09:47:52 AM: 	# validation passes without improvement: 0
09/16 09:47:52 AM: edges-ner-ontonotes_loss: training: 0.047422 validation: 0.034811
09/16 09:47:52 AM: macro_avg: validation: 0.894370
09/16 09:47:52 AM: micro_avg: validation: 0.000000
09/16 09:47:52 AM: edges-ner-ontonotes_mcc: training: 0.849140 validation: 0.889419
09/16 09:47:52 AM: edges-ner-ontonotes_acc: training: 0.781912 validation: 0.843267
09/16 09:47:52 AM: edges-ner-ontonotes_precision: training: 0.920672 validation: 0.935791
09/16 09:47:52 AM: edges-ner-ontonotes_recall: training: 0.797397 validation: 0.856460
09/16 09:47:52 AM: edges-ner-ontonotes_f1: training: 0.854612 validation: 0.894370
09/16 09:47:52 AM: Global learning rate: 0.0001
09/16 09:47:52 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:47:54 AM: Update 2018: task edges-ner-ontonotes, batch 18 (2018): mcc: 0.8807, acc: 0.8280, precision: 0.9331, recall: 0.8431, f1: 0.8858, edges-ner-ontonotes_loss: 0.0357
09/16 09:48:04 AM: Update 2146: task edges-ner-ontonotes, batch 146 (2146): mcc: 0.8839, acc: 0.8318, precision: 0.9332, recall: 0.8488, f1: 0.8890, edges-ner-ontonotes_loss: 0.0362
09/16 09:48:14 AM: Update 2245: task edges-ner-ontonotes, batch 245 (2245): mcc: 0.8844, acc: 0.8303, precision: 0.9345, recall: 0.8485, f1: 0.8894, edges-ner-ontonotes_loss: 0.0357
09/16 09:48:24 AM: Update 2366: task edges-ner-ontonotes, batch 366 (2366): mcc: 0.8904, acc: 0.8386, precision: 0.9372, recall: 0.8570, f1: 0.8953, edges-ner-ontonotes_loss: 0.0339
09/16 09:48:34 AM: Update 2486: task edges-ner-ontonotes, batch 486 (2486): mcc: 0.8935, acc: 0.8430, precision: 0.9385, recall: 0.8615, f1: 0.8983, edges-ner-ontonotes_loss: 0.0330
09/16 09:48:44 AM: Update 2587: task edges-ner-ontonotes, batch 587 (2587): mcc: 0.8937, acc: 0.8433, precision: 0.9383, recall: 0.8620, f1: 0.8985, edges-ner-ontonotes_loss: 0.0329
09/16 09:48:54 AM: Update 2712: task edges-ner-ontonotes, batch 712 (2712): mcc: 0.8949, acc: 0.8448, precision: 0.9386, recall: 0.8639, f1: 0.8997, edges-ner-ontonotes_loss: 0.0325
09/16 09:49:04 AM: Update 2810: task edges-ner-ontonotes, batch 810 (2810): mcc: 0.8961, acc: 0.8463, precision: 0.9390, recall: 0.8656, f1: 0.9008, edges-ner-ontonotes_loss: 0.0321
09/16 09:49:14 AM: Update 2929: task edges-ner-ontonotes, batch 929 (2929): mcc: 0.8921, acc: 0.8411, precision: 0.9368, recall: 0.8604, f1: 0.8970, edges-ner-ontonotes_loss: 0.0338
09/16 09:49:20 AM: ***** Step 3000 / Validation 3 *****
09/16 09:49:20 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:49:20 AM: Validating...
09/16 09:49:25 AM: Evaluate: task edges-ner-ontonotes, batch 45 (157): mcc: 0.8686, acc: 0.8247, precision: 0.9100, recall: 0.8425, f1: 0.8750, edges-ner-ontonotes_loss: 0.0403
09/16 09:49:35 AM: Evaluate: task edges-ner-ontonotes, batch 121 (157): mcc: 0.9002, acc: 0.8602, precision: 0.9362, recall: 0.8759, f1: 0.9051, edges-ner-ontonotes_loss: 0.0325
09/16 09:49:39 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:49:39 AM: Best result seen so far for macro.
09/16 09:49:39 AM: Updating LR scheduler:
09/16 09:49:39 AM: 	Best result seen so far for macro_avg: 0.910
09/16 09:49:39 AM: 	# validation passes without improvement: 0
09/16 09:49:39 AM: edges-ner-ontonotes_loss: training: 0.034607 validation: 0.030751
09/16 09:49:39 AM: macro_avg: validation: 0.909796
09/16 09:49:39 AM: micro_avg: validation: 0.000000
09/16 09:49:39 AM: edges-ner-ontonotes_mcc: training: 0.890128 validation: 0.905247
09/16 09:49:39 AM: edges-ner-ontonotes_acc: training: 0.838481 validation: 0.864726
09/16 09:49:39 AM: edges-ner-ontonotes_precision: training: 0.935606 validation: 0.941372
09/16 09:49:39 AM: edges-ner-ontonotes_recall: training: 0.857929 validation: 0.880270
09/16 09:49:39 AM: edges-ner-ontonotes_f1: training: 0.895085 validation: 0.909796
09/16 09:49:39 AM: Global learning rate: 0.0001
09/16 09:49:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:49:45 AM: Update 3079: task edges-ner-ontonotes, batch 79 (3079): mcc: 0.8602, acc: 0.8029, precision: 0.9165, recall: 0.8212, f1: 0.8662, edges-ner-ontonotes_loss: 0.0457
09/16 09:49:55 AM: Update 3189: task edges-ner-ontonotes, batch 189 (3189): mcc: 0.8632, acc: 0.8041, precision: 0.9184, recall: 0.8247, f1: 0.8690, edges-ner-ontonotes_loss: 0.0435
09/16 09:50:05 AM: Update 3333: task edges-ner-ontonotes, batch 333 (3333): mcc: 0.8683, acc: 0.8105, precision: 0.9211, recall: 0.8314, f1: 0.8740, edges-ner-ontonotes_loss: 0.0412
09/16 09:50:15 AM: Update 3450: task edges-ner-ontonotes, batch 450 (3450): mcc: 0.8722, acc: 0.8166, precision: 0.9231, recall: 0.8368, f1: 0.8778, edges-ner-ontonotes_loss: 0.0399
09/16 09:50:25 AM: Update 3573: task edges-ner-ontonotes, batch 573 (3573): mcc: 0.8768, acc: 0.8231, precision: 0.9254, recall: 0.8431, f1: 0.8823, edges-ner-ontonotes_loss: 0.0386
09/16 09:50:35 AM: Update 3703: task edges-ner-ontonotes, batch 703 (3703): mcc: 0.8812, acc: 0.8293, precision: 0.9275, recall: 0.8492, f1: 0.8866, edges-ner-ontonotes_loss: 0.0374
09/16 09:50:45 AM: Update 3799: task edges-ner-ontonotes, batch 799 (3799): mcc: 0.8846, acc: 0.8338, precision: 0.9295, recall: 0.8535, f1: 0.8899, edges-ner-ontonotes_loss: 0.0364
09/16 09:50:55 AM: Update 3927: task edges-ner-ontonotes, batch 927 (3927): mcc: 0.8888, acc: 0.8390, precision: 0.9319, recall: 0.8590, f1: 0.8939, edges-ner-ontonotes_loss: 0.0351
09/16 09:51:01 AM: ***** Step 4000 / Validation 4 *****
09/16 09:51:01 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:51:01 AM: Validating...
09/16 09:51:05 AM: Evaluate: task edges-ner-ontonotes, batch 43 (157): mcc: 0.8748, acc: 0.8349, precision: 0.9136, recall: 0.8505, f1: 0.8809, edges-ner-ontonotes_loss: 0.0383
09/16 09:51:15 AM: Evaluate: task edges-ner-ontonotes, batch 118 (157): mcc: 0.9065, acc: 0.8691, precision: 0.9380, recall: 0.8858, f1: 0.9112, edges-ner-ontonotes_loss: 0.0302
09/16 09:51:19 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:51:19 AM: Best result seen so far for macro.
09/16 09:51:19 AM: Updating LR scheduler:
09/16 09:51:19 AM: 	Best result seen so far for macro_avg: 0.917
09/16 09:51:19 AM: 	# validation passes without improvement: 0
09/16 09:51:19 AM: edges-ner-ontonotes_loss: training: 0.034351 validation: 0.027952
09/16 09:51:19 AM: macro_avg: validation: 0.917111
09/16 09:51:19 AM: micro_avg: validation: 0.000000
09/16 09:51:19 AM: edges-ner-ontonotes_mcc: training: 0.890998 validation: 0.912743
09/16 09:51:19 AM: edges-ner-ontonotes_acc: training: 0.841668 validation: 0.875341
09/16 09:51:19 AM: edges-ner-ontonotes_precision: training: 0.933241 validation: 0.942466
09/16 09:51:19 AM: edges-ner-ontonotes_recall: training: 0.861737 validation: 0.893085
09/16 09:51:19 AM: edges-ner-ontonotes_f1: training: 0.896065 validation: 0.917111
09/16 09:51:19 AM: Global learning rate: 0.0001
09/16 09:51:19 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:51:26 AM: Update 4052: task edges-ner-ontonotes, batch 52 (4052): mcc: 0.9170, acc: 0.8747, precision: 0.9482, recall: 0.8954, f1: 0.9210, edges-ner-ontonotes_loss: 0.0255
09/16 09:51:36 AM: Update 4172: task edges-ner-ontonotes, batch 172 (4172): mcc: 0.9134, acc: 0.8706, precision: 0.9447, recall: 0.8922, f1: 0.9177, edges-ner-ontonotes_loss: 0.0267
09/16 09:51:46 AM: Update 4295: task edges-ner-ontonotes, batch 295 (4295): mcc: 0.9131, acc: 0.8701, precision: 0.9438, recall: 0.8924, f1: 0.9174, edges-ner-ontonotes_loss: 0.0268
09/16 09:51:56 AM: Update 4404: task edges-ner-ontonotes, batch 404 (4404): mcc: 0.9091, acc: 0.8653, precision: 0.9415, recall: 0.8873, f1: 0.9136, edges-ner-ontonotes_loss: 0.0284
09/16 09:52:06 AM: Update 4535: task edges-ner-ontonotes, batch 535 (4535): mcc: 0.9021, acc: 0.8560, precision: 0.9375, recall: 0.8781, f1: 0.9069, edges-ner-ontonotes_loss: 0.0317
09/16 09:52:16 AM: Update 4659: task edges-ner-ontonotes, batch 659 (4659): mcc: 0.8968, acc: 0.8495, precision: 0.9343, recall: 0.8714, f1: 0.9018, edges-ner-ontonotes_loss: 0.0336
09/16 09:52:26 AM: Update 4784: task edges-ner-ontonotes, batch 784 (4784): mcc: 0.8950, acc: 0.8476, precision: 0.9329, recall: 0.8693, f1: 0.9000, edges-ner-ontonotes_loss: 0.0342
09/16 09:52:36 AM: Update 4929: task edges-ner-ontonotes, batch 929 (4929): mcc: 0.8938, acc: 0.8459, precision: 0.9323, recall: 0.8678, f1: 0.8989, edges-ner-ontonotes_loss: 0.0343
09/16 09:52:44 AM: ***** Step 5000 / Validation 5 *****
09/16 09:52:44 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:52:44 AM: Validating...
09/16 09:52:46 AM: Evaluate: task edges-ner-ontonotes, batch 24 (157): mcc: 0.8551, acc: 0.8164, precision: 0.8892, recall: 0.8374, f1: 0.8625, edges-ner-ontonotes_loss: 0.0413
09/16 09:52:56 AM: Evaluate: task edges-ner-ontonotes, batch 111 (157): mcc: 0.9100, acc: 0.8758, precision: 0.9381, recall: 0.8922, f1: 0.9146, edges-ner-ontonotes_loss: 0.0285
09/16 09:53:02 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:53:02 AM: Best result seen so far for macro.
09/16 09:53:02 AM: Updating LR scheduler:
09/16 09:53:02 AM: 	Best result seen so far for macro_avg: 0.923
09/16 09:53:02 AM: 	# validation passes without improvement: 0
09/16 09:53:02 AM: edges-ner-ontonotes_loss: training: 0.034440 validation: 0.026368
09/16 09:53:02 AM: macro_avg: validation: 0.922796
09/16 09:53:02 AM: micro_avg: validation: 0.000000
09/16 09:53:02 AM: edges-ner-ontonotes_mcc: training: 0.893073 validation: 0.918706
09/16 09:53:02 AM: edges-ner-ontonotes_acc: training: 0.845026 validation: 0.884820
09/16 09:53:02 AM: edges-ner-ontonotes_precision: training: 0.931760 validation: 0.946793
09/16 09:53:02 AM: edges-ner-ontonotes_recall: training: 0.866948 validation: 0.899985
09/16 09:53:02 AM: edges-ner-ontonotes_f1: training: 0.898187 validation: 0.922796
09/16 09:53:02 AM: Global learning rate: 0.0001
09/16 09:53:02 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:53:06 AM: Update 5047: task edges-ner-ontonotes, batch 47 (5047): mcc: 0.8908, acc: 0.8468, precision: 0.9241, recall: 0.8700, f1: 0.8962, edges-ner-ontonotes_loss: 0.0338
09/16 09:53:16 AM: Update 5169: task edges-ner-ontonotes, batch 169 (5169): mcc: 0.8963, acc: 0.8505, precision: 0.9304, recall: 0.8742, f1: 0.9015, edges-ner-ontonotes_loss: 0.0316
09/16 09:53:27 AM: Update 5295: task edges-ner-ontonotes, batch 295 (5295): mcc: 0.9014, acc: 0.8566, precision: 0.9347, recall: 0.8796, f1: 0.9063, edges-ner-ontonotes_loss: 0.0305
09/16 09:53:37 AM: Update 5417: task edges-ner-ontonotes, batch 417 (5417): mcc: 0.9060, acc: 0.8628, precision: 0.9378, recall: 0.8851, f1: 0.9107, edges-ner-ontonotes_loss: 0.0288
09/16 09:53:47 AM: Update 5537: task edges-ner-ontonotes, batch 537 (5537): mcc: 0.9095, acc: 0.8673, precision: 0.9399, recall: 0.8896, f1: 0.9141, edges-ner-ontonotes_loss: 0.0280
09/16 09:53:58 AM: Update 5623: task edges-ner-ontonotes, batch 623 (5623): mcc: 0.9111, acc: 0.8696, precision: 0.9411, recall: 0.8913, f1: 0.9155, edges-ner-ontonotes_loss: 0.0275
09/16 09:54:08 AM: Update 5743: task edges-ner-ontonotes, batch 743 (5743): mcc: 0.9128, acc: 0.8713, precision: 0.9421, recall: 0.8936, f1: 0.9172, edges-ner-ontonotes_loss: 0.0270
09/16 09:54:18 AM: Update 5871: task edges-ner-ontonotes, batch 871 (5871): mcc: 0.9137, acc: 0.8724, precision: 0.9429, recall: 0.8944, f1: 0.9180, edges-ner-ontonotes_loss: 0.0269
09/16 09:54:28 AM: Update 5971: task edges-ner-ontonotes, batch 971 (5971): mcc: 0.9122, acc: 0.8701, precision: 0.9419, recall: 0.8926, f1: 0.9166, edges-ner-ontonotes_loss: 0.0276
09/16 09:54:30 AM: ***** Step 6000 / Validation 6 *****
09/16 09:54:30 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:54:30 AM: Validating...
09/16 09:54:38 AM: Evaluate: task edges-ner-ontonotes, batch 69 (157): mcc: 0.8999, acc: 0.8629, precision: 0.9321, recall: 0.8792, f1: 0.9049, edges-ner-ontonotes_loss: 0.0328
09/16 09:54:48 AM: Evaluate: task edges-ner-ontonotes, batch 147 (157): mcc: 0.9199, acc: 0.8864, precision: 0.9464, recall: 0.9025, f1: 0.9239, edges-ner-ontonotes_loss: 0.0265
09/16 09:54:49 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:54:49 AM: Best result seen so far for macro.
09/16 09:54:49 AM: Updating LR scheduler:
09/16 09:54:49 AM: 	Best result seen so far for macro_avg: 0.925
09/16 09:54:49 AM: 	# validation passes without improvement: 0
09/16 09:54:49 AM: edges-ner-ontonotes_loss: training: 0.027988 validation: 0.026053
09/16 09:54:49 AM: macro_avg: validation: 0.925059
09/16 09:54:49 AM: micro_avg: validation: 0.000000
09/16 09:54:49 AM: edges-ner-ontonotes_mcc: training: 0.911327 validation: 0.921049
09/16 09:54:49 AM: edges-ner-ontonotes_acc: training: 0.869031 validation: 0.887701
09/16 09:54:49 AM: edges-ner-ontonotes_precision: training: 0.941401 validation: 0.947452
09/16 09:54:49 AM: edges-ner-ontonotes_recall: training: 0.891481 validation: 0.903700
09/16 09:54:49 AM: edges-ner-ontonotes_f1: training: 0.915761 validation: 0.925059
09/16 09:54:49 AM: Global learning rate: 0.0001
09/16 09:54:49 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:54:58 AM: Update 6115: task edges-ner-ontonotes, batch 115 (6115): mcc: 0.8792, acc: 0.8283, precision: 0.9198, recall: 0.8527, f1: 0.8850, edges-ner-ontonotes_loss: 0.0399
09/16 09:55:08 AM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.8786, acc: 0.8275, precision: 0.9204, recall: 0.8510, f1: 0.8843, edges-ner-ontonotes_loss: 0.0405
09/16 09:55:18 AM: Update 6371: task edges-ner-ontonotes, batch 371 (6371): mcc: 0.8809, acc: 0.8303, precision: 0.9217, recall: 0.8541, f1: 0.8866, edges-ner-ontonotes_loss: 0.0383
09/16 09:55:28 AM: Update 6523: task edges-ner-ontonotes, batch 523 (6523): mcc: 0.8840, acc: 0.8343, precision: 0.9242, recall: 0.8574, f1: 0.8896, edges-ner-ontonotes_loss: 0.0370
09/16 09:55:38 AM: Update 6626: task edges-ner-ontonotes, batch 626 (6626): mcc: 0.8881, acc: 0.8395, precision: 0.9265, recall: 0.8627, f1: 0.8935, edges-ner-ontonotes_loss: 0.0357
09/16 09:55:48 AM: Update 6759: task edges-ner-ontonotes, batch 759 (6759): mcc: 0.8916, acc: 0.8441, precision: 0.9286, recall: 0.8672, f1: 0.8969, edges-ner-ontonotes_loss: 0.0346
09/16 09:55:58 AM: Update 6856: task edges-ner-ontonotes, batch 856 (6856): mcc: 0.8937, acc: 0.8467, precision: 0.9298, recall: 0.8700, f1: 0.8989, edges-ner-ontonotes_loss: 0.0339
09/16 09:56:08 AM: Update 6981: task edges-ner-ontonotes, batch 981 (6981): mcc: 0.8979, acc: 0.8523, precision: 0.9323, recall: 0.8753, f1: 0.9029, edges-ner-ontonotes_loss: 0.0327
09/16 09:56:10 AM: ***** Step 7000 / Validation 7 *****
09/16 09:56:10 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:56:10 AM: Validating...
09/16 09:56:18 AM: Evaluate: task edges-ner-ontonotes, batch 78 (157): mcc: 0.9056, acc: 0.8728, precision: 0.9314, recall: 0.8905, f1: 0.9105, edges-ner-ontonotes_loss: 0.0311
09/16 09:56:28 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:56:28 AM: Best result seen so far for macro.
09/16 09:56:28 AM: Updating LR scheduler:
09/16 09:56:28 AM: 	Best result seen so far for macro_avg: 0.926
09/16 09:56:28 AM: 	# validation passes without improvement: 0
09/16 09:56:28 AM: edges-ner-ontonotes_loss: training: 0.032492 validation: 0.025536
09/16 09:56:28 AM: macro_avg: validation: 0.926109
09/16 09:56:28 AM: micro_avg: validation: 0.000000
09/16 09:56:28 AM: edges-ner-ontonotes_mcc: training: 0.898461 validation: 0.921994
09/16 09:56:28 AM: edges-ner-ontonotes_acc: training: 0.853072 validation: 0.891796
09/16 09:56:28 AM: edges-ner-ontonotes_precision: training: 0.932691 validation: 0.942317
09/16 09:56:28 AM: edges-ner-ontonotes_recall: training: 0.876004 validation: 0.910449
09/16 09:56:28 AM: edges-ner-ontonotes_f1: training: 0.903459 validation: 0.926109
09/16 09:56:28 AM: Global learning rate: 0.0001
09/16 09:56:28 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:56:29 AM: Update 7001: task edges-ner-ontonotes, batch 1 (7001): mcc: 0.9619, acc: 0.9353, precision: 0.9640, recall: 0.9640, f1: 0.9640, edges-ner-ontonotes_loss: 0.0156
09/16 09:56:39 AM: Update 7126: task edges-ner-ontonotes, batch 126 (7126): mcc: 0.9260, acc: 0.8894, precision: 0.9489, recall: 0.9114, f1: 0.9298, edges-ner-ontonotes_loss: 0.0231
09/16 09:56:49 AM: Update 7232: task edges-ner-ontonotes, batch 232 (7232): mcc: 0.9247, acc: 0.8867, precision: 0.9479, recall: 0.9099, f1: 0.9286, edges-ner-ontonotes_loss: 0.0237
09/16 09:56:59 AM: Update 7356: task edges-ner-ontonotes, batch 356 (7356): mcc: 0.9245, acc: 0.8864, precision: 0.9471, recall: 0.9105, f1: 0.9285, edges-ner-ontonotes_loss: 0.0237
09/16 09:57:10 AM: Update 7477: task edges-ner-ontonotes, batch 477 (7477): mcc: 0.9228, acc: 0.8845, precision: 0.9457, recall: 0.9087, f1: 0.9268, edges-ner-ontonotes_loss: 0.0242
09/16 09:57:20 AM: Update 7598: task edges-ner-ontonotes, batch 598 (7598): mcc: 0.9147, acc: 0.8740, precision: 0.9405, recall: 0.8986, f1: 0.9191, edges-ner-ontonotes_loss: 0.0276
09/16 09:57:30 AM: Update 7725: task edges-ner-ontonotes, batch 725 (7725): mcc: 0.9094, acc: 0.8671, precision: 0.9374, recall: 0.8917, f1: 0.9140, edges-ner-ontonotes_loss: 0.0296
09/16 09:57:40 AM: Update 7848: task edges-ner-ontonotes, batch 848 (7848): mcc: 0.9063, acc: 0.8633, precision: 0.9356, recall: 0.8878, f1: 0.9111, edges-ner-ontonotes_loss: 0.0308
09/16 09:57:50 AM: Update 7998: task edges-ner-ontonotes, batch 998 (7998): mcc: 0.9051, acc: 0.8617, precision: 0.9349, recall: 0.8862, f1: 0.9099, edges-ner-ontonotes_loss: 0.0312
09/16 09:57:50 AM: ***** Step 8000 / Validation 8 *****
09/16 09:57:50 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:57:50 AM: Validating...
09/16 09:58:00 AM: Evaluate: task edges-ner-ontonotes, batch 90 (157): mcc: 0.9166, acc: 0.8823, precision: 0.9444, recall: 0.8983, f1: 0.9208, edges-ner-ontonotes_loss: 0.0277
09/16 09:58:09 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:58:09 AM: Best result seen so far for macro.
09/16 09:58:09 AM: Updating LR scheduler:
09/16 09:58:09 AM: 	Best result seen so far for macro_avg: 0.929
09/16 09:58:09 AM: 	# validation passes without improvement: 0
09/16 09:58:09 AM: edges-ner-ontonotes_loss: training: 0.031224 validation: 0.024741
09/16 09:58:09 AM: macro_avg: validation: 0.928807
09/16 09:58:09 AM: micro_avg: validation: 0.000000
09/16 09:58:09 AM: edges-ner-ontonotes_mcc: training: 0.905136 validation: 0.925004
09/16 09:58:09 AM: edges-ner-ontonotes_acc: training: 0.861738 validation: 0.893009
09/16 09:58:09 AM: edges-ner-ontonotes_precision: training: 0.934899 validation: 0.950981
09/16 09:58:09 AM: edges-ner-ontonotes_recall: training: 0.886248 validation: 0.907643
09/16 09:58:09 AM: edges-ner-ontonotes_f1: training: 0.909924 validation: 0.928807
09/16 09:58:09 AM: Global learning rate: 0.0001
09/16 09:58:09 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 09:58:10 AM: Update 8014: task edges-ner-ontonotes, batch 14 (8014): mcc: 0.8936, acc: 0.8520, precision: 0.9335, recall: 0.8662, f1: 0.8986, edges-ner-ontonotes_loss: 0.0321
09/16 09:58:20 AM: Update 8123: task edges-ner-ontonotes, batch 123 (8123): mcc: 0.8944, acc: 0.8475, precision: 0.9291, recall: 0.8721, f1: 0.8997, edges-ner-ontonotes_loss: 0.0321
09/16 09:58:31 AM: Update 8252: task edges-ner-ontonotes, batch 252 (8252): mcc: 0.9024, acc: 0.8579, precision: 0.9330, recall: 0.8829, f1: 0.9073, edges-ner-ontonotes_loss: 0.0304
09/16 09:58:41 AM: Update 8378: task edges-ner-ontonotes, batch 378 (8378): mcc: 0.9040, acc: 0.8604, precision: 0.9337, recall: 0.8853, f1: 0.9089, edges-ner-ontonotes_loss: 0.0299
09/16 09:58:51 AM: Update 8484: task edges-ner-ontonotes, batch 484 (8484): mcc: 0.9082, acc: 0.8662, precision: 0.9365, recall: 0.8905, f1: 0.9129, edges-ner-ontonotes_loss: 0.0288
09/16 09:59:01 AM: Update 8606: task edges-ner-ontonotes, batch 606 (8606): mcc: 0.9116, acc: 0.8708, precision: 0.9383, recall: 0.8950, f1: 0.9162, edges-ner-ontonotes_loss: 0.0277
09/16 09:59:11 AM: Update 8720: task edges-ner-ontonotes, batch 720 (8720): mcc: 0.9147, acc: 0.8748, precision: 0.9406, recall: 0.8985, f1: 0.9190, edges-ner-ontonotes_loss: 0.0268
09/16 09:59:21 AM: Update 8839: task edges-ner-ontonotes, batch 839 (8839): mcc: 0.9158, acc: 0.8763, precision: 0.9414, recall: 0.8998, f1: 0.9201, edges-ner-ontonotes_loss: 0.0265
09/16 09:59:31 AM: Update 8968: task edges-ner-ontonotes, batch 968 (8968): mcc: 0.9167, acc: 0.8776, precision: 0.9417, recall: 0.9011, f1: 0.9210, edges-ner-ontonotes_loss: 0.0261
09/16 09:59:34 AM: ***** Step 9000 / Validation 9 *****
09/16 09:59:34 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 09:59:34 AM: Validating...
09/16 09:59:42 AM: Evaluate: task edges-ner-ontonotes, batch 62 (157): mcc: 0.9051, acc: 0.8720, precision: 0.9276, recall: 0.8932, f1: 0.9101, edges-ner-ontonotes_loss: 0.0318
09/16 09:59:52 AM: Evaluate: task edges-ner-ontonotes, batch 141 (157): mcc: 0.9252, acc: 0.8962, precision: 0.9443, recall: 0.9145, f1: 0.9292, edges-ner-ontonotes_loss: 0.0253
09/16 09:59:53 AM: Best result seen so far for edges-ner-ontonotes.
09/16 09:59:53 AM: Best result seen so far for macro.
09/16 09:59:53 AM: Updating LR scheduler:
09/16 09:59:53 AM: 	Best result seen so far for macro_avg: 0.930
09/16 09:59:53 AM: 	# validation passes without improvement: 0
09/16 09:59:53 AM: edges-ner-ontonotes_loss: training: 0.025945 validation: 0.024800
09/16 09:59:53 AM: macro_avg: validation: 0.929648
09/16 09:59:53 AM: micro_avg: validation: 0.000000
09/16 09:59:53 AM: edges-ner-ontonotes_mcc: training: 0.917229 validation: 0.925729
09/16 09:59:53 AM: edges-ner-ontonotes_acc: training: 0.878272 validation: 0.895966
09/16 09:59:53 AM: edges-ner-ontonotes_precision: training: 0.941993 validation: 0.945503
09/16 09:59:53 AM: edges-ner-ontonotes_recall: training: 0.901875 validation: 0.914316
09/16 09:59:53 AM: edges-ner-ontonotes_f1: training: 0.921498 validation: 0.929648
09/16 09:59:53 AM: Global learning rate: 0.0001
09/16 09:59:53 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:00:02 AM: Update 9072: task edges-ner-ontonotes, batch 72 (9072): mcc: 0.8975, acc: 0.8496, precision: 0.9286, recall: 0.8781, f1: 0.9027, edges-ner-ontonotes_loss: 0.0339
09/16 10:00:12 AM: Update 9202: task edges-ner-ontonotes, batch 202 (9202): mcc: 0.8911, acc: 0.8446, precision: 0.9241, recall: 0.8707, f1: 0.8966, edges-ner-ontonotes_loss: 0.0365
09/16 10:00:22 AM: Update 9331: task edges-ner-ontonotes, batch 331 (9331): mcc: 0.8873, acc: 0.8392, precision: 0.9225, recall: 0.8651, f1: 0.8929, edges-ner-ontonotes_loss: 0.0378
09/16 10:00:32 AM: Update 9459: task edges-ner-ontonotes, batch 459 (9459): mcc: 0.8882, acc: 0.8404, precision: 0.9231, recall: 0.8662, f1: 0.8937, edges-ner-ontonotes_loss: 0.0364
09/16 10:00:42 AM: Update 9612: task edges-ner-ontonotes, batch 612 (9612): mcc: 0.8899, acc: 0.8425, precision: 0.9241, recall: 0.8683, f1: 0.8953, edges-ner-ontonotes_loss: 0.0355
09/16 10:00:52 AM: Update 9724: task edges-ner-ontonotes, batch 724 (9724): mcc: 0.8916, acc: 0.8447, precision: 0.9254, recall: 0.8703, f1: 0.8970, edges-ner-ontonotes_loss: 0.0347
09/16 10:01:02 AM: Update 9857: task edges-ner-ontonotes, batch 857 (9857): mcc: 0.8953, acc: 0.8495, precision: 0.9277, recall: 0.8749, f1: 0.9005, edges-ner-ontonotes_loss: 0.0335
09/16 10:01:12 AM: Update 9963: task edges-ner-ontonotes, batch 963 (9963): mcc: 0.8970, acc: 0.8517, precision: 0.9287, recall: 0.8772, f1: 0.9022, edges-ner-ontonotes_loss: 0.0330
09/16 10:01:15 AM: ***** Step 10000 / Validation 10 *****
09/16 10:01:15 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:01:15 AM: Validating...
09/16 10:01:22 AM: Evaluate: task edges-ner-ontonotes, batch 65 (157): mcc: 0.9078, acc: 0.8741, precision: 0.9301, recall: 0.8959, f1: 0.9127, edges-ner-ontonotes_loss: 0.0301
09/16 10:01:32 AM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.9266, acc: 0.8975, precision: 0.9450, recall: 0.9163, f1: 0.9305, edges-ner-ontonotes_loss: 0.0243
09/16 10:01:34 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:01:34 AM: Best result seen so far for macro.
09/16 10:01:34 AM: Updating LR scheduler:
09/16 10:01:34 AM: 	Best result seen so far for macro_avg: 0.931
09/16 10:01:34 AM: 	# validation passes without improvement: 0
09/16 10:01:34 AM: edges-ner-ontonotes_loss: training: 0.032520 validation: 0.024080
09/16 10:01:34 AM: macro_avg: validation: 0.930827
09/16 10:01:34 AM: micro_avg: validation: 0.000000
09/16 10:01:34 AM: edges-ner-ontonotes_mcc: training: 0.898417 validation: 0.926957
09/16 10:01:34 AM: edges-ner-ontonotes_acc: training: 0.853371 validation: 0.897558
09/16 10:01:34 AM: edges-ner-ontonotes_precision: training: 0.929706 validation: 0.945836
09/16 10:01:34 AM: edges-ner-ontonotes_recall: training: 0.878777 validation: 0.916288
09/16 10:01:34 AM: edges-ner-ontonotes_f1: training: 0.903525 validation: 0.930827
09/16 10:01:34 AM: Global learning rate: 0.0001
09/16 10:01:34 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:01:42 AM: Update 10106: task edges-ner-ontonotes, batch 106 (10106): mcc: 0.9281, acc: 0.8899, precision: 0.9520, recall: 0.9125, f1: 0.9318, edges-ner-ontonotes_loss: 0.0228
09/16 10:01:52 AM: Update 10234: task edges-ner-ontonotes, batch 234 (10234): mcc: 0.9269, acc: 0.8898, precision: 0.9498, recall: 0.9123, f1: 0.9307, edges-ner-ontonotes_loss: 0.0225
09/16 10:02:02 AM: Update 10335: task edges-ner-ontonotes, batch 335 (10335): mcc: 0.9257, acc: 0.8889, precision: 0.9480, recall: 0.9118, f1: 0.9296, edges-ner-ontonotes_loss: 0.0228
09/16 10:02:12 AM: Update 10458: task edges-ner-ontonotes, batch 458 (10458): mcc: 0.9267, acc: 0.8901, precision: 0.9486, recall: 0.9130, f1: 0.9305, edges-ner-ontonotes_loss: 0.0226
09/16 10:02:22 AM: Update 10584: task edges-ner-ontonotes, batch 584 (10584): mcc: 0.9263, acc: 0.8899, precision: 0.9482, recall: 0.9128, f1: 0.9302, edges-ner-ontonotes_loss: 0.0230
09/16 10:02:32 AM: Update 10687: task edges-ner-ontonotes, batch 687 (10687): mcc: 0.9208, acc: 0.8827, precision: 0.9450, recall: 0.9057, f1: 0.9249, edges-ner-ontonotes_loss: 0.0253
09/16 10:02:43 AM: Update 10819: task edges-ner-ontonotes, batch 819 (10819): mcc: 0.9159, acc: 0.8767, precision: 0.9417, recall: 0.8998, f1: 0.9203, edges-ner-ontonotes_loss: 0.0273
09/16 10:02:53 AM: Update 10919: task edges-ner-ontonotes, batch 919 (10919): mcc: 0.9129, acc: 0.8730, precision: 0.9394, recall: 0.8963, f1: 0.9174, edges-ner-ontonotes_loss: 0.0285
09/16 10:02:58 AM: ***** Step 11000 / Validation 11 *****
09/16 10:02:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:02:58 AM: Validating...
09/16 10:03:03 AM: Evaluate: task edges-ner-ontonotes, batch 46 (157): mcc: 0.9029, acc: 0.8685, precision: 0.9282, recall: 0.8886, f1: 0.9080, edges-ner-ontonotes_loss: 0.0315
09/16 10:03:13 AM: Evaluate: task edges-ner-ontonotes, batch 121 (157): mcc: 0.9211, acc: 0.8882, precision: 0.9461, recall: 0.9052, f1: 0.9252, edges-ner-ontonotes_loss: 0.0256
09/16 10:03:17 AM: Updating LR scheduler:
09/16 10:03:17 AM: 	Best result seen so far for macro_avg: 0.931
09/16 10:03:17 AM: 	# validation passes without improvement: 1
09/16 10:03:17 AM: edges-ner-ontonotes_loss: training: 0.028878 validation: 0.023949
09/16 10:03:17 AM: macro_avg: validation: 0.930761
09/16 10:03:17 AM: micro_avg: validation: 0.000000
09/16 10:03:17 AM: edges-ner-ontonotes_mcc: training: 0.911499 validation: 0.927032
09/16 10:03:17 AM: edges-ner-ontonotes_acc: training: 0.871266 validation: 0.895587
09/16 10:03:17 AM: edges-ner-ontonotes_precision: training: 0.938524 validation: 0.951668
09/16 10:03:17 AM: edges-ner-ontonotes_recall: training: 0.894570 validation: 0.910752
09/16 10:03:17 AM: edges-ner-ontonotes_f1: training: 0.916020 validation: 0.930761
09/16 10:03:17 AM: Global learning rate: 0.0001
09/16 10:03:17 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:03:23 AM: Update 11094: task edges-ner-ontonotes, batch 94 (11094): mcc: 0.9013, acc: 0.8567, precision: 0.9346, recall: 0.8793, f1: 0.9061, edges-ner-ontonotes_loss: 0.0310
09/16 10:03:33 AM: Update 11218: task edges-ner-ontonotes, batch 218 (11218): mcc: 0.8995, acc: 0.8546, precision: 0.9325, recall: 0.8781, f1: 0.9045, edges-ner-ontonotes_loss: 0.0318
09/16 10:03:43 AM: Update 11344: task edges-ner-ontonotes, batch 344 (11344): mcc: 0.9044, acc: 0.8625, precision: 0.9344, recall: 0.8854, f1: 0.9092, edges-ner-ontonotes_loss: 0.0304
09/16 10:03:53 AM: Update 11474: task edges-ner-ontonotes, batch 474 (11474): mcc: 0.9066, acc: 0.8657, precision: 0.9350, recall: 0.8888, f1: 0.9113, edges-ner-ontonotes_loss: 0.0297
09/16 10:04:03 AM: Update 11580: task edges-ner-ontonotes, batch 580 (11580): mcc: 0.9100, acc: 0.8697, precision: 0.9376, recall: 0.8927, f1: 0.9146, edges-ner-ontonotes_loss: 0.0286
09/16 10:04:13 AM: Update 11703: task edges-ner-ontonotes, batch 703 (11703): mcc: 0.9139, acc: 0.8747, precision: 0.9405, recall: 0.8971, f1: 0.9183, edges-ner-ontonotes_loss: 0.0274
09/16 10:04:23 AM: Update 11822: task edges-ner-ontonotes, batch 822 (11822): mcc: 0.9167, acc: 0.8784, precision: 0.9422, recall: 0.9007, f1: 0.9210, edges-ner-ontonotes_loss: 0.0264
09/16 10:04:33 AM: Update 11933: task edges-ner-ontonotes, batch 933 (11933): mcc: 0.9177, acc: 0.8798, precision: 0.9429, recall: 0.9020, f1: 0.9220, edges-ner-ontonotes_loss: 0.0261
09/16 10:04:38 AM: ***** Step 12000 / Validation 12 *****
09/16 10:04:38 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:04:38 AM: Validating...
09/16 10:04:43 AM: Evaluate: task edges-ner-ontonotes, batch 47 (157): mcc: 0.8929, acc: 0.8616, precision: 0.9143, recall: 0.8833, f1: 0.8986, edges-ner-ontonotes_loss: 0.0353
09/16 10:04:53 AM: Evaluate: task edges-ner-ontonotes, batch 124 (157): mcc: 0.9223, acc: 0.8940, precision: 0.9400, recall: 0.9134, f1: 0.9265, edges-ner-ontonotes_loss: 0.0261
09/16 10:04:57 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:04:57 AM: Best result seen so far for macro.
09/16 10:04:57 AM: Updating LR scheduler:
09/16 10:04:57 AM: 	Best result seen so far for macro_avg: 0.932
09/16 10:04:57 AM: 	# validation passes without improvement: 0
09/16 10:04:57 AM: edges-ner-ontonotes_loss: training: 0.025954 validation: 0.023958
09/16 10:04:57 AM: macro_avg: validation: 0.932201
09/16 10:04:57 AM: micro_avg: validation: 0.000000
09/16 10:04:57 AM: edges-ner-ontonotes_mcc: training: 0.918178 validation: 0.928369
09/16 10:04:57 AM: edges-ner-ontonotes_acc: training: 0.880339 validation: 0.900819
09/16 10:04:57 AM: edges-ner-ontonotes_precision: training: 0.942935 validation: 0.945207
09/16 10:04:57 AM: edges-ner-ontonotes_recall: training: 0.902730 validation: 0.919548
09/16 10:04:57 AM: edges-ner-ontonotes_f1: training: 0.922395 validation: 0.932201
09/16 10:04:57 AM: Global learning rate: 0.0001
09/16 10:04:57 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:05:03 AM: Update 12075: task edges-ner-ontonotes, batch 75 (12075): mcc: 0.9306, acc: 0.8964, precision: 0.9528, recall: 0.9163, f1: 0.9342, edges-ner-ontonotes_loss: 0.0221
09/16 10:05:13 AM: Update 12175: task edges-ner-ontonotes, batch 175 (12175): mcc: 0.9216, acc: 0.8840, precision: 0.9459, recall: 0.9062, f1: 0.9257, edges-ner-ontonotes_loss: 0.0256
09/16 10:05:23 AM: Update 12300: task edges-ner-ontonotes, batch 300 (12300): mcc: 0.9073, acc: 0.8660, precision: 0.9356, recall: 0.8895, f1: 0.9120, edges-ner-ontonotes_loss: 0.0308
09/16 10:05:33 AM: Update 12431: task edges-ner-ontonotes, batch 431 (12431): mcc: 0.9024, acc: 0.8600, precision: 0.9320, recall: 0.8839, f1: 0.9073, edges-ner-ontonotes_loss: 0.0326
09/16 10:05:43 AM: Update 12552: task edges-ner-ontonotes, batch 552 (12552): mcc: 0.9007, acc: 0.8570, precision: 0.9311, recall: 0.8817, f1: 0.9057, edges-ner-ontonotes_loss: 0.0328
09/16 10:05:53 AM: Update 12710: task edges-ner-ontonotes, batch 710 (12710): mcc: 0.9010, acc: 0.8574, precision: 0.9317, recall: 0.8816, f1: 0.9060, edges-ner-ontonotes_loss: 0.0325
09/16 10:06:03 AM: Update 12816: task edges-ner-ontonotes, batch 816 (12816): mcc: 0.9010, acc: 0.8574, precision: 0.9312, recall: 0.8820, f1: 0.9060, edges-ner-ontonotes_loss: 0.0323
09/16 10:06:13 AM: Update 12943: task edges-ner-ontonotes, batch 943 (12943): mcc: 0.9029, acc: 0.8600, precision: 0.9324, recall: 0.8845, f1: 0.9078, edges-ner-ontonotes_loss: 0.0316
09/16 10:06:18 AM: ***** Step 13000 / Validation 13 *****
09/16 10:06:18 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:06:18 AM: Validating...
09/16 10:06:23 AM: Evaluate: task edges-ner-ontonotes, batch 56 (157): mcc: 0.9132, acc: 0.8825, precision: 0.9326, recall: 0.9034, f1: 0.9178, edges-ner-ontonotes_loss: 0.0292
09/16 10:06:34 AM: Evaluate: task edges-ner-ontonotes, batch 135 (157): mcc: 0.9282, acc: 0.8997, precision: 0.9469, recall: 0.9175, f1: 0.9320, edges-ner-ontonotes_loss: 0.0238
09/16 10:06:36 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:06:36 AM: Best result seen so far for macro.
09/16 10:06:36 AM: Updating LR scheduler:
09/16 10:06:36 AM: 	Best result seen so far for macro_avg: 0.933
09/16 10:06:36 AM: 	# validation passes without improvement: 0
09/16 10:06:36 AM: edges-ner-ontonotes_loss: training: 0.031318 validation: 0.023241
09/16 10:06:36 AM: macro_avg: validation: 0.932979
09/16 10:06:36 AM: micro_avg: validation: 0.000000
09/16 10:06:36 AM: edges-ner-ontonotes_mcc: training: 0.903666 validation: 0.929247
09/16 10:06:36 AM: edges-ner-ontonotes_acc: training: 0.861200 validation: 0.900212
09/16 10:06:36 AM: edges-ner-ontonotes_precision: training: 0.932733 validation: 0.948664
09/16 10:06:36 AM: edges-ner-ontonotes_recall: training: 0.885606 validation: 0.917804
09/16 10:06:36 AM: edges-ner-ontonotes_f1: training: 0.908559 validation: 0.932979
09/16 10:06:36 AM: Global learning rate: 0.0001
09/16 10:06:36 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:06:44 AM: Update 13080: task edges-ner-ontonotes, batch 80 (13080): mcc: 0.9125, acc: 0.8733, precision: 0.9377, recall: 0.8973, f1: 0.9171, edges-ner-ontonotes_loss: 0.0271
09/16 10:06:54 AM: Update 13205: task edges-ner-ontonotes, batch 205 (13205): mcc: 0.9229, acc: 0.8852, precision: 0.9446, recall: 0.9100, f1: 0.9270, edges-ner-ontonotes_loss: 0.0239
09/16 10:07:04 AM: Update 13332: task edges-ner-ontonotes, batch 332 (13332): mcc: 0.9261, acc: 0.8893, precision: 0.9472, recall: 0.9132, f1: 0.9299, edges-ner-ontonotes_loss: 0.0231
09/16 10:07:14 AM: Update 13438: task edges-ner-ontonotes, batch 438 (13438): mcc: 0.9263, acc: 0.8901, precision: 0.9471, recall: 0.9138, f1: 0.9302, edges-ner-ontonotes_loss: 0.0231
09/16 10:07:24 AM: Update 13568: task edges-ner-ontonotes, batch 568 (13568): mcc: 0.9265, acc: 0.8902, precision: 0.9473, recall: 0.9141, f1: 0.9304, edges-ner-ontonotes_loss: 0.0229
09/16 10:07:34 AM: Update 13696: task edges-ner-ontonotes, batch 696 (13696): mcc: 0.9271, acc: 0.8911, precision: 0.9475, recall: 0.9149, f1: 0.9309, edges-ner-ontonotes_loss: 0.0228
09/16 10:07:44 AM: Update 13813: task edges-ner-ontonotes, batch 813 (13813): mcc: 0.9217, acc: 0.8840, precision: 0.9439, recall: 0.9083, f1: 0.9258, edges-ner-ontonotes_loss: 0.0250
09/16 10:07:54 AM: Update 13940: task edges-ner-ontonotes, batch 940 (13940): mcc: 0.9174, acc: 0.8786, precision: 0.9409, recall: 0.9033, f1: 0.9217, edges-ner-ontonotes_loss: 0.0268
09/16 10:07:58 AM: ***** Step 14000 / Validation 14 *****
09/16 10:07:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:07:58 AM: Validating...
09/16 10:08:04 AM: Evaluate: task edges-ner-ontonotes, batch 56 (157): mcc: 0.9084, acc: 0.8743, precision: 0.9309, recall: 0.8962, f1: 0.9132, edges-ner-ontonotes_loss: 0.0307
09/16 10:08:14 AM: Evaluate: task edges-ner-ontonotes, batch 135 (157): mcc: 0.9254, acc: 0.8946, precision: 0.9465, recall: 0.9127, f1: 0.9293, edges-ner-ontonotes_loss: 0.0248
09/16 10:08:16 AM: Updating LR scheduler:
09/16 10:08:16 AM: 	Best result seen so far for macro_avg: 0.933
09/16 10:08:16 AM: 	# validation passes without improvement: 1
09/16 10:08:16 AM: edges-ner-ontonotes_loss: training: 0.027446 validation: 0.024184
09/16 10:08:16 AM: macro_avg: validation: 0.930279
09/16 10:08:16 AM: micro_avg: validation: 0.000000
09/16 10:08:16 AM: edges-ner-ontonotes_mcc: training: 0.916227 validation: 0.926443
09/16 10:08:16 AM: edges-ner-ontonotes_acc: training: 0.877010 validation: 0.895511
09/16 10:08:16 AM: edges-ner-ontonotes_precision: training: 0.940307 validation: 0.948114
09/16 10:08:16 AM: edges-ner-ontonotes_recall: training: 0.901645 validation: 0.913103
09/16 10:08:16 AM: edges-ner-ontonotes_f1: training: 0.920570 validation: 0.930279
09/16 10:08:16 AM: Global learning rate: 0.0001
09/16 10:08:16 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:08:24 AM: Update 14091: task edges-ner-ontonotes, batch 91 (14091): mcc: 0.8910, acc: 0.8431, precision: 0.9241, recall: 0.8703, f1: 0.8964, edges-ner-ontonotes_loss: 0.0329
09/16 10:08:34 AM: Update 14247: task edges-ner-ontonotes, batch 247 (14247): mcc: 0.9008, acc: 0.8558, precision: 0.9312, recall: 0.8817, f1: 0.9058, edges-ner-ontonotes_loss: 0.0311
09/16 10:08:44 AM: Update 14359: task edges-ner-ontonotes, batch 359 (14359): mcc: 0.8985, acc: 0.8538, precision: 0.9292, recall: 0.8794, f1: 0.9036, edges-ner-ontonotes_loss: 0.0316
09/16 10:08:54 AM: Update 14492: task edges-ner-ontonotes, batch 492 (14492): mcc: 0.9033, acc: 0.8600, precision: 0.9321, recall: 0.8854, f1: 0.9082, edges-ner-ontonotes_loss: 0.0304
09/16 10:09:04 AM: Update 14625: task edges-ner-ontonotes, batch 625 (14625): mcc: 0.9063, acc: 0.8644, precision: 0.9340, recall: 0.8893, f1: 0.9111, edges-ner-ontonotes_loss: 0.0295
09/16 10:09:14 AM: Update 14728: task edges-ner-ontonotes, batch 728 (14728): mcc: 0.9102, acc: 0.8695, precision: 0.9368, recall: 0.8939, f1: 0.9148, edges-ner-ontonotes_loss: 0.0284
09/16 10:09:24 AM: Update 14859: task edges-ner-ontonotes, batch 859 (14859): mcc: 0.9139, acc: 0.8741, precision: 0.9390, recall: 0.8985, f1: 0.9183, edges-ner-ontonotes_loss: 0.0273
09/16 10:09:34 AM: Update 14973: task edges-ner-ontonotes, batch 973 (14973): mcc: 0.9158, acc: 0.8766, precision: 0.9402, recall: 0.9009, f1: 0.9202, edges-ner-ontonotes_loss: 0.0267
09/16 10:09:37 AM: ***** Step 15000 / Validation 15 *****
09/16 10:09:37 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:09:37 AM: Validating...
09/16 10:09:44 AM: Evaluate: task edges-ner-ontonotes, batch 70 (157): mcc: 0.9108, acc: 0.8784, precision: 0.9349, recall: 0.8968, f1: 0.9154, edges-ner-ontonotes_loss: 0.0308
09/16 10:09:54 AM: Evaluate: task edges-ner-ontonotes, batch 153 (157): mcc: 0.9302, acc: 0.9025, precision: 0.9486, recall: 0.9197, f1: 0.9339, edges-ner-ontonotes_loss: 0.0237
09/16 10:09:55 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:09:55 AM: Best result seen so far for macro.
09/16 10:09:55 AM: Updating LR scheduler:
09/16 10:09:55 AM: 	Best result seen so far for macro_avg: 0.934
09/16 10:09:55 AM: 	# validation passes without improvement: 0
09/16 10:09:55 AM: edges-ner-ontonotes_loss: training: 0.026602 validation: 0.023523
09/16 10:09:55 AM: macro_avg: validation: 0.934252
09/16 10:09:55 AM: micro_avg: validation: 0.000000
09/16 10:09:55 AM: edges-ner-ontonotes_mcc: training: 0.916031 validation: 0.930569
09/16 10:09:55 AM: edges-ner-ontonotes_acc: training: 0.876729 validation: 0.902866
09/16 10:09:55 AM: edges-ner-ontonotes_precision: training: 0.940362 validation: 0.948788
09/16 10:09:55 AM: edges-ner-ontonotes_recall: training: 0.901227 validation: 0.920155
09/16 10:09:55 AM: edges-ner-ontonotes_f1: training: 0.920379 validation: 0.934252
09/16 10:09:55 AM: Global learning rate: 0.0001
09/16 10:09:55 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:10:05 AM: Update 15129: task edges-ner-ontonotes, batch 129 (15129): mcc: 0.9259, acc: 0.8879, precision: 0.9469, recall: 0.9134, f1: 0.9298, edges-ner-ontonotes_loss: 0.0227
09/16 10:10:16 AM: Update 15257: task edges-ner-ontonotes, batch 257 (15257): mcc: 0.9267, acc: 0.8904, precision: 0.9467, recall: 0.9149, f1: 0.9306, edges-ner-ontonotes_loss: 0.0223
09/16 10:10:26 AM: Update 15390: task edges-ner-ontonotes, batch 390 (15390): mcc: 0.9155, acc: 0.8761, precision: 0.9400, recall: 0.9005, f1: 0.9198, edges-ner-ontonotes_loss: 0.0275
09/16 10:10:36 AM: Update 15530: task edges-ner-ontonotes, batch 530 (15530): mcc: 0.9091, acc: 0.8681, precision: 0.9358, recall: 0.8928, f1: 0.9138, edges-ner-ontonotes_loss: 0.0304
09/16 10:10:46 AM: Update 15666: task edges-ner-ontonotes, batch 666 (15666): mcc: 0.9062, acc: 0.8641, precision: 0.9337, recall: 0.8893, f1: 0.9110, edges-ner-ontonotes_loss: 0.0310
09/16 10:10:56 AM: Update 15823: task edges-ner-ontonotes, batch 823 (15823): mcc: 0.9051, acc: 0.8625, precision: 0.9330, recall: 0.8880, f1: 0.9100, edges-ner-ontonotes_loss: 0.0311
09/16 10:11:06 AM: Update 15948: task edges-ner-ontonotes, batch 948 (15948): mcc: 0.9055, acc: 0.8629, precision: 0.9334, recall: 0.8882, f1: 0.9103, edges-ner-ontonotes_loss: 0.0308
09/16 10:11:10 AM: ***** Step 16000 / Validation 16 *****
09/16 10:11:10 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:11:10 AM: Validating...
09/16 10:11:16 AM: Evaluate: task edges-ner-ontonotes, batch 60 (157): mcc: 0.9166, acc: 0.8868, precision: 0.9359, recall: 0.9066, f1: 0.9210, edges-ner-ontonotes_loss: 0.0281
09/16 10:11:26 AM: Evaluate: task edges-ner-ontonotes, batch 138 (157): mcc: 0.9286, acc: 0.9001, precision: 0.9480, recall: 0.9173, f1: 0.9324, edges-ner-ontonotes_loss: 0.0237
09/16 10:11:28 AM: Updating LR scheduler:
09/16 10:11:28 AM: 	Best result seen so far for macro_avg: 0.934
09/16 10:11:28 AM: 	# validation passes without improvement: 1
09/16 10:11:28 AM: edges-ner-ontonotes_loss: training: 0.030478 validation: 0.023084
09/16 10:11:28 AM: macro_avg: validation: 0.933894
09/16 10:11:28 AM: micro_avg: validation: 0.000000
09/16 10:11:28 AM: edges-ner-ontonotes_mcc: training: 0.906307 validation: 0.930232
09/16 10:11:28 AM: edges-ner-ontonotes_acc: training: 0.864110 validation: 0.901577
09/16 10:11:28 AM: edges-ner-ontonotes_precision: training: 0.933867 validation: 0.950314
09/16 10:11:28 AM: edges-ner-ontonotes_recall: training: 0.889418 validation: 0.918032
09/16 10:11:28 AM: edges-ner-ontonotes_f1: training: 0.911101 validation: 0.933894
09/16 10:11:28 AM: Global learning rate: 0.0001
09/16 10:11:28 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:11:36 AM: Update 16101: task edges-ner-ontonotes, batch 101 (16101): mcc: 0.9094, acc: 0.8711, precision: 0.9347, recall: 0.8943, f1: 0.9141, edges-ner-ontonotes_loss: 0.0279
09/16 10:11:46 AM: Update 16215: task edges-ner-ontonotes, batch 215 (16215): mcc: 0.9129, acc: 0.8740, precision: 0.9363, recall: 0.8992, f1: 0.9174, edges-ner-ontonotes_loss: 0.0267
09/16 10:11:56 AM: Update 16356: task edges-ner-ontonotes, batch 356 (16356): mcc: 0.9197, acc: 0.8829, precision: 0.9419, recall: 0.9067, f1: 0.9239, edges-ner-ontonotes_loss: 0.0247
09/16 10:12:06 AM: Update 16482: task edges-ner-ontonotes, batch 482 (16482): mcc: 0.9237, acc: 0.8878, precision: 0.9450, recall: 0.9111, f1: 0.9277, edges-ner-ontonotes_loss: 0.0236
09/16 10:12:17 AM: Update 16586: task edges-ner-ontonotes, batch 586 (16586): mcc: 0.9250, acc: 0.8894, precision: 0.9458, recall: 0.9127, f1: 0.9290, edges-ner-ontonotes_loss: 0.0234
09/16 10:12:27 AM: Update 16717: task edges-ner-ontonotes, batch 717 (16717): mcc: 0.9257, acc: 0.8901, precision: 0.9460, recall: 0.9137, f1: 0.9296, edges-ner-ontonotes_loss: 0.0232
09/16 10:12:37 AM: Update 16830: task edges-ner-ontonotes, batch 830 (16830): mcc: 0.9251, acc: 0.8890, precision: 0.9459, recall: 0.9127, f1: 0.9290, edges-ner-ontonotes_loss: 0.0234
09/16 10:12:47 AM: Update 16963: task edges-ner-ontonotes, batch 963 (16963): mcc: 0.9205, acc: 0.8833, precision: 0.9429, recall: 0.9072, f1: 0.9247, edges-ner-ontonotes_loss: 0.0254
09/16 10:12:49 AM: ***** Step 17000 / Validation 17 *****
09/16 10:12:49 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:12:49 AM: Validating...
09/16 10:12:57 AM: Evaluate: task edges-ner-ontonotes, batch 71 (157): mcc: 0.9072, acc: 0.8695, precision: 0.9345, recall: 0.8906, f1: 0.9120, edges-ner-ontonotes_loss: 0.0306
09/16 10:13:07 AM: Evaluate: task edges-ner-ontonotes, batch 153 (157): mcc: 0.9252, acc: 0.8932, precision: 0.9485, recall: 0.9104, f1: 0.9290, edges-ner-ontonotes_loss: 0.0244
09/16 10:13:07 AM: Updating LR scheduler:
09/16 10:13:07 AM: 	Best result seen so far for macro_avg: 0.934
09/16 10:13:07 AM: 	# validation passes without improvement: 2
09/16 10:13:07 AM: edges-ner-ontonotes_loss: training: 0.025856 validation: 0.024306
09/16 10:13:07 AM: macro_avg: validation: 0.929398
09/16 10:13:07 AM: micro_avg: validation: 0.000000
09/16 10:13:07 AM: edges-ner-ontonotes_mcc: training: 0.919546 validation: 0.925549
09/16 10:13:07 AM: edges-ner-ontonotes_acc: training: 0.882326 validation: 0.893691
09/16 10:13:07 AM: edges-ner-ontonotes_precision: training: 0.942130 validation: 0.948740
09/16 10:13:07 AM: edges-ner-ontonotes_recall: training: 0.906060 validation: 0.910828
09/16 10:13:07 AM: edges-ner-ontonotes_f1: training: 0.923743 validation: 0.929398
09/16 10:13:07 AM: Global learning rate: 0.0001
09/16 10:13:07 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:13:17 AM: Update 17118: task edges-ner-ontonotes, batch 118 (17118): mcc: 0.8895, acc: 0.8434, precision: 0.9228, recall: 0.8689, f1: 0.8950, edges-ner-ontonotes_loss: 0.0369
09/16 10:13:27 AM: Update 17277: task edges-ner-ontonotes, batch 277 (17277): mcc: 0.8955, acc: 0.8509, precision: 0.9260, recall: 0.8769, f1: 0.9008, edges-ner-ontonotes_loss: 0.0340
09/16 10:13:38 AM: Update 17430: task edges-ner-ontonotes, batch 430 (17430): mcc: 0.8977, acc: 0.8534, precision: 0.9280, recall: 0.8791, f1: 0.9029, edges-ner-ontonotes_loss: 0.0330
09/16 10:13:48 AM: Update 17562: task edges-ner-ontonotes, batch 562 (17562): mcc: 0.9013, acc: 0.8579, precision: 0.9298, recall: 0.8840, f1: 0.9063, edges-ner-ontonotes_loss: 0.0318
09/16 10:13:58 AM: Update 17698: task edges-ner-ontonotes, batch 698 (17698): mcc: 0.9048, acc: 0.8623, precision: 0.9324, recall: 0.8880, f1: 0.9097, edges-ner-ontonotes_loss: 0.0305
09/16 10:14:08 AM: Update 17820: task edges-ner-ontonotes, batch 820 (17820): mcc: 0.9082, acc: 0.8670, precision: 0.9348, recall: 0.8921, f1: 0.9130, edges-ner-ontonotes_loss: 0.0294
09/16 10:14:18 AM: Update 17950: task edges-ner-ontonotes, batch 950 (17950): mcc: 0.9120, acc: 0.8717, precision: 0.9372, recall: 0.8968, f1: 0.9165, edges-ner-ontonotes_loss: 0.0283
09/16 10:14:22 AM: ***** Step 18000 / Validation 18 *****
09/16 10:14:22 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:14:22 AM: Validating...
09/16 10:14:28 AM: Evaluate: task edges-ner-ontonotes, batch 62 (157): mcc: 0.9125, acc: 0.8814, precision: 0.9339, recall: 0.9010, f1: 0.9172, edges-ner-ontonotes_loss: 0.0297
09/16 10:14:38 AM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.9304, acc: 0.9027, precision: 0.9498, recall: 0.9188, f1: 0.9340, edges-ner-ontonotes_loss: 0.0237
09/16 10:14:40 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:14:40 AM: Best result seen so far for macro.
09/16 10:14:40 AM: Updating LR scheduler:
09/16 10:14:40 AM: 	Best result seen so far for macro_avg: 0.935
09/16 10:14:40 AM: 	# validation passes without improvement: 0
09/16 10:14:40 AM: edges-ner-ontonotes_loss: training: 0.027845 validation: 0.023197
09/16 10:14:40 AM: macro_avg: validation: 0.934628
09/16 10:14:40 AM: micro_avg: validation: 0.000000
09/16 10:14:40 AM: edges-ner-ontonotes_mcc: training: 0.913381 validation: 0.930995
09/16 10:14:40 AM: edges-ner-ontonotes_acc: training: 0.873635 validation: 0.902715
09/16 10:14:40 AM: edges-ner-ontonotes_precision: training: 0.938058 validation: 0.950455
09/16 10:14:40 AM: edges-ner-ontonotes_recall: training: 0.898525 validation: 0.919321
09/16 10:14:40 AM: edges-ner-ontonotes_f1: training: 0.917866 validation: 0.934628
09/16 10:14:40 AM: Global learning rate: 0.0001
09/16 10:14:40 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:14:48 AM: Update 18081: task edges-ner-ontonotes, batch 81 (18081): mcc: 0.9270, acc: 0.8897, precision: 0.9450, recall: 0.9171, f1: 0.9309, edges-ner-ontonotes_loss: 0.0228
09/16 10:14:58 AM: Update 18213: task edges-ner-ontonotes, batch 213 (18213): mcc: 0.9284, acc: 0.8921, precision: 0.9481, recall: 0.9167, f1: 0.9321, edges-ner-ontonotes_loss: 0.0222
09/16 10:15:08 AM: Update 18343: task edges-ner-ontonotes, batch 343 (18343): mcc: 0.9290, acc: 0.8928, precision: 0.9485, recall: 0.9175, f1: 0.9327, edges-ner-ontonotes_loss: 0.0220
09/16 10:15:18 AM: Update 18453: task edges-ner-ontonotes, batch 453 (18453): mcc: 0.9203, acc: 0.8817, precision: 0.9426, recall: 0.9070, f1: 0.9244, edges-ner-ontonotes_loss: 0.0255
09/16 10:15:29 AM: Update 18592: task edges-ner-ontonotes, batch 592 (18592): mcc: 0.9143, acc: 0.8740, precision: 0.9389, recall: 0.8994, f1: 0.9187, edges-ner-ontonotes_loss: 0.0281
09/16 10:15:39 AM: Update 18710: task edges-ner-ontonotes, batch 710 (18710): mcc: 0.9111, acc: 0.8700, precision: 0.9368, recall: 0.8955, f1: 0.9157, edges-ner-ontonotes_loss: 0.0294
09/16 10:15:49 AM: Update 18873: task edges-ner-ontonotes, batch 873 (18873): mcc: 0.9100, acc: 0.8685, precision: 0.9361, recall: 0.8940, f1: 0.9146, edges-ner-ontonotes_loss: 0.0295
09/16 10:15:58 AM: ***** Step 19000 / Validation 19 *****
09/16 10:15:58 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:15:58 AM: Validating...
09/16 10:15:59 AM: Evaluate: task edges-ner-ontonotes, batch 2 (157): mcc: 0.7967, acc: 0.7154, precision: 0.8475, recall: 0.7692, f1: 0.8065, edges-ner-ontonotes_loss: 0.0525
09/16 10:16:09 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.9255, acc: 0.8956, precision: 0.9440, recall: 0.9153, f1: 0.9294, edges-ner-ontonotes_loss: 0.0255
09/16 10:16:16 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:16:16 AM: Best result seen so far for macro.
09/16 10:16:16 AM: Updating LR scheduler:
09/16 10:16:16 AM: 	Best result seen so far for macro_avg: 0.936
09/16 10:16:16 AM: 	# validation passes without improvement: 0
09/16 10:16:16 AM: edges-ner-ontonotes_loss: training: 0.029603 validation: 0.022621
09/16 10:16:16 AM: macro_avg: validation: 0.935936
09/16 10:16:16 AM: micro_avg: validation: 0.000000
09/16 10:16:16 AM: edges-ner-ontonotes_mcc: training: 0.909178 validation: 0.932319
09/16 10:16:16 AM: edges-ner-ontonotes_acc: training: 0.867338 validation: 0.903852
09/16 10:16:16 AM: edges-ner-ontonotes_precision: training: 0.935833 validation: 0.948882
09/16 10:16:16 AM: edges-ner-ontonotes_recall: training: 0.892856 validation: 0.923339
09/16 10:16:16 AM: edges-ner-ontonotes_f1: training: 0.913840 validation: 0.935936
09/16 10:16:16 AM: Global learning rate: 0.0001
09/16 10:16:16 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:16:19 AM: Update 19035: task edges-ner-ontonotes, batch 35 (19035): mcc: 0.9165, acc: 0.8813, precision: 0.9398, recall: 0.9028, f1: 0.9209, edges-ner-ontonotes_loss: 0.0257
09/16 10:16:29 AM: Update 19166: task edges-ner-ontonotes, batch 166 (19166): mcc: 0.9124, acc: 0.8721, precision: 0.9377, recall: 0.8970, f1: 0.9169, edges-ner-ontonotes_loss: 0.0272
09/16 10:16:40 AM: Update 19299: task edges-ner-ontonotes, batch 299 (19299): mcc: 0.9150, acc: 0.8753, precision: 0.9390, recall: 0.9006, f1: 0.9194, edges-ner-ontonotes_loss: 0.0265
09/16 10:16:50 AM: Update 19426: task edges-ner-ontonotes, batch 426 (19426): mcc: 0.9211, acc: 0.8837, precision: 0.9430, recall: 0.9082, f1: 0.9253, edges-ner-ontonotes_loss: 0.0249
09/16 10:17:00 AM: Update 19554: task edges-ner-ontonotes, batch 554 (19554): mcc: 0.9231, acc: 0.8864, precision: 0.9443, recall: 0.9105, f1: 0.9271, edges-ner-ontonotes_loss: 0.0239
09/16 10:17:10 AM: Update 19672: task edges-ner-ontonotes, batch 672 (19672): mcc: 0.9246, acc: 0.8880, precision: 0.9455, recall: 0.9122, f1: 0.9286, edges-ner-ontonotes_loss: 0.0235
09/16 10:17:20 AM: Update 19805: task edges-ner-ontonotes, batch 805 (19805): mcc: 0.9253, acc: 0.8891, precision: 0.9458, recall: 0.9132, f1: 0.9292, edges-ner-ontonotes_loss: 0.0233
09/16 10:17:31 AM: Update 19925: task edges-ner-ontonotes, batch 925 (19925): mcc: 0.9260, acc: 0.8899, precision: 0.9463, recall: 0.9141, f1: 0.9299, edges-ner-ontonotes_loss: 0.0231
09/16 10:17:37 AM: ***** Step 20000 / Validation 20 *****
09/16 10:17:37 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:17:37 AM: Validating...
09/16 10:17:42 AM: Evaluate: task edges-ner-ontonotes, batch 48 (157): mcc: 0.9056, acc: 0.8749, precision: 0.9236, recall: 0.8981, f1: 0.9107, edges-ner-ontonotes_loss: 0.0318
09/16 10:17:52 AM: Evaluate: task edges-ner-ontonotes, batch 127 (157): mcc: 0.9254, acc: 0.8966, precision: 0.9434, recall: 0.9158, f1: 0.9294, edges-ner-ontonotes_loss: 0.0246
09/16 10:17:55 AM: Updating LR scheduler:
09/16 10:17:55 AM: 	Best result seen so far for macro_avg: 0.936
09/16 10:17:55 AM: 	# validation passes without improvement: 1
09/16 10:17:55 AM: edges-ner-ontonotes_loss: training: 0.024332 validation: 0.023078
09/16 10:17:55 AM: macro_avg: validation: 0.933805
09/16 10:17:55 AM: micro_avg: validation: 0.000000
09/16 10:17:55 AM: edges-ner-ontonotes_mcc: training: 0.923222 validation: 0.930092
09/16 10:17:55 AM: edges-ner-ontonotes_acc: training: 0.886411 validation: 0.902411
09/16 10:17:55 AM: edges-ner-ontonotes_precision: training: 0.944359 validation: 0.948109
09/16 10:17:55 AM: edges-ner-ontonotes_recall: training: 0.910750 validation: 0.919927
09/16 10:17:55 AM: edges-ner-ontonotes_f1: training: 0.927250 validation: 0.933805
09/16 10:17:55 AM: Global learning rate: 0.0001
09/16 10:17:55 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:18:02 AM: Update 20095: task edges-ner-ontonotes, batch 95 (20095): mcc: 0.8979, acc: 0.8527, precision: 0.9293, recall: 0.8781, f1: 0.9030, edges-ner-ontonotes_loss: 0.0342
09/16 10:18:13 AM: Update 20229: task edges-ner-ontonotes, batch 229 (20229): mcc: 0.8936, acc: 0.8492, precision: 0.9258, recall: 0.8736, f1: 0.8989, edges-ner-ontonotes_loss: 0.0359
09/16 10:18:23 AM: Update 20388: task edges-ner-ontonotes, batch 388 (20388): mcc: 0.8961, acc: 0.8517, precision: 0.9269, recall: 0.8772, f1: 0.9014, edges-ner-ontonotes_loss: 0.0338
09/16 10:18:35 AM: Update 20542: task edges-ner-ontonotes, batch 542 (20542): mcc: 0.8976, acc: 0.8536, precision: 0.9279, recall: 0.8790, f1: 0.9028, edges-ner-ontonotes_loss: 0.0329
09/16 10:18:45 AM: Update 20673: task edges-ner-ontonotes, batch 673 (20673): mcc: 0.9015, acc: 0.8583, precision: 0.9303, recall: 0.8840, f1: 0.9066, edges-ner-ontonotes_loss: 0.0317
09/16 10:18:55 AM: Update 20807: task edges-ner-ontonotes, batch 807 (20807): mcc: 0.9038, acc: 0.8615, precision: 0.9315, recall: 0.8871, f1: 0.9088, edges-ner-ontonotes_loss: 0.0309
09/16 10:19:05 AM: Update 20921: task edges-ner-ontonotes, batch 921 (20921): mcc: 0.9073, acc: 0.8659, precision: 0.9338, recall: 0.8912, f1: 0.9120, edges-ner-ontonotes_loss: 0.0298
09/16 10:19:11 AM: ***** Step 21000 / Validation 21 *****
09/16 10:19:11 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:19:11 AM: Validating...
09/16 10:19:15 AM: Evaluate: task edges-ner-ontonotes, batch 43 (157): mcc: 0.9016, acc: 0.8708, precision: 0.9223, recall: 0.8918, f1: 0.9068, edges-ner-ontonotes_loss: 0.0326
09/16 10:19:25 AM: Evaluate: task edges-ner-ontonotes, batch 122 (157): mcc: 0.9252, acc: 0.8967, precision: 0.9427, recall: 0.9160, f1: 0.9292, edges-ner-ontonotes_loss: 0.0250
09/16 10:19:29 AM: Updating LR scheduler:
09/16 10:19:29 AM: 	Best result seen so far for macro_avg: 0.936
09/16 10:19:29 AM: 	# validation passes without improvement: 2
09/16 10:19:29 AM: edges-ner-ontonotes_loss: training: 0.029041 validation: 0.022874
09/16 10:19:29 AM: macro_avg: validation: 0.935352
09/16 10:19:29 AM: micro_avg: validation: 0.000000
09/16 10:19:29 AM: edges-ner-ontonotes_mcc: training: 0.910102 validation: 0.931704
09/16 10:19:29 AM: edges-ner-ontonotes_acc: training: 0.869445 validation: 0.904686
09/16 10:19:29 AM: edges-ner-ontonotes_precision: training: 0.935822 validation: 0.948402
09/16 10:19:29 AM: edges-ner-ontonotes_recall: training: 0.894586 validation: 0.922657
09/16 10:19:29 AM: edges-ner-ontonotes_f1: training: 0.914740 validation: 0.935352
09/16 10:19:29 AM: Global learning rate: 0.0001
09/16 10:19:29 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:19:35 AM: Update 21081: task edges-ner-ontonotes, batch 81 (21081): mcc: 0.9307, acc: 0.8969, precision: 0.9478, recall: 0.9213, f1: 0.9344, edges-ner-ontonotes_loss: 0.0211
09/16 10:19:45 AM: Update 21190: task edges-ner-ontonotes, batch 190 (21190): mcc: 0.9283, acc: 0.8936, precision: 0.9467, recall: 0.9180, f1: 0.9321, edges-ner-ontonotes_loss: 0.0214
09/16 10:19:55 AM: Update 21322: task edges-ner-ontonotes, batch 322 (21322): mcc: 0.9289, acc: 0.8932, precision: 0.9471, recall: 0.9187, f1: 0.9327, edges-ner-ontonotes_loss: 0.0214
09/16 10:20:05 AM: Update 21453: task edges-ner-ontonotes, batch 453 (21453): mcc: 0.9296, acc: 0.8941, precision: 0.9477, recall: 0.9194, f1: 0.9333, edges-ner-ontonotes_loss: 0.0215
09/16 10:20:15 AM: Update 21579: task edges-ner-ontonotes, batch 579 (21579): mcc: 0.9234, acc: 0.8863, precision: 0.9439, recall: 0.9115, f1: 0.9274, edges-ner-ontonotes_loss: 0.0244
09/16 10:20:25 AM: Update 21714: task edges-ner-ontonotes, batch 714 (21714): mcc: 0.9175, acc: 0.8788, precision: 0.9400, recall: 0.9042, f1: 0.9218, edges-ner-ontonotes_loss: 0.0268
09/16 10:20:35 AM: Update 21838: task edges-ner-ontonotes, batch 838 (21838): mcc: 0.9148, acc: 0.8752, precision: 0.9384, recall: 0.9008, f1: 0.9192, edges-ner-ontonotes_loss: 0.0277
09/16 10:20:45 AM: Update 21998: task edges-ner-ontonotes, batch 998 (21998): mcc: 0.9130, acc: 0.8728, precision: 0.9371, recall: 0.8987, f1: 0.9175, edges-ner-ontonotes_loss: 0.0283
09/16 10:20:46 AM: ***** Step 22000 / Validation 22 *****
09/16 10:20:46 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:20:46 AM: Validating...
09/16 10:20:55 AM: Evaluate: task edges-ner-ontonotes, batch 94 (157): mcc: 0.9220, acc: 0.8900, precision: 0.9451, recall: 0.9077, f1: 0.9260, edges-ner-ontonotes_loss: 0.0255
09/16 10:21:03 AM: Updating LR scheduler:
09/16 10:21:03 AM: 	Best result seen so far for macro_avg: 0.936
09/16 10:21:03 AM: 	# validation passes without improvement: 3
09/16 10:21:03 AM: edges-ner-ontonotes_loss: training: 0.028294 validation: 0.022630
09/16 10:21:03 AM: macro_avg: validation: 0.934132
09/16 10:21:03 AM: micro_avg: validation: 0.000000
09/16 10:21:03 AM: edges-ner-ontonotes_mcc: training: 0.912941 validation: 0.930522
09/16 10:21:03 AM: edges-ner-ontonotes_acc: training: 0.872779 validation: 0.901350
09/16 10:21:03 AM: edges-ner-ontonotes_precision: training: 0.937149 validation: 0.952193
09/16 10:21:03 AM: edges-ner-ontonotes_recall: training: 0.898588 validation: 0.916743
09/16 10:21:03 AM: edges-ner-ontonotes_f1: training: 0.917463 validation: 0.934132
09/16 10:21:03 AM: Global learning rate: 0.0001
09/16 10:21:03 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:21:05 AM: Update 22033: task edges-ner-ontonotes, batch 33 (22033): mcc: 0.9066, acc: 0.8617, precision: 0.9342, recall: 0.8896, f1: 0.9114, edges-ner-ontonotes_loss: 0.0277
09/16 10:21:16 AM: Update 22145: task edges-ner-ontonotes, batch 145 (22145): mcc: 0.9073, acc: 0.8644, precision: 0.9349, recall: 0.8903, f1: 0.9121, edges-ner-ontonotes_loss: 0.0284
09/16 10:21:26 AM: Update 22279: task edges-ner-ontonotes, batch 279 (22279): mcc: 0.9107, acc: 0.8702, precision: 0.9352, recall: 0.8963, f1: 0.9153, edges-ner-ontonotes_loss: 0.0274
09/16 10:21:37 AM: Update 22411: task edges-ner-ontonotes, batch 411 (22411): mcc: 0.9113, acc: 0.8715, precision: 0.9360, recall: 0.8967, f1: 0.9159, edges-ner-ontonotes_loss: 0.0272
09/16 10:21:47 AM: Update 22535: task edges-ner-ontonotes, batch 535 (22535): mcc: 0.9166, acc: 0.8780, precision: 0.9399, recall: 0.9028, f1: 0.9210, edges-ner-ontonotes_loss: 0.0259
09/16 10:21:57 AM: Update 22663: task edges-ner-ontonotes, batch 663 (22663): mcc: 0.9206, acc: 0.8833, precision: 0.9425, recall: 0.9077, f1: 0.9248, edges-ner-ontonotes_loss: 0.0247
09/16 10:22:07 AM: Update 22773: task edges-ner-ontonotes, batch 773 (22773): mcc: 0.9229, acc: 0.8864, precision: 0.9441, recall: 0.9104, f1: 0.9269, edges-ner-ontonotes_loss: 0.0241
09/16 10:22:17 AM: Update 22902: task edges-ner-ontonotes, batch 902 (22902): mcc: 0.9238, acc: 0.8875, precision: 0.9445, recall: 0.9118, f1: 0.9278, edges-ner-ontonotes_loss: 0.0237
09/16 10:22:24 AM: ***** Step 23000 / Validation 23 *****
09/16 10:22:24 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:22:24 AM: Validating...
09/16 10:22:27 AM: Evaluate: task edges-ner-ontonotes, batch 26 (157): mcc: 0.8718, acc: 0.8364, precision: 0.8957, recall: 0.8623, f1: 0.8787, edges-ner-ontonotes_loss: 0.0418
09/16 10:22:37 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9235, acc: 0.8963, precision: 0.9392, recall: 0.9163, f1: 0.9276, edges-ner-ontonotes_loss: 0.0261
09/16 10:22:42 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:22:42 AM: Best result seen so far for macro.
09/16 10:22:42 AM: Updating LR scheduler:
09/16 10:22:42 AM: 	Best result seen so far for macro_avg: 0.936
09/16 10:22:42 AM: 	# validation passes without improvement: 0
09/16 10:22:42 AM: edges-ner-ontonotes_loss: training: 0.023554 validation: 0.022897
09/16 10:22:42 AM: macro_avg: validation: 0.936496
09/16 10:22:42 AM: micro_avg: validation: 0.000000
09/16 10:22:42 AM: edges-ner-ontonotes_mcc: training: 0.924430 validation: 0.932865
09/16 10:22:42 AM: edges-ner-ontonotes_acc: training: 0.888347 validation: 0.907264
09/16 10:22:42 AM: edges-ner-ontonotes_precision: training: 0.945054 validation: 0.946765
09/16 10:22:42 AM: edges-ner-ontonotes_recall: training: 0.912328 validation: 0.926448
09/16 10:22:42 AM: edges-ner-ontonotes_f1: training: 0.928403 validation: 0.936496
09/16 10:22:42 AM: Global learning rate: 0.0001
09/16 10:22:42 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:22:48 AM: Update 23039: task edges-ner-ontonotes, batch 39 (23039): mcc: 0.9227, acc: 0.8839, precision: 0.9432, recall: 0.9108, f1: 0.9267, edges-ner-ontonotes_loss: 0.0222
09/16 10:22:58 AM: Update 23175: task edges-ner-ontonotes, batch 175 (23175): mcc: 0.8990, acc: 0.8559, precision: 0.9274, recall: 0.8820, f1: 0.9041, edges-ner-ontonotes_loss: 0.0336
09/16 10:23:08 AM: Update 23306: task edges-ner-ontonotes, batch 306 (23306): mcc: 0.8975, acc: 0.8541, precision: 0.9274, recall: 0.8792, f1: 0.9027, edges-ner-ontonotes_loss: 0.0345
09/16 10:23:18 AM: Update 23418: task edges-ner-ontonotes, batch 418 (23418): mcc: 0.8968, acc: 0.8533, precision: 0.9269, recall: 0.8784, f1: 0.9020, edges-ner-ontonotes_loss: 0.0340
09/16 10:23:28 AM: Update 23580: task edges-ner-ontonotes, batch 580 (23580): mcc: 0.8981, acc: 0.8549, precision: 0.9279, recall: 0.8799, f1: 0.9033, edges-ner-ontonotes_loss: 0.0329
09/16 10:23:38 AM: Update 23704: task edges-ner-ontonotes, batch 704 (23704): mcc: 0.9003, acc: 0.8575, precision: 0.9293, recall: 0.8826, f1: 0.9054, edges-ner-ontonotes_loss: 0.0321
09/16 10:23:48 AM: Update 23843: task edges-ner-ontonotes, batch 843 (23843): mcc: 0.9030, acc: 0.8608, precision: 0.9307, recall: 0.8863, f1: 0.9079, edges-ner-ontonotes_loss: 0.0312
09/16 10:23:59 AM: Update 23967: task edges-ner-ontonotes, batch 967 (23967): mcc: 0.9047, acc: 0.8629, precision: 0.9317, recall: 0.8885, f1: 0.9096, edges-ner-ontonotes_loss: 0.0306
09/16 10:24:02 AM: ***** Step 24000 / Validation 24 *****
09/16 10:24:02 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:24:02 AM: Validating...
09/16 10:24:09 AM: Evaluate: task edges-ner-ontonotes, batch 65 (157): mcc: 0.9154, acc: 0.8844, precision: 0.9331, recall: 0.9072, f1: 0.9200, edges-ner-ontonotes_loss: 0.0288
09/16 10:24:19 AM: Evaluate: task edges-ner-ontonotes, batch 147 (157): mcc: 0.9312, acc: 0.9038, precision: 0.9452, recall: 0.9249, f1: 0.9350, edges-ner-ontonotes_loss: 0.0232
09/16 10:24:20 AM: Updating LR scheduler:
09/16 10:24:20 AM: 	Best result seen so far for macro_avg: 0.936
09/16 10:24:20 AM: 	# validation passes without improvement: 1
09/16 10:24:20 AM: edges-ner-ontonotes_loss: training: 0.030263 validation: 0.022808
09/16 10:24:20 AM: macro_avg: validation: 0.936087
09/16 10:24:20 AM: micro_avg: validation: 0.000000
09/16 10:24:20 AM: edges-ner-ontonotes_mcc: training: 0.905894 validation: 0.932429
09/16 10:24:20 AM: edges-ner-ontonotes_acc: training: 0.864399 validation: 0.905520
09/16 10:24:20 AM: edges-ner-ontonotes_precision: training: 0.932393 validation: 0.946166
09/16 10:24:20 AM: edges-ner-ontonotes_recall: training: 0.890074 validation: 0.926221
09/16 10:24:20 AM: edges-ner-ontonotes_f1: training: 0.910742 validation: 0.936087
09/16 10:24:20 AM: Global learning rate: 0.0001
09/16 10:24:20 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:24:29 AM: Update 24114: task edges-ner-ontonotes, batch 114 (24114): mcc: 0.9348, acc: 0.9007, precision: 0.9523, recall: 0.9246, f1: 0.9382, edges-ner-ontonotes_loss: 0.0199
09/16 10:24:39 AM: Update 24252: task edges-ner-ontonotes, batch 252 (24252): mcc: 0.9340, acc: 0.9002, precision: 0.9513, recall: 0.9240, f1: 0.9375, edges-ner-ontonotes_loss: 0.0203
09/16 10:24:49 AM: Update 24353: task edges-ner-ontonotes, batch 353 (24353): mcc: 0.9314, acc: 0.8964, precision: 0.9493, recall: 0.9212, f1: 0.9350, edges-ner-ontonotes_loss: 0.0210
09/16 10:24:59 AM: Update 24487: task edges-ner-ontonotes, batch 487 (24487): mcc: 0.9319, acc: 0.8970, precision: 0.9498, recall: 0.9215, f1: 0.9355, edges-ner-ontonotes_loss: 0.0212
09/16 10:25:09 AM: Update 24608: task edges-ner-ontonotes, batch 608 (24608): mcc: 0.9302, acc: 0.8948, precision: 0.9485, recall: 0.9196, f1: 0.9339, edges-ner-ontonotes_loss: 0.0217
09/16 10:25:19 AM: Update 24747: task edges-ner-ontonotes, batch 747 (24747): mcc: 0.9231, acc: 0.8857, precision: 0.9439, recall: 0.9109, f1: 0.9271, edges-ner-ontonotes_loss: 0.0249
09/16 10:25:29 AM: Update 24883: task edges-ner-ontonotes, batch 883 (24883): mcc: 0.9192, acc: 0.8810, precision: 0.9415, recall: 0.9059, f1: 0.9234, edges-ner-ontonotes_loss: 0.0266
09/16 10:25:38 AM: ***** Step 25000 / Validation 25 *****
09/16 10:25:38 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:25:38 AM: Validating...
09/16 10:25:39 AM: Evaluate: task edges-ner-ontonotes, batch 9 (157): mcc: 0.8589, acc: 0.8044, precision: 0.8919, recall: 0.8418, f1: 0.8661, edges-ner-ontonotes_loss: 0.0414
09/16 10:25:49 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.9239, acc: 0.8936, precision: 0.9471, recall: 0.9094, f1: 0.9279, edges-ner-ontonotes_loss: 0.0254
09/16 10:25:57 AM: Updating LR scheduler:
09/16 10:25:57 AM: 	Best result seen so far for macro_avg: 0.936
09/16 10:25:57 AM: 	# validation passes without improvement: 2
09/16 10:25:57 AM: edges-ner-ontonotes_loss: training: 0.027145 validation: 0.022406
09/16 10:25:57 AM: macro_avg: validation: 0.935763
09/16 10:25:57 AM: micro_avg: validation: 0.000000
09/16 10:25:57 AM: edges-ner-ontonotes_mcc: training: 0.917513 validation: 0.932230
09/16 10:25:57 AM: edges-ner-ontonotes_acc: training: 0.878798 validation: 0.904079
09/16 10:25:57 AM: edges-ner-ontonotes_precision: training: 0.940439 validation: 0.953130
09/16 10:25:57 AM: edges-ner-ontonotes_recall: training: 0.903913 validation: 0.919017
09/16 10:25:57 AM: edges-ner-ontonotes_f1: training: 0.921815 validation: 0.935763
09/16 10:25:57 AM: Global learning rate: 0.0001
09/16 10:25:57 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:25:59 AM: Update 25037: task edges-ner-ontonotes, batch 37 (25037): mcc: 0.8968, acc: 0.8490, precision: 0.9267, recall: 0.8787, f1: 0.9021, edges-ner-ontonotes_loss: 0.0313
09/16 10:26:09 AM: Update 25202: task edges-ner-ontonotes, batch 202 (25202): mcc: 0.9032, acc: 0.8602, precision: 0.9304, recall: 0.8870, f1: 0.9082, edges-ner-ontonotes_loss: 0.0303
09/16 10:26:19 AM: Update 25310: task edges-ner-ontonotes, batch 310 (25310): mcc: 0.9064, acc: 0.8643, precision: 0.9322, recall: 0.8912, f1: 0.9113, edges-ner-ontonotes_loss: 0.0294
09/16 10:26:29 AM: Update 25454: task edges-ner-ontonotes, batch 454 (25454): mcc: 0.9113, acc: 0.8710, precision: 0.9353, recall: 0.8972, f1: 0.9159, edges-ner-ontonotes_loss: 0.0280
09/16 10:26:40 AM: Update 25558: task edges-ner-ontonotes, batch 558 (25558): mcc: 0.9130, acc: 0.8736, precision: 0.9366, recall: 0.8993, f1: 0.9176, edges-ner-ontonotes_loss: 0.0275
09/16 10:26:50 AM: Update 25693: task edges-ner-ontonotes, batch 693 (25693): mcc: 0.9173, acc: 0.8789, precision: 0.9399, recall: 0.9041, f1: 0.9216, edges-ner-ontonotes_loss: 0.0261
09/16 10:27:00 AM: Update 25826: task edges-ner-ontonotes, batch 826 (25826): mcc: 0.9207, acc: 0.8834, precision: 0.9421, recall: 0.9082, f1: 0.9249, edges-ner-ontonotes_loss: 0.0251
09/16 10:27:10 AM: Update 25939: task edges-ner-ontonotes, batch 939 (25939): mcc: 0.9219, acc: 0.8846, precision: 0.9432, recall: 0.9095, f1: 0.9260, edges-ner-ontonotes_loss: 0.0247
09/16 10:27:14 AM: ***** Step 26000 / Validation 26 *****
09/16 10:27:14 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:27:14 AM: Validating...
09/16 10:27:20 AM: Evaluate: task edges-ner-ontonotes, batch 55 (157): mcc: 0.9123, acc: 0.8826, precision: 0.9311, recall: 0.9034, f1: 0.9170, edges-ner-ontonotes_loss: 0.0308
09/16 10:27:30 AM: Evaluate: task edges-ner-ontonotes, batch 136 (157): mcc: 0.9330, acc: 0.9075, precision: 0.9478, recall: 0.9256, f1: 0.9366, edges-ner-ontonotes_loss: 0.0235
09/16 10:27:32 AM: Best result seen so far for edges-ner-ontonotes.
09/16 10:27:32 AM: Best result seen so far for macro.
09/16 10:27:32 AM: Updating LR scheduler:
09/16 10:27:32 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:27:32 AM: 	# validation passes without improvement: 0
09/16 10:27:32 AM: edges-ner-ontonotes_loss: training: 0.024603 validation: 0.022650
09/16 10:27:32 AM: macro_avg: validation: 0.938035
09/16 10:27:32 AM: micro_avg: validation: 0.000000
09/16 10:27:32 AM: edges-ner-ontonotes_mcc: training: 0.922391 validation: 0.934512
09/16 10:27:32 AM: edges-ner-ontonotes_acc: training: 0.885187 validation: 0.909008
09/16 10:27:32 AM: edges-ner-ontonotes_precision: training: 0.943353 validation: 0.949437
09/16 10:27:32 AM: edges-ner-ontonotes_recall: training: 0.910179 validation: 0.926903
09/16 10:27:32 AM: edges-ner-ontonotes_f1: training: 0.926469 validation: 0.938035
09/16 10:27:32 AM: Global learning rate: 0.0001
09/16 10:27:32 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:27:40 AM: Update 26104: task edges-ner-ontonotes, batch 104 (26104): mcc: 0.9298, acc: 0.8955, precision: 0.9468, recall: 0.9207, f1: 0.9335, edges-ner-ontonotes_loss: 0.0218
09/16 10:27:50 AM: Update 26211: task edges-ner-ontonotes, batch 211 (26211): mcc: 0.9175, acc: 0.8786, precision: 0.9395, recall: 0.9048, f1: 0.9218, edges-ner-ontonotes_loss: 0.0271
09/16 10:28:00 AM: Update 26340: task edges-ner-ontonotes, batch 340 (26340): mcc: 0.9079, acc: 0.8663, precision: 0.9329, recall: 0.8934, f1: 0.9127, edges-ner-ontonotes_loss: 0.0302
09/16 10:28:10 AM: Update 26454: task edges-ner-ontonotes, batch 454 (26454): mcc: 0.9055, acc: 0.8632, precision: 0.9321, recall: 0.8896, f1: 0.9103, edges-ner-ontonotes_loss: 0.0316
09/16 10:28:20 AM: Update 26615: task edges-ner-ontonotes, batch 615 (26615): mcc: 0.9037, acc: 0.8609, precision: 0.9306, recall: 0.8878, f1: 0.9087, edges-ner-ontonotes_loss: 0.0316
09/16 10:28:31 AM: Update 26766: task edges-ner-ontonotes, batch 766 (26766): mcc: 0.9046, acc: 0.8621, precision: 0.9314, recall: 0.8886, f1: 0.9095, edges-ner-ontonotes_loss: 0.0311
09/16 10:28:41 AM: Update 26898: task edges-ner-ontonotes, batch 898 (26898): mcc: 0.9061, acc: 0.8640, precision: 0.9319, recall: 0.8909, f1: 0.9110, edges-ner-ontonotes_loss: 0.0304
09/16 10:28:49 AM: ***** Step 27000 / Validation 27 *****
09/16 10:28:49 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:28:49 AM: Validating...
09/16 10:28:51 AM: Evaluate: task edges-ner-ontonotes, batch 27 (157): mcc: 0.8845, acc: 0.8474, precision: 0.9124, recall: 0.8697, f1: 0.8905, edges-ner-ontonotes_loss: 0.0364
09/16 10:29:02 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9250, acc: 0.8960, precision: 0.9445, recall: 0.9139, f1: 0.9290, edges-ner-ontonotes_loss: 0.0247
09/16 10:29:07 AM: Updating LR scheduler:
09/16 10:29:07 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:29:07 AM: 	# validation passes without improvement: 1
09/16 10:29:07 AM: edges-ner-ontonotes_loss: training: 0.029872 validation: 0.022207
09/16 10:29:07 AM: macro_avg: validation: 0.936598
09/16 10:29:07 AM: micro_avg: validation: 0.000000
09/16 10:29:07 AM: edges-ner-ontonotes_mcc: training: 0.907270 validation: 0.933052
09/16 10:29:07 AM: edges-ner-ontonotes_acc: training: 0.865490 validation: 0.905823
09/16 10:29:07 AM: edges-ner-ontonotes_precision: training: 0.932559 validation: 0.951208
09/16 10:29:07 AM: edges-ner-ontonotes_recall: training: 0.892476 validation: 0.922430
09/16 10:29:07 AM: edges-ner-ontonotes_f1: training: 0.912077 validation: 0.936598
09/16 10:29:07 AM: Global learning rate: 0.0001
09/16 10:29:07 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:29:12 AM: Update 27068: task edges-ner-ontonotes, batch 68 (27068): mcc: 0.9146, acc: 0.8774, precision: 0.9378, recall: 0.9011, f1: 0.9191, edges-ner-ontonotes_loss: 0.0270
09/16 10:29:22 AM: Update 27171: task edges-ner-ontonotes, batch 171 (27171): mcc: 0.9226, acc: 0.8870, precision: 0.9425, recall: 0.9114, f1: 0.9267, edges-ner-ontonotes_loss: 0.0239
09/16 10:29:32 AM: Update 27299: task edges-ner-ontonotes, batch 299 (27299): mcc: 0.9279, acc: 0.8932, precision: 0.9470, recall: 0.9169, f1: 0.9317, edges-ner-ontonotes_loss: 0.0220
09/16 10:29:42 AM: Update 27419: task edges-ner-ontonotes, batch 419 (27419): mcc: 0.9307, acc: 0.8969, precision: 0.9487, recall: 0.9205, f1: 0.9344, edges-ner-ontonotes_loss: 0.0216
09/16 10:29:52 AM: Update 27550: task edges-ner-ontonotes, batch 550 (27550): mcc: 0.9317, acc: 0.8980, precision: 0.9497, recall: 0.9214, f1: 0.9353, edges-ner-ontonotes_loss: 0.0212
09/16 10:30:02 AM: Update 27682: task edges-ner-ontonotes, batch 682 (27682): mcc: 0.9310, acc: 0.8971, precision: 0.9490, recall: 0.9208, f1: 0.9347, edges-ner-ontonotes_loss: 0.0215
09/16 10:30:12 AM: Update 27784: task edges-ner-ontonotes, batch 784 (27784): mcc: 0.9272, acc: 0.8920, precision: 0.9466, recall: 0.9159, f1: 0.9310, edges-ner-ontonotes_loss: 0.0229
09/16 10:30:22 AM: Update 27930: task edges-ner-ontonotes, batch 930 (27930): mcc: 0.9221, acc: 0.8858, precision: 0.9431, recall: 0.9099, f1: 0.9262, edges-ner-ontonotes_loss: 0.0252
09/16 10:30:27 AM: ***** Step 28000 / Validation 28 *****
09/16 10:30:27 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:30:27 AM: Validating...
09/16 10:30:32 AM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.9138, acc: 0.8837, precision: 0.9307, recall: 0.9065, f1: 0.9184, edges-ner-ontonotes_loss: 0.0300
09/16 10:30:42 AM: Evaluate: task edges-ner-ontonotes, batch 133 (157): mcc: 0.9308, acc: 0.9032, precision: 0.9463, recall: 0.9230, f1: 0.9345, edges-ner-ontonotes_loss: 0.0235
09/16 10:30:45 AM: Updating LR scheduler:
09/16 10:30:45 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:30:45 AM: 	# validation passes without improvement: 2
09/16 10:30:45 AM: edges-ner-ontonotes_loss: training: 0.026041 validation: 0.022592
09/16 10:30:45 AM: macro_avg: validation: 0.936742
09/16 10:30:45 AM: micro_avg: validation: 0.000000
09/16 10:30:45 AM: edges-ner-ontonotes_mcc: training: 0.920196 validation: 0.933151
09/16 10:30:45 AM: edges-ner-ontonotes_acc: training: 0.883206 validation: 0.906278
09/16 10:30:45 AM: edges-ner-ontonotes_precision: training: 0.941973 validation: 0.948539
09/16 10:30:45 AM: edges-ner-ontonotes_recall: training: 0.907426 validation: 0.925235
09/16 10:30:45 AM: edges-ner-ontonotes_f1: training: 0.924377 validation: 0.936742
09/16 10:30:45 AM: Global learning rate: 0.0001
09/16 10:30:45 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:30:52 AM: Update 28082: task edges-ner-ontonotes, batch 82 (28082): mcc: 0.8997, acc: 0.8561, precision: 0.9283, recall: 0.8826, f1: 0.9049, edges-ner-ontonotes_loss: 0.0316
09/16 10:31:02 AM: Update 28243: task edges-ner-ontonotes, batch 243 (28243): mcc: 0.9002, acc: 0.8561, precision: 0.9276, recall: 0.8841, f1: 0.9053, edges-ner-ontonotes_loss: 0.0313
09/16 10:31:12 AM: Update 28365: task edges-ner-ontonotes, batch 365 (28365): mcc: 0.9035, acc: 0.8602, precision: 0.9304, recall: 0.8875, f1: 0.9084, edges-ner-ontonotes_loss: 0.0302
09/16 10:31:22 AM: Update 28498: task edges-ner-ontonotes, batch 498 (28498): mcc: 0.9072, acc: 0.8655, precision: 0.9327, recall: 0.8923, f1: 0.9120, edges-ner-ontonotes_loss: 0.0292
09/16 10:31:33 AM: Update 28635: task edges-ner-ontonotes, batch 635 (28635): mcc: 0.9099, acc: 0.8692, precision: 0.9344, recall: 0.8956, f1: 0.9146, edges-ner-ontonotes_loss: 0.0282
09/16 10:31:43 AM: Update 28763: task edges-ner-ontonotes, batch 763 (28763): mcc: 0.9150, acc: 0.8759, precision: 0.9381, recall: 0.9016, f1: 0.9195, edges-ner-ontonotes_loss: 0.0269
09/16 10:31:54 AM: Update 28898: task edges-ner-ontonotes, batch 898 (28898): mcc: 0.9191, acc: 0.8811, precision: 0.9409, recall: 0.9064, f1: 0.9234, edges-ner-ontonotes_loss: 0.0257
09/16 10:32:03 AM: ***** Step 29000 / Validation 29 *****
09/16 10:32:03 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:32:03 AM: Validating...
09/16 10:32:04 AM: Evaluate: task edges-ner-ontonotes, batch 2 (157): mcc: 0.7895, acc: 0.7154, precision: 0.8584, recall: 0.7462, f1: 0.7984, edges-ner-ontonotes_loss: 0.0554
09/16 10:32:14 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.9237, acc: 0.8958, precision: 0.9427, recall: 0.9134, f1: 0.9278, edges-ner-ontonotes_loss: 0.0271
09/16 10:32:21 AM: Updating LR scheduler:
09/16 10:32:21 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:32:21 AM: 	# validation passes without improvement: 3
09/16 10:32:21 AM: edges-ner-ontonotes_loss: training: 0.025250 validation: 0.022897
09/16 10:32:21 AM: macro_avg: validation: 0.936802
09/16 10:32:21 AM: micro_avg: validation: 0.000000
09/16 10:32:21 AM: edges-ner-ontonotes_mcc: training: 0.920311 validation: 0.933208
09/16 10:32:21 AM: edges-ner-ontonotes_acc: training: 0.882686 validation: 0.906885
09/16 10:32:21 AM: edges-ner-ontonotes_precision: training: 0.941811 validation: 0.948264
09/16 10:32:21 AM: edges-ner-ontonotes_recall: training: 0.907798 validation: 0.925614
09/16 10:32:21 AM: edges-ner-ontonotes_f1: training: 0.924492 validation: 0.936802
09/16 10:32:21 AM: Global learning rate: 0.0001
09/16 10:32:21 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:32:24 AM: Update 29034: task edges-ner-ontonotes, batch 34 (29034): mcc: 0.9243, acc: 0.8892, precision: 0.9424, recall: 0.9147, f1: 0.9283, edges-ner-ontonotes_loss: 0.0245
09/16 10:32:34 AM: Update 29166: task edges-ner-ontonotes, batch 166 (29166): mcc: 0.9318, acc: 0.8991, precision: 0.9487, recall: 0.9225, f1: 0.9354, edges-ner-ontonotes_loss: 0.0221
09/16 10:32:44 AM: Update 29274: task edges-ner-ontonotes, batch 274 (29274): mcc: 0.9311, acc: 0.8975, precision: 0.9488, recall: 0.9211, f1: 0.9347, edges-ner-ontonotes_loss: 0.0218
09/16 10:32:54 AM: Update 29415: task edges-ner-ontonotes, batch 415 (29415): mcc: 0.9177, acc: 0.8809, precision: 0.9397, recall: 0.9049, f1: 0.9220, edges-ner-ontonotes_loss: 0.0272
09/16 10:33:04 AM: Update 29551: task edges-ner-ontonotes, batch 551 (29551): mcc: 0.9124, acc: 0.8738, precision: 0.9362, recall: 0.8985, f1: 0.9170, edges-ner-ontonotes_loss: 0.0293
09/16 10:33:14 AM: Update 29679: task edges-ner-ontonotes, batch 679 (29679): mcc: 0.9105, acc: 0.8713, precision: 0.9351, recall: 0.8961, f1: 0.9152, edges-ner-ontonotes_loss: 0.0298
09/16 10:33:24 AM: Update 29839: task edges-ner-ontonotes, batch 839 (29839): mcc: 0.9098, acc: 0.8701, precision: 0.9345, recall: 0.8954, f1: 0.9145, edges-ner-ontonotes_loss: 0.0298
09/16 10:33:34 AM: Update 29950: task edges-ner-ontonotes, batch 950 (29950): mcc: 0.9096, acc: 0.8694, precision: 0.9343, recall: 0.8952, f1: 0.9143, edges-ner-ontonotes_loss: 0.0296
09/16 10:33:37 AM: ***** Step 30000 / Validation 30 *****
09/16 10:33:37 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:33:37 AM: Validating...
09/16 10:33:44 AM: Evaluate: task edges-ner-ontonotes, batch 63 (157): mcc: 0.9212, acc: 0.8924, precision: 0.9383, recall: 0.9128, f1: 0.9254, edges-ner-ontonotes_loss: 0.0265
09/16 10:33:54 AM: Evaluate: task edges-ner-ontonotes, batch 146 (157): mcc: 0.9321, acc: 0.9054, precision: 0.9501, recall: 0.9217, f1: 0.9357, edges-ner-ontonotes_loss: 0.0226
09/16 10:33:55 AM: Updating LR scheduler:
09/16 10:33:55 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:33:55 AM: 	# validation passes without improvement: 0
09/16 10:33:55 AM: edges-ner-ontonotes_loss: training: 0.029455 validation: 0.022103
09/16 10:33:55 AM: macro_avg: validation: 0.937012
09/16 10:33:55 AM: micro_avg: validation: 0.000000
09/16 10:33:55 AM: edges-ner-ontonotes_mcc: training: 0.909889 validation: 0.933481
09/16 10:33:55 AM: edges-ner-ontonotes_acc: training: 0.869888 validation: 0.907037
09/16 10:33:55 AM: edges-ner-ontonotes_precision: training: 0.934366 validation: 0.951176
09/16 10:33:55 AM: edges-ner-ontonotes_recall: training: 0.895601 validation: 0.923264
09/16 10:33:55 AM: edges-ner-ontonotes_f1: training: 0.914573 validation: 0.937012
09/16 10:33:55 AM: Global learning rate: 5e-05
09/16 10:33:55 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:34:04 AM: Update 30124: task edges-ner-ontonotes, batch 124 (30124): mcc: 0.9182, acc: 0.8823, precision: 0.9404, recall: 0.9052, f1: 0.9224, edges-ner-ontonotes_loss: 0.0261
09/16 10:34:14 AM: Update 30229: task edges-ner-ontonotes, batch 229 (30229): mcc: 0.9185, acc: 0.8822, precision: 0.9400, recall: 0.9062, f1: 0.9228, edges-ner-ontonotes_loss: 0.0257
09/16 10:34:24 AM: Update 30354: task edges-ner-ontonotes, batch 354 (30354): mcc: 0.9251, acc: 0.8904, precision: 0.9452, recall: 0.9135, f1: 0.9291, edges-ner-ontonotes_loss: 0.0237
09/16 10:34:34 AM: Update 30476: task edges-ner-ontonotes, batch 476 (30476): mcc: 0.9287, acc: 0.8944, precision: 0.9481, recall: 0.9173, f1: 0.9325, edges-ner-ontonotes_loss: 0.0227
09/16 10:34:44 AM: Update 30581: task edges-ner-ontonotes, batch 581 (30581): mcc: 0.9284, acc: 0.8937, precision: 0.9480, recall: 0.9169, f1: 0.9322, edges-ner-ontonotes_loss: 0.0227
09/16 10:34:54 AM: Update 30702: task edges-ner-ontonotes, batch 702 (30702): mcc: 0.9291, acc: 0.8945, precision: 0.9484, recall: 0.9179, f1: 0.9329, edges-ner-ontonotes_loss: 0.0225
09/16 10:35:05 AM: Update 30817: task edges-ner-ontonotes, batch 817 (30817): mcc: 0.9294, acc: 0.8946, precision: 0.9483, recall: 0.9184, f1: 0.9331, edges-ner-ontonotes_loss: 0.0223
09/16 10:35:15 AM: Update 30937: task edges-ner-ontonotes, batch 937 (30937): mcc: 0.9245, acc: 0.8884, precision: 0.9449, recall: 0.9126, f1: 0.9285, edges-ner-ontonotes_loss: 0.0243
09/16 10:35:20 AM: ***** Step 31000 / Validation 31 *****
09/16 10:35:20 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:35:20 AM: Validating...
09/16 10:35:25 AM: Evaluate: task edges-ner-ontonotes, batch 50 (157): mcc: 0.9111, acc: 0.8799, precision: 0.9310, recall: 0.9012, f1: 0.9158, edges-ner-ontonotes_loss: 0.0306
09/16 10:35:35 AM: Evaluate: task edges-ner-ontonotes, batch 125 (157): mcc: 0.9279, acc: 0.8993, precision: 0.9454, recall: 0.9184, f1: 0.9317, edges-ner-ontonotes_loss: 0.0242
09/16 10:35:39 AM: Updating LR scheduler:
09/16 10:35:39 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:35:39 AM: 	# validation passes without improvement: 1
09/16 10:35:39 AM: edges-ner-ontonotes_loss: training: 0.024830 validation: 0.022444
09/16 10:35:39 AM: macro_avg: validation: 0.936430
09/16 10:35:39 AM: micro_avg: validation: 0.000000
09/16 10:35:39 AM: edges-ner-ontonotes_mcc: training: 0.923030 validation: 0.932855
09/16 10:35:39 AM: edges-ner-ontonotes_acc: training: 0.886619 validation: 0.905748
09/16 10:35:39 AM: edges-ner-ontonotes_precision: training: 0.943859 validation: 0.950059
09/16 10:35:39 AM: edges-ner-ontonotes_recall: training: 0.910879 validation: 0.923188
09/16 10:35:39 AM: edges-ner-ontonotes_f1: training: 0.927076 validation: 0.936430
09/16 10:35:39 AM: Global learning rate: 5e-05
09/16 10:35:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:35:45 AM: Update 31082: task edges-ner-ontonotes, batch 82 (31082): mcc: 0.8951, acc: 0.8515, precision: 0.9257, recall: 0.8764, f1: 0.9004, edges-ner-ontonotes_loss: 0.0366
09/16 10:35:56 AM: Update 31190: task edges-ner-ontonotes, batch 190 (31190): mcc: 0.8973, acc: 0.8551, precision: 0.9261, recall: 0.8802, f1: 0.9026, edges-ner-ontonotes_loss: 0.0339
09/16 10:36:06 AM: Update 31333: task edges-ner-ontonotes, batch 333 (31333): mcc: 0.9013, acc: 0.8600, precision: 0.9285, recall: 0.8853, f1: 0.9064, edges-ner-ontonotes_loss: 0.0318
09/16 10:36:16 AM: Update 31448: task edges-ner-ontonotes, batch 448 (31448): mcc: 0.9025, acc: 0.8604, precision: 0.9295, recall: 0.8865, f1: 0.9075, edges-ner-ontonotes_loss: 0.0310
09/16 10:36:26 AM: Update 31575: task edges-ner-ontonotes, batch 575 (31575): mcc: 0.9076, acc: 0.8671, precision: 0.9332, recall: 0.8924, f1: 0.9124, edges-ner-ontonotes_loss: 0.0296
09/16 10:36:36 AM: Update 31698: task edges-ner-ontonotes, batch 698 (31698): mcc: 0.9098, acc: 0.8699, precision: 0.9342, recall: 0.8955, f1: 0.9144, edges-ner-ontonotes_loss: 0.0289
09/16 10:36:46 AM: Update 31791: task edges-ner-ontonotes, batch 791 (31791): mcc: 0.9117, acc: 0.8720, precision: 0.9357, recall: 0.8977, f1: 0.9163, edges-ner-ontonotes_loss: 0.0283
09/16 10:36:56 AM: Update 31913: task edges-ner-ontonotes, batch 913 (31913): mcc: 0.9153, acc: 0.8766, precision: 0.9382, recall: 0.9019, f1: 0.9197, edges-ner-ontonotes_loss: 0.0272
09/16 10:37:03 AM: ***** Step 32000 / Validation 32 *****
09/16 10:37:03 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:37:03 AM: Validating...
09/16 10:37:06 AM: Evaluate: task edges-ner-ontonotes, batch 28 (157): mcc: 0.8807, acc: 0.8426, precision: 0.9087, recall: 0.8661, f1: 0.8869, edges-ner-ontonotes_loss: 0.0387
09/16 10:37:16 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.9233, acc: 0.8952, precision: 0.9428, recall: 0.9124, f1: 0.9274, edges-ner-ontonotes_loss: 0.0258
09/16 10:37:22 AM: Updating LR scheduler:
09/16 10:37:22 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:37:22 AM: 	# validation passes without improvement: 2
09/16 10:37:22 AM: edges-ner-ontonotes_loss: training: 0.026554 validation: 0.022692
09/16 10:37:22 AM: macro_avg: validation: 0.936836
09/16 10:37:22 AM: micro_avg: validation: 0.000000
09/16 10:37:22 AM: edges-ner-ontonotes_mcc: training: 0.917234 validation: 0.933290
09/16 10:37:22 AM: edges-ner-ontonotes_acc: training: 0.878926 validation: 0.907113
09/16 10:37:22 AM: edges-ner-ontonotes_precision: training: 0.939630 validation: 0.950734
09/16 10:37:22 AM: edges-ner-ontonotes_recall: training: 0.904181 validation: 0.923339
09/16 10:37:22 AM: edges-ner-ontonotes_f1: training: 0.921565 validation: 0.936836
09/16 10:37:22 AM: Global learning rate: 5e-05
09/16 10:37:22 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:37:26 AM: Update 32050: task edges-ner-ontonotes, batch 50 (32050): mcc: 0.9368, acc: 0.9020, precision: 0.9512, recall: 0.9294, f1: 0.9402, edges-ner-ontonotes_loss: 0.0195
09/16 10:37:36 AM: Update 32147: task edges-ner-ontonotes, batch 147 (32147): mcc: 0.9320, acc: 0.8974, precision: 0.9498, recall: 0.9219, f1: 0.9356, edges-ner-ontonotes_loss: 0.0216
09/16 10:37:46 AM: Update 32271: task edges-ner-ontonotes, batch 271 (32271): mcc: 0.9308, acc: 0.8954, precision: 0.9489, recall: 0.9204, f1: 0.9344, edges-ner-ontonotes_loss: 0.0217
09/16 10:37:56 AM: Update 32374: task edges-ner-ontonotes, batch 374 (32374): mcc: 0.9307, acc: 0.8957, precision: 0.9491, recall: 0.9202, f1: 0.9344, edges-ner-ontonotes_loss: 0.0215
09/16 10:38:06 AM: Update 32498: task edges-ner-ontonotes, batch 498 (32498): mcc: 0.9221, acc: 0.8848, precision: 0.9438, recall: 0.9092, f1: 0.9262, edges-ner-ontonotes_loss: 0.0252
09/16 10:38:16 AM: Update 32627: task edges-ner-ontonotes, batch 627 (32627): mcc: 0.9176, acc: 0.8792, precision: 0.9408, recall: 0.9038, f1: 0.9219, edges-ner-ontonotes_loss: 0.0275
09/16 10:38:26 AM: Update 32742: task edges-ner-ontonotes, batch 742 (32742): mcc: 0.9152, acc: 0.8763, precision: 0.9391, recall: 0.9009, f1: 0.9196, edges-ner-ontonotes_loss: 0.0282
09/16 10:38:36 AM: Update 32892: task edges-ner-ontonotes, batch 892 (32892): mcc: 0.9138, acc: 0.8745, precision: 0.9381, recall: 0.8993, f1: 0.9183, edges-ner-ontonotes_loss: 0.0283
09/16 10:38:46 AM: ***** Step 33000 / Validation 33 *****
09/16 10:38:46 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:38:46 AM: Validating...
09/16 10:38:46 AM: Evaluate: task edges-ner-ontonotes, batch 4 (157): mcc: 0.7998, acc: 0.7246, precision: 0.8548, recall: 0.7681, f1: 0.8092, edges-ner-ontonotes_loss: 0.0590
09/16 10:38:57 AM: Evaluate: task edges-ner-ontonotes, batch 94 (157): mcc: 0.9252, acc: 0.8956, precision: 0.9478, recall: 0.9111, f1: 0.9291, edges-ner-ontonotes_loss: 0.0251
09/16 10:39:05 AM: Updating LR scheduler:
09/16 10:39:05 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:39:05 AM: 	# validation passes without improvement: 3
09/16 10:39:05 AM: edges-ner-ontonotes_loss: training: 0.028481 validation: 0.021982
09/16 10:39:05 AM: macro_avg: validation: 0.936975
09/16 10:39:05 AM: micro_avg: validation: 0.000000
09/16 10:39:05 AM: edges-ner-ontonotes_mcc: training: 0.913087 validation: 0.933492
09/16 10:39:05 AM: edges-ner-ontonotes_acc: training: 0.873559 validation: 0.906506
09/16 10:39:05 AM: edges-ner-ontonotes_precision: training: 0.937457 validation: 0.953525
09/16 10:39:05 AM: edges-ner-ontonotes_recall: training: 0.898559 validation: 0.920989
09/16 10:39:05 AM: edges-ner-ontonotes_f1: training: 0.917596 validation: 0.936975
09/16 10:39:05 AM: Global learning rate: 5e-05
09/16 10:39:05 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:39:07 AM: Update 33019: task edges-ner-ontonotes, batch 19 (33019): mcc: 0.8965, acc: 0.8488, precision: 0.9262, recall: 0.8785, f1: 0.9017, edges-ner-ontonotes_loss: 0.0315
09/16 10:39:17 AM: Update 33143: task edges-ner-ontonotes, batch 143 (33143): mcc: 0.9142, acc: 0.8755, precision: 0.9373, recall: 0.9008, f1: 0.9187, edges-ner-ontonotes_loss: 0.0265
09/16 10:39:27 AM: Update 33273: task edges-ner-ontonotes, batch 273 (33273): mcc: 0.9174, acc: 0.8802, precision: 0.9392, recall: 0.9050, f1: 0.9218, edges-ner-ontonotes_loss: 0.0258
09/16 10:39:37 AM: Update 33363: task edges-ner-ontonotes, batch 363 (33363): mcc: 0.9196, acc: 0.8830, precision: 0.9405, recall: 0.9077, f1: 0.9238, edges-ner-ontonotes_loss: 0.0250
09/16 10:39:47 AM: Update 33489: task edges-ner-ontonotes, batch 489 (33489): mcc: 0.9241, acc: 0.8888, precision: 0.9438, recall: 0.9129, f1: 0.9281, edges-ner-ontonotes_loss: 0.0236
09/16 10:39:57 AM: Update 33608: task edges-ner-ontonotes, batch 608 (33608): mcc: 0.9276, acc: 0.8933, precision: 0.9461, recall: 0.9171, f1: 0.9314, edges-ner-ontonotes_loss: 0.0227
09/16 10:40:07 AM: Update 33716: task edges-ner-ontonotes, batch 716 (33716): mcc: 0.9277, acc: 0.8929, precision: 0.9466, recall: 0.9169, f1: 0.9315, edges-ner-ontonotes_loss: 0.0225
09/16 10:40:17 AM: Update 33834: task edges-ner-ontonotes, batch 834 (33834): mcc: 0.9285, acc: 0.8939, precision: 0.9473, recall: 0.9177, f1: 0.9323, edges-ner-ontonotes_loss: 0.0223
09/16 10:40:27 AM: Update 33930: task edges-ner-ontonotes, batch 930 (33930): mcc: 0.9284, acc: 0.8937, precision: 0.9471, recall: 0.9177, f1: 0.9322, edges-ner-ontonotes_loss: 0.0224
09/16 10:40:33 AM: ***** Step 34000 / Validation 34 *****
09/16 10:40:33 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:40:33 AM: Validating...
09/16 10:40:37 AM: Evaluate: task edges-ner-ontonotes, batch 44 (157): mcc: 0.9100, acc: 0.8806, precision: 0.9311, recall: 0.8990, f1: 0.9148, edges-ner-ontonotes_loss: 0.0308
09/16 10:40:47 AM: Evaluate: task edges-ner-ontonotes, batch 121 (157): mcc: 0.9259, acc: 0.8971, precision: 0.9472, recall: 0.9130, f1: 0.9298, edges-ner-ontonotes_loss: 0.0248
09/16 10:40:51 AM: Updating LR scheduler:
09/16 10:40:51 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:40:51 AM: 	# validation passes without improvement: 0
09/16 10:40:51 AM: edges-ner-ontonotes_loss: training: 0.023376 validation: 0.022726
09/16 10:40:51 AM: macro_avg: validation: 0.935150
09/16 10:40:51 AM: micro_avg: validation: 0.000000
09/16 10:40:51 AM: edges-ner-ontonotes_mcc: training: 0.926016 validation: 0.931580
09/16 10:40:51 AM: edges-ner-ontonotes_acc: training: 0.890552 validation: 0.904155
09/16 10:40:51 AM: edges-ner-ontonotes_precision: training: 0.945579 validation: 0.952430
09/16 10:40:51 AM: edges-ner-ontonotes_recall: training: 0.914778 validation: 0.918486
09/16 10:40:51 AM: edges-ner-ontonotes_f1: training: 0.929924 validation: 0.935150
09/16 10:40:51 AM: Global learning rate: 2.5e-05
09/16 10:40:51 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:40:57 AM: Update 34074: task edges-ner-ontonotes, batch 74 (34074): mcc: 0.8887, acc: 0.8420, precision: 0.9241, recall: 0.8663, f1: 0.8942, edges-ner-ontonotes_loss: 0.0379
09/16 10:41:07 AM: Update 34205: task edges-ner-ontonotes, batch 205 (34205): mcc: 0.8945, acc: 0.8507, precision: 0.9248, recall: 0.8761, f1: 0.8998, edges-ner-ontonotes_loss: 0.0362
09/16 10:41:17 AM: Update 34316: task edges-ner-ontonotes, batch 316 (34316): mcc: 0.8966, acc: 0.8527, precision: 0.9257, recall: 0.8794, f1: 0.9019, edges-ner-ontonotes_loss: 0.0346
09/16 10:41:27 AM: Update 34465: task edges-ner-ontonotes, batch 465 (34465): mcc: 0.8996, acc: 0.8562, precision: 0.9276, recall: 0.8830, f1: 0.9048, edges-ner-ontonotes_loss: 0.0330
09/16 10:41:37 AM: Update 34576: task edges-ner-ontonotes, batch 576 (34576): mcc: 0.9013, acc: 0.8584, precision: 0.9288, recall: 0.8849, f1: 0.9063, edges-ner-ontonotes_loss: 0.0323
09/16 10:41:48 AM: Update 34702: task edges-ner-ontonotes, batch 702 (34702): mcc: 0.9040, acc: 0.8618, precision: 0.9305, recall: 0.8883, f1: 0.9089, edges-ner-ontonotes_loss: 0.0313
09/16 10:41:58 AM: Update 34827: task edges-ner-ontonotes, batch 827 (34827): mcc: 0.9063, acc: 0.8648, precision: 0.9325, recall: 0.8907, f1: 0.9111, edges-ner-ontonotes_loss: 0.0304
09/16 10:42:08 AM: Update 34932: task edges-ner-ontonotes, batch 932 (34932): mcc: 0.9090, acc: 0.8683, precision: 0.9343, recall: 0.8941, f1: 0.9138, edges-ner-ontonotes_loss: 0.0295
09/16 10:42:13 AM: ***** Step 35000 / Validation 35 *****
09/16 10:42:13 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:42:13 AM: Validating...
09/16 10:42:18 AM: Evaluate: task edges-ner-ontonotes, batch 43 (157): mcc: 0.9064, acc: 0.8772, precision: 0.9239, recall: 0.8993, f1: 0.9115, edges-ner-ontonotes_loss: 0.0315
09/16 10:42:28 AM: Evaluate: task edges-ner-ontonotes, batch 116 (157): mcc: 0.9261, acc: 0.8985, precision: 0.9437, recall: 0.9168, f1: 0.9301, edges-ner-ontonotes_loss: 0.0247
09/16 10:42:33 AM: Updating LR scheduler:
09/16 10:42:33 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:42:33 AM: 	# validation passes without improvement: 1
09/16 10:42:33 AM: edges-ner-ontonotes_loss: training: 0.028762 validation: 0.022121
09/16 10:42:33 AM: macro_avg: validation: 0.937827
09/16 10:42:33 AM: micro_avg: validation: 0.000000
09/16 10:42:33 AM: edges-ner-ontonotes_mcc: training: 0.911504 validation: 0.934317
09/16 10:42:33 AM: edges-ner-ontonotes_acc: training: 0.871468 validation: 0.908477
09/16 10:42:33 AM: edges-ner-ontonotes_precision: training: 0.936009 validation: 0.950686
09/16 10:42:33 AM: edges-ner-ontonotes_recall: training: 0.897017 validation: 0.925311
09/16 10:42:33 AM: edges-ner-ontonotes_f1: training: 0.916098 validation: 0.937827
09/16 10:42:33 AM: Global learning rate: 2.5e-05
09/16 10:42:33 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:42:38 AM: Update 35060: task edges-ner-ontonotes, batch 60 (35060): mcc: 0.9321, acc: 0.8962, precision: 0.9495, recall: 0.9222, f1: 0.9357, edges-ner-ontonotes_loss: 0.0204
09/16 10:42:49 AM: Update 35172: task edges-ner-ontonotes, batch 172 (35172): mcc: 0.9380, acc: 0.9050, precision: 0.9538, recall: 0.9290, f1: 0.9413, edges-ner-ontonotes_loss: 0.0195
09/16 10:42:59 AM: Update 35289: task edges-ner-ontonotes, batch 289 (35289): mcc: 0.9352, acc: 0.9014, precision: 0.9526, recall: 0.9252, f1: 0.9387, edges-ner-ontonotes_loss: 0.0203
09/16 10:43:09 AM: Update 35413: task edges-ner-ontonotes, batch 413 (35413): mcc: 0.9352, acc: 0.9016, precision: 0.9527, recall: 0.9250, f1: 0.9387, edges-ner-ontonotes_loss: 0.0202
09/16 10:43:19 AM: Update 35515: task edges-ner-ontonotes, batch 515 (35515): mcc: 0.9314, acc: 0.8970, precision: 0.9499, recall: 0.9205, f1: 0.9350, edges-ner-ontonotes_loss: 0.0217
09/16 10:43:29 AM: Update 35634: task edges-ner-ontonotes, batch 634 (35634): mcc: 0.9254, acc: 0.8894, precision: 0.9459, recall: 0.9132, f1: 0.9293, edges-ner-ontonotes_loss: 0.0242
09/16 10:43:39 AM: Update 35757: task edges-ner-ontonotes, batch 757 (35757): mcc: 0.9209, acc: 0.8839, precision: 0.9428, recall: 0.9079, f1: 0.9250, edges-ner-ontonotes_loss: 0.0259
09/16 10:43:49 AM: Update 35876: task edges-ner-ontonotes, batch 876 (35876): mcc: 0.9180, acc: 0.8805, precision: 0.9406, recall: 0.9046, f1: 0.9223, edges-ner-ontonotes_loss: 0.0270
09/16 10:43:57 AM: ***** Step 36000 / Validation 36 *****
09/16 10:43:57 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 10:43:57 AM: Validating...
09/16 10:43:59 AM: Evaluate: task edges-ner-ontonotes, batch 19 (157): mcc: 0.8902, acc: 0.8501, precision: 0.9163, recall: 0.8765, f1: 0.8960, edges-ner-ontonotes_loss: 0.0319
09/16 10:44:09 AM: Evaluate: task edges-ner-ontonotes, batch 105 (157): mcc: 0.9238, acc: 0.8950, precision: 0.9452, recall: 0.9110, f1: 0.9278, edges-ner-ontonotes_loss: 0.0250
09/16 10:44:16 AM: Updating LR scheduler:
09/16 10:44:16 AM: 	Best result seen so far for macro_avg: 0.938
09/16 10:44:16 AM: 	# validation passes without improvement: 2
09/16 10:44:16 AM: Ran out of early stopping patience. Stopping training.
09/16 10:44:16 AM: edges-ner-ontonotes_loss: training: 0.027191 validation: 0.021801
09/16 10:44:16 AM: macro_avg: validation: 0.937616
09/16 10:44:16 AM: micro_avg: validation: 0.000000
09/16 10:44:16 AM: edges-ner-ontonotes_mcc: training: 0.917043 validation: 0.934162
09/16 10:44:16 AM: edges-ner-ontonotes_acc: training: 0.879169 validation: 0.907719
09/16 10:44:16 AM: edges-ner-ontonotes_precision: training: 0.939948 validation: 0.953797
09/16 10:44:16 AM: edges-ner-ontonotes_recall: training: 0.903515 validation: 0.921975
09/16 10:44:16 AM: edges-ner-ontonotes_f1: training: 0.921371 validation: 0.937616
09/16 10:44:16 AM: Global learning rate: 2.5e-05
09/16 10:44:16 AM: Saving checkpoints to: ./experiments/ner-ontonotes-mnli-top/run
09/16 10:44:16 AM: Stopped training after 36 validation checks
09/16 10:44:16 AM: Trained edges-ner-ontonotes for 36000 batches or 23.166 epochs
09/16 10:44:16 AM: ***** VALIDATION RESULTS *****
09/16 10:44:16 AM: edges-ner-ontonotes_f1 (for best val pass 26): edges-ner-ontonotes_loss: 0.02265, macro_avg: 0.93803, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.93451, edges-ner-ontonotes_acc: 0.90901, edges-ner-ontonotes_precision: 0.94944, edges-ner-ontonotes_recall: 0.92690, edges-ner-ontonotes_f1: 0.93803
09/16 10:44:16 AM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.04897, macro_avg: 0.86255, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.85674, edges-ner-ontonotes_acc: 0.80042, edges-ner-ontonotes_precision: 0.91824, edges-ner-ontonotes_recall: 0.81324, edges-ner-ontonotes_f1: 0.86255
09/16 10:44:16 AM: macro_avg (for best val pass 26): edges-ner-ontonotes_loss: 0.02265, macro_avg: 0.93803, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.93451, edges-ner-ontonotes_acc: 0.90901, edges-ner-ontonotes_precision: 0.94944, edges-ner-ontonotes_recall: 0.92690, edges-ner-ontonotes_f1: 0.93803
09/16 10:44:16 AM: Evaluating...
09/16 10:44:16 AM: Loaded model state from ./experiments/ner-ontonotes-mnli-top/run/edges-ner-ontonotes/model_state_target_train_val_26.best.th
09/16 10:44:16 AM: Evaluating on: edges-ner-ontonotes, split: val
09/16 10:44:46 AM: 	Task edges-ner-ontonotes: batch 233
09/16 10:44:47 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 10:44:47 AM: Finished evaluating on: edges-ner-ontonotes
09/16 10:44:47 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
09/16 10:44:48 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:44:48 AM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:44:48 AM: Evaluating on: edges-ner-ontonotes, split: test
09/16 10:45:07 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 10:45:07 AM: Finished evaluating on: edges-ner-ontonotes
09/16 10:45:07 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
09/16 10:45:08 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:45:08 AM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-mnli-top/run
09/16 10:45:08 AM: Writing results for split 'val' to ./experiments/ner-ontonotes-mnli-top/results.tsv
09/16 10:45:08 AM: micro_avg: 0.000, macro_avg: 0.934, edges-ner-ontonotes_mcc: 0.931, edges-ner-ontonotes_acc: 0.905, edges-ner-ontonotes_precision: 0.946, edges-ner-ontonotes_recall: 0.924, edges-ner-ontonotes_f1: 0.934
09/16 10:45:08 AM: Done!
09/16 10:45:08 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
