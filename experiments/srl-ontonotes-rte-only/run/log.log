09/16 09:10:58 AM: Git branch: master
09/16 09:10:58 AM: Git SHA: 1a42459c6cbb693793b9c0d01bca567d99b0baac
09/16 09:10:59 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/srl-ontonotes-rte-only/",
  "exp_name": "experiments/srl-ontonotes-rte-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/srl-ontonotes-rte-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/rte",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/srl-ontonotes-rte-only__run",
  "run_dir": "./experiments/srl-ontonotes-rte-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-srl-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 09:10:59 AM: Saved config to ./experiments/srl-ontonotes-rte-only/run/params.conf
09/16 09:10:59 AM: Using random seed 1234
09/16 09:10:59 AM: Using GPU 0
09/16 09:10:59 AM: Loading tasks...
09/16 09:10:59 AM: Writing pre-preprocessed tasks to ./experiments/srl-ontonotes-rte-only/
09/16 09:10:59 AM: 	Creating task edges-srl-ontonotes from scratch.
09/16 09:11:04 AM: Read=231480, Skip=21590, Total=253070 from ./probing_data/edges/ontonotes/srl/train.json.retokenized.bert-base-uncased
09/16 09:11:04 AM: Read=32486, Skip=2811, Total=35297 from ./probing_data/edges/ontonotes/srl/development.json.retokenized.bert-base-uncased
09/16 09:11:05 AM: Read=23800, Skip=2915, Total=26715 from ./probing_data/edges/ontonotes/srl/test.json.retokenized.bert-base-uncased
09/16 09:11:08 AM: 	Task 'edges-srl-ontonotes': |train|=231480 |val|=32486 |test|=23800
09/16 09:11:08 AM: 	Finished loading tasks: edges-srl-ontonotes.
09/16 09:11:08 AM: 	Building vocab from scratch.
09/16 09:11:08 AM: 	Counting units for task edges-srl-ontonotes.
09/16 09:11:15 AM: 	Task 'edges-srl-ontonotes': adding vocab namespace 'edges-srl-ontonotes_labels'
09/16 09:11:16 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:11:16 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 09:11:16 AM: 	Saved vocab to ./experiments/srl-ontonotes-rte-only/vocab
09/16 09:11:16 AM: Loading token dictionary from ./experiments/srl-ontonotes-rte-only/vocab.
09/16 09:11:16 AM: 	Loaded vocab from ./experiments/srl-ontonotes-rte-only/vocab
09/16 09:11:16 AM: 	Vocab namespace bert_uncased: size 30524
09/16 09:11:16 AM: 	Vocab namespace tokens: size 23662
09/16 09:11:16 AM: 	Vocab namespace edges-srl-ontonotes_labels: size 66
09/16 09:11:16 AM: 	Vocab namespace chars: size 76
09/16 09:11:16 AM: 	Finished building vocab.
09/16 09:11:16 AM: 	Task edges-srl-ontonotes (train): Indexing from scratch.
09/16 09:11:48 AM: 	Task edges-srl-ontonotes (train): Saved 231480 instances to ./experiments/srl-ontonotes-rte-only/preproc/edges-srl-ontonotes__train_data
09/16 09:11:48 AM: 	Task edges-srl-ontonotes (val): Indexing from scratch.
09/16 09:11:52 AM: 	Task edges-srl-ontonotes (val): Saved 32486 instances to ./experiments/srl-ontonotes-rte-only/preproc/edges-srl-ontonotes__val_data
09/16 09:11:52 AM: 	Task edges-srl-ontonotes (test): Indexing from scratch.
09/16 09:11:55 AM: 	Task edges-srl-ontonotes (test): Saved 23800 instances to ./experiments/srl-ontonotes-rte-only/preproc/edges-srl-ontonotes__test_data
09/16 09:11:55 AM: 	Finished indexing tasks
09/16 09:11:55 AM: 	Creating trimmed target-only version of edges-srl-ontonotes train.
09/16 09:11:55 AM: 	  Training on 
09/16 09:11:55 AM: 	  Evaluating on edges-srl-ontonotes
09/16 09:11:55 AM: 	Finished loading tasks in 56.336s
09/16 09:11:55 AM: 	 Tasks: ['edges-srl-ontonotes']
09/16 09:11:55 AM: Building model...
09/16 09:11:55 AM: Using BERT model (bert-base-uncased).
09/16 09:11:55 AM: LOADING A FUNETUNED MODEL from: 
09/16 09:11:55 AM: models/rte
09/16 09:11:55 AM: loading configuration file models/rte/config.json
09/16 09:11:55 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 09:11:55 AM: loading weights file models/rte/pytorch_model.bin
09/16 09:11:59 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpqyjacjy_
09/16 09:12:01 AM: copying /tmp/tmpqyjacjy_ to cache at ./experiments/srl-ontonotes-rte-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:01 AM: creating metadata file for ./experiments/srl-ontonotes-rte-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:01 AM: removing temp file /tmp/tmpqyjacjy_
09/16 09:12:01 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/srl-ontonotes-rte-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 09:12:02 AM: Initializing parameters
09/16 09:12:02 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 09:12:02 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 09:12:02 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 09:12:02 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 09:12:02 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 09:12:02 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 09:12:02 AM: 	Task 'edges-srl-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-srl-ontonotes"
}
09/16 09:12:05 AM: Model specification:
09/16 09:12:05 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-srl-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=66, bias=True)
      )
    )
  )
)
09/16 09:12:05 AM: Model parameters:
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 09:12:05 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 16896 with torch.Size([66, 256])
09/16 09:12:05 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 66 with torch.Size([66])
09/16 09:12:05 AM: Total number of parameters: 110155842 (1.10156e+08)
09/16 09:12:05 AM: Number of trainable parameters: 673602 (673602)
09/16 09:12:05 AM: Finished building model in 9.969s
09/16 09:12:05 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-srl-ontonotes 

09/16 09:12:19 AM: patience = 9
09/16 09:12:19 AM: val_interval = 1000
09/16 09:12:19 AM: max_vals = 250
09/16 09:12:19 AM: cuda_device = 0
09/16 09:12:19 AM: grad_norm = 5.0
09/16 09:12:19 AM: grad_clipping = None
09/16 09:12:19 AM: lr_decay = 0.99
09/16 09:12:19 AM: min_lr = 1e-06
09/16 09:12:19 AM: keep_all_checkpoints = 0
09/16 09:12:19 AM: val_data_limit = 5000
09/16 09:12:19 AM: max_epochs = -1
09/16 09:12:19 AM: dec_val_scale = 250
09/16 09:12:19 AM: training_data_fraction = 1
09/16 09:12:19 AM: type = adam
09/16 09:12:19 AM: parameter_groups = None
09/16 09:12:19 AM: Number of trainable parameters: 673602
09/16 09:12:19 AM: infer_type_and_cast = True
09/16 09:12:19 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:19 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:19 AM: lr = 0.0001
09/16 09:12:19 AM: amsgrad = True
09/16 09:12:19 AM: type = reduce_on_plateau
09/16 09:12:19 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:19 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:19 AM: mode = max
09/16 09:12:19 AM: factor = 0.5
09/16 09:12:19 AM: patience = 3
09/16 09:12:19 AM: threshold = 0.0001
09/16 09:12:19 AM: threshold_mode = abs
09/16 09:12:19 AM: verbose = True
09/16 09:12:19 AM: type = adam
09/16 09:12:19 AM: parameter_groups = None
09/16 09:12:19 AM: Number of trainable parameters: 673602
09/16 09:12:19 AM: infer_type_and_cast = True
09/16 09:12:19 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:19 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:19 AM: lr = 0.0001
09/16 09:12:19 AM: amsgrad = True
09/16 09:12:19 AM: type = reduce_on_plateau
09/16 09:12:19 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 09:12:19 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 09:12:19 AM: mode = max
09/16 09:12:19 AM: factor = 0.5
09/16 09:12:19 AM: patience = 3
09/16 09:12:19 AM: threshold = 0.0001
09/16 09:12:19 AM: threshold_mode = abs
09/16 09:12:19 AM: verbose = True
09/16 09:12:19 AM: Starting training without restoring from a checkpoint.
09/16 09:12:19 AM: Training examples per task, before any subsampling: {'edges-srl-ontonotes': 231480}
09/16 09:12:19 AM: Beginning training with stopping criteria based on metric: edges-srl-ontonotes_f1
09/16 09:12:29 AM: Update 207: task edges-srl-ontonotes, batch 207 (207): mcc: 0.0560, acc: 0.0484, precision: 0.0578, recall: 0.0957, f1: 0.0720, edges-srl-ontonotes_loss: 0.1593
09/16 09:12:40 AM: Update 419: task edges-srl-ontonotes, batch 419 (419): mcc: 0.1565, acc: 0.1428, precision: 0.1707, recall: 0.1677, f1: 0.1692, edges-srl-ontonotes_loss: 0.1031
09/16 09:12:50 AM: Update 627: task edges-srl-ontonotes, batch 627 (627): mcc: 0.2696, acc: 0.2364, precision: 0.3048, recall: 0.2567, f1: 0.2787, edges-srl-ontonotes_loss: 0.0807
09/16 09:13:00 AM: Update 821: task edges-srl-ontonotes, batch 821 (821): mcc: 0.3292, acc: 0.2784, precision: 0.3809, recall: 0.3002, f1: 0.3358, edges-srl-ontonotes_loss: 0.0695
09/16 09:13:10 AM: Update 990: task edges-srl-ontonotes, batch 990 (990): mcc: 0.3699, acc: 0.3080, precision: 0.4318, recall: 0.3314, f1: 0.3750, edges-srl-ontonotes_loss: 0.0628
09/16 09:13:11 AM: ***** Step 1000 / Validation 1 *****
09/16 09:13:11 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:13:11 AM: Validating...
09/16 09:13:15 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:13:15 AM: Best result seen so far for micro.
09/16 09:13:15 AM: Best result seen so far for macro.
09/16 09:13:15 AM: Updating LR scheduler:
09/16 09:13:15 AM: 	Best result seen so far for macro_avg: 0.638
09/16 09:13:15 AM: 	# validation passes without improvement: 0
09/16 09:13:15 AM: edges-srl-ontonotes_loss: training: 0.062514 validation: 0.027538
09/16 09:13:15 AM: macro_avg: validation: 0.638005
09/16 09:13:15 AM: micro_avg: validation: 0.000000
09/16 09:13:15 AM: edges-srl-ontonotes_mcc: training: 0.371994 validation: 0.646033
09/16 09:13:15 AM: edges-srl-ontonotes_acc: training: 0.309376 validation: 0.519590
09/16 09:13:15 AM: edges-srl-ontonotes_precision: training: 0.434355 validation: 0.791002
09/16 09:13:15 AM: edges-srl-ontonotes_recall: training: 0.332938 validation: 0.534601
09/16 09:13:15 AM: edges-srl-ontonotes_f1: training: 0.376944 validation: 0.638005
09/16 09:13:15 AM: Global learning rate: 0.0001
09/16 09:13:15 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:13:20 AM: Update 1105: task edges-srl-ontonotes, batch 105 (1105): mcc: 0.5930, acc: 0.4560, precision: 0.7304, recall: 0.4893, f1: 0.5861, edges-srl-ontonotes_loss: 0.0304
09/16 09:13:30 AM: Update 1295: task edges-srl-ontonotes, batch 295 (1295): mcc: 0.6028, acc: 0.4672, precision: 0.7327, recall: 0.5039, f1: 0.5972, edges-srl-ontonotes_loss: 0.0294
09/16 09:13:40 AM: Update 1513: task edges-srl-ontonotes, batch 513 (1513): mcc: 0.6119, acc: 0.4784, precision: 0.7349, recall: 0.5176, f1: 0.6074, edges-srl-ontonotes_loss: 0.0285
09/16 09:13:50 AM: Update 1736: task edges-srl-ontonotes, batch 736 (1736): mcc: 0.6128, acc: 0.4798, precision: 0.7327, recall: 0.5206, f1: 0.6087, edges-srl-ontonotes_loss: 0.0283
09/16 09:14:00 AM: Update 1899: task edges-srl-ontonotes, batch 899 (1899): mcc: 0.6128, acc: 0.4802, precision: 0.7325, recall: 0.5207, f1: 0.6087, edges-srl-ontonotes_loss: 0.0283
09/16 09:14:05 AM: ***** Step 2000 / Validation 2 *****
09/16 09:14:05 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:14:05 AM: Validating...
09/16 09:14:10 AM: Evaluate: task edges-srl-ontonotes, batch 144 (157): mcc: 0.6766, acc: 0.5633, precision: 0.8003, recall: 0.5789, f1: 0.6718, edges-srl-ontonotes_loss: 0.0237
09/16 09:14:11 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:14:11 AM: Best result seen so far for macro.
09/16 09:14:11 AM: Updating LR scheduler:
09/16 09:14:11 AM: 	Best result seen so far for macro_avg: 0.670
09/16 09:14:11 AM: 	# validation passes without improvement: 0
09/16 09:14:11 AM: edges-srl-ontonotes_loss: training: 0.028125 validation: 0.023903
09/16 09:14:11 AM: macro_avg: validation: 0.669884
09/16 09:14:11 AM: micro_avg: validation: 0.000000
09/16 09:14:11 AM: edges-srl-ontonotes_mcc: training: 0.613999 validation: 0.674677
09/16 09:14:11 AM: edges-srl-ontonotes_acc: training: 0.481860 validation: 0.561543
09/16 09:14:11 AM: edges-srl-ontonotes_precision: training: 0.733131 validation: 0.798530
09/16 09:14:11 AM: edges-srl-ontonotes_recall: training: 0.522333 validation: 0.576938
09/16 09:14:11 AM: edges-srl-ontonotes_f1: training: 0.610035 validation: 0.669884
09/16 09:14:11 AM: Global learning rate: 0.0001
09/16 09:14:11 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:14:21 AM: Update 2192: task edges-srl-ontonotes, batch 192 (2192): mcc: 0.6326, acc: 0.5062, precision: 0.7437, recall: 0.5461, f1: 0.6298, edges-srl-ontonotes_loss: 0.0267
09/16 09:14:31 AM: Update 2407: task edges-srl-ontonotes, batch 407 (2407): mcc: 0.6352, acc: 0.5109, precision: 0.7391, recall: 0.5540, f1: 0.6333, edges-srl-ontonotes_loss: 0.0262
09/16 09:14:41 AM: Update 2607: task edges-srl-ontonotes, batch 607 (2607): mcc: 0.6402, acc: 0.5180, precision: 0.7403, recall: 0.5616, f1: 0.6387, edges-srl-ontonotes_loss: 0.0258
09/16 09:14:51 AM: Update 2812: task edges-srl-ontonotes, batch 812 (2812): mcc: 0.6466, acc: 0.5264, precision: 0.7442, recall: 0.5699, f1: 0.6455, edges-srl-ontonotes_loss: 0.0255
09/16 09:15:01 AM: Update 2981: task edges-srl-ontonotes, batch 981 (2981): mcc: 0.6508, acc: 0.5319, precision: 0.7464, recall: 0.5754, f1: 0.6498, edges-srl-ontonotes_loss: 0.0253
09/16 09:15:02 AM: ***** Step 3000 / Validation 3 *****
09/16 09:15:02 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:15:02 AM: Validating...
09/16 09:15:08 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:15:08 AM: Best result seen so far for macro.
09/16 09:15:08 AM: Updating LR scheduler:
09/16 09:15:08 AM: 	Best result seen so far for macro_avg: 0.683
09/16 09:15:08 AM: 	# validation passes without improvement: 0
09/16 09:15:08 AM: edges-srl-ontonotes_loss: training: 0.025239 validation: 0.023142
09/16 09:15:08 AM: macro_avg: validation: 0.683315
09/16 09:15:08 AM: micro_avg: validation: 0.000000
09/16 09:15:08 AM: edges-srl-ontonotes_mcc: training: 0.651058 validation: 0.685094
09/16 09:15:08 AM: edges-srl-ontonotes_acc: training: 0.532370 validation: 0.592102
09/16 09:15:08 AM: edges-srl-ontonotes_precision: training: 0.746628 validation: 0.785771
09/16 09:15:08 AM: edges-srl-ontonotes_recall: training: 0.575716 validation: 0.604495
09/16 09:15:08 AM: edges-srl-ontonotes_f1: training: 0.650127 validation: 0.683315
09/16 09:15:08 AM: Global learning rate: 0.0001
09/16 09:15:08 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:15:11 AM: Update 3091: task edges-srl-ontonotes, batch 91 (3091): mcc: 0.6846, acc: 0.5804, precision: 0.7672, recall: 0.6185, f1: 0.6848, edges-srl-ontonotes_loss: 0.0232
09/16 09:15:21 AM: Update 3306: task edges-srl-ontonotes, batch 306 (3306): mcc: 0.6666, acc: 0.5568, precision: 0.7529, recall: 0.5981, f1: 0.6666, edges-srl-ontonotes_loss: 0.0241
09/16 09:15:31 AM: Update 3509: task edges-srl-ontonotes, batch 509 (3509): mcc: 0.6659, acc: 0.5558, precision: 0.7540, recall: 0.5959, f1: 0.6657, edges-srl-ontonotes_loss: 0.0241
09/16 09:15:41 AM: Update 3708: task edges-srl-ontonotes, batch 708 (3708): mcc: 0.6656, acc: 0.5556, precision: 0.7544, recall: 0.5951, f1: 0.6653, edges-srl-ontonotes_loss: 0.0241
09/16 09:15:51 AM: Update 3915: task edges-srl-ontonotes, batch 915 (3915): mcc: 0.6667, acc: 0.5571, precision: 0.7548, recall: 0.5968, f1: 0.6666, edges-srl-ontonotes_loss: 0.0240
09/16 09:15:56 AM: ***** Step 4000 / Validation 4 *****
09/16 09:15:56 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:15:56 AM: Validating...
09/16 09:16:01 AM: Evaluate: task edges-srl-ontonotes, batch 118 (157): mcc: 0.6849, acc: 0.5927, precision: 0.7839, recall: 0.6056, f1: 0.6833, edges-srl-ontonotes_loss: 0.0228
09/16 09:16:03 AM: Updating LR scheduler:
09/16 09:16:03 AM: 	Best result seen so far for macro_avg: 0.683
09/16 09:16:03 AM: 	# validation passes without improvement: 1
09/16 09:16:03 AM: edges-srl-ontonotes_loss: training: 0.024006 validation: 0.022887
09/16 09:16:03 AM: macro_avg: validation: 0.680929
09/16 09:16:03 AM: micro_avg: validation: 0.000000
09/16 09:16:03 AM: edges-srl-ontonotes_mcc: training: 0.666290 validation: 0.682369
09/16 09:16:03 AM: edges-srl-ontonotes_acc: training: 0.557105 validation: 0.591102
09/16 09:16:03 AM: edges-srl-ontonotes_precision: training: 0.754147 validation: 0.780386
09/16 09:16:03 AM: edges-srl-ontonotes_recall: training: 0.596530 validation: 0.603957
09/16 09:16:03 AM: edges-srl-ontonotes_f1: training: 0.666142 validation: 0.680929
09/16 09:16:03 AM: Global learning rate: 0.0001
09/16 09:16:03 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:16:11 AM: Update 4164: task edges-srl-ontonotes, batch 164 (4164): mcc: 0.6664, acc: 0.5618, precision: 0.7522, recall: 0.5983, f1: 0.6665, edges-srl-ontonotes_loss: 0.0239
09/16 09:16:21 AM: Update 4379: task edges-srl-ontonotes, batch 379 (4379): mcc: 0.6762, acc: 0.5726, precision: 0.7592, recall: 0.6100, f1: 0.6765, edges-srl-ontonotes_loss: 0.0234
09/16 09:16:31 AM: Update 4574: task edges-srl-ontonotes, batch 574 (4574): mcc: 0.6798, acc: 0.5774, precision: 0.7613, recall: 0.6147, f1: 0.6802, edges-srl-ontonotes_loss: 0.0231
09/16 09:16:42 AM: Update 4730: task edges-srl-ontonotes, batch 730 (4730): mcc: 0.6807, acc: 0.5783, precision: 0.7619, recall: 0.6158, f1: 0.6811, edges-srl-ontonotes_loss: 0.0232
09/16 09:16:52 AM: Update 4915: task edges-srl-ontonotes, batch 915 (4915): mcc: 0.6794, acc: 0.5766, precision: 0.7614, recall: 0.6139, f1: 0.6797, edges-srl-ontonotes_loss: 0.0233
09/16 09:16:56 AM: ***** Step 5000 / Validation 5 *****
09/16 09:16:56 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:16:56 AM: Validating...
09/16 09:17:01 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:17:01 AM: Best result seen so far for macro.
09/16 09:17:01 AM: Updating LR scheduler:
09/16 09:17:01 AM: 	Best result seen so far for macro_avg: 0.685
09/16 09:17:01 AM: 	# validation passes without improvement: 0
09/16 09:17:01 AM: edges-srl-ontonotes_loss: training: 0.023248 validation: 0.022608
09/16 09:17:01 AM: macro_avg: validation: 0.685354
09/16 09:17:01 AM: micro_avg: validation: 0.000000
09/16 09:17:01 AM: edges-srl-ontonotes_mcc: training: 0.679605 validation: 0.684541
09/16 09:17:01 AM: edges-srl-ontonotes_acc: training: 0.576322 validation: 0.600493
09/16 09:17:01 AM: edges-srl-ontonotes_precision: training: 0.762023 validation: 0.761622
09/16 09:17:01 AM: edges-srl-ontonotes_recall: training: 0.613798 validation: 0.622970
09/16 09:17:01 AM: edges-srl-ontonotes_f1: training: 0.679926 validation: 0.685354
09/16 09:17:01 AM: Global learning rate: 0.0001
09/16 09:17:01 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:17:04 AM: Update 5009: task edges-srl-ontonotes, batch 9 (5009): mcc: 0.6776, acc: 0.5816, precision: 0.7625, recall: 0.6098, f1: 0.6777, edges-srl-ontonotes_loss: 0.0248
09/16 09:17:14 AM: Update 5202: task edges-srl-ontonotes, batch 202 (5202): mcc: 0.7056, acc: 0.6088, precision: 0.7788, recall: 0.6466, f1: 0.7066, edges-srl-ontonotes_loss: 0.0213
09/16 09:17:24 AM: Update 5389: task edges-srl-ontonotes, batch 389 (5389): mcc: 0.7205, acc: 0.6269, precision: 0.7900, recall: 0.6640, f1: 0.7216, edges-srl-ontonotes_loss: 0.0205
09/16 09:17:34 AM: Update 5574: task edges-srl-ontonotes, batch 574 (5574): mcc: 0.7352, acc: 0.6451, precision: 0.8002, recall: 0.6823, f1: 0.7366, edges-srl-ontonotes_loss: 0.0196
09/16 09:17:44 AM: Update 5762: task edges-srl-ontonotes, batch 762 (5762): mcc: 0.7421, acc: 0.6541, precision: 0.8047, recall: 0.6910, f1: 0.7436, edges-srl-ontonotes_loss: 0.0192
09/16 09:17:56 AM: Update 5948: task edges-srl-ontonotes, batch 948 (5948): mcc: 0.7472, acc: 0.6605, precision: 0.8085, recall: 0.6971, f1: 0.7487, edges-srl-ontonotes_loss: 0.0189
09/16 09:17:58 AM: ***** Step 6000 / Validation 6 *****
09/16 09:17:58 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:17:58 AM: Validating...
09/16 09:18:03 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:18:03 AM: Best result seen so far for macro.
09/16 09:18:03 AM: Updating LR scheduler:
09/16 09:18:03 AM: 	Best result seen so far for macro_avg: 0.720
09/16 09:18:03 AM: 	# validation passes without improvement: 0
09/16 09:18:03 AM: edges-srl-ontonotes_loss: training: 0.018787 validation: 0.021199
09/16 09:18:03 AM: macro_avg: validation: 0.719654
09/16 09:18:03 AM: micro_avg: validation: 0.000000
09/16 09:18:03 AM: edges-srl-ontonotes_mcc: training: 0.748907 validation: 0.719642
09/16 09:18:03 AM: edges-srl-ontonotes_acc: training: 0.662776 validation: 0.644677
09/16 09:18:03 AM: edges-srl-ontonotes_precision: training: 0.809792 validation: 0.801360
09/16 09:18:03 AM: edges-srl-ontonotes_recall: training: 0.699100 validation: 0.653068
09/16 09:18:03 AM: edges-srl-ontonotes_f1: training: 0.750386 validation: 0.719654
09/16 09:18:03 AM: Global learning rate: 0.0001
09/16 09:18:03 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:18:06 AM: Update 6051: task edges-srl-ontonotes, batch 51 (6051): mcc: 0.7714, acc: 0.6945, precision: 0.8216, recall: 0.7303, f1: 0.7733, edges-srl-ontonotes_loss: 0.0174
09/16 09:18:16 AM: Update 6247: task edges-srl-ontonotes, batch 247 (6247): mcc: 0.7815, acc: 0.7071, precision: 0.8318, recall: 0.7401, f1: 0.7833, edges-srl-ontonotes_loss: 0.0168
09/16 09:18:26 AM: Update 6435: task edges-srl-ontonotes, batch 435 (6435): mcc: 0.7963, acc: 0.7261, precision: 0.8445, recall: 0.7563, f1: 0.7980, edges-srl-ontonotes_loss: 0.0162
09/16 09:18:36 AM: Update 6607: task edges-srl-ontonotes, batch 607 (6607): mcc: 0.7983, acc: 0.7301, precision: 0.8455, recall: 0.7591, f1: 0.8000, edges-srl-ontonotes_loss: 0.0162
09/16 09:18:46 AM: Update 6832: task edges-srl-ontonotes, batch 832 (6832): mcc: 0.7784, acc: 0.7053, precision: 0.8295, recall: 0.7364, f1: 0.7802, edges-srl-ontonotes_loss: 0.0174
09/16 09:18:55 AM: ***** Step 7000 / Validation 7 *****
09/16 09:18:55 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:18:55 AM: Validating...
09/16 09:18:56 AM: Evaluate: task edges-srl-ontonotes, batch 22 (157): mcc: 0.7451, acc: 0.6665, precision: 0.8218, recall: 0.6819, f1: 0.7453, edges-srl-ontonotes_loss: 0.0193
09/16 09:19:02 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:19:02 AM: Best result seen so far for macro.
09/16 09:19:02 AM: Updating LR scheduler:
09/16 09:19:02 AM: 	Best result seen so far for macro_avg: 0.732
09/16 09:19:02 AM: 	# validation passes without improvement: 0
09/16 09:19:02 AM: edges-srl-ontonotes_loss: training: 0.018220 validation: 0.020161
09/16 09:19:02 AM: macro_avg: validation: 0.731744
09/16 09:19:02 AM: micro_avg: validation: 0.000000
09/16 09:19:02 AM: edges-srl-ontonotes_mcc: training: 0.763982 validation: 0.731211
09/16 09:19:02 AM: edges-srl-ontonotes_acc: training: 0.687109 validation: 0.658610
09/16 09:19:02 AM: edges-srl-ontonotes_precision: training: 0.818519 validation: 0.806132
09/16 09:19:02 AM: edges-srl-ontonotes_recall: training: 0.719307 validation: 0.669925
09/16 09:19:02 AM: edges-srl-ontonotes_f1: training: 0.765712 validation: 0.731744
09/16 09:19:02 AM: Global learning rate: 0.0001
09/16 09:19:02 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:19:06 AM: Update 7074: task edges-srl-ontonotes, batch 74 (7074): mcc: 0.6985, acc: 0.6035, precision: 0.7670, recall: 0.6438, f1: 0.7000, edges-srl-ontonotes_loss: 0.0221
09/16 09:19:17 AM: Update 7247: task edges-srl-ontonotes, batch 247 (7247): mcc: 0.6983, acc: 0.6013, precision: 0.7719, recall: 0.6392, f1: 0.6993, edges-srl-ontonotes_loss: 0.0221
09/16 09:19:27 AM: Update 7465: task edges-srl-ontonotes, batch 465 (7465): mcc: 0.7148, acc: 0.6228, precision: 0.7845, recall: 0.6585, f1: 0.7160, edges-srl-ontonotes_loss: 0.0210
09/16 09:19:37 AM: Update 7617: task edges-srl-ontonotes, batch 617 (7617): mcc: 0.7220, acc: 0.6317, precision: 0.7891, recall: 0.6676, f1: 0.7233, edges-srl-ontonotes_loss: 0.0205
09/16 09:19:47 AM: Update 7790: task edges-srl-ontonotes, batch 790 (7790): mcc: 0.7285, acc: 0.6400, precision: 0.7942, recall: 0.6752, f1: 0.7298, edges-srl-ontonotes_loss: 0.0200
09/16 09:19:57 AM: Update 7949: task edges-srl-ontonotes, batch 949 (7949): mcc: 0.7300, acc: 0.6422, precision: 0.7950, recall: 0.6773, f1: 0.7314, edges-srl-ontonotes_loss: 0.0200
09/16 09:20:00 AM: ***** Step 8000 / Validation 8 *****
09/16 09:20:00 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:20:00 AM: Validating...
09/16 09:20:07 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:20:07 AM: Best result seen so far for macro.
09/16 09:20:07 AM: Updating LR scheduler:
09/16 09:20:07 AM: 	Best result seen so far for macro_avg: 0.748
09/16 09:20:07 AM: 	# validation passes without improvement: 0
09/16 09:20:07 AM: edges-srl-ontonotes_loss: training: 0.019959 validation: 0.018774
09/16 09:20:07 AM: macro_avg: validation: 0.748090
09/16 09:20:07 AM: micro_avg: validation: 0.000000
09/16 09:20:07 AM: edges-srl-ontonotes_mcc: training: 0.729599 validation: 0.747322
09/16 09:20:07 AM: edges-srl-ontonotes_acc: training: 0.641672 validation: 0.679086
09/16 09:20:07 AM: edges-srl-ontonotes_precision: training: 0.794280 validation: 0.817485
09/16 09:20:07 AM: edges-srl-ontonotes_recall: training: 0.677091 validation: 0.689554
09/16 09:20:07 AM: edges-srl-ontonotes_f1: training: 0.731019 validation: 0.748090
09/16 09:20:07 AM: Global learning rate: 0.0001
09/16 09:20:07 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:20:07 AM: Update 8003: task edges-srl-ontonotes, batch 3 (8003): mcc: 0.7342, acc: 0.6628, precision: 0.7937, recall: 0.6860, f1: 0.7360, edges-srl-ontonotes_loss: 0.0196
09/16 09:20:19 AM: Update 8186: task edges-srl-ontonotes, batch 186 (8186): mcc: 0.7123, acc: 0.6242, precision: 0.7763, recall: 0.6609, f1: 0.7140, edges-srl-ontonotes_loss: 0.0209
09/16 09:20:29 AM: Update 8382: task edges-srl-ontonotes, batch 382 (8382): mcc: 0.7047, acc: 0.6136, precision: 0.7720, recall: 0.6508, f1: 0.7062, edges-srl-ontonotes_loss: 0.0214
09/16 09:20:39 AM: Update 8538: task edges-srl-ontonotes, batch 538 (8538): mcc: 0.7009, acc: 0.6087, precision: 0.7698, recall: 0.6457, f1: 0.7023, edges-srl-ontonotes_loss: 0.0216
09/16 09:20:49 AM: Update 8762: task edges-srl-ontonotes, batch 762 (8762): mcc: 0.7015, acc: 0.6086, precision: 0.7702, recall: 0.6465, f1: 0.7029, edges-srl-ontonotes_loss: 0.0215
09/16 09:20:59 AM: Update 8938: task edges-srl-ontonotes, batch 938 (8938): mcc: 0.6973, acc: 0.6033, precision: 0.7670, recall: 0.6416, f1: 0.6987, edges-srl-ontonotes_loss: 0.0217
09/16 09:21:02 AM: ***** Step 9000 / Validation 9 *****
09/16 09:21:02 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:21:02 AM: Validating...
09/16 09:21:08 AM: Updating LR scheduler:
09/16 09:21:08 AM: 	Best result seen so far for macro_avg: 0.748
09/16 09:21:08 AM: 	# validation passes without improvement: 1
09/16 09:21:08 AM: edges-srl-ontonotes_loss: training: 0.021825 validation: 0.018741
09/16 09:21:08 AM: macro_avg: validation: 0.742725
09/16 09:21:08 AM: micro_avg: validation: 0.000000
09/16 09:21:08 AM: edges-srl-ontonotes_mcc: training: 0.695862 validation: 0.742664
09/16 09:21:08 AM: edges-srl-ontonotes_acc: training: 0.601454 validation: 0.667154
09/16 09:21:08 AM: edges-srl-ontonotes_precision: training: 0.766024 validation: 0.821439
09/16 09:21:08 AM: edges-srl-ontonotes_recall: training: 0.639727 validation: 0.677777
09/16 09:21:08 AM: edges-srl-ontonotes_f1: training: 0.697202 validation: 0.742725
09/16 09:21:08 AM: Global learning rate: 0.0001
09/16 09:21:08 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:21:09 AM: Update 9025: task edges-srl-ontonotes, batch 25 (9025): mcc: 0.6681, acc: 0.5620, precision: 0.7526, recall: 0.6010, f1: 0.6683, edges-srl-ontonotes_loss: 0.0234
09/16 09:21:19 AM: Update 9151: task edges-srl-ontonotes, batch 151 (9151): mcc: 0.6776, acc: 0.5797, precision: 0.7564, recall: 0.6149, f1: 0.6783, edges-srl-ontonotes_loss: 0.0230
09/16 09:21:29 AM: Update 9383: task edges-srl-ontonotes, batch 383 (9383): mcc: 0.6808, acc: 0.5818, precision: 0.7599, recall: 0.6176, f1: 0.6814, edges-srl-ontonotes_loss: 0.0228
09/16 09:21:39 AM: Update 9585: task edges-srl-ontonotes, batch 585 (9585): mcc: 0.6863, acc: 0.5893, precision: 0.7634, recall: 0.6246, f1: 0.6871, edges-srl-ontonotes_loss: 0.0224
09/16 09:21:49 AM: Update 9752: task edges-srl-ontonotes, batch 752 (9752): mcc: 0.6862, acc: 0.5896, precision: 0.7619, recall: 0.6258, f1: 0.6872, edges-srl-ontonotes_loss: 0.0224
09/16 09:21:59 AM: Update 9933: task edges-srl-ontonotes, batch 933 (9933): mcc: 0.6896, acc: 0.5932, precision: 0.7636, recall: 0.6305, f1: 0.6907, edges-srl-ontonotes_loss: 0.0221
09/16 09:22:03 AM: ***** Step 10000 / Validation 10 *****
09/16 09:22:03 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:22:03 AM: Validating...
09/16 09:22:09 AM: Evaluate: task edges-srl-ontonotes, batch 151 (157): mcc: 0.7258, acc: 0.6557, precision: 0.7993, recall: 0.6659, f1: 0.7266, edges-srl-ontonotes_loss: 0.0195
09/16 09:22:10 AM: Updating LR scheduler:
09/16 09:22:10 AM: 	Best result seen so far for macro_avg: 0.748
09/16 09:22:10 AM: 	# validation passes without improvement: 2
09/16 09:22:10 AM: edges-srl-ontonotes_loss: training: 0.022097 validation: 0.019617
09/16 09:22:10 AM: macro_avg: validation: 0.725714
09/16 09:22:10 AM: micro_avg: validation: 0.000000
09/16 09:22:10 AM: edges-srl-ontonotes_mcc: training: 0.690228 validation: 0.725026
09/16 09:22:10 AM: edges-srl-ontonotes_acc: training: 0.593904 validation: 0.654453
09/16 09:22:10 AM: edges-srl-ontonotes_precision: training: 0.763888 validation: 0.798964
09/16 09:22:10 AM: edges-srl-ontonotes_recall: training: 0.631326 validation: 0.664768
09/16 09:22:10 AM: edges-srl-ontonotes_f1: training: 0.691310 validation: 0.725714
09/16 09:22:10 AM: Global learning rate: 0.0001
09/16 09:22:10 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:22:19 AM: Update 10152: task edges-srl-ontonotes, batch 152 (10152): mcc: 0.7141, acc: 0.6257, precision: 0.7794, recall: 0.6615, f1: 0.7157, edges-srl-ontonotes_loss: 0.0209
09/16 09:22:29 AM: Update 10327: task edges-srl-ontonotes, batch 327 (10327): mcc: 0.7120, acc: 0.6216, precision: 0.7788, recall: 0.6582, f1: 0.7135, edges-srl-ontonotes_loss: 0.0210
09/16 09:22:39 AM: Update 10524: task edges-srl-ontonotes, batch 524 (10524): mcc: 0.7095, acc: 0.6187, precision: 0.7777, recall: 0.6545, f1: 0.7108, edges-srl-ontonotes_loss: 0.0211
09/16 09:22:49 AM: Update 10714: task edges-srl-ontonotes, batch 714 (10714): mcc: 0.7069, acc: 0.6155, precision: 0.7754, recall: 0.6519, f1: 0.7083, edges-srl-ontonotes_loss: 0.0213
09/16 09:22:59 AM: Update 10938: task edges-srl-ontonotes, batch 938 (10938): mcc: 0.7059, acc: 0.6147, precision: 0.7749, recall: 0.6505, f1: 0.7073, edges-srl-ontonotes_loss: 0.0213
09/16 09:23:02 AM: ***** Step 11000 / Validation 11 *****
09/16 09:23:02 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:23:02 AM: Validating...
09/16 09:23:08 AM: Updating LR scheduler:
09/16 09:23:08 AM: 	Best result seen so far for macro_avg: 0.748
09/16 09:23:08 AM: 	# validation passes without improvement: 3
09/16 09:23:08 AM: edges-srl-ontonotes_loss: training: 0.021310 validation: 0.020084
09/16 09:23:08 AM: macro_avg: validation: 0.722066
09/16 09:23:08 AM: micro_avg: validation: 0.000000
09/16 09:23:08 AM: edges-srl-ontonotes_mcc: training: 0.705823 validation: 0.721381
09/16 09:23:08 AM: edges-srl-ontonotes_acc: training: 0.614604 validation: 0.650296
09/16 09:23:08 AM: edges-srl-ontonotes_precision: training: 0.774913 validation: 0.795792
09/16 09:23:08 AM: edges-srl-ontonotes_recall: training: 0.650283 validation: 0.660842
09/16 09:23:08 AM: edges-srl-ontonotes_f1: training: 0.707149 validation: 0.722066
09/16 09:23:08 AM: Global learning rate: 0.0001
09/16 09:23:08 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:23:10 AM: Update 11024: task edges-srl-ontonotes, batch 24 (11024): mcc: 0.6964, acc: 0.6045, precision: 0.7571, recall: 0.6484, f1: 0.6985, edges-srl-ontonotes_loss: 0.0216
09/16 09:23:20 AM: Update 11235: task edges-srl-ontonotes, batch 235 (11235): mcc: 0.6985, acc: 0.6070, precision: 0.7682, recall: 0.6426, f1: 0.6998, edges-srl-ontonotes_loss: 0.0215
09/16 09:23:30 AM: Update 11392: task edges-srl-ontonotes, batch 392 (11392): mcc: 0.7019, acc: 0.6107, precision: 0.7712, recall: 0.6463, f1: 0.7032, edges-srl-ontonotes_loss: 0.0214
09/16 09:23:40 AM: Update 11590: task edges-srl-ontonotes, batch 590 (11590): mcc: 0.7048, acc: 0.6153, precision: 0.7725, recall: 0.6504, f1: 0.7062, edges-srl-ontonotes_loss: 0.0212
09/16 09:23:50 AM: Update 11785: task edges-srl-ontonotes, batch 785 (11785): mcc: 0.7061, acc: 0.6160, precision: 0.7736, recall: 0.6518, f1: 0.7075, edges-srl-ontonotes_loss: 0.0211
09/16 09:24:00 AM: Update 11968: task edges-srl-ontonotes, batch 968 (11968): mcc: 0.7062, acc: 0.6165, precision: 0.7738, recall: 0.6519, f1: 0.7076, edges-srl-ontonotes_loss: 0.0211
09/16 09:24:01 AM: ***** Step 12000 / Validation 12 *****
09/16 09:24:01 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:24:01 AM: Validating...
09/16 09:24:07 AM: Updating LR scheduler:
09/16 09:24:07 AM: 	Best result seen so far for macro_avg: 0.748
09/16 09:24:07 AM: 	# validation passes without improvement: 0
09/16 09:24:07 AM: edges-srl-ontonotes_loss: training: 0.021169 validation: 0.020074
09/16 09:24:07 AM: macro_avg: validation: 0.715743
09/16 09:24:07 AM: micro_avg: validation: 0.000000
09/16 09:24:07 AM: edges-srl-ontonotes_mcc: training: 0.705450 validation: 0.715489
09/16 09:24:07 AM: edges-srl-ontonotes_acc: training: 0.615550 validation: 0.643215
09/16 09:24:07 AM: edges-srl-ontonotes_precision: training: 0.773403 validation: 0.795147
09/16 09:24:07 AM: edges-srl-ontonotes_recall: training: 0.650890 validation: 0.650758
09/16 09:24:07 AM: edges-srl-ontonotes_f1: training: 0.706877 validation: 0.715743
09/16 09:24:07 AM: Global learning rate: 5e-05
09/16 09:24:07 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:24:10 AM: Update 12048: task edges-srl-ontonotes, batch 48 (12048): mcc: 0.7003, acc: 0.6112, precision: 0.7768, recall: 0.6387, f1: 0.7010, edges-srl-ontonotes_loss: 0.0220
09/16 09:24:20 AM: Update 12236: task edges-srl-ontonotes, batch 236 (12236): mcc: 0.6983, acc: 0.6046, precision: 0.7717, recall: 0.6393, f1: 0.6993, edges-srl-ontonotes_loss: 0.0216
09/16 09:24:30 AM: Update 12364: task edges-srl-ontonotes, batch 364 (12364): mcc: 0.7081, acc: 0.6172, precision: 0.7776, recall: 0.6521, f1: 0.7094, edges-srl-ontonotes_loss: 0.0210
09/16 09:24:40 AM: Update 12559: task edges-srl-ontonotes, batch 559 (12559): mcc: 0.7213, acc: 0.6334, precision: 0.7863, recall: 0.6688, f1: 0.7228, edges-srl-ontonotes_loss: 0.0201
09/16 09:24:50 AM: Update 12750: task edges-srl-ontonotes, batch 750 (12750): mcc: 0.7348, acc: 0.6493, precision: 0.7953, recall: 0.6857, f1: 0.7365, edges-srl-ontonotes_loss: 0.0193
09/16 09:25:00 AM: Update 12926: task edges-srl-ontonotes, batch 926 (12926): mcc: 0.7444, acc: 0.6612, precision: 0.8021, recall: 0.6976, f1: 0.7462, edges-srl-ontonotes_loss: 0.0187
09/16 09:25:03 AM: ***** Step 13000 / Validation 13 *****
09/16 09:25:03 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:25:03 AM: Validating...
09/16 09:25:09 AM: Updating LR scheduler:
09/16 09:25:09 AM: 	Best result seen so far for macro_avg: 0.748
09/16 09:25:09 AM: 	# validation passes without improvement: 1
09/16 09:25:09 AM: edges-srl-ontonotes_loss: training: 0.018596 validation: 0.019433
09/16 09:25:09 AM: macro_avg: validation: 0.742216
09/16 09:25:09 AM: micro_avg: validation: 0.000000
09/16 09:25:09 AM: edges-srl-ontonotes_mcc: training: 0.746463 validation: 0.741021
09/16 09:25:09 AM: edges-srl-ontonotes_acc: training: 0.663826 validation: 0.674467
09/16 09:25:09 AM: edges-srl-ontonotes_precision: training: 0.803802 validation: 0.806851
09/16 09:25:09 AM: edges-srl-ontonotes_recall: training: 0.699839 validation: 0.687168
09/16 09:25:09 AM: edges-srl-ontonotes_f1: training: 0.748227 validation: 0.742216
09/16 09:25:09 AM: Global learning rate: 5e-05
09/16 09:25:09 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:25:10 AM: Update 13016: task edges-srl-ontonotes, batch 16 (13016): mcc: 0.7627, acc: 0.6940, precision: 0.8132, recall: 0.7217, f1: 0.7647, edges-srl-ontonotes_loss: 0.0173
09/16 09:25:22 AM: Update 13194: task edges-srl-ontonotes, batch 194 (13194): mcc: 0.7761, acc: 0.7022, precision: 0.8254, recall: 0.7357, f1: 0.7780, edges-srl-ontonotes_loss: 0.0166
09/16 09:25:32 AM: Update 13397: task edges-srl-ontonotes, batch 397 (13397): mcc: 0.7837, acc: 0.7125, precision: 0.8305, recall: 0.7454, f1: 0.7857, edges-srl-ontonotes_loss: 0.0161
09/16 09:25:42 AM: Update 13582: task edges-srl-ontonotes, batch 582 (13582): mcc: 0.7900, acc: 0.7205, precision: 0.8366, recall: 0.7517, f1: 0.7919, edges-srl-ontonotes_loss: 0.0159
09/16 09:25:52 AM: Update 13781: task edges-srl-ontonotes, batch 781 (13781): mcc: 0.8002, acc: 0.7337, precision: 0.8446, recall: 0.7635, f1: 0.8020, edges-srl-ontonotes_loss: 0.0154
09/16 09:26:02 AM: Update 13972: task edges-srl-ontonotes, batch 972 (13972): mcc: 0.7916, acc: 0.7230, precision: 0.8380, recall: 0.7533, f1: 0.7934, edges-srl-ontonotes_loss: 0.0160
09/16 09:26:03 AM: ***** Step 14000 / Validation 14 *****
09/16 09:26:03 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:26:03 AM: Validating...
09/16 09:26:09 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:26:09 AM: Best result seen so far for macro.
09/16 09:26:09 AM: Updating LR scheduler:
09/16 09:26:09 AM: 	Best result seen so far for macro_avg: 0.752
09/16 09:26:09 AM: 	# validation passes without improvement: 0
09/16 09:26:09 AM: edges-srl-ontonotes_loss: training: 0.016123 validation: 0.018519
09/16 09:26:09 AM: macro_avg: validation: 0.752286
09/16 09:26:09 AM: micro_avg: validation: 0.000000
09/16 09:26:09 AM: edges-srl-ontonotes_mcc: training: 0.789523 validation: 0.751253
09/16 09:26:09 AM: edges-srl-ontonotes_acc: training: 0.720386 validation: 0.685398
09/16 09:26:09 AM: edges-srl-ontonotes_precision: training: 0.836496 validation: 0.817599
09/16 09:26:09 AM: edges-srl-ontonotes_recall: training: 0.750870 validation: 0.696636
09/16 09:26:09 AM: edges-srl-ontonotes_f1: training: 0.791374 validation: 0.752286
09/16 09:26:09 AM: Global learning rate: 5e-05
09/16 09:26:09 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:26:12 AM: Update 14076: task edges-srl-ontonotes, batch 76 (14076): mcc: 0.7477, acc: 0.6697, precision: 0.8042, recall: 0.7018, f1: 0.7495, edges-srl-ontonotes_loss: 0.0187
09/16 09:26:22 AM: Update 14282: task edges-srl-ontonotes, batch 282 (14282): mcc: 0.7197, acc: 0.6350, precision: 0.7823, recall: 0.6693, f1: 0.7214, edges-srl-ontonotes_loss: 0.0203
09/16 09:26:32 AM: Update 14492: task edges-srl-ontonotes, batch 492 (14492): mcc: 0.7158, acc: 0.6296, precision: 0.7806, recall: 0.6636, f1: 0.7174, edges-srl-ontonotes_loss: 0.0206
09/16 09:26:42 AM: Update 14635: task edges-srl-ontonotes, batch 635 (14635): mcc: 0.7242, acc: 0.6395, precision: 0.7879, recall: 0.6726, f1: 0.7257, edges-srl-ontonotes_loss: 0.0200
09/16 09:26:52 AM: Update 14819: task edges-srl-ontonotes, batch 819 (14819): mcc: 0.7303, acc: 0.6466, precision: 0.7932, recall: 0.6793, f1: 0.7318, edges-srl-ontonotes_loss: 0.0196
09/16 09:27:01 AM: ***** Step 15000 / Validation 15 *****
09/16 09:27:01 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:27:01 AM: Validating...
09/16 09:27:02 AM: Evaluate: task edges-srl-ontonotes, batch 20 (157): mcc: 0.7675, acc: 0.6979, precision: 0.8399, recall: 0.7071, f1: 0.7678, edges-srl-ontonotes_loss: 0.0173
09/16 09:27:07 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:27:07 AM: Best result seen so far for macro.
09/16 09:27:07 AM: Updating LR scheduler:
09/16 09:27:07 AM: 	Best result seen so far for macro_avg: 0.760
09/16 09:27:07 AM: 	# validation passes without improvement: 0
09/16 09:27:07 AM: edges-srl-ontonotes_loss: training: 0.019249 validation: 0.017789
09/16 09:27:07 AM: macro_avg: validation: 0.759611
09/16 09:27:07 AM: micro_avg: validation: 0.000000
09/16 09:27:07 AM: edges-srl-ontonotes_mcc: training: 0.736066 validation: 0.758778
09/16 09:27:07 AM: edges-srl-ontonotes_acc: training: 0.653430 validation: 0.691556
09/16 09:27:07 AM: edges-srl-ontonotes_precision: training: 0.797768 validation: 0.826528
09/16 09:27:07 AM: edges-srl-ontonotes_recall: training: 0.685941 validation: 0.702717
09/16 09:27:07 AM: edges-srl-ontonotes_f1: training: 0.737640 validation: 0.759611
09/16 09:27:07 AM: Global learning rate: 5e-05
09/16 09:27:07 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:27:12 AM: Update 15116: task edges-srl-ontonotes, batch 116 (15116): mcc: 0.7707, acc: 0.6941, precision: 0.8228, recall: 0.7281, f1: 0.7725, edges-srl-ontonotes_loss: 0.0172
09/16 09:27:22 AM: Update 15303: task edges-srl-ontonotes, batch 303 (15303): mcc: 0.7425, acc: 0.6602, precision: 0.8007, recall: 0.6953, f1: 0.7443, edges-srl-ontonotes_loss: 0.0190
09/16 09:27:32 AM: Update 15432: task edges-srl-ontonotes, batch 432 (15432): mcc: 0.7392, acc: 0.6567, precision: 0.7976, recall: 0.6918, f1: 0.7410, edges-srl-ontonotes_loss: 0.0191
09/16 09:27:42 AM: Update 15643: task edges-srl-ontonotes, batch 643 (15643): mcc: 0.7300, acc: 0.6456, precision: 0.7911, recall: 0.6805, f1: 0.7317, edges-srl-ontonotes_loss: 0.0197
09/16 09:27:53 AM: Update 15827: task edges-srl-ontonotes, batch 827 (15827): mcc: 0.7268, acc: 0.6412, precision: 0.7884, recall: 0.6771, f1: 0.7285, edges-srl-ontonotes_loss: 0.0199
09/16 09:28:01 AM: ***** Step 16000 / Validation 16 *****
09/16 09:28:01 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:28:01 AM: Validating...
09/16 09:28:03 AM: Evaluate: task edges-srl-ontonotes, batch 47 (157): mcc: 0.7503, acc: 0.6863, precision: 0.8110, recall: 0.7006, f1: 0.7517, edges-srl-ontonotes_loss: 0.0188
09/16 09:28:06 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:28:06 AM: Best result seen so far for macro.
09/16 09:28:06 AM: Updating LR scheduler:
09/16 09:28:06 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:28:06 AM: 	# validation passes without improvement: 0
09/16 09:28:06 AM: edges-srl-ontonotes_loss: training: 0.019974 validation: 0.017712
09/16 09:28:06 AM: macro_avg: validation: 0.761952
09/16 09:28:06 AM: micro_avg: validation: 0.000000
09/16 09:28:06 AM: edges-srl-ontonotes_mcc: training: 0.724025 validation: 0.760355
09/16 09:28:06 AM: edges-srl-ontonotes_acc: training: 0.637854 validation: 0.699099
09/16 09:28:06 AM: edges-srl-ontonotes_precision: training: 0.785660 validation: 0.817572
09/16 09:28:06 AM: edges-srl-ontonotes_recall: training: 0.674328 validation: 0.713417
09/16 09:28:06 AM: edges-srl-ontonotes_f1: training: 0.725749 validation: 0.761952
09/16 09:28:06 AM: Global learning rate: 5e-05
09/16 09:28:06 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:28:13 AM: Update 16101: task edges-srl-ontonotes, batch 101 (16101): mcc: 0.7063, acc: 0.6155, precision: 0.7704, recall: 0.6550, f1: 0.7080, edges-srl-ontonotes_loss: 0.0210
09/16 09:28:23 AM: Update 16319: task edges-srl-ontonotes, batch 319 (16319): mcc: 0.6914, acc: 0.5981, precision: 0.7603, recall: 0.6364, f1: 0.6929, edges-srl-ontonotes_loss: 0.0219
09/16 09:28:33 AM: Update 16491: task edges-srl-ontonotes, batch 491 (16491): mcc: 0.6915, acc: 0.5980, precision: 0.7624, recall: 0.6349, f1: 0.6928, edges-srl-ontonotes_loss: 0.0219
09/16 09:28:43 AM: Update 16688: task edges-srl-ontonotes, batch 688 (16688): mcc: 0.6920, acc: 0.5984, precision: 0.7640, recall: 0.6344, f1: 0.6932, edges-srl-ontonotes_loss: 0.0219
09/16 09:28:53 AM: Update 16908: task edges-srl-ontonotes, batch 908 (16908): mcc: 0.6960, acc: 0.6034, precision: 0.7660, recall: 0.6400, f1: 0.6974, edges-srl-ontonotes_loss: 0.0215
09/16 09:28:58 AM: ***** Step 17000 / Validation 17 *****
09/16 09:28:58 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:28:58 AM: Validating...
09/16 09:29:03 AM: Evaluate: task edges-srl-ontonotes, batch 115 (157): mcc: 0.7435, acc: 0.6725, precision: 0.8182, recall: 0.6820, f1: 0.7439, edges-srl-ontonotes_loss: 0.0181
09/16 09:29:04 AM: Updating LR scheduler:
09/16 09:29:04 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:29:04 AM: 	# validation passes without improvement: 1
09/16 09:29:04 AM: edges-srl-ontonotes_loss: training: 0.021485 validation: 0.018340
09/16 09:29:04 AM: macro_avg: validation: 0.742226
09/16 09:29:04 AM: micro_avg: validation: 0.000000
09/16 09:29:04 AM: edges-srl-ontonotes_mcc: training: 0.696797 validation: 0.741854
09/16 09:29:04 AM: edges-srl-ontonotes_acc: training: 0.604692 validation: 0.670464
09/16 09:29:04 AM: edges-srl-ontonotes_precision: training: 0.766170 validation: 0.817307
09/16 09:29:04 AM: edges-srl-ontonotes_recall: training: 0.641302 validation: 0.679778
09/16 09:29:04 AM: edges-srl-ontonotes_f1: training: 0.698197 validation: 0.742226
09/16 09:29:04 AM: Global learning rate: 5e-05
09/16 09:29:04 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:29:13 AM: Update 17191: task edges-srl-ontonotes, batch 191 (17191): mcc: 0.7189, acc: 0.6339, precision: 0.7807, recall: 0.6693, f1: 0.7207, edges-srl-ontonotes_loss: 0.0203
09/16 09:29:23 AM: Update 17391: task edges-srl-ontonotes, batch 391 (17391): mcc: 0.7179, acc: 0.6320, precision: 0.7803, recall: 0.6678, f1: 0.7197, edges-srl-ontonotes_loss: 0.0203
09/16 09:29:33 AM: Update 17595: task edges-srl-ontonotes, batch 595 (17595): mcc: 0.7217, acc: 0.6365, precision: 0.7838, recall: 0.6716, f1: 0.7234, edges-srl-ontonotes_loss: 0.0201
09/16 09:29:43 AM: Update 17762: task edges-srl-ontonotes, batch 762 (17762): mcc: 0.7187, acc: 0.6327, precision: 0.7812, recall: 0.6684, f1: 0.7204, edges-srl-ontonotes_loss: 0.0203
09/16 09:29:53 AM: Update 17966: task edges-srl-ontonotes, batch 966 (17966): mcc: 0.7164, acc: 0.6293, precision: 0.7798, recall: 0.6654, f1: 0.7181, edges-srl-ontonotes_loss: 0.0205
09/16 09:29:54 AM: ***** Step 18000 / Validation 18 *****
09/16 09:29:54 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:29:54 AM: Validating...
09/16 09:29:59 AM: Updating LR scheduler:
09/16 09:29:59 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:29:59 AM: 	# validation passes without improvement: 2
09/16 09:29:59 AM: edges-srl-ontonotes_loss: training: 0.020477 validation: 0.018633
09/16 09:29:59 AM: macro_avg: validation: 0.742816
09/16 09:29:59 AM: micro_avg: validation: 0.000000
09/16 09:29:59 AM: edges-srl-ontonotes_mcc: training: 0.716233 validation: 0.742195
09/16 09:29:59 AM: edges-srl-ontonotes_acc: training: 0.629145 validation: 0.673851
09/16 09:29:59 AM: edges-srl-ontonotes_precision: training: 0.779891 validation: 0.814757
09/16 09:29:59 AM: edges-srl-ontonotes_recall: training: 0.665025 validation: 0.682549
09/16 09:29:59 AM: edges-srl-ontonotes_f1: training: 0.717892 validation: 0.742816
09/16 09:29:59 AM: Global learning rate: 5e-05
09/16 09:29:59 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:30:03 AM: Update 18071: task edges-srl-ontonotes, batch 71 (18071): mcc: 0.7152, acc: 0.6286, precision: 0.7845, recall: 0.6593, f1: 0.7164, edges-srl-ontonotes_loss: 0.0207
09/16 09:30:13 AM: Update 18249: task edges-srl-ontonotes, batch 249 (18249): mcc: 0.7168, acc: 0.6288, precision: 0.7822, recall: 0.6641, f1: 0.7183, edges-srl-ontonotes_loss: 0.0204
09/16 09:30:23 AM: Update 18430: task edges-srl-ontonotes, batch 430 (18430): mcc: 0.7130, acc: 0.6241, precision: 0.7788, recall: 0.6602, f1: 0.7146, edges-srl-ontonotes_loss: 0.0206
09/16 09:30:34 AM: Update 18562: task edges-srl-ontonotes, batch 562 (18562): mcc: 0.7133, acc: 0.6245, precision: 0.7792, recall: 0.6602, f1: 0.7148, edges-srl-ontonotes_loss: 0.0206
09/16 09:30:44 AM: Update 18785: task edges-srl-ontonotes, batch 785 (18785): mcc: 0.7152, acc: 0.6277, precision: 0.7801, recall: 0.6630, f1: 0.7168, edges-srl-ontonotes_loss: 0.0205
09/16 09:30:53 AM: ***** Step 19000 / Validation 19 *****
09/16 09:30:53 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:30:53 AM: Validating...
09/16 09:30:54 AM: Evaluate: task edges-srl-ontonotes, batch 3 (157): mcc: 0.7412, acc: 0.6566, precision: 0.8211, recall: 0.6755, f1: 0.7412, edges-srl-ontonotes_loss: 0.0191
09/16 09:30:59 AM: Updating LR scheduler:
09/16 09:30:59 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:30:59 AM: 	# validation passes without improvement: 3
09/16 09:30:59 AM: edges-srl-ontonotes_loss: training: 0.020456 validation: 0.018903
09/16 09:30:59 AM: macro_avg: validation: 0.740446
09/16 09:30:59 AM: micro_avg: validation: 0.000000
09/16 09:30:59 AM: edges-srl-ontonotes_mcc: training: 0.716583 validation: 0.739017
09/16 09:30:59 AM: edges-srl-ontonotes_acc: training: 0.629379 validation: 0.674698
09/16 09:30:59 AM: edges-srl-ontonotes_precision: training: 0.781115 validation: 0.802155
09/16 09:30:59 AM: edges-srl-ontonotes_recall: training: 0.664608 validation: 0.687553
09/16 09:30:59 AM: edges-srl-ontonotes_f1: training: 0.718167 validation: 0.740446
09/16 09:30:59 AM: Global learning rate: 5e-05
09/16 09:30:59 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:31:04 AM: Update 19102: task edges-srl-ontonotes, batch 102 (19102): mcc: 0.7235, acc: 0.6395, precision: 0.7851, recall: 0.6739, f1: 0.7252, edges-srl-ontonotes_loss: 0.0201
09/16 09:31:14 AM: Update 19288: task edges-srl-ontonotes, batch 288 (19288): mcc: 0.7124, acc: 0.6259, precision: 0.7777, recall: 0.6600, f1: 0.7140, edges-srl-ontonotes_loss: 0.0207
09/16 09:31:24 AM: Update 19493: task edges-srl-ontonotes, batch 493 (19493): mcc: 0.7115, acc: 0.6235, precision: 0.7800, recall: 0.6563, f1: 0.7128, edges-srl-ontonotes_loss: 0.0209
09/16 09:31:34 AM: Update 19684: task edges-srl-ontonotes, batch 684 (19684): mcc: 0.7229, acc: 0.6367, precision: 0.7884, recall: 0.6698, f1: 0.7243, edges-srl-ontonotes_loss: 0.0201
09/16 09:31:44 AM: Update 19833: task edges-srl-ontonotes, batch 833 (19833): mcc: 0.7295, acc: 0.6445, precision: 0.7928, recall: 0.6782, f1: 0.7310, edges-srl-ontonotes_loss: 0.0197
09/16 09:31:51 AM: ***** Step 20000 / Validation 20 *****
09/16 09:31:51 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:31:51 AM: Validating...
09/16 09:31:54 AM: Evaluate: task edges-srl-ontonotes, batch 56 (157): mcc: 0.7413, acc: 0.6754, precision: 0.8037, recall: 0.6903, f1: 0.7427, edges-srl-ontonotes_loss: 0.0192
09/16 09:31:58 AM: Updating LR scheduler:
09/16 09:31:58 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:31:58 AM: 	# validation passes without improvement: 0
09/16 09:31:58 AM: edges-srl-ontonotes_loss: training: 0.019083 validation: 0.018659
09/16 09:31:58 AM: macro_avg: validation: 0.750360
09/16 09:31:58 AM: micro_avg: validation: 0.000000
09/16 09:31:58 AM: edges-srl-ontonotes_mcc: training: 0.739474 validation: 0.748568
09/16 09:31:58 AM: edges-srl-ontonotes_acc: training: 0.656572 validation: 0.685629
09/16 09:31:58 AM: edges-srl-ontonotes_precision: training: 0.799780 validation: 0.805134
09/16 09:31:58 AM: edges-srl-ontonotes_recall: training: 0.690461 validation: 0.702563
09/16 09:31:58 AM: edges-srl-ontonotes_f1: training: 0.741111 validation: 0.750360
09/16 09:31:58 AM: Global learning rate: 2.5e-05
09/16 09:31:58 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:32:04 AM: Update 20125: task edges-srl-ontonotes, batch 125 (20125): mcc: 0.7950, acc: 0.7217, precision: 0.8399, recall: 0.7581, f1: 0.7969, edges-srl-ontonotes_loss: 0.0152
09/16 09:32:14 AM: Update 20313: task edges-srl-ontonotes, batch 313 (20313): mcc: 0.7858, acc: 0.7137, precision: 0.8330, recall: 0.7471, f1: 0.7877, edges-srl-ontonotes_loss: 0.0158
09/16 09:32:24 AM: Update 20520: task edges-srl-ontonotes, batch 520 (20520): mcc: 0.7859, acc: 0.7140, precision: 0.8323, recall: 0.7480, f1: 0.7879, edges-srl-ontonotes_loss: 0.0159
09/16 09:32:34 AM: Update 20742: task edges-srl-ontonotes, batch 742 (20742): mcc: 0.7890, acc: 0.7178, precision: 0.8347, recall: 0.7515, f1: 0.7909, edges-srl-ontonotes_loss: 0.0157
09/16 09:32:44 AM: Update 20895: task edges-srl-ontonotes, batch 895 (20895): mcc: 0.7947, acc: 0.7253, precision: 0.8395, recall: 0.7579, f1: 0.7966, edges-srl-ontonotes_loss: 0.0155
09/16 09:32:48 AM: ***** Step 21000 / Validation 21 *****
09/16 09:32:48 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:32:48 AM: Validating...
09/16 09:32:54 AM: Updating LR scheduler:
09/16 09:32:54 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:32:54 AM: 	# validation passes without improvement: 1
09/16 09:32:54 AM: edges-srl-ontonotes_loss: training: 0.015334 validation: 0.018365
09/16 09:32:54 AM: macro_avg: validation: 0.751181
09/16 09:32:54 AM: micro_avg: validation: 0.000000
09/16 09:32:54 AM: edges-srl-ontonotes_mcc: training: 0.798450 validation: 0.749950
09/16 09:32:54 AM: edges-srl-ontonotes_acc: training: 0.730415 validation: 0.685398
09/16 09:32:54 AM: edges-srl-ontonotes_precision: training: 0.842745 validation: 0.813943
09/16 09:32:54 AM: edges-srl-ontonotes_recall: training: 0.761968 validation: 0.697406
09/16 09:32:54 AM: edges-srl-ontonotes_f1: training: 0.800323 validation: 0.751181
09/16 09:32:54 AM: Global learning rate: 2.5e-05
09/16 09:32:54 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:32:54 AM: Update 21005: task edges-srl-ontonotes, batch 5 (21005): mcc: 0.7807, acc: 0.7091, precision: 0.8379, recall: 0.7332, f1: 0.7821, edges-srl-ontonotes_loss: 0.0158
09/16 09:33:04 AM: Update 21208: task edges-srl-ontonotes, batch 208 (21208): mcc: 0.7761, acc: 0.7062, precision: 0.8281, recall: 0.7334, f1: 0.7779, edges-srl-ontonotes_loss: 0.0170
09/16 09:33:14 AM: Update 21416: task edges-srl-ontonotes, batch 416 (21416): mcc: 0.7545, acc: 0.6793, precision: 0.8089, recall: 0.7102, f1: 0.7563, edges-srl-ontonotes_loss: 0.0183
09/16 09:33:24 AM: Update 21630: task edges-srl-ontonotes, batch 630 (21630): mcc: 0.7402, acc: 0.6611, precision: 0.7978, recall: 0.6936, f1: 0.7421, edges-srl-ontonotes_loss: 0.0191
09/16 09:33:34 AM: Update 21775: task edges-srl-ontonotes, batch 775 (21775): mcc: 0.7363, acc: 0.6557, precision: 0.7955, recall: 0.6884, f1: 0.7381, edges-srl-ontonotes_loss: 0.0193
09/16 09:33:44 AM: Update 21972: task edges-srl-ontonotes, batch 972 (21972): mcc: 0.7419, acc: 0.6617, precision: 0.8006, recall: 0.6943, f1: 0.7436, edges-srl-ontonotes_loss: 0.0190
09/16 09:33:45 AM: ***** Step 22000 / Validation 22 *****
09/16 09:33:45 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:33:45 AM: Validating...
09/16 09:33:52 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:33:52 AM: Best result seen so far for macro.
09/16 09:33:52 AM: Updating LR scheduler:
09/16 09:33:52 AM: 	Best result seen so far for macro_avg: 0.762
09/16 09:33:52 AM: 	# validation passes without improvement: 2
09/16 09:33:52 AM: edges-srl-ontonotes_loss: training: 0.018955 validation: 0.017554
09/16 09:33:52 AM: macro_avg: validation: 0.762000
09/16 09:33:52 AM: micro_avg: validation: 0.000000
09/16 09:33:52 AM: edges-srl-ontonotes_mcc: training: 0.742142 validation: 0.761004
09/16 09:33:52 AM: edges-srl-ontonotes_acc: training: 0.661945 validation: 0.695251
09/16 09:33:52 AM: edges-srl-ontonotes_precision: training: 0.800675 validation: 0.826345
09/16 09:33:52 AM: edges-srl-ontonotes_recall: training: 0.694600 validation: 0.706951
09/16 09:33:52 AM: edges-srl-ontonotes_f1: training: 0.743875 validation: 0.762000
09/16 09:33:52 AM: Global learning rate: 2.5e-05
09/16 09:33:52 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:33:54 AM: Update 22040: task edges-srl-ontonotes, batch 40 (22040): mcc: 0.7568, acc: 0.6785, precision: 0.8113, recall: 0.7124, f1: 0.7587, edges-srl-ontonotes_loss: 0.0183
09/16 09:34:04 AM: Update 22268: task edges-srl-ontonotes, batch 268 (22268): mcc: 0.7674, acc: 0.6909, precision: 0.8214, recall: 0.7231, f1: 0.7691, edges-srl-ontonotes_loss: 0.0173
09/16 09:34:14 AM: Update 22443: task edges-srl-ontonotes, batch 443 (22443): mcc: 0.7612, acc: 0.6836, precision: 0.8171, recall: 0.7154, f1: 0.7629, edges-srl-ontonotes_loss: 0.0177
09/16 09:34:24 AM: Update 22635: task edges-srl-ontonotes, batch 635 (22635): mcc: 0.7539, acc: 0.6747, precision: 0.8111, recall: 0.7072, f1: 0.7556, edges-srl-ontonotes_loss: 0.0182
09/16 09:34:34 AM: Update 22787: task edges-srl-ontonotes, batch 787 (22787): mcc: 0.7475, acc: 0.6669, precision: 0.8055, recall: 0.7002, f1: 0.7492, edges-srl-ontonotes_loss: 0.0186
09/16 09:34:44 AM: Update 22962: task edges-srl-ontonotes, batch 962 (22962): mcc: 0.7425, acc: 0.6613, precision: 0.8010, recall: 0.6949, f1: 0.7442, edges-srl-ontonotes_loss: 0.0189
09/16 09:34:47 AM: ***** Step 23000 / Validation 23 *****
09/16 09:34:47 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:34:47 AM: Validating...
09/16 09:34:52 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:34:52 AM: Best result seen so far for macro.
09/16 09:34:52 AM: Updating LR scheduler:
09/16 09:34:52 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:34:52 AM: 	# validation passes without improvement: 0
09/16 09:34:52 AM: edges-srl-ontonotes_loss: training: 0.018997 validation: 0.017259
09/16 09:34:52 AM: macro_avg: validation: 0.767161
09/16 09:34:52 AM: micro_avg: validation: 0.000000
09/16 09:34:52 AM: edges-srl-ontonotes_mcc: training: 0.741376 validation: 0.766114
09/16 09:34:52 AM: edges-srl-ontonotes_acc: training: 0.659969 validation: 0.702332
09/16 09:34:52 AM: edges-srl-ontonotes_precision: training: 0.800096 validation: 0.829974
09/16 09:34:52 AM: edges-srl-ontonotes_recall: training: 0.693693 validation: 0.713186
09/16 09:34:52 AM: edges-srl-ontonotes_f1: training: 0.743105 validation: 0.767161
09/16 09:34:52 AM: Global learning rate: 2.5e-05
09/16 09:34:52 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:34:54 AM: Update 23033: task edges-srl-ontonotes, batch 33 (23033): mcc: 0.7125, acc: 0.6244, precision: 0.7741, recall: 0.6632, f1: 0.7144, edges-srl-ontonotes_loss: 0.0203
09/16 09:35:04 AM: Update 23210: task edges-srl-ontonotes, batch 210 (23210): mcc: 0.7164, acc: 0.6277, precision: 0.7786, recall: 0.6665, f1: 0.7182, edges-srl-ontonotes_loss: 0.0201
09/16 09:35:14 AM: Update 23368: task edges-srl-ontonotes, batch 368 (23368): mcc: 0.7097, acc: 0.6221, precision: 0.7726, recall: 0.6593, f1: 0.7115, edges-srl-ontonotes_loss: 0.0205
09/16 09:35:24 AM: Update 23569: task edges-srl-ontonotes, batch 569 (23569): mcc: 0.7035, acc: 0.6146, precision: 0.7692, recall: 0.6510, f1: 0.7052, edges-srl-ontonotes_loss: 0.0210
09/16 09:35:34 AM: Update 23749: task edges-srl-ontonotes, batch 749 (23749): mcc: 0.7026, acc: 0.6130, precision: 0.7695, recall: 0.6490, f1: 0.7041, edges-srl-ontonotes_loss: 0.0211
09/16 09:35:46 AM: Update 23930: task edges-srl-ontonotes, batch 930 (23930): mcc: 0.7006, acc: 0.6105, precision: 0.7687, recall: 0.6461, f1: 0.7021, edges-srl-ontonotes_loss: 0.0212
09/16 09:35:50 AM: ***** Step 24000 / Validation 24 *****
09/16 09:35:50 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:35:50 AM: Validating...
09/16 09:35:56 AM: Updating LR scheduler:
09/16 09:35:56 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:35:56 AM: 	# validation passes without improvement: 1
09/16 09:35:56 AM: edges-srl-ontonotes_loss: training: 0.021199 validation: 0.017683
09/16 09:35:56 AM: macro_avg: validation: 0.757303
09/16 09:35:56 AM: micro_avg: validation: 0.000000
09/16 09:35:56 AM: edges-srl-ontonotes_mcc: training: 0.701086 validation: 0.756448
09/16 09:35:56 AM: edges-srl-ontonotes_acc: training: 0.610867 validation: 0.688554
09/16 09:35:56 AM: edges-srl-ontonotes_precision: training: 0.768843 validation: 0.824259
09/16 09:35:56 AM: edges-srl-ontonotes_recall: training: 0.646831 validation: 0.700408
09/16 09:35:56 AM: edges-srl-ontonotes_f1: training: 0.702579 validation: 0.757303
09/16 09:35:56 AM: Global learning rate: 2.5e-05
09/16 09:35:56 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:35:56 AM: Update 24010: task edges-srl-ontonotes, batch 10 (24010): mcc: 0.7250, acc: 0.6281, precision: 0.7953, recall: 0.6678, f1: 0.7260, edges-srl-ontonotes_loss: 0.0196
09/16 09:36:06 AM: Update 24197: task edges-srl-ontonotes, batch 197 (24197): mcc: 0.7155, acc: 0.6276, precision: 0.7815, recall: 0.6623, f1: 0.7170, edges-srl-ontonotes_loss: 0.0202
09/16 09:36:16 AM: Update 24364: task edges-srl-ontonotes, batch 364 (24364): mcc: 0.7167, acc: 0.6295, precision: 0.7811, recall: 0.6648, f1: 0.7183, edges-srl-ontonotes_loss: 0.0202
09/16 09:36:27 AM: Update 24556: task edges-srl-ontonotes, batch 556 (24556): mcc: 0.7191, acc: 0.6316, precision: 0.7832, recall: 0.6674, f1: 0.7207, edges-srl-ontonotes_loss: 0.0201
09/16 09:36:37 AM: Update 24731: task edges-srl-ontonotes, batch 731 (24731): mcc: 0.7218, acc: 0.6354, precision: 0.7856, recall: 0.6703, f1: 0.7234, edges-srl-ontonotes_loss: 0.0200
09/16 09:36:48 AM: Update 24869: task edges-srl-ontonotes, batch 869 (24869): mcc: 0.7238, acc: 0.6378, precision: 0.7869, recall: 0.6729, f1: 0.7254, edges-srl-ontonotes_loss: 0.0199
09/16 09:36:55 AM: ***** Step 25000 / Validation 25 *****
09/16 09:36:55 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:36:55 AM: Validating...
09/16 09:36:58 AM: Evaluate: task edges-srl-ontonotes, batch 75 (157): mcc: 0.7452, acc: 0.6773, precision: 0.8155, recall: 0.6873, f1: 0.7459, edges-srl-ontonotes_loss: 0.0183
09/16 09:37:01 AM: Updating LR scheduler:
09/16 09:37:01 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:37:01 AM: 	# validation passes without improvement: 2
09/16 09:37:01 AM: edges-srl-ontonotes_loss: training: 0.020087 validation: 0.017994
09/16 09:37:01 AM: macro_avg: validation: 0.750823
09/16 09:37:01 AM: micro_avg: validation: 0.000000
09/16 09:37:01 AM: edges-srl-ontonotes_mcc: training: 0.721649 validation: 0.749911
09/16 09:37:01 AM: edges-srl-ontonotes_acc: training: 0.635521 validation: 0.685013
09/16 09:37:01 AM: edges-srl-ontonotes_precision: training: 0.785147 validation: 0.817967
09/16 09:37:01 AM: edges-srl-ontonotes_recall: training: 0.670409 validation: 0.693865
09/16 09:37:01 AM: edges-srl-ontonotes_f1: training: 0.723256 validation: 0.750823
09/16 09:37:01 AM: Global learning rate: 2.5e-05
09/16 09:37:01 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:37:08 AM: Update 25152: task edges-srl-ontonotes, batch 152 (25152): mcc: 0.7153, acc: 0.6287, precision: 0.7768, recall: 0.6660, f1: 0.7172, edges-srl-ontonotes_loss: 0.0205
09/16 09:37:19 AM: Update 25367: task edges-srl-ontonotes, batch 367 (25367): mcc: 0.7172, acc: 0.6305, precision: 0.7801, recall: 0.6666, f1: 0.7189, edges-srl-ontonotes_loss: 0.0204
09/16 09:37:29 AM: Update 25575: task edges-srl-ontonotes, batch 575 (25575): mcc: 0.7166, acc: 0.6292, precision: 0.7803, recall: 0.6653, f1: 0.7182, edges-srl-ontonotes_loss: 0.0204
09/16 09:37:39 AM: Update 25775: task edges-srl-ontonotes, batch 775 (25775): mcc: 0.7168, acc: 0.6296, precision: 0.7807, recall: 0.6653, f1: 0.7184, edges-srl-ontonotes_loss: 0.0204
09/16 09:37:49 AM: Update 25933: task edges-srl-ontonotes, batch 933 (25933): mcc: 0.7177, acc: 0.6303, precision: 0.7819, recall: 0.6661, f1: 0.7193, edges-srl-ontonotes_loss: 0.0204
09/16 09:37:52 AM: ***** Step 26000 / Validation 26 *****
09/16 09:37:52 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:37:52 AM: Validating...
09/16 09:37:58 AM: Updating LR scheduler:
09/16 09:37:58 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:37:58 AM: 	# validation passes without improvement: 3
09/16 09:37:58 AM: edges-srl-ontonotes_loss: training: 0.020306 validation: 0.018347
09/16 09:37:58 AM: macro_avg: validation: 0.747904
09/16 09:37:58 AM: micro_avg: validation: 0.000000
09/16 09:37:58 AM: edges-srl-ontonotes_mcc: training: 0.718011 validation: 0.746695
09/16 09:37:58 AM: edges-srl-ontonotes_acc: training: 0.631023 validation: 0.683319
09/16 09:37:58 AM: edges-srl-ontonotes_precision: training: 0.781719 validation: 0.811492
09/16 09:37:58 AM: edges-srl-ontonotes_recall: training: 0.666704 validation: 0.693557
09/16 09:37:58 AM: edges-srl-ontonotes_f1: training: 0.719645 validation: 0.747904
09/16 09:37:58 AM: Global learning rate: 2.5e-05
09/16 09:37:58 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:37:59 AM: Update 26016: task edges-srl-ontonotes, batch 16 (26016): mcc: 0.7248, acc: 0.6411, precision: 0.7823, recall: 0.6787, f1: 0.7268, edges-srl-ontonotes_loss: 0.0202
09/16 09:38:09 AM: Update 26210: task edges-srl-ontonotes, batch 210 (26210): mcc: 0.7236, acc: 0.6386, precision: 0.7843, recall: 0.6748, f1: 0.7254, edges-srl-ontonotes_loss: 0.0200
09/16 09:38:19 AM: Update 26416: task edges-srl-ontonotes, batch 416 (26416): mcc: 0.7224, acc: 0.6370, precision: 0.7834, recall: 0.6734, f1: 0.7242, edges-srl-ontonotes_loss: 0.0200
09/16 09:38:29 AM: Update 26624: task edges-srl-ontonotes, batch 624 (26624): mcc: 0.7175, acc: 0.6305, precision: 0.7812, recall: 0.6662, f1: 0.7191, edges-srl-ontonotes_loss: 0.0204
09/16 09:38:39 AM: Update 26843: task edges-srl-ontonotes, batch 843 (26843): mcc: 0.7210, acc: 0.6346, precision: 0.7848, recall: 0.6695, f1: 0.7226, edges-srl-ontonotes_loss: 0.0201
09/16 09:38:48 AM: ***** Step 27000 / Validation 27 *****
09/16 09:38:48 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:38:48 AM: Validating...
09/16 09:38:49 AM: Evaluate: task edges-srl-ontonotes, batch 15 (157): mcc: 0.7529, acc: 0.6791, precision: 0.8262, recall: 0.6924, f1: 0.7534, edges-srl-ontonotes_loss: 0.0177
09/16 09:38:56 AM: Updating LR scheduler:
09/16 09:38:56 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:38:56 AM: 	# validation passes without improvement: 0
09/16 09:38:56 AM: edges-srl-ontonotes_loss: training: 0.019675 validation: 0.018115
09/16 09:38:56 AM: macro_avg: validation: 0.753708
09/16 09:38:56 AM: micro_avg: validation: 0.000000
09/16 09:38:56 AM: edges-srl-ontonotes_mcc: training: 0.727985 validation: 0.752486
09/16 09:38:56 AM: edges-srl-ontonotes_acc: training: 0.643160 validation: 0.687861
09/16 09:38:56 AM: edges-srl-ontonotes_precision: training: 0.789883 validation: 0.816207
09/16 09:38:56 AM: edges-srl-ontonotes_recall: training: 0.677938 validation: 0.700100
09/16 09:38:56 AM: edges-srl-ontonotes_f1: training: 0.729642 validation: 0.753708
09/16 09:38:56 AM: Global learning rate: 1.25e-05
09/16 09:38:56 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:39:01 AM: Update 27060: task edges-srl-ontonotes, batch 60 (27060): mcc: 0.7634, acc: 0.6848, precision: 0.8127, recall: 0.7233, f1: 0.7654, edges-srl-ontonotes_loss: 0.0174
09/16 09:39:11 AM: Update 27265: task edges-srl-ontonotes, batch 265 (27265): mcc: 0.7789, acc: 0.7044, precision: 0.8268, recall: 0.7397, f1: 0.7808, edges-srl-ontonotes_loss: 0.0162
09/16 09:39:21 AM: Update 27468: task edges-srl-ontonotes, batch 468 (27468): mcc: 0.7802, acc: 0.7067, precision: 0.8271, recall: 0.7419, f1: 0.7822, edges-srl-ontonotes_loss: 0.0161
09/16 09:39:31 AM: Update 27683: task edges-srl-ontonotes, batch 683 (27683): mcc: 0.7813, acc: 0.7081, precision: 0.8283, recall: 0.7428, f1: 0.7832, edges-srl-ontonotes_loss: 0.0161
09/16 09:39:42 AM: Update 27875: task edges-srl-ontonotes, batch 875 (27875): mcc: 0.7839, acc: 0.7118, precision: 0.8305, recall: 0.7458, f1: 0.7859, edges-srl-ontonotes_loss: 0.0159
09/16 09:39:50 AM: ***** Step 28000 / Validation 28 *****
09/16 09:39:50 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:39:50 AM: Validating...
09/16 09:39:52 AM: Evaluate: task edges-srl-ontonotes, batch 27 (157): mcc: 0.7626, acc: 0.6964, precision: 0.8267, recall: 0.7095, f1: 0.7637, edges-srl-ontonotes_loss: 0.0177
09/16 09:39:57 AM: Updating LR scheduler:
09/16 09:39:57 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:39:57 AM: 	# validation passes without improvement: 1
09/16 09:39:57 AM: edges-srl-ontonotes_loss: training: 0.015860 validation: 0.017901
09/16 09:39:57 AM: macro_avg: validation: 0.759547
09/16 09:39:57 AM: micro_avg: validation: 0.000000
09/16 09:39:57 AM: edges-srl-ontonotes_mcc: training: 0.785711 validation: 0.758253
09/16 09:39:57 AM: edges-srl-ontonotes_acc: training: 0.714201 validation: 0.695712
09/16 09:39:57 AM: edges-srl-ontonotes_precision: training: 0.831721 validation: 0.820080
09/16 09:39:57 AM: edges-srl-ontonotes_recall: training: 0.748044 validation: 0.707336
09/16 09:39:57 AM: edges-srl-ontonotes_f1: training: 0.787666 validation: 0.759547
09/16 09:39:57 AM: Global learning rate: 1.25e-05
09/16 09:39:57 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:40:02 AM: Update 28084: task edges-srl-ontonotes, batch 84 (28084): mcc: 0.8154, acc: 0.7520, precision: 0.8590, recall: 0.7790, f1: 0.8171, edges-srl-ontonotes_loss: 0.0145
09/16 09:40:12 AM: Update 28270: task edges-srl-ontonotes, batch 270 (28270): mcc: 0.8224, acc: 0.7628, precision: 0.8636, recall: 0.7881, f1: 0.8241, edges-srl-ontonotes_loss: 0.0142
09/16 09:40:22 AM: Update 28467: task edges-srl-ontonotes, batch 467 (28467): mcc: 0.7968, acc: 0.7312, precision: 0.8435, recall: 0.7583, f1: 0.7986, edges-srl-ontonotes_loss: 0.0157
09/16 09:40:32 AM: Update 28657: task edges-srl-ontonotes, batch 657 (28657): mcc: 0.7797, acc: 0.7100, precision: 0.8290, recall: 0.7392, f1: 0.7815, edges-srl-ontonotes_loss: 0.0167
09/16 09:40:42 AM: Update 28871: task edges-srl-ontonotes, batch 871 (28871): mcc: 0.7641, acc: 0.6905, precision: 0.8170, recall: 0.7209, f1: 0.7659, edges-srl-ontonotes_loss: 0.0177
09/16 09:40:51 AM: ***** Step 29000 / Validation 29 *****
09/16 09:40:51 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:40:51 AM: Validating...
09/16 09:40:52 AM: Evaluate: task edges-srl-ontonotes, batch 26 (157): mcc: 0.7685, acc: 0.7031, precision: 0.8314, recall: 0.7163, f1: 0.7696, edges-srl-ontonotes_loss: 0.0173
09/16 09:40:56 AM: Updating LR scheduler:
09/16 09:40:56 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:40:56 AM: 	# validation passes without improvement: 2
09/16 09:40:56 AM: edges-srl-ontonotes_loss: training: 0.017997 validation: 0.017613
09/16 09:40:56 AM: macro_avg: validation: 0.763431
09/16 09:40:56 AM: micro_avg: validation: 0.000000
09/16 09:40:56 AM: edges-srl-ontonotes_mcc: training: 0.759204 validation: 0.762000
09/16 09:40:56 AM: edges-srl-ontonotes_acc: training: 0.684027 validation: 0.699946
09/16 09:40:56 AM: edges-srl-ontonotes_precision: training: 0.813663 validation: 0.821292
09/16 09:40:56 AM: edges-srl-ontonotes_recall: training: 0.714743 validation: 0.713186
09/16 09:40:56 AM: edges-srl-ontonotes_f1: training: 0.761002 validation: 0.763431
09/16 09:40:56 AM: Global learning rate: 1.25e-05
09/16 09:40:56 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:41:02 AM: Update 29119: task edges-srl-ontonotes, batch 119 (29119): mcc: 0.7595, acc: 0.6838, precision: 0.8137, recall: 0.7152, f1: 0.7613, edges-srl-ontonotes_loss: 0.0181
09/16 09:41:12 AM: Update 29317: task edges-srl-ontonotes, batch 317 (29317): mcc: 0.7603, acc: 0.6842, precision: 0.8161, recall: 0.7146, f1: 0.7620, edges-srl-ontonotes_loss: 0.0178
09/16 09:41:22 AM: Update 29541: task edges-srl-ontonotes, batch 541 (29541): mcc: 0.7642, acc: 0.6883, precision: 0.8190, recall: 0.7193, f1: 0.7659, edges-srl-ontonotes_loss: 0.0175
09/16 09:41:32 AM: Update 29741: task edges-srl-ontonotes, batch 741 (29741): mcc: 0.7578, acc: 0.6809, precision: 0.8136, recall: 0.7122, f1: 0.7595, edges-srl-ontonotes_loss: 0.0179
09/16 09:41:42 AM: Update 29924: task edges-srl-ontonotes, batch 924 (29924): mcc: 0.7542, acc: 0.6768, precision: 0.8104, recall: 0.7083, f1: 0.7559, edges-srl-ontonotes_loss: 0.0181
09/16 09:41:45 AM: ***** Step 30000 / Validation 30 *****
09/16 09:41:45 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:41:45 AM: Validating...
09/16 09:41:50 AM: Best result seen so far for edges-srl-ontonotes.
09/16 09:41:50 AM: Best result seen so far for macro.
09/16 09:41:50 AM: Updating LR scheduler:
09/16 09:41:50 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:41:50 AM: 	# validation passes without improvement: 0
09/16 09:41:50 AM: edges-srl-ontonotes_loss: training: 0.018274 validation: 0.017242
09/16 09:41:50 AM: macro_avg: validation: 0.767485
09/16 09:41:50 AM: micro_avg: validation: 0.000000
09/16 09:41:50 AM: edges-srl-ontonotes_mcc: training: 0.751654 validation: 0.766220
09/16 09:41:50 AM: edges-srl-ontonotes_acc: training: 0.673803 validation: 0.703179
09/16 09:41:50 AM: edges-srl-ontonotes_precision: training: 0.808383 validation: 0.827108
09/16 09:41:50 AM: edges-srl-ontonotes_recall: training: 0.705411 validation: 0.715880
09/16 09:41:50 AM: edges-srl-ontonotes_f1: training: 0.753395 validation: 0.767485
09/16 09:41:50 AM: Global learning rate: 1.25e-05
09/16 09:41:50 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:41:52 AM: Update 30052: task edges-srl-ontonotes, batch 52 (30052): mcc: 0.7194, acc: 0.6324, precision: 0.7787, recall: 0.6718, f1: 0.7213, edges-srl-ontonotes_loss: 0.0198
09/16 09:42:02 AM: Update 30244: task edges-srl-ontonotes, batch 244 (30244): mcc: 0.7162, acc: 0.6306, precision: 0.7789, recall: 0.6658, f1: 0.7179, edges-srl-ontonotes_loss: 0.0204
09/16 09:42:12 AM: Update 30469: task edges-srl-ontonotes, batch 469 (30469): mcc: 0.7187, acc: 0.6316, precision: 0.7804, recall: 0.6692, f1: 0.7205, edges-srl-ontonotes_loss: 0.0201
09/16 09:42:22 AM: Update 30693: task edges-srl-ontonotes, batch 693 (30693): mcc: 0.7121, acc: 0.6241, precision: 0.7749, recall: 0.6617, f1: 0.7138, edges-srl-ontonotes_loss: 0.0204
09/16 09:42:32 AM: Update 30905: task edges-srl-ontonotes, batch 905 (30905): mcc: 0.7083, acc: 0.6199, precision: 0.7723, recall: 0.6570, f1: 0.7100, edges-srl-ontonotes_loss: 0.0208
09/16 09:42:37 AM: ***** Step 31000 / Validation 31 *****
09/16 09:42:37 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:42:37 AM: Validating...
09/16 09:42:42 AM: Evaluate: task edges-srl-ontonotes, batch 144 (157): mcc: 0.7651, acc: 0.6984, precision: 0.8325, recall: 0.7092, f1: 0.7659, edges-srl-ontonotes_loss: 0.0170
09/16 09:42:43 AM: Updating LR scheduler:
09/16 09:42:43 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:42:43 AM: 	# validation passes without improvement: 1
09/16 09:42:43 AM: edges-srl-ontonotes_loss: training: 0.020841 validation: 0.017338
09/16 09:42:43 AM: macro_avg: validation: 0.763104
09/16 09:42:43 AM: micro_avg: validation: 0.000000
09/16 09:42:43 AM: edges-srl-ontonotes_mcc: training: 0.707422 validation: 0.762270
09/16 09:42:43 AM: edges-srl-ontonotes_acc: training: 0.618809 validation: 0.695558
09/16 09:42:43 AM: edges-srl-ontonotes_precision: training: 0.771810 validation: 0.829478
09/16 09:42:43 AM: edges-srl-ontonotes_recall: training: 0.655857 validation: 0.706566
09/16 09:42:43 AM: edges-srl-ontonotes_f1: training: 0.709124 validation: 0.763104
09/16 09:42:43 AM: Global learning rate: 1.25e-05
09/16 09:42:43 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:42:53 AM: Update 31176: task edges-srl-ontonotes, batch 176 (31176): mcc: 0.7037, acc: 0.6128, precision: 0.7726, recall: 0.6483, f1: 0.7050, edges-srl-ontonotes_loss: 0.0213
09/16 09:43:03 AM: Update 31396: task edges-srl-ontonotes, batch 396 (31396): mcc: 0.7137, acc: 0.6252, precision: 0.7791, recall: 0.6610, f1: 0.7152, edges-srl-ontonotes_loss: 0.0205
09/16 09:43:13 AM: Update 31576: task edges-srl-ontonotes, batch 576 (31576): mcc: 0.7148, acc: 0.6265, precision: 0.7795, recall: 0.6627, f1: 0.7164, edges-srl-ontonotes_loss: 0.0204
09/16 09:43:24 AM: Update 31746: task edges-srl-ontonotes, batch 746 (31746): mcc: 0.7168, acc: 0.6293, precision: 0.7805, recall: 0.6655, f1: 0.7184, edges-srl-ontonotes_loss: 0.0203
09/16 09:43:34 AM: Update 31929: task edges-srl-ontonotes, batch 929 (31929): mcc: 0.7183, acc: 0.6309, precision: 0.7813, recall: 0.6677, f1: 0.7200, edges-srl-ontonotes_loss: 0.0202
09/16 09:43:38 AM: ***** Step 32000 / Validation 32 *****
09/16 09:43:38 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:43:38 AM: Validating...
09/16 09:43:43 AM: Updating LR scheduler:
09/16 09:43:43 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:43:43 AM: 	# validation passes without improvement: 2
09/16 09:43:43 AM: edges-srl-ontonotes_loss: training: 0.020109 validation: 0.017655
09/16 09:43:43 AM: macro_avg: validation: 0.755946
09/16 09:43:43 AM: micro_avg: validation: 0.000000
09/16 09:43:43 AM: edges-srl-ontonotes_mcc: training: 0.719451 validation: 0.754762
09/16 09:43:43 AM: edges-srl-ontonotes_acc: training: 0.632204 validation: 0.691633
09/16 09:43:43 AM: edges-srl-ontonotes_precision: training: 0.782296 validation: 0.818631
09/16 09:43:43 AM: edges-srl-ontonotes_recall: training: 0.668846 validation: 0.702178
09/16 09:43:43 AM: edges-srl-ontonotes_f1: training: 0.721136 validation: 0.755946
09/16 09:43:43 AM: Global learning rate: 1.25e-05
09/16 09:43:43 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:43:44 AM: Update 32016: task edges-srl-ontonotes, batch 16 (32016): mcc: 0.7153, acc: 0.6231, precision: 0.7857, recall: 0.6584, f1: 0.7164, edges-srl-ontonotes_loss: 0.0202
09/16 09:43:54 AM: Update 32165: task edges-srl-ontonotes, batch 165 (32165): mcc: 0.7245, acc: 0.6408, precision: 0.7844, recall: 0.6763, f1: 0.7263, edges-srl-ontonotes_loss: 0.0200
09/16 09:44:04 AM: Update 32369: task edges-srl-ontonotes, batch 369 (32369): mcc: 0.7183, acc: 0.6327, precision: 0.7808, recall: 0.6680, f1: 0.7200, edges-srl-ontonotes_loss: 0.0203
09/16 09:44:14 AM: Update 32558: task edges-srl-ontonotes, batch 558 (32558): mcc: 0.7183, acc: 0.6325, precision: 0.7811, recall: 0.6679, f1: 0.7200, edges-srl-ontonotes_loss: 0.0203
09/16 09:44:24 AM: Update 32744: task edges-srl-ontonotes, batch 744 (32744): mcc: 0.7189, acc: 0.6333, precision: 0.7813, recall: 0.6688, f1: 0.7207, edges-srl-ontonotes_loss: 0.0203
09/16 09:44:34 AM: Update 32946: task edges-srl-ontonotes, batch 946 (32946): mcc: 0.7185, acc: 0.6327, precision: 0.7813, recall: 0.6680, f1: 0.7202, edges-srl-ontonotes_loss: 0.0203
09/16 09:44:36 AM: ***** Step 33000 / Validation 33 *****
09/16 09:44:36 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:44:36 AM: Validating...
09/16 09:44:41 AM: Updating LR scheduler:
09/16 09:44:41 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:44:41 AM: 	# validation passes without improvement: 3
09/16 09:44:41 AM: edges-srl-ontonotes_loss: training: 0.020286 validation: 0.017916
09/16 09:44:41 AM: macro_avg: validation: 0.753564
09/16 09:44:41 AM: micro_avg: validation: 0.000000
09/16 09:44:41 AM: edges-srl-ontonotes_mcc: training: 0.718637 validation: 0.752357
09/16 09:44:41 AM: edges-srl-ontonotes_acc: training: 0.632731 validation: 0.690632
09/16 09:44:41 AM: edges-srl-ontonotes_precision: training: 0.781592 validation: 0.816288
09/16 09:44:41 AM: edges-srl-ontonotes_recall: training: 0.667962 validation: 0.699792
09/16 09:44:41 AM: edges-srl-ontonotes_f1: training: 0.720323 validation: 0.753564
09/16 09:44:41 AM: Global learning rate: 1.25e-05
09/16 09:44:41 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:44:46 AM: Update 33054: task edges-srl-ontonotes, batch 54 (33054): mcc: 0.7058, acc: 0.6163, precision: 0.7728, recall: 0.6520, f1: 0.7073, edges-srl-ontonotes_loss: 0.0209
09/16 09:44:56 AM: Update 33292: task edges-srl-ontonotes, batch 292 (33292): mcc: 0.7188, acc: 0.6326, precision: 0.7815, recall: 0.6684, f1: 0.7205, edges-srl-ontonotes_loss: 0.0201
09/16 09:45:06 AM: Update 33502: task edges-srl-ontonotes, batch 502 (33502): mcc: 0.7202, acc: 0.6348, precision: 0.7825, recall: 0.6701, f1: 0.7219, edges-srl-ontonotes_loss: 0.0200
09/16 09:45:16 AM: Update 33685: task edges-srl-ontonotes, batch 685 (33685): mcc: 0.7221, acc: 0.6372, precision: 0.7843, recall: 0.6720, f1: 0.7238, edges-srl-ontonotes_loss: 0.0200
09/16 09:45:26 AM: Update 33884: task edges-srl-ontonotes, batch 884 (33884): mcc: 0.7195, acc: 0.6342, precision: 0.7832, recall: 0.6683, f1: 0.7212, edges-srl-ontonotes_loss: 0.0202
09/16 09:45:34 AM: ***** Step 34000 / Validation 34 *****
09/16 09:45:34 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:45:34 AM: Validating...
09/16 09:45:36 AM: Evaluate: task edges-srl-ontonotes, batch 32 (157): mcc: 0.7518, acc: 0.6850, precision: 0.8224, recall: 0.6935, f1: 0.7525, edges-srl-ontonotes_loss: 0.0180
09/16 09:45:40 AM: Updating LR scheduler:
09/16 09:45:40 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:45:40 AM: 	# validation passes without improvement: 0
09/16 09:45:40 AM: edges-srl-ontonotes_loss: training: 0.020214 validation: 0.018058
09/16 09:45:40 AM: macro_avg: validation: 0.749418
09/16 09:45:40 AM: micro_avg: validation: 0.000000
09/16 09:45:40 AM: edges-srl-ontonotes_mcc: training: 0.719112 validation: 0.748394
09/16 09:45:40 AM: edges-srl-ontonotes_acc: training: 0.633720 validation: 0.684705
09/16 09:45:40 AM: edges-srl-ontonotes_precision: training: 0.783211 validation: 0.815277
09/16 09:45:40 AM: edges-srl-ontonotes_recall: training: 0.667434 validation: 0.693403
09/16 09:45:40 AM: edges-srl-ontonotes_f1: training: 0.720702 validation: 0.749418
09/16 09:45:40 AM: Global learning rate: 6.25e-06
09/16 09:45:40 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:45:46 AM: Update 34124: task edges-srl-ontonotes, batch 124 (34124): mcc: 0.7492, acc: 0.6690, precision: 0.8058, recall: 0.7031, f1: 0.7509, edges-srl-ontonotes_loss: 0.0179
09/16 09:45:58 AM: Update 34306: task edges-srl-ontonotes, batch 306 (34306): mcc: 0.7560, acc: 0.6776, precision: 0.8125, recall: 0.7099, f1: 0.7577, edges-srl-ontonotes_loss: 0.0176
09/16 09:46:08 AM: Update 34502: task edges-srl-ontonotes, batch 502 (34502): mcc: 0.7638, acc: 0.6878, precision: 0.8173, recall: 0.7201, f1: 0.7656, edges-srl-ontonotes_loss: 0.0171
09/16 09:46:18 AM: Update 34707: task edges-srl-ontonotes, batch 707 (34707): mcc: 0.7672, acc: 0.6922, precision: 0.8188, recall: 0.7252, f1: 0.7691, edges-srl-ontonotes_loss: 0.0169
09/16 09:46:28 AM: Update 34913: task edges-srl-ontonotes, batch 913 (34913): mcc: 0.7707, acc: 0.6962, precision: 0.8212, recall: 0.7293, f1: 0.7725, edges-srl-ontonotes_loss: 0.0167
09/16 09:46:32 AM: ***** Step 35000 / Validation 35 *****
09/16 09:46:32 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:46:32 AM: Validating...
09/16 09:46:37 AM: Updating LR scheduler:
09/16 09:46:37 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:46:37 AM: 	# validation passes without improvement: 1
09/16 09:46:37 AM: edges-srl-ontonotes_loss: training: 0.016563 validation: 0.017768
09/16 09:46:37 AM: macro_avg: validation: 0.759898
09/16 09:46:37 AM: micro_avg: validation: 0.000000
09/16 09:46:37 AM: edges-srl-ontonotes_mcc: training: 0.772851 validation: 0.758604
09/16 09:46:37 AM: edges-srl-ontonotes_acc: training: 0.698848 validation: 0.695943
09/16 09:46:37 AM: edges-srl-ontonotes_precision: training: 0.822977 validation: 0.820380
09/16 09:46:37 AM: edges-srl-ontonotes_recall: training: 0.731850 validation: 0.707721
09/16 09:46:37 AM: edges-srl-ontonotes_f1: training: 0.774743 validation: 0.759898
09/16 09:46:37 AM: Global learning rate: 6.25e-06
09/16 09:46:37 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:46:38 AM: Update 35023: task edges-srl-ontonotes, batch 23 (35023): mcc: 0.7942, acc: 0.7245, precision: 0.8385, recall: 0.7578, f1: 0.7961, edges-srl-ontonotes_loss: 0.0153
09/16 09:46:50 AM: Update 35245: task edges-srl-ontonotes, batch 245 (35245): mcc: 0.7955, acc: 0.7281, precision: 0.8407, recall: 0.7583, f1: 0.7973, edges-srl-ontonotes_loss: 0.0153
09/16 09:47:00 AM: Update 35425: task edges-srl-ontonotes, batch 425 (35425): mcc: 0.8066, acc: 0.7418, precision: 0.8494, recall: 0.7713, f1: 0.8085, edges-srl-ontonotes_loss: 0.0149
09/16 09:47:10 AM: Update 35594: task edges-srl-ontonotes, batch 594 (35594): mcc: 0.8062, acc: 0.7415, precision: 0.8496, recall: 0.7704, f1: 0.8081, edges-srl-ontonotes_loss: 0.0150
09/16 09:47:20 AM: Update 35767: task edges-srl-ontonotes, batch 767 (35767): mcc: 0.7913, acc: 0.7238, precision: 0.8378, recall: 0.7530, f1: 0.7931, edges-srl-ontonotes_loss: 0.0159
09/16 09:47:30 AM: Update 35947: task edges-srl-ontonotes, batch 947 (35947): mcc: 0.7813, acc: 0.7117, precision: 0.8296, recall: 0.7417, f1: 0.7832, edges-srl-ontonotes_loss: 0.0165
09/16 09:47:33 AM: ***** Step 36000 / Validation 36 *****
09/16 09:47:33 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:47:33 AM: Validating...
09/16 09:47:38 AM: Updating LR scheduler:
09/16 09:47:38 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:47:38 AM: 	# validation passes without improvement: 2
09/16 09:47:38 AM: edges-srl-ontonotes_loss: training: 0.016739 validation: 0.017490
09/16 09:47:38 AM: macro_avg: validation: 0.764109
09/16 09:47:38 AM: micro_avg: validation: 0.000000
09/16 09:47:38 AM: edges-srl-ontonotes_mcc: training: 0.778280 validation: 0.763018
09/16 09:47:38 AM: edges-srl-ontonotes_acc: training: 0.707730 validation: 0.698791
09/16 09:47:38 AM: edges-srl-ontonotes_precision: training: 0.827356 validation: 0.826792
09/16 09:47:38 AM: edges-srl-ontonotes_recall: training: 0.738061 validation: 0.710261
09/16 09:47:38 AM: edges-srl-ontonotes_f1: training: 0.780162 validation: 0.764109
09/16 09:47:38 AM: Global learning rate: 6.25e-06
09/16 09:47:38 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:47:40 AM: Update 36038: task edges-srl-ontonotes, batch 38 (36038): mcc: 0.7106, acc: 0.6279, precision: 0.7755, recall: 0.6585, f1: 0.7122, edges-srl-ontonotes_loss: 0.0211
09/16 09:47:51 AM: Update 36231: task edges-srl-ontonotes, batch 231 (36231): mcc: 0.7166, acc: 0.6302, precision: 0.7827, recall: 0.6633, f1: 0.7180, edges-srl-ontonotes_loss: 0.0208
09/16 09:48:01 AM: Update 36443: task edges-srl-ontonotes, batch 443 (36443): mcc: 0.7374, acc: 0.6555, precision: 0.7977, recall: 0.6884, f1: 0.7390, edges-srl-ontonotes_loss: 0.0193
09/16 09:48:11 AM: Update 36604: task edges-srl-ontonotes, batch 604 (36604): mcc: 0.7448, acc: 0.6641, precision: 0.8033, recall: 0.6973, f1: 0.7465, edges-srl-ontonotes_loss: 0.0188
09/16 09:48:21 AM: Update 36801: task edges-srl-ontonotes, batch 801 (36801): mcc: 0.7505, acc: 0.6717, precision: 0.8077, recall: 0.7040, f1: 0.7523, edges-srl-ontonotes_loss: 0.0185
09/16 09:48:31 AM: Update 36992: task edges-srl-ontonotes, batch 992 (36992): mcc: 0.7496, acc: 0.6706, precision: 0.8070, recall: 0.7028, f1: 0.7513, edges-srl-ontonotes_loss: 0.0185
09/16 09:48:31 AM: ***** Step 37000 / Validation 37 *****
09/16 09:48:31 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:48:31 AM: Validating...
09/16 09:48:37 AM: Updating LR scheduler:
09/16 09:48:37 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:48:37 AM: 	# validation passes without improvement: 3
09/16 09:48:37 AM: edges-srl-ontonotes_loss: training: 0.018498 validation: 0.017237
09/16 09:48:37 AM: macro_avg: validation: 0.765378
09/16 09:48:37 AM: micro_avg: validation: 0.000000
09/16 09:48:37 AM: edges-srl-ontonotes_mcc: training: 0.749719 validation: 0.764289
09/16 09:48:37 AM: edges-srl-ontonotes_acc: training: 0.670809 validation: 0.699561
09/16 09:48:37 AM: edges-srl-ontonotes_precision: training: 0.807160 validation: 0.827886
09/16 09:48:37 AM: edges-srl-ontonotes_recall: training: 0.702907 validation: 0.711646
09/16 09:48:37 AM: edges-srl-ontonotes_f1: training: 0.751434 validation: 0.765378
09/16 09:48:37 AM: Global learning rate: 6.25e-06
09/16 09:48:37 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:48:41 AM: Update 37090: task edges-srl-ontonotes, batch 90 (37090): mcc: 0.7411, acc: 0.6593, precision: 0.8021, recall: 0.6915, f1: 0.7427, edges-srl-ontonotes_loss: 0.0189
09/16 09:48:51 AM: Update 37292: task edges-srl-ontonotes, batch 292 (37292): mcc: 0.7327, acc: 0.6488, precision: 0.7939, recall: 0.6832, f1: 0.7344, edges-srl-ontonotes_loss: 0.0194
09/16 09:49:02 AM: Update 37483: task edges-srl-ontonotes, batch 483 (37483): mcc: 0.7278, acc: 0.6441, precision: 0.7899, recall: 0.6777, f1: 0.7295, edges-srl-ontonotes_loss: 0.0198
09/16 09:49:12 AM: Update 37726: task edges-srl-ontonotes, batch 726 (37726): mcc: 0.7257, acc: 0.6408, precision: 0.7872, recall: 0.6760, f1: 0.7274, edges-srl-ontonotes_loss: 0.0198
09/16 09:49:22 AM: Update 37938: task edges-srl-ontonotes, batch 938 (37938): mcc: 0.7207, acc: 0.6342, precision: 0.7831, recall: 0.6705, f1: 0.7224, edges-srl-ontonotes_loss: 0.0201
09/16 09:49:25 AM: ***** Step 38000 / Validation 38 *****
09/16 09:49:25 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:49:25 AM: Validating...
09/16 09:49:31 AM: Updating LR scheduler:
09/16 09:49:31 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:49:31 AM: 	# validation passes without improvement: 0
09/16 09:49:31 AM: edges-srl-ontonotes_loss: training: 0.020179 validation: 0.017283
09/16 09:49:31 AM: macro_avg: validation: 0.763943
09/16 09:49:31 AM: micro_avg: validation: 0.000000
09/16 09:49:31 AM: edges-srl-ontonotes_mcc: training: 0.719340 validation: 0.762850
09/16 09:49:31 AM: edges-srl-ontonotes_acc: training: 0.632837 validation: 0.698638
09/16 09:49:31 AM: edges-srl-ontonotes_precision: training: 0.781919 validation: 0.826613
09/16 09:49:31 AM: edges-srl-ontonotes_recall: training: 0.668969 validation: 0.710107
09/16 09:49:31 AM: edges-srl-ontonotes_f1: training: 0.721048 validation: 0.763943
09/16 09:49:31 AM: Global learning rate: 3.125e-06
09/16 09:49:31 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:49:32 AM: Update 38034: task edges-srl-ontonotes, batch 34 (38034): mcc: 0.6984, acc: 0.6104, precision: 0.7624, recall: 0.6474, f1: 0.7002, edges-srl-ontonotes_loss: 0.0216
09/16 09:49:42 AM: Update 38233: task edges-srl-ontonotes, batch 233 (38233): mcc: 0.6916, acc: 0.6007, precision: 0.7608, recall: 0.6364, f1: 0.6931, edges-srl-ontonotes_loss: 0.0219
09/16 09:49:54 AM: Update 38422: task edges-srl-ontonotes, batch 422 (38422): mcc: 0.6990, acc: 0.6091, precision: 0.7671, recall: 0.6445, f1: 0.7005, edges-srl-ontonotes_loss: 0.0216
09/16 09:50:04 AM: Update 38616: task edges-srl-ontonotes, batch 616 (38616): mcc: 0.7073, acc: 0.6181, precision: 0.7740, recall: 0.6538, f1: 0.7089, edges-srl-ontonotes_loss: 0.0210
09/16 09:50:14 AM: Update 38818: task edges-srl-ontonotes, batch 818 (38818): mcc: 0.7102, acc: 0.6216, precision: 0.7756, recall: 0.6576, f1: 0.7118, edges-srl-ontonotes_loss: 0.0207
09/16 09:50:22 AM: ***** Step 39000 / Validation 39 *****
09/16 09:50:22 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:50:22 AM: Validating...
09/16 09:50:24 AM: Evaluate: task edges-srl-ontonotes, batch 44 (157): mcc: 0.7546, acc: 0.6886, precision: 0.8180, recall: 0.7024, f1: 0.7558, edges-srl-ontonotes_loss: 0.0183
09/16 09:50:28 AM: Updating LR scheduler:
09/16 09:50:28 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:50:28 AM: 	# validation passes without improvement: 1
09/16 09:50:28 AM: edges-srl-ontonotes_loss: training: 0.020579 validation: 0.017369
09/16 09:50:28 AM: macro_avg: validation: 0.762636
09/16 09:50:28 AM: micro_avg: validation: 0.000000
09/16 09:50:28 AM: edges-srl-ontonotes_mcc: training: 0.712426 validation: 0.761340
09/16 09:50:28 AM: edges-srl-ontonotes_acc: training: 0.624012 validation: 0.698099
09/16 09:50:28 AM: edges-srl-ontonotes_precision: training: 0.777046 validation: 0.822628
09/16 09:50:28 AM: edges-srl-ontonotes_recall: training: 0.660505 validation: 0.710800
09/16 09:50:28 AM: edges-srl-ontonotes_f1: training: 0.714052 validation: 0.762636
09/16 09:50:28 AM: Global learning rate: 3.125e-06
09/16 09:50:28 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:50:34 AM: Update 39119: task edges-srl-ontonotes, batch 119 (39119): mcc: 0.7284, acc: 0.6430, precision: 0.7870, recall: 0.6813, f1: 0.7303, edges-srl-ontonotes_loss: 0.0197
09/16 09:50:44 AM: Update 39333: task edges-srl-ontonotes, batch 333 (39333): mcc: 0.7304, acc: 0.6451, precision: 0.7906, recall: 0.6818, f1: 0.7322, edges-srl-ontonotes_loss: 0.0194
09/16 09:50:54 AM: Update 39550: task edges-srl-ontonotes, batch 550 (39550): mcc: 0.7248, acc: 0.6389, precision: 0.7853, recall: 0.6760, f1: 0.7266, edges-srl-ontonotes_loss: 0.0198
09/16 09:51:04 AM: Update 39723: task edges-srl-ontonotes, batch 723 (39723): mcc: 0.7218, acc: 0.6356, precision: 0.7830, recall: 0.6725, f1: 0.7236, edges-srl-ontonotes_loss: 0.0200
09/16 09:51:14 AM: Update 39944: task edges-srl-ontonotes, batch 944 (39944): mcc: 0.7210, acc: 0.6344, precision: 0.7830, recall: 0.6711, f1: 0.7227, edges-srl-ontonotes_loss: 0.0201
09/16 09:51:18 AM: ***** Step 40000 / Validation 40 *****
09/16 09:51:18 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 09:51:18 AM: Validating...
09/16 09:51:23 AM: Updating LR scheduler:
09/16 09:51:23 AM: 	Best result seen so far for macro_avg: 0.767
09/16 09:51:23 AM: 	# validation passes without improvement: 2
09/16 09:51:23 AM: Ran out of early stopping patience. Stopping training.
09/16 09:51:23 AM: edges-srl-ontonotes_loss: training: 0.020123 validation: 0.017455
09/16 09:51:23 AM: macro_avg: validation: 0.760406
09/16 09:51:23 AM: micro_avg: validation: 0.000000
09/16 09:51:23 AM: edges-srl-ontonotes_mcc: training: 0.720220 validation: 0.759268
09/16 09:51:23 AM: edges-srl-ontonotes_acc: training: 0.633502 validation: 0.696251
09/16 09:51:23 AM: edges-srl-ontonotes_precision: training: 0.782618 validation: 0.823023
09/16 09:51:23 AM: edges-srl-ontonotes_recall: training: 0.669979 validation: 0.706643
09/16 09:51:23 AM: edges-srl-ontonotes_f1: training: 0.721931 validation: 0.760406
09/16 09:51:23 AM: Global learning rate: 3.125e-06
09/16 09:51:23 AM: Saving checkpoints to: ./experiments/srl-ontonotes-rte-only/run
09/16 09:51:23 AM: Stopped training after 40 validation checks
09/16 09:51:23 AM: Trained edges-srl-ontonotes for 40000 batches or 5.529 epochs
09/16 09:51:23 AM: ***** VALIDATION RESULTS *****
09/16 09:51:23 AM: edges-srl-ontonotes_f1 (for best val pass 30): edges-srl-ontonotes_loss: 0.01724, macro_avg: 0.76749, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.76622, edges-srl-ontonotes_acc: 0.70318, edges-srl-ontonotes_precision: 0.82711, edges-srl-ontonotes_recall: 0.71588, edges-srl-ontonotes_f1: 0.76749
09/16 09:51:23 AM: micro_avg (for best val pass 1): edges-srl-ontonotes_loss: 0.02754, macro_avg: 0.63800, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.64603, edges-srl-ontonotes_acc: 0.51959, edges-srl-ontonotes_precision: 0.79100, edges-srl-ontonotes_recall: 0.53460, edges-srl-ontonotes_f1: 0.63800
09/16 09:51:23 AM: macro_avg (for best val pass 30): edges-srl-ontonotes_loss: 0.01724, macro_avg: 0.76749, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.76622, edges-srl-ontonotes_acc: 0.70318, edges-srl-ontonotes_precision: 0.82711, edges-srl-ontonotes_recall: 0.71588, edges-srl-ontonotes_f1: 0.76749
09/16 09:51:23 AM: Evaluating...
09/16 09:51:24 AM: Loaded model state from ./experiments/srl-ontonotes-rte-only/run/edges-srl-ontonotes/model_state_target_train_val_30.best.th
09/16 09:51:24 AM: Evaluating on: edges-srl-ontonotes, split: val
09/16 09:51:54 AM: 	Task edges-srl-ontonotes: batch 686
09/16 09:52:07 AM: Task 'edges-srl-ontonotes': sorting predictions by 'idx'
09/16 09:52:07 AM: Finished evaluating on: edges-srl-ontonotes
09/16 09:52:08 AM: Task 'edges-srl-ontonotes': joining predictions with input split 'val'
09/16 09:52:14 AM: Task 'edges-srl-ontonotes': Wrote predictions to ./experiments/srl-ontonotes-rte-only/run
09/16 09:52:14 AM: Wrote all preds for split 'val' to ./experiments/srl-ontonotes-rte-only/run
09/16 09:52:14 AM: Evaluating on: edges-srl-ontonotes, split: test
09/16 09:52:42 AM: Task 'edges-srl-ontonotes': sorting predictions by 'idx'
09/16 09:52:42 AM: Finished evaluating on: edges-srl-ontonotes
09/16 09:52:43 AM: Task 'edges-srl-ontonotes': joining predictions with input split 'test'
09/16 09:52:46 AM: Task 'edges-srl-ontonotes': Wrote predictions to ./experiments/srl-ontonotes-rte-only/run
09/16 09:52:46 AM: Wrote all preds for split 'test' to ./experiments/srl-ontonotes-rte-only/run
09/16 09:52:46 AM: Writing results for split 'val' to ./experiments/srl-ontonotes-rte-only/results.tsv
09/16 09:52:46 AM: micro_avg: 0.000, macro_avg: 0.750, edges-srl-ontonotes_mcc: 0.748, edges-srl-ontonotes_acc: 0.684, edges-srl-ontonotes_precision: 0.813, edges-srl-ontonotes_recall: 0.696, edges-srl-ontonotes_f1: 0.750
09/16 09:52:46 AM: Done!
09/16 09:52:46 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
