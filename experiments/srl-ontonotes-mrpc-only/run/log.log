09/16 10:55:44 AM: Git branch: master
09/16 10:55:44 AM: Git SHA: 092d4f2e0b7152db74aa328af35fdb8b3f73d06a
09/16 10:55:44 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/srl-ontonotes-mrpc-only/",
  "exp_name": "experiments/srl-ontonotes-mrpc-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/srl-ontonotes-mrpc-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/mrpc",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/srl-ontonotes-mrpc-only__run",
  "run_dir": "./experiments/srl-ontonotes-mrpc-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-srl-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 10:55:44 AM: Saved config to ./experiments/srl-ontonotes-mrpc-only/run/params.conf
09/16 10:55:44 AM: Using random seed 1234
09/16 10:55:45 AM: Using GPU 0
09/16 10:55:45 AM: Loading tasks...
09/16 10:55:45 AM: Writing pre-preprocessed tasks to ./experiments/srl-ontonotes-mrpc-only/
09/16 10:55:45 AM: 	Creating task edges-srl-ontonotes from scratch.
09/16 10:55:52 AM: Read=231480, Skip=21590, Total=253070 from ./probing_data/edges/ontonotes/srl/train.json.retokenized.bert-base-uncased
09/16 10:55:52 AM: Read=32486, Skip=2811, Total=35297 from ./probing_data/edges/ontonotes/srl/development.json.retokenized.bert-base-uncased
09/16 10:55:53 AM: Read=23800, Skip=2915, Total=26715 from ./probing_data/edges/ontonotes/srl/test.json.retokenized.bert-base-uncased
09/16 10:55:57 AM: 	Task 'edges-srl-ontonotes': |train|=231480 |val|=32486 |test|=23800
09/16 10:55:57 AM: 	Finished loading tasks: edges-srl-ontonotes.
09/16 10:55:57 AM: 	Building vocab from scratch.
09/16 10:55:57 AM: 	Counting units for task edges-srl-ontonotes.
09/16 10:56:05 AM: 	Task 'edges-srl-ontonotes': adding vocab namespace 'edges-srl-ontonotes_labels'
09/16 10:56:06 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:06 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 10:56:06 AM: 	Saved vocab to ./experiments/srl-ontonotes-mrpc-only/vocab
09/16 10:56:06 AM: Loading token dictionary from ./experiments/srl-ontonotes-mrpc-only/vocab.
09/16 10:56:06 AM: 	Loaded vocab from ./experiments/srl-ontonotes-mrpc-only/vocab
09/16 10:56:06 AM: 	Vocab namespace bert_uncased: size 30524
09/16 10:56:06 AM: 	Vocab namespace tokens: size 23662
09/16 10:56:06 AM: 	Vocab namespace edges-srl-ontonotes_labels: size 66
09/16 10:56:06 AM: 	Vocab namespace chars: size 76
09/16 10:56:06 AM: 	Finished building vocab.
09/16 10:56:06 AM: 	Task edges-srl-ontonotes (train): Indexing from scratch.
09/16 10:56:45 AM: 	Task edges-srl-ontonotes (train): Saved 231480 instances to ./experiments/srl-ontonotes-mrpc-only/preproc/edges-srl-ontonotes__train_data
09/16 10:56:45 AM: 	Task edges-srl-ontonotes (val): Indexing from scratch.
09/16 10:56:50 AM: 	Task edges-srl-ontonotes (val): Saved 32486 instances to ./experiments/srl-ontonotes-mrpc-only/preproc/edges-srl-ontonotes__val_data
09/16 10:56:50 AM: 	Task edges-srl-ontonotes (test): Indexing from scratch.
09/16 10:56:53 AM: 	Task edges-srl-ontonotes (test): Saved 23800 instances to ./experiments/srl-ontonotes-mrpc-only/preproc/edges-srl-ontonotes__test_data
09/16 10:56:53 AM: 	Finished indexing tasks
09/16 10:56:53 AM: 	Creating trimmed target-only version of edges-srl-ontonotes train.
09/16 10:56:53 AM: 	  Training on 
09/16 10:56:53 AM: 	  Evaluating on edges-srl-ontonotes
09/16 10:56:53 AM: 	Finished loading tasks in 68.073s
09/16 10:56:53 AM: 	 Tasks: ['edges-srl-ontonotes']
09/16 10:56:53 AM: Building model...
09/16 10:56:53 AM: Using BERT model (bert-base-uncased).
09/16 10:56:53 AM: LOADING A FUNETUNED MODEL from: 
09/16 10:56:53 AM: models/mrpc
09/16 10:56:53 AM: loading configuration file models/mrpc/config.json
09/16 10:56:53 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 10:56:53 AM: loading weights file models/mrpc/pytorch_model.bin
09/16 10:56:56 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpfz527awe
09/16 10:56:58 AM: copying /tmp/tmpfz527awe to cache at ./experiments/srl-ontonotes-mrpc-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:58 AM: creating metadata file for ./experiments/srl-ontonotes-mrpc-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:58 AM: removing temp file /tmp/tmpfz527awe
09/16 10:56:58 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/srl-ontonotes-mrpc-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:58 AM: Initializing parameters
09/16 10:56:58 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 10:56:58 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 10:56:58 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 10:56:58 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 10:56:58 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 10:56:58 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 10:56:58 AM: 	Task 'edges-srl-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-srl-ontonotes"
}
09/16 10:57:02 AM: Model specification:
09/16 10:57:02 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-srl-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=66, bias=True)
      )
    )
  )
)
09/16 10:57:02 AM: Model parameters:
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:57:02 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 16896 with torch.Size([66, 256])
09/16 10:57:02 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 66 with torch.Size([66])
09/16 10:57:02 AM: Total number of parameters: 110155842 (1.10156e+08)
09/16 10:57:02 AM: Number of trainable parameters: 673602 (673602)
09/16 10:57:02 AM: Finished building model in 9.094s
09/16 10:57:02 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-srl-ontonotes 

09/16 10:57:17 AM: patience = 9
09/16 10:57:17 AM: val_interval = 1000
09/16 10:57:17 AM: max_vals = 250
09/16 10:57:17 AM: cuda_device = 0
09/16 10:57:17 AM: grad_norm = 5.0
09/16 10:57:17 AM: grad_clipping = None
09/16 10:57:17 AM: lr_decay = 0.99
09/16 10:57:17 AM: min_lr = 1e-06
09/16 10:57:17 AM: keep_all_checkpoints = 0
09/16 10:57:17 AM: val_data_limit = 5000
09/16 10:57:17 AM: max_epochs = -1
09/16 10:57:17 AM: dec_val_scale = 250
09/16 10:57:17 AM: training_data_fraction = 1
09/16 10:57:17 AM: type = adam
09/16 10:57:17 AM: parameter_groups = None
09/16 10:57:17 AM: Number of trainable parameters: 673602
09/16 10:57:17 AM: infer_type_and_cast = True
09/16 10:57:17 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:17 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:17 AM: lr = 0.0001
09/16 10:57:17 AM: amsgrad = True
09/16 10:57:17 AM: type = reduce_on_plateau
09/16 10:57:17 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:17 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:17 AM: mode = max
09/16 10:57:17 AM: factor = 0.5
09/16 10:57:17 AM: patience = 3
09/16 10:57:17 AM: threshold = 0.0001
09/16 10:57:17 AM: threshold_mode = abs
09/16 10:57:17 AM: verbose = True
09/16 10:57:17 AM: type = adam
09/16 10:57:17 AM: parameter_groups = None
09/16 10:57:17 AM: Number of trainable parameters: 673602
09/16 10:57:17 AM: infer_type_and_cast = True
09/16 10:57:17 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:17 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:17 AM: lr = 0.0001
09/16 10:57:17 AM: amsgrad = True
09/16 10:57:17 AM: type = reduce_on_plateau
09/16 10:57:17 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:17 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:17 AM: mode = max
09/16 10:57:17 AM: factor = 0.5
09/16 10:57:17 AM: patience = 3
09/16 10:57:17 AM: threshold = 0.0001
09/16 10:57:17 AM: threshold_mode = abs
09/16 10:57:17 AM: verbose = True
09/16 10:57:17 AM: Starting training without restoring from a checkpoint.
09/16 10:57:17 AM: Training examples per task, before any subsampling: {'edges-srl-ontonotes': 231480}
09/16 10:57:17 AM: Beginning training with stopping criteria based on metric: edges-srl-ontonotes_f1
09/16 10:57:27 AM: Update 236: task edges-srl-ontonotes, batch 236 (236): mcc: 0.0646, acc: 0.0575, precision: 0.0673, recall: 0.0994, f1: 0.0803, edges-srl-ontonotes_loss: 0.1469
09/16 10:57:37 AM: Update 464: task edges-srl-ontonotes, batch 464 (464): mcc: 0.1819, acc: 0.1644, precision: 0.2004, recall: 0.1876, f1: 0.1938, edges-srl-ontonotes_loss: 0.0969
09/16 10:57:47 AM: Update 627: task edges-srl-ontonotes, batch 627 (627): mcc: 0.2691, acc: 0.2360, precision: 0.3042, recall: 0.2564, f1: 0.2783, edges-srl-ontonotes_loss: 0.0807
09/16 10:57:57 AM: Update 883: task edges-srl-ontonotes, batch 883 (883): mcc: 0.3457, acc: 0.2902, precision: 0.4015, recall: 0.3128, f1: 0.3516, edges-srl-ontonotes_loss: 0.0668
09/16 10:58:02 AM: ***** Step 1000 / Validation 1 *****
09/16 10:58:02 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 10:58:02 AM: Validating...
09/16 10:58:07 AM: Evaluate: task edges-srl-ontonotes, batch 152 (157): mcc: 0.6467, acc: 0.5202, precision: 0.7916, recall: 0.5354, f1: 0.6387, edges-srl-ontonotes_loss: 0.0275
09/16 10:58:07 AM: Best result seen so far for edges-srl-ontonotes.
09/16 10:58:07 AM: Best result seen so far for micro.
09/16 10:58:07 AM: Best result seen so far for macro.
09/16 10:58:07 AM: Updating LR scheduler:
09/16 10:58:07 AM: 	Best result seen so far for macro_avg: 0.639
09/16 10:58:07 AM: 	# validation passes without improvement: 0
09/16 10:58:07 AM: edges-srl-ontonotes_loss: training: 0.062522 validation: 0.027541
09/16 10:58:07 AM: macro_avg: validation: 0.638710
09/16 10:58:07 AM: micro_avg: validation: 0.000000
09/16 10:58:07 AM: edges-srl-ontonotes_mcc: training: 0.371732 validation: 0.646754
09/16 10:58:07 AM: edges-srl-ontonotes_acc: training: 0.309185 validation: 0.520283
09/16 10:58:07 AM: edges-srl-ontonotes_precision: training: 0.433986 validation: 0.791823
09/16 10:58:07 AM: edges-srl-ontonotes_recall: training: 0.332770 validation: 0.535217
09/16 10:58:07 AM: edges-srl-ontonotes_f1: training: 0.376698 validation: 0.638710
09/16 10:58:07 AM: Global learning rate: 0.0001
09/16 10:58:07 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 10:58:17 AM: Update 1220: task edges-srl-ontonotes, batch 220 (1220): mcc: 0.5984, acc: 0.4629, precision: 0.7322, recall: 0.4971, f1: 0.5921, edges-srl-ontonotes_loss: 0.0298
09/16 10:58:27 AM: Update 1430: task edges-srl-ontonotes, batch 430 (1430): mcc: 0.6086, acc: 0.4744, precision: 0.7343, recall: 0.5125, f1: 0.6036, edges-srl-ontonotes_loss: 0.0289
09/16 10:58:37 AM: Update 1637: task edges-srl-ontonotes, batch 637 (1637): mcc: 0.6122, acc: 0.4788, precision: 0.7328, recall: 0.5196, f1: 0.6081, edges-srl-ontonotes_loss: 0.0284
09/16 10:58:47 AM: Update 1846: task edges-srl-ontonotes, batch 846 (1846): mcc: 0.6127, acc: 0.4798, precision: 0.7325, recall: 0.5205, f1: 0.6086, edges-srl-ontonotes_loss: 0.0283
09/16 10:58:56 AM: ***** Step 2000 / Validation 2 *****
09/16 10:58:56 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 10:58:56 AM: Validating...
09/16 10:58:57 AM: Evaluate: task edges-srl-ontonotes, batch 34 (157): mcc: 0.6706, acc: 0.5580, precision: 0.7916, recall: 0.5752, f1: 0.6663, edges-srl-ontonotes_loss: 0.0244
09/16 10:59:01 AM: Best result seen so far for edges-srl-ontonotes.
09/16 10:59:01 AM: Best result seen so far for macro.
09/16 10:59:01 AM: Updating LR scheduler:
09/16 10:59:01 AM: 	Best result seen so far for macro_avg: 0.670
09/16 10:59:01 AM: 	# validation passes without improvement: 0
09/16 10:59:01 AM: edges-srl-ontonotes_loss: training: 0.028129 validation: 0.023906
09/16 10:59:01 AM: macro_avg: validation: 0.669557
09/16 10:59:01 AM: micro_avg: validation: 0.000000
09/16 10:59:01 AM: edges-srl-ontonotes_mcc: training: 0.613870 validation: 0.674347
09/16 10:59:01 AM: edges-srl-ontonotes_acc: training: 0.481785 validation: 0.560927
09/16 10:59:01 AM: edges-srl-ontonotes_precision: training: 0.733058 validation: 0.798189
09/16 10:59:01 AM: edges-srl-ontonotes_recall: training: 0.522171 validation: 0.576630
09/16 10:59:01 AM: edges-srl-ontonotes_f1: training: 0.609899 validation: 0.669557
09/16 10:59:01 AM: Global learning rate: 0.0001
09/16 10:59:01 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 10:59:07 AM: Update 2169: task edges-srl-ontonotes, batch 169 (2169): mcc: 0.6325, acc: 0.5065, precision: 0.7427, recall: 0.5467, f1: 0.6298, edges-srl-ontonotes_loss: 0.0268
09/16 10:59:17 AM: Update 2377: task edges-srl-ontonotes, batch 377 (2377): mcc: 0.6352, acc: 0.5105, precision: 0.7392, recall: 0.5539, f1: 0.6333, edges-srl-ontonotes_loss: 0.0263
09/16 10:59:27 AM: Update 2586: task edges-srl-ontonotes, batch 586 (2586): mcc: 0.6401, acc: 0.5179, precision: 0.7406, recall: 0.5614, f1: 0.6387, edges-srl-ontonotes_loss: 0.0259
09/16 10:59:40 AM: Update 2818: task edges-srl-ontonotes, batch 818 (2818): mcc: 0.6469, acc: 0.5269, precision: 0.7444, recall: 0.5703, f1: 0.6458, edges-srl-ontonotes_loss: 0.0255
09/16 10:59:47 AM: ***** Step 3000 / Validation 3 *****
09/16 10:59:47 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 10:59:47 AM: Validating...
09/16 10:59:50 AM: Evaluate: task edges-srl-ontonotes, batch 80 (157): mcc: 0.6813, acc: 0.5871, precision: 0.7823, recall: 0.6007, f1: 0.6796, edges-srl-ontonotes_loss: 0.0237
09/16 10:59:52 AM: Best result seen so far for edges-srl-ontonotes.
09/16 10:59:52 AM: Best result seen so far for macro.
09/16 10:59:52 AM: Updating LR scheduler:
09/16 10:59:52 AM: 	Best result seen so far for macro_avg: 0.684
09/16 10:59:52 AM: 	# validation passes without improvement: 0
09/16 10:59:52 AM: edges-srl-ontonotes_loss: training: 0.025240 validation: 0.023144
09/16 10:59:52 AM: macro_avg: validation: 0.683549
09/16 10:59:52 AM: micro_avg: validation: 0.000000
09/16 10:59:52 AM: edges-srl-ontonotes_mcc: training: 0.651107 validation: 0.685342
09/16 10:59:52 AM: edges-srl-ontonotes_acc: training: 0.532540 validation: 0.592256
09/16 10:59:52 AM: edges-srl-ontonotes_precision: training: 0.746660 validation: 0.786129
09/16 10:59:52 AM: edges-srl-ontonotes_recall: training: 0.575777 validation: 0.604649
09/16 10:59:52 AM: edges-srl-ontonotes_f1: training: 0.650178 validation: 0.683549
09/16 10:59:52 AM: Global learning rate: 0.0001
09/16 10:59:52 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:00:00 AM: Update 3161: task edges-srl-ontonotes, batch 161 (3161): mcc: 0.6774, acc: 0.5693, precision: 0.7609, recall: 0.6108, f1: 0.6776, edges-srl-ontonotes_loss: 0.0236
09/16 11:00:10 AM: Update 3392: task edges-srl-ontonotes, batch 392 (3392): mcc: 0.6653, acc: 0.5557, precision: 0.7532, recall: 0.5956, f1: 0.6652, edges-srl-ontonotes_loss: 0.0242
09/16 11:00:20 AM: Update 3622: task edges-srl-ontonotes, batch 622 (3622): mcc: 0.6661, acc: 0.5566, precision: 0.7545, recall: 0.5960, f1: 0.6659, edges-srl-ontonotes_loss: 0.0241
09/16 11:00:30 AM: Update 3835: task edges-srl-ontonotes, batch 835 (3835): mcc: 0.6664, acc: 0.5565, precision: 0.7546, recall: 0.5964, f1: 0.6663, edges-srl-ontonotes_loss: 0.0240
09/16 11:00:37 AM: ***** Step 4000 / Validation 4 *****
09/16 11:00:37 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:00:37 AM: Validating...
09/16 11:00:40 AM: Evaluate: task edges-srl-ontonotes, batch 55 (157): mcc: 0.6816, acc: 0.5923, precision: 0.7782, recall: 0.6042, f1: 0.6803, edges-srl-ontonotes_loss: 0.0237
09/16 11:00:43 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:00:43 AM: Best result seen so far for macro.
09/16 11:00:43 AM: Updating LR scheduler:
09/16 11:00:43 AM: 	Best result seen so far for macro_avg: 0.684
09/16 11:00:43 AM: 	# validation passes without improvement: 0
09/16 11:00:43 AM: edges-srl-ontonotes_loss: training: 0.024007 validation: 0.022886
09/16 11:00:43 AM: macro_avg: validation: 0.683862
09/16 11:00:43 AM: micro_avg: validation: 0.000000
09/16 11:00:43 AM: edges-srl-ontonotes_mcc: training: 0.666568 validation: 0.685080
09/16 11:00:43 AM: edges-srl-ontonotes_acc: training: 0.557521 validation: 0.595412
09/16 11:00:43 AM: edges-srl-ontonotes_precision: training: 0.754316 validation: 0.781040
09/16 11:00:43 AM: edges-srl-ontonotes_recall: training: 0.596886 validation: 0.608190
09/16 11:00:43 AM: edges-srl-ontonotes_f1: training: 0.666430 validation: 0.683862
09/16 11:00:43 AM: Global learning rate: 0.0001
09/16 11:00:43 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:00:50 AM: Update 4153: task edges-srl-ontonotes, batch 153 (4153): mcc: 0.6655, acc: 0.5615, precision: 0.7520, recall: 0.5969, f1: 0.6655, edges-srl-ontonotes_loss: 0.0239
09/16 11:01:00 AM: Update 4357: task edges-srl-ontonotes, batch 357 (4357): mcc: 0.6759, acc: 0.5725, precision: 0.7588, recall: 0.6098, f1: 0.6762, edges-srl-ontonotes_loss: 0.0234
09/16 11:01:10 AM: Update 4558: task edges-srl-ontonotes, batch 558 (4558): mcc: 0.6802, acc: 0.5780, precision: 0.7616, recall: 0.6151, f1: 0.6806, edges-srl-ontonotes_loss: 0.0231
09/16 11:01:20 AM: Update 4769: task edges-srl-ontonotes, batch 769 (4769): mcc: 0.6797, acc: 0.5774, precision: 0.7612, recall: 0.6147, f1: 0.6802, edges-srl-ontonotes_loss: 0.0232
09/16 11:01:30 AM: Update 4984: task edges-srl-ontonotes, batch 984 (4984): mcc: 0.6798, acc: 0.5766, precision: 0.7621, recall: 0.6141, f1: 0.6801, edges-srl-ontonotes_loss: 0.0232
09/16 11:01:31 AM: ***** Step 5000 / Validation 5 *****
09/16 11:01:31 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:01:31 AM: Validating...
09/16 11:01:36 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:01:36 AM: Best result seen so far for macro.
09/16 11:01:36 AM: Updating LR scheduler:
09/16 11:01:36 AM: 	Best result seen so far for macro_avg: 0.685
09/16 11:01:36 AM: 	# validation passes without improvement: 0
09/16 11:01:36 AM: edges-srl-ontonotes_loss: training: 0.023249 validation: 0.022607
09/16 11:01:36 AM: macro_avg: validation: 0.685104
09/16 11:01:36 AM: micro_avg: validation: 0.000000
09/16 11:01:36 AM: edges-srl-ontonotes_mcc: training: 0.679581 validation: 0.684309
09/16 11:01:36 AM: edges-srl-ontonotes_acc: training: 0.576298 validation: 0.600339
09/16 11:01:36 AM: edges-srl-ontonotes_precision: training: 0.761957 validation: 0.761582
09/16 11:01:36 AM: edges-srl-ontonotes_recall: training: 0.613810 validation: 0.622585
09/16 11:01:36 AM: edges-srl-ontonotes_f1: training: 0.679907 validation: 0.685104
09/16 11:01:36 AM: Global learning rate: 0.0001
09/16 11:01:36 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:01:40 AM: Update 5035: task edges-srl-ontonotes, batch 35 (5035): mcc: 0.6895, acc: 0.5938, precision: 0.7709, recall: 0.6241, f1: 0.6898, edges-srl-ontonotes_loss: 0.0229
09/16 11:01:50 AM: Update 5292: task edges-srl-ontonotes, batch 292 (5292): mcc: 0.7127, acc: 0.6166, precision: 0.7848, recall: 0.6545, f1: 0.7137, edges-srl-ontonotes_loss: 0.0209
09/16 11:02:00 AM: Update 5495: task edges-srl-ontonotes, batch 495 (5495): mcc: 0.7286, acc: 0.6370, precision: 0.7956, recall: 0.6742, f1: 0.7299, edges-srl-ontonotes_loss: 0.0200
09/16 11:02:10 AM: Update 5701: task edges-srl-ontonotes, batch 701 (5701): mcc: 0.7401, acc: 0.6513, precision: 0.8035, recall: 0.6884, f1: 0.7415, edges-srl-ontonotes_loss: 0.0193
09/16 11:02:20 AM: Update 5924: task edges-srl-ontonotes, batch 924 (5924): mcc: 0.7471, acc: 0.6603, precision: 0.8086, recall: 0.6968, f1: 0.7486, edges-srl-ontonotes_loss: 0.0189
09/16 11:02:26 AM: ***** Step 6000 / Validation 6 *****
09/16 11:02:26 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:02:26 AM: Validating...
09/16 11:02:30 AM: Evaluate: task edges-srl-ontonotes, batch 147 (157): mcc: 0.7222, acc: 0.6478, precision: 0.8031, recall: 0.6562, f1: 0.7223, edges-srl-ontonotes_loss: 0.0210
09/16 11:02:30 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:02:30 AM: Best result seen so far for macro.
09/16 11:02:30 AM: Updating LR scheduler:
09/16 11:02:30 AM: 	Best result seen so far for macro_avg: 0.720
09/16 11:02:30 AM: 	# validation passes without improvement: 0
09/16 11:02:30 AM: edges-srl-ontonotes_loss: training: 0.018791 validation: 0.021201
09/16 11:02:30 AM: macro_avg: validation: 0.719871
09/16 11:02:30 AM: micro_avg: validation: 0.000000
09/16 11:02:30 AM: edges-srl-ontonotes_mcc: training: 0.748830 validation: 0.719848
09/16 11:02:30 AM: edges-srl-ontonotes_acc: training: 0.662658 validation: 0.645062
09/16 11:02:30 AM: edges-srl-ontonotes_precision: training: 0.809886 validation: 0.801435
09/16 11:02:30 AM: edges-srl-ontonotes_recall: training: 0.698876 validation: 0.653375
09/16 11:02:30 AM: edges-srl-ontonotes_f1: training: 0.750298 validation: 0.719871
09/16 11:02:30 AM: Global learning rate: 0.0001
09/16 11:02:30 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:02:40 AM: Update 6237: task edges-srl-ontonotes, batch 237 (6237): mcc: 0.7812, acc: 0.7069, precision: 0.8312, recall: 0.7401, f1: 0.7830, edges-srl-ontonotes_loss: 0.0168
09/16 11:02:50 AM: Update 6438: task edges-srl-ontonotes, batch 438 (6438): mcc: 0.7960, acc: 0.7263, precision: 0.8442, recall: 0.7561, f1: 0.7977, edges-srl-ontonotes_loss: 0.0163
09/16 11:03:00 AM: Update 6632: task edges-srl-ontonotes, batch 632 (6632): mcc: 0.7949, acc: 0.7262, precision: 0.8426, recall: 0.7555, f1: 0.7966, edges-srl-ontonotes_loss: 0.0164
09/16 11:03:10 AM: Update 6833: task edges-srl-ontonotes, batch 833 (6833): mcc: 0.7783, acc: 0.7050, precision: 0.8294, recall: 0.7363, f1: 0.7801, edges-srl-ontonotes_loss: 0.0174
09/16 11:03:20 AM: ***** Step 7000 / Validation 7 *****
09/16 11:03:20 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:03:20 AM: Validating...
09/16 11:03:20 AM: Evaluate: task edges-srl-ontonotes, batch 3 (157): mcc: 0.7444, acc: 0.6642, precision: 0.8190, recall: 0.6830, f1: 0.7449, edges-srl-ontonotes_loss: 0.0198
09/16 11:03:27 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:03:27 AM: Best result seen so far for macro.
09/16 11:03:27 AM: Updating LR scheduler:
09/16 11:03:27 AM: 	Best result seen so far for macro_avg: 0.732
09/16 11:03:27 AM: 	# validation passes without improvement: 0
09/16 11:03:27 AM: edges-srl-ontonotes_loss: training: 0.018221 validation: 0.020164
09/16 11:03:27 AM: macro_avg: validation: 0.732029
09/16 11:03:27 AM: micro_avg: validation: 0.000000
09/16 11:03:27 AM: edges-srl-ontonotes_mcc: training: 0.763981 validation: 0.731542
09/16 11:03:27 AM: edges-srl-ontonotes_acc: training: 0.687014 validation: 0.658764
09/16 11:03:27 AM: edges-srl-ontonotes_precision: training: 0.818491 validation: 0.806936
09/16 11:03:27 AM: edges-srl-ontonotes_recall: training: 0.719330 validation: 0.669848
09/16 11:03:27 AM: edges-srl-ontonotes_f1: training: 0.765714 validation: 0.732029
09/16 11:03:27 AM: Global learning rate: 0.0001
09/16 11:03:27 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:03:30 AM: Update 7073: task edges-srl-ontonotes, batch 73 (7073): mcc: 0.6975, acc: 0.6022, precision: 0.7655, recall: 0.6431, f1: 0.6990, edges-srl-ontonotes_loss: 0.0221
09/16 11:03:40 AM: Update 7240: task edges-srl-ontonotes, batch 240 (7240): mcc: 0.6967, acc: 0.5998, precision: 0.7704, recall: 0.6376, f1: 0.6977, edges-srl-ontonotes_loss: 0.0222
09/16 11:03:51 AM: Update 7437: task edges-srl-ontonotes, batch 437 (7437): mcc: 0.7122, acc: 0.6193, precision: 0.7824, recall: 0.6555, f1: 0.7133, edges-srl-ontonotes_loss: 0.0211
09/16 11:04:01 AM: Update 7654: task edges-srl-ontonotes, batch 654 (7654): mcc: 0.7243, acc: 0.6345, precision: 0.7906, recall: 0.6705, f1: 0.7256, edges-srl-ontonotes_loss: 0.0204
09/16 11:04:11 AM: Update 7879: task edges-srl-ontonotes, batch 879 (7879): mcc: 0.7315, acc: 0.6435, precision: 0.7962, recall: 0.6789, f1: 0.7329, edges-srl-ontonotes_loss: 0.0198
09/16 11:04:17 AM: ***** Step 8000 / Validation 8 *****
09/16 11:04:17 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:04:17 AM: Validating...
09/16 11:04:21 AM: Evaluate: task edges-srl-ontonotes, batch 103 (157): mcc: 0.7560, acc: 0.6895, precision: 0.8256, recall: 0.6985, f1: 0.7567, edges-srl-ontonotes_loss: 0.0183
09/16 11:04:22 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:04:22 AM: Best result seen so far for macro.
09/16 11:04:22 AM: Updating LR scheduler:
09/16 11:04:22 AM: 	Best result seen so far for macro_avg: 0.748
09/16 11:04:22 AM: 	# validation passes without improvement: 0
09/16 11:04:22 AM: edges-srl-ontonotes_loss: training: 0.019961 validation: 0.018773
09/16 11:04:22 AM: macro_avg: validation: 0.748236
09/16 11:04:22 AM: micro_avg: validation: 0.000000
09/16 11:04:22 AM: edges-srl-ontonotes_mcc: training: 0.729521 validation: 0.747452
09/16 11:04:22 AM: edges-srl-ontonotes_acc: training: 0.641530 validation: 0.679316
09/16 11:04:22 AM: edges-srl-ontonotes_precision: training: 0.794170 validation: 0.817402
09/16 11:04:22 AM: edges-srl-ontonotes_recall: training: 0.677044 validation: 0.689862
09/16 11:04:22 AM: edges-srl-ontonotes_f1: training: 0.730944 validation: 0.748236
09/16 11:04:22 AM: Global learning rate: 0.0001
09/16 11:04:22 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:04:33 AM: Update 8186: task edges-srl-ontonotes, batch 186 (8186): mcc: 0.7128, acc: 0.6248, precision: 0.7764, recall: 0.6617, f1: 0.7145, edges-srl-ontonotes_loss: 0.0209
09/16 11:04:43 AM: Update 8438: task edges-srl-ontonotes, batch 438 (8438): mcc: 0.7031, acc: 0.6121, precision: 0.7709, recall: 0.6488, f1: 0.7046, edges-srl-ontonotes_loss: 0.0215
09/16 11:04:53 AM: Update 8631: task edges-srl-ontonotes, batch 631 (8631): mcc: 0.7011, acc: 0.6085, precision: 0.7698, recall: 0.6460, f1: 0.7025, edges-srl-ontonotes_loss: 0.0216
09/16 11:05:03 AM: Update 8863: task edges-srl-ontonotes, batch 863 (8863): mcc: 0.7003, acc: 0.6069, precision: 0.7690, recall: 0.6452, f1: 0.7017, edges-srl-ontonotes_loss: 0.0216
09/16 11:05:09 AM: ***** Step 9000 / Validation 9 *****
09/16 11:05:09 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:05:09 AM: Validating...
09/16 11:05:13 AM: Evaluate: task edges-srl-ontonotes, batch 137 (157): mcc: 0.7486, acc: 0.6745, precision: 0.8260, recall: 0.6846, f1: 0.7487, edges-srl-ontonotes_loss: 0.0184
09/16 11:05:14 AM: Updating LR scheduler:
09/16 11:05:14 AM: 	Best result seen so far for macro_avg: 0.748
09/16 11:05:14 AM: 	# validation passes without improvement: 1
09/16 11:05:14 AM: edges-srl-ontonotes_loss: training: 0.021825 validation: 0.018744
09/16 11:05:14 AM: macro_avg: validation: 0.742004
09/16 11:05:14 AM: micro_avg: validation: 0.000000
09/16 11:05:14 AM: edges-srl-ontonotes_mcc: training: 0.695979 validation: 0.741973
09/16 11:05:14 AM: edges-srl-ontonotes_acc: training: 0.601392 validation: 0.666384
09/16 11:05:14 AM: edges-srl-ontonotes_precision: training: 0.766128 validation: 0.821145
09/16 11:05:14 AM: edges-srl-ontonotes_recall: training: 0.639852 validation: 0.676776
09/16 11:05:14 AM: edges-srl-ontonotes_f1: training: 0.697319 validation: 0.742004
09/16 11:05:14 AM: Global learning rate: 0.0001
09/16 11:05:14 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:05:23 AM: Update 9158: task edges-srl-ontonotes, batch 158 (9158): mcc: 0.6778, acc: 0.5803, precision: 0.7564, recall: 0.6152, f1: 0.6785, edges-srl-ontonotes_loss: 0.0230
09/16 11:05:33 AM: Update 9390: task edges-srl-ontonotes, batch 390 (9390): mcc: 0.6804, acc: 0.5815, precision: 0.7598, recall: 0.6170, f1: 0.6810, edges-srl-ontonotes_loss: 0.0228
09/16 11:05:43 AM: Update 9590: task edges-srl-ontonotes, batch 590 (9590): mcc: 0.6861, acc: 0.5892, precision: 0.7632, recall: 0.6245, f1: 0.6869, edges-srl-ontonotes_loss: 0.0224
09/16 11:05:53 AM: Update 9836: task edges-srl-ontonotes, batch 836 (9836): mcc: 0.6876, acc: 0.5911, precision: 0.7625, recall: 0.6278, f1: 0.6886, edges-srl-ontonotes_loss: 0.0223
09/16 11:06:01 AM: ***** Step 10000 / Validation 10 *****
09/16 11:06:01 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:06:01 AM: Validating...
09/16 11:06:03 AM: Evaluate: task edges-srl-ontonotes, batch 63 (157): mcc: 0.7182, acc: 0.6437, precision: 0.7979, recall: 0.6533, f1: 0.7184, edges-srl-ontonotes_loss: 0.0203
09/16 11:06:06 AM: Updating LR scheduler:
09/16 11:06:06 AM: 	Best result seen so far for macro_avg: 0.748
09/16 11:06:06 AM: 	# validation passes without improvement: 2
09/16 11:06:06 AM: edges-srl-ontonotes_loss: training: 0.022095 validation: 0.019621
09/16 11:06:06 AM: macro_avg: validation: 0.725569
09/16 11:06:06 AM: micro_avg: validation: 0.000000
09/16 11:06:06 AM: edges-srl-ontonotes_mcc: training: 0.690211 validation: 0.724908
09/16 11:06:06 AM: edges-srl-ontonotes_acc: training: 0.593965 validation: 0.653991
09/16 11:06:06 AM: edges-srl-ontonotes_precision: training: 0.763808 validation: 0.799167
09/16 11:06:06 AM: edges-srl-ontonotes_recall: training: 0.631363 validation: 0.664383
09/16 11:06:06 AM: edges-srl-ontonotes_f1: training: 0.691299 validation: 0.725569
09/16 11:06:06 AM: Global learning rate: 0.0001
09/16 11:06:06 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:06:13 AM: Update 10114: task edges-srl-ontonotes, batch 114 (10114): mcc: 0.7176, acc: 0.6285, precision: 0.7830, recall: 0.6649, f1: 0.7191, edges-srl-ontonotes_loss: 0.0209
09/16 11:06:23 AM: Update 10345: task edges-srl-ontonotes, batch 345 (10345): mcc: 0.7128, acc: 0.6224, precision: 0.7802, recall: 0.6585, f1: 0.7142, edges-srl-ontonotes_loss: 0.0209
09/16 11:06:33 AM: Update 10570: task edges-srl-ontonotes, batch 570 (10570): mcc: 0.7079, acc: 0.6169, precision: 0.7765, recall: 0.6527, f1: 0.7093, edges-srl-ontonotes_loss: 0.0212
09/16 11:06:43 AM: Update 10806: task edges-srl-ontonotes, batch 806 (10806): mcc: 0.7055, acc: 0.6140, precision: 0.7743, recall: 0.6502, f1: 0.7069, edges-srl-ontonotes_loss: 0.0213
09/16 11:06:51 AM: ***** Step 11000 / Validation 11 *****
09/16 11:06:51 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:06:51 AM: Validating...
09/16 11:06:53 AM: Evaluate: task edges-srl-ontonotes, batch 85 (157): mcc: 0.7149, acc: 0.6437, precision: 0.7913, recall: 0.6529, f1: 0.7155, edges-srl-ontonotes_loss: 0.0204
09/16 11:06:56 AM: Updating LR scheduler:
09/16 11:06:56 AM: 	Best result seen so far for macro_avg: 0.748
09/16 11:06:56 AM: 	# validation passes without improvement: 3
09/16 11:06:56 AM: edges-srl-ontonotes_loss: training: 0.021310 validation: 0.020088
09/16 11:06:56 AM: macro_avg: validation: 0.721834
09/16 11:06:56 AM: micro_avg: validation: 0.000000
09/16 11:06:56 AM: edges-srl-ontonotes_mcc: training: 0.705974 validation: 0.721176
09/16 11:06:56 AM: edges-srl-ontonotes_acc: training: 0.614677 validation: 0.650142
09/16 11:06:56 AM: edges-srl-ontonotes_precision: training: 0.775294 validation: 0.795899
09/16 11:06:56 AM: edges-srl-ontonotes_recall: training: 0.650234 validation: 0.660380
09/16 11:06:56 AM: edges-srl-ontonotes_f1: training: 0.707278 validation: 0.721834
09/16 11:06:56 AM: Global learning rate: 0.0001
09/16 11:06:56 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:07:03 AM: Update 11178: task edges-srl-ontonotes, batch 178 (11178): mcc: 0.7004, acc: 0.6092, precision: 0.7694, recall: 0.6452, f1: 0.7018, edges-srl-ontonotes_loss: 0.0215
09/16 11:07:13 AM: Update 11330: task edges-srl-ontonotes, batch 330 (11330): mcc: 0.6998, acc: 0.6078, precision: 0.7698, recall: 0.6437, f1: 0.7011, edges-srl-ontonotes_loss: 0.0214
09/16 11:07:23 AM: Update 11576: task edges-srl-ontonotes, batch 576 (11576): mcc: 0.7051, acc: 0.6158, precision: 0.7729, recall: 0.6507, f1: 0.7066, edges-srl-ontonotes_loss: 0.0212
09/16 11:07:33 AM: Update 11810: task edges-srl-ontonotes, batch 810 (11810): mcc: 0.7066, acc: 0.6166, precision: 0.7742, recall: 0.6522, f1: 0.7080, edges-srl-ontonotes_loss: 0.0211
09/16 11:07:42 AM: ***** Step 12000 / Validation 12 *****
09/16 11:07:42 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:07:42 AM: Validating...
09/16 11:07:43 AM: Evaluate: task edges-srl-ontonotes, batch 27 (157): mcc: 0.7252, acc: 0.6514, precision: 0.8059, recall: 0.6592, f1: 0.7252, edges-srl-ontonotes_loss: 0.0197
09/16 11:07:47 AM: Updating LR scheduler:
09/16 11:07:47 AM: 	Best result seen so far for macro_avg: 0.748
09/16 11:07:47 AM: 	# validation passes without improvement: 0
09/16 11:07:47 AM: edges-srl-ontonotes_loss: training: 0.021169 validation: 0.020072
09/16 11:07:47 AM: macro_avg: validation: 0.715603
09/16 11:07:47 AM: micro_avg: validation: 0.000000
09/16 11:07:47 AM: edges-srl-ontonotes_mcc: training: 0.705501 validation: 0.715299
09/16 11:07:47 AM: edges-srl-ontonotes_acc: training: 0.615732 validation: 0.643292
09/16 11:07:47 AM: edges-srl-ontonotes_precision: training: 0.773442 validation: 0.794457
09/16 11:07:47 AM: edges-srl-ontonotes_recall: training: 0.650951 validation: 0.650989
09/16 11:07:47 AM: edges-srl-ontonotes_f1: training: 0.706929 validation: 0.715603
09/16 11:07:47 AM: Global learning rate: 5e-05
09/16 11:07:47 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:07:53 AM: Update 12137: task edges-srl-ontonotes, batch 137 (12137): mcc: 0.6936, acc: 0.6015, precision: 0.7679, recall: 0.6341, f1: 0.6946, edges-srl-ontonotes_loss: 0.0220
09/16 11:08:03 AM: Update 12333: task edges-srl-ontonotes, batch 333 (12333): mcc: 0.7048, acc: 0.6134, precision: 0.7750, recall: 0.6483, f1: 0.7060, edges-srl-ontonotes_loss: 0.0212
09/16 11:08:13 AM: Update 12564: task edges-srl-ontonotes, batch 564 (12564): mcc: 0.7213, acc: 0.6337, precision: 0.7861, recall: 0.6690, f1: 0.7228, edges-srl-ontonotes_loss: 0.0201
09/16 11:08:23 AM: Update 12797: task edges-srl-ontonotes, batch 797 (12797): mcc: 0.7377, acc: 0.6530, precision: 0.7975, recall: 0.6893, f1: 0.7394, edges-srl-ontonotes_loss: 0.0191
09/16 11:08:33 AM: ***** Step 13000 / Validation 13 *****
09/16 11:08:33 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:08:33 AM: Validating...
09/16 11:08:33 AM: Evaluate: task edges-srl-ontonotes, batch 10 (157): mcc: 0.7470, acc: 0.6749, precision: 0.8145, recall: 0.6916, f1: 0.7480, edges-srl-ontonotes_loss: 0.0184
09/16 11:08:38 AM: Updating LR scheduler:
09/16 11:08:38 AM: 	Best result seen so far for macro_avg: 0.748
09/16 11:08:38 AM: 	# validation passes without improvement: 1
09/16 11:08:38 AM: edges-srl-ontonotes_loss: training: 0.018598 validation: 0.019433
09/16 11:08:38 AM: macro_avg: validation: 0.741640
09/16 11:08:38 AM: micro_avg: validation: 0.000000
09/16 11:08:38 AM: edges-srl-ontonotes_mcc: training: 0.746163 validation: 0.740468
09/16 11:08:38 AM: edges-srl-ontonotes_acc: training: 0.663465 validation: 0.673697
09/16 11:08:38 AM: edges-srl-ontonotes_precision: training: 0.803568 validation: 0.806659
09/16 11:08:38 AM: edges-srl-ontonotes_recall: training: 0.699490 validation: 0.686321
09/16 11:08:38 AM: edges-srl-ontonotes_f1: training: 0.747926 validation: 0.741640
09/16 11:08:38 AM: Global learning rate: 5e-05
09/16 11:08:38 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:08:43 AM: Update 13125: task edges-srl-ontonotes, batch 125 (13125): mcc: 0.7766, acc: 0.7035, precision: 0.8261, recall: 0.7360, f1: 0.7785, edges-srl-ontonotes_loss: 0.0166
09/16 11:08:53 AM: Update 13285: task edges-srl-ontonotes, batch 285 (13285): mcc: 0.7799, acc: 0.7072, precision: 0.8286, recall: 0.7400, f1: 0.7818, edges-srl-ontonotes_loss: 0.0164
09/16 11:09:04 AM: Update 13507: task edges-srl-ontonotes, batch 507 (13507): mcc: 0.7859, acc: 0.7154, precision: 0.8328, recall: 0.7475, f1: 0.7878, edges-srl-ontonotes_loss: 0.0160
09/16 11:09:14 AM: Update 13721: task edges-srl-ontonotes, batch 721 (13721): mcc: 0.7975, acc: 0.7303, precision: 0.8426, recall: 0.7604, f1: 0.7994, edges-srl-ontonotes_loss: 0.0155
09/16 11:09:24 AM: Update 13944: task edges-srl-ontonotes, batch 944 (13944): mcc: 0.7931, acc: 0.7249, precision: 0.8392, recall: 0.7551, f1: 0.7949, edges-srl-ontonotes_loss: 0.0159
09/16 11:09:27 AM: ***** Step 14000 / Validation 14 *****
09/16 11:09:27 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:09:27 AM: Validating...
09/16 11:09:33 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:09:33 AM: Best result seen so far for macro.
09/16 11:09:33 AM: Updating LR scheduler:
09/16 11:09:33 AM: 	Best result seen so far for macro_avg: 0.752
09/16 11:09:33 AM: 	# validation passes without improvement: 0
09/16 11:09:33 AM: edges-srl-ontonotes_loss: training: 0.016123 validation: 0.018521
09/16 11:09:33 AM: macro_avg: validation: 0.752275
09/16 11:09:33 AM: micro_avg: validation: 0.000000
09/16 11:09:33 AM: edges-srl-ontonotes_mcc: training: 0.789447 validation: 0.751233
09/16 11:09:33 AM: edges-srl-ontonotes_acc: training: 0.720316 validation: 0.685475
09/16 11:09:33 AM: edges-srl-ontonotes_precision: training: 0.836417 validation: 0.817468
09/16 11:09:33 AM: edges-srl-ontonotes_recall: training: 0.750799 validation: 0.696713
09/16 11:09:33 AM: edges-srl-ontonotes_f1: training: 0.791299 validation: 0.752275
09/16 11:09:33 AM: Global learning rate: 5e-05
09/16 11:09:33 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:09:34 AM: Update 14040: task edges-srl-ontonotes, batch 40 (14040): mcc: 0.7560, acc: 0.6782, precision: 0.8106, recall: 0.7116, f1: 0.7579, edges-srl-ontonotes_loss: 0.0184
09/16 11:09:44 AM: Update 14231: task edges-srl-ontonotes, batch 231 (14231): mcc: 0.7229, acc: 0.6391, precision: 0.7841, recall: 0.6736, f1: 0.7247, edges-srl-ontonotes_loss: 0.0201
09/16 11:09:54 AM: Update 14451: task edges-srl-ontonotes, batch 451 (14451): mcc: 0.7145, acc: 0.6281, precision: 0.7786, recall: 0.6631, f1: 0.7162, edges-srl-ontonotes_loss: 0.0206
09/16 11:10:04 AM: Update 14621: task edges-srl-ontonotes, batch 621 (14621): mcc: 0.7232, acc: 0.6383, precision: 0.7869, recall: 0.6717, f1: 0.7248, edges-srl-ontonotes_loss: 0.0201
09/16 11:10:14 AM: Update 14825: task edges-srl-ontonotes, batch 825 (14825): mcc: 0.7304, acc: 0.6467, precision: 0.7933, recall: 0.6794, f1: 0.7320, edges-srl-ontonotes_loss: 0.0196
09/16 11:10:23 AM: ***** Step 15000 / Validation 15 *****
09/16 11:10:23 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:10:23 AM: Validating...
09/16 11:10:24 AM: Evaluate: task edges-srl-ontonotes, batch 53 (157): mcc: 0.7520, acc: 0.6816, precision: 0.8253, recall: 0.6914, f1: 0.7524, edges-srl-ontonotes_loss: 0.0186
09/16 11:10:28 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:10:28 AM: Best result seen so far for macro.
09/16 11:10:28 AM: Updating LR scheduler:
09/16 11:10:28 AM: 	Best result seen so far for macro_avg: 0.760
09/16 11:10:28 AM: 	# validation passes without improvement: 0
09/16 11:10:28 AM: edges-srl-ontonotes_loss: training: 0.019250 validation: 0.017788
09/16 11:10:28 AM: macro_avg: validation: 0.759629
09/16 11:10:28 AM: micro_avg: validation: 0.000000
09/16 11:10:28 AM: edges-srl-ontonotes_mcc: training: 0.736045 validation: 0.758854
09/16 11:10:28 AM: edges-srl-ontonotes_acc: training: 0.653252 validation: 0.691479
09/16 11:10:28 AM: edges-srl-ontonotes_precision: training: 0.797724 validation: 0.827317
09/16 11:10:28 AM: edges-srl-ontonotes_recall: training: 0.685941 validation: 0.702178
09/16 11:10:28 AM: edges-srl-ontonotes_f1: training: 0.737621 validation: 0.759629
09/16 11:10:28 AM: Global learning rate: 5e-05
09/16 11:10:28 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:10:35 AM: Update 15124: task edges-srl-ontonotes, batch 124 (15124): mcc: 0.7667, acc: 0.6899, precision: 0.8197, recall: 0.7233, f1: 0.7685, edges-srl-ontonotes_loss: 0.0175
09/16 11:10:45 AM: Update 15346: task edges-srl-ontonotes, batch 346 (15346): mcc: 0.7408, acc: 0.6584, precision: 0.7993, recall: 0.6933, f1: 0.7426, edges-srl-ontonotes_loss: 0.0190
09/16 11:10:55 AM: Update 15525: task edges-srl-ontonotes, batch 525 (15525): mcc: 0.7341, acc: 0.6508, precision: 0.7934, recall: 0.6861, f1: 0.7358, edges-srl-ontonotes_loss: 0.0195
09/16 11:11:05 AM: Update 15745: task edges-srl-ontonotes, batch 745 (15745): mcc: 0.7279, acc: 0.6429, precision: 0.7894, recall: 0.6783, f1: 0.7296, edges-srl-ontonotes_loss: 0.0198
09/16 11:11:15 AM: Update 15994: task edges-srl-ontonotes, batch 994 (15994): mcc: 0.7244, acc: 0.6384, precision: 0.7858, recall: 0.6748, f1: 0.7261, edges-srl-ontonotes_loss: 0.0200
09/16 11:11:15 AM: ***** Step 16000 / Validation 16 *****
09/16 11:11:15 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:11:15 AM: Validating...
09/16 11:11:20 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:11:20 AM: Best result seen so far for macro.
09/16 11:11:20 AM: Updating LR scheduler:
09/16 11:11:20 AM: 	Best result seen so far for macro_avg: 0.762
09/16 11:11:20 AM: 	# validation passes without improvement: 0
09/16 11:11:20 AM: edges-srl-ontonotes_loss: training: 0.019974 validation: 0.017711
09/16 11:11:20 AM: macro_avg: validation: 0.761638
09/16 11:11:20 AM: micro_avg: validation: 0.000000
09/16 11:11:20 AM: edges-srl-ontonotes_mcc: training: 0.724194 validation: 0.760064
09/16 11:11:20 AM: edges-srl-ontonotes_acc: training: 0.638177 validation: 0.698253
09/16 11:11:20 AM: edges-srl-ontonotes_precision: training: 0.785603 validation: 0.817660
09/16 11:11:20 AM: edges-srl-ontonotes_recall: training: 0.674689 validation: 0.712801
09/16 11:11:20 AM: edges-srl-ontonotes_f1: training: 0.725934 validation: 0.761638
09/16 11:11:20 AM: Global learning rate: 5e-05
09/16 11:11:20 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:11:25 AM: Update 16106: task edges-srl-ontonotes, batch 106 (16106): mcc: 0.7040, acc: 0.6128, precision: 0.7685, recall: 0.6524, f1: 0.7057, edges-srl-ontonotes_loss: 0.0212
09/16 11:11:35 AM: Update 16322: task edges-srl-ontonotes, batch 322 (16322): mcc: 0.6911, acc: 0.5979, precision: 0.7602, recall: 0.6360, f1: 0.6926, edges-srl-ontonotes_loss: 0.0219
09/16 11:11:45 AM: Update 16494: task edges-srl-ontonotes, batch 494 (16494): mcc: 0.6920, acc: 0.5984, precision: 0.7631, recall: 0.6352, f1: 0.6933, edges-srl-ontonotes_loss: 0.0219
09/16 11:11:55 AM: Update 16723: task edges-srl-ontonotes, batch 723 (16723): mcc: 0.6929, acc: 0.5994, precision: 0.7651, recall: 0.6351, f1: 0.6941, edges-srl-ontonotes_loss: 0.0218
09/16 11:12:05 AM: Update 16977: task edges-srl-ontonotes, batch 977 (16977): mcc: 0.6969, acc: 0.6047, precision: 0.7668, recall: 0.6410, f1: 0.6983, edges-srl-ontonotes_loss: 0.0215
09/16 11:12:07 AM: ***** Step 17000 / Validation 17 *****
09/16 11:12:07 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:12:07 AM: Validating...
09/16 11:12:12 AM: Updating LR scheduler:
09/16 11:12:12 AM: 	Best result seen so far for macro_avg: 0.762
09/16 11:12:12 AM: 	# validation passes without improvement: 1
09/16 11:12:12 AM: edges-srl-ontonotes_loss: training: 0.021485 validation: 0.018341
09/16 11:12:12 AM: macro_avg: validation: 0.742444
09/16 11:12:12 AM: micro_avg: validation: 0.000000
09/16 11:12:12 AM: edges-srl-ontonotes_mcc: training: 0.696771 validation: 0.742101
09/16 11:12:12 AM: edges-srl-ontonotes_acc: training: 0.604631 validation: 0.670541
09/16 11:12:12 AM: edges-srl-ontonotes_precision: training: 0.766404 validation: 0.817837
09/16 11:12:12 AM: edges-srl-ontonotes_recall: training: 0.641057 validation: 0.679778
09/16 11:12:12 AM: edges-srl-ontonotes_f1: training: 0.698149 validation: 0.742444
09/16 11:12:12 AM: Global learning rate: 5e-05
09/16 11:12:12 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:12:15 AM: Update 17077: task edges-srl-ontonotes, batch 77 (17077): mcc: 0.7170, acc: 0.6312, precision: 0.7792, recall: 0.6669, f1: 0.7187, edges-srl-ontonotes_loss: 0.0204
09/16 11:12:25 AM: Update 17287: task edges-srl-ontonotes, batch 287 (17287): mcc: 0.7168, acc: 0.6302, precision: 0.7799, recall: 0.6661, f1: 0.7185, edges-srl-ontonotes_loss: 0.0204
09/16 11:12:35 AM: Update 17489: task edges-srl-ontonotes, batch 489 (17489): mcc: 0.7209, acc: 0.6351, precision: 0.7829, recall: 0.6710, f1: 0.7226, edges-srl-ontonotes_loss: 0.0202
09/16 11:12:45 AM: Update 17672: task edges-srl-ontonotes, batch 672 (17672): mcc: 0.7196, acc: 0.6338, precision: 0.7819, recall: 0.6694, f1: 0.7213, edges-srl-ontonotes_loss: 0.0203
09/16 11:12:55 AM: Update 17910: task edges-srl-ontonotes, batch 910 (17910): mcc: 0.7172, acc: 0.6304, precision: 0.7806, recall: 0.6663, f1: 0.7189, edges-srl-ontonotes_loss: 0.0204
09/16 11:13:00 AM: ***** Step 18000 / Validation 18 *****
09/16 11:13:00 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:13:00 AM: Validating...
09/16 11:13:05 AM: Updating LR scheduler:
09/16 11:13:05 AM: 	Best result seen so far for macro_avg: 0.762
09/16 11:13:05 AM: 	# validation passes without improvement: 2
09/16 11:13:05 AM: edges-srl-ontonotes_loss: training: 0.020477 validation: 0.018632
09/16 11:13:05 AM: macro_avg: validation: 0.743197
09/16 11:13:05 AM: micro_avg: validation: 0.000000
09/16 11:13:05 AM: edges-srl-ontonotes_mcc: training: 0.716368 validation: 0.742543
09/16 11:13:05 AM: edges-srl-ontonotes_acc: training: 0.629229 validation: 0.673466
09/16 11:13:05 AM: edges-srl-ontonotes_precision: training: 0.779952 validation: 0.814686
09/16 11:13:05 AM: edges-srl-ontonotes_recall: training: 0.665219 validation: 0.683242
09/16 11:13:05 AM: edges-srl-ontonotes_f1: training: 0.718031 validation: 0.743197
09/16 11:13:05 AM: Global learning rate: 5e-05
09/16 11:13:05 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:13:05 AM: Update 18002: task edges-srl-ontonotes, batch 2 (18002): mcc: 0.7160, acc: 0.6429, precision: 0.7914, recall: 0.6548, f1: 0.7166, edges-srl-ontonotes_loss: 0.0200
09/16 11:13:15 AM: Update 18230: task edges-srl-ontonotes, batch 230 (18230): mcc: 0.7179, acc: 0.6301, precision: 0.7838, recall: 0.6648, f1: 0.7194, edges-srl-ontonotes_loss: 0.0204
09/16 11:13:25 AM: Update 18458: task edges-srl-ontonotes, batch 458 (18458): mcc: 0.7137, acc: 0.6251, precision: 0.7790, recall: 0.6612, f1: 0.7153, edges-srl-ontonotes_loss: 0.0206
09/16 11:13:35 AM: Update 18614: task edges-srl-ontonotes, batch 614 (18614): mcc: 0.7147, acc: 0.6264, precision: 0.7803, recall: 0.6618, f1: 0.7162, edges-srl-ontonotes_loss: 0.0206
09/16 11:13:45 AM: Update 18847: task edges-srl-ontonotes, batch 847 (18847): mcc: 0.7160, acc: 0.6289, precision: 0.7805, recall: 0.6641, f1: 0.7176, edges-srl-ontonotes_loss: 0.0205
09/16 11:13:52 AM: ***** Step 19000 / Validation 19 *****
09/16 11:13:52 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:13:52 AM: Validating...
09/16 11:13:55 AM: Evaluate: task edges-srl-ontonotes, batch 88 (157): mcc: 0.7360, acc: 0.6699, precision: 0.8020, recall: 0.6822, f1: 0.7373, edges-srl-ontonotes_loss: 0.0190
09/16 11:13:57 AM: Updating LR scheduler:
09/16 11:13:57 AM: 	Best result seen so far for macro_avg: 0.762
09/16 11:13:57 AM: 	# validation passes without improvement: 3
09/16 11:13:57 AM: edges-srl-ontonotes_loss: training: 0.020454 validation: 0.018903
09/16 11:13:57 AM: macro_avg: validation: 0.740765
09/16 11:13:57 AM: micro_avg: validation: 0.000000
09/16 11:13:57 AM: edges-srl-ontonotes_mcc: training: 0.716703 validation: 0.739356
09/16 11:13:57 AM: edges-srl-ontonotes_acc: training: 0.629745 validation: 0.674929
09/16 11:13:57 AM: edges-srl-ontonotes_precision: training: 0.781115 validation: 0.802695
09/16 11:13:57 AM: edges-srl-ontonotes_recall: training: 0.664828 validation: 0.687707
09/16 11:13:57 AM: edges-srl-ontonotes_f1: training: 0.718296 validation: 0.740765
09/16 11:13:57 AM: Global learning rate: 5e-05
09/16 11:13:57 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:14:05 AM: Update 19185: task edges-srl-ontonotes, batch 185 (19185): mcc: 0.7217, acc: 0.6377, precision: 0.7837, recall: 0.6718, f1: 0.7234, edges-srl-ontonotes_loss: 0.0201
09/16 11:14:15 AM: Update 19424: task edges-srl-ontonotes, batch 424 (19424): mcc: 0.7105, acc: 0.6228, precision: 0.7780, recall: 0.6562, f1: 0.7119, edges-srl-ontonotes_loss: 0.0209
09/16 11:14:25 AM: Update 19633: task edges-srl-ontonotes, batch 633 (19633): mcc: 0.7198, acc: 0.6335, precision: 0.7859, recall: 0.6665, f1: 0.7213, edges-srl-ontonotes_loss: 0.0203
09/16 11:14:36 AM: Update 19814: task edges-srl-ontonotes, batch 814 (19814): mcc: 0.7284, acc: 0.6430, precision: 0.7924, recall: 0.6766, f1: 0.7299, edges-srl-ontonotes_loss: 0.0198
09/16 11:14:43 AM: ***** Step 20000 / Validation 20 *****
09/16 11:14:43 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:14:43 AM: Validating...
09/16 11:14:46 AM: Evaluate: task edges-srl-ontonotes, batch 75 (157): mcc: 0.7467, acc: 0.6829, precision: 0.8065, recall: 0.6978, f1: 0.7482, edges-srl-ontonotes_loss: 0.0189
09/16 11:14:48 AM: Updating LR scheduler:
09/16 11:14:48 AM: 	Best result seen so far for macro_avg: 0.762
09/16 11:14:48 AM: 	# validation passes without improvement: 0
09/16 11:14:48 AM: edges-srl-ontonotes_loss: training: 0.019084 validation: 0.018657
09/16 11:14:48 AM: macro_avg: validation: 0.750329
09/16 11:14:48 AM: micro_avg: validation: 0.000000
09/16 11:14:48 AM: edges-srl-ontonotes_mcc: training: 0.739383 validation: 0.748575
09/16 11:14:48 AM: edges-srl-ontonotes_acc: training: 0.656524 validation: 0.685705
09/16 11:14:48 AM: edges-srl-ontonotes_precision: training: 0.799673 validation: 0.805671
09/16 11:14:48 AM: edges-srl-ontonotes_recall: training: 0.690388 validation: 0.702101
09/16 11:14:48 AM: edges-srl-ontonotes_f1: training: 0.741023 validation: 0.750329
09/16 11:14:48 AM: Global learning rate: 2.5e-05
09/16 11:14:48 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:14:56 AM: Update 20140: task edges-srl-ontonotes, batch 140 (20140): mcc: 0.7934, acc: 0.7209, precision: 0.8381, recall: 0.7568, f1: 0.7954, edges-srl-ontonotes_loss: 0.0153
09/16 11:15:06 AM: Update 20388: task edges-srl-ontonotes, batch 388 (20388): mcc: 0.7852, acc: 0.7129, precision: 0.8319, recall: 0.7470, f1: 0.7872, edges-srl-ontonotes_loss: 0.0159
09/16 11:15:16 AM: Update 20621: task edges-srl-ontonotes, batch 621 (20621): mcc: 0.7871, acc: 0.7155, precision: 0.8331, recall: 0.7495, f1: 0.7891, edges-srl-ontonotes_loss: 0.0158
09/16 11:15:26 AM: Update 20829: task edges-srl-ontonotes, batch 829 (20829): mcc: 0.7915, acc: 0.7212, precision: 0.8368, recall: 0.7543, f1: 0.7934, edges-srl-ontonotes_loss: 0.0156
09/16 11:15:32 AM: ***** Step 21000 / Validation 21 *****
09/16 11:15:32 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:15:32 AM: Validating...
09/16 11:15:36 AM: Evaluate: task edges-srl-ontonotes, batch 136 (157): mcc: 0.7567, acc: 0.6936, precision: 0.8186, recall: 0.7058, f1: 0.7580, edges-srl-ontonotes_loss: 0.0179
09/16 11:15:36 AM: Updating LR scheduler:
09/16 11:15:36 AM: 	Best result seen so far for macro_avg: 0.762
09/16 11:15:36 AM: 	# validation passes without improvement: 1
09/16 11:15:36 AM: edges-srl-ontonotes_loss: training: 0.015336 validation: 0.018364
09/16 11:15:36 AM: macro_avg: validation: 0.750943
09/16 11:15:36 AM: micro_avg: validation: 0.000000
09/16 11:15:36 AM: edges-srl-ontonotes_mcc: training: 0.798293 validation: 0.749716
09/16 11:15:36 AM: edges-srl-ontonotes_acc: training: 0.730298 validation: 0.685321
09/16 11:15:36 AM: edges-srl-ontonotes_precision: training: 0.842663 validation: 0.813803
09/16 11:15:36 AM: edges-srl-ontonotes_recall: training: 0.761746 validation: 0.697098
09/16 11:15:36 AM: edges-srl-ontonotes_f1: training: 0.800164 validation: 0.750943
09/16 11:15:36 AM: Global learning rate: 2.5e-05
09/16 11:15:36 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:15:46 AM: Update 21215: task edges-srl-ontonotes, batch 215 (21215): mcc: 0.7758, acc: 0.7059, precision: 0.8282, recall: 0.7327, f1: 0.7775, edges-srl-ontonotes_loss: 0.0171
09/16 11:15:56 AM: Update 21431: task edges-srl-ontonotes, batch 431 (21431): mcc: 0.7530, acc: 0.6773, precision: 0.8074, recall: 0.7088, f1: 0.7549, edges-srl-ontonotes_loss: 0.0184
09/16 11:16:06 AM: Update 21659: task edges-srl-ontonotes, batch 659 (21659): mcc: 0.7387, acc: 0.6590, precision: 0.7964, recall: 0.6919, f1: 0.7405, edges-srl-ontonotes_loss: 0.0192
09/16 11:16:16 AM: Update 21852: task edges-srl-ontonotes, batch 852 (21852): mcc: 0.7382, acc: 0.6577, precision: 0.7970, recall: 0.6905, f1: 0.7399, edges-srl-ontonotes_loss: 0.0192
09/16 11:16:22 AM: ***** Step 22000 / Validation 22 *****
09/16 11:16:22 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:16:22 AM: Validating...
09/16 11:16:26 AM: Evaluate: task edges-srl-ontonotes, batch 101 (157): mcc: 0.7699, acc: 0.7042, precision: 0.8351, recall: 0.7157, f1: 0.7708, edges-srl-ontonotes_loss: 0.0171
09/16 11:16:27 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:16:27 AM: Best result seen so far for macro.
09/16 11:16:27 AM: Updating LR scheduler:
09/16 11:16:27 AM: 	Best result seen so far for macro_avg: 0.763
09/16 11:16:27 AM: 	# validation passes without improvement: 0
09/16 11:16:27 AM: edges-srl-ontonotes_loss: training: 0.018957 validation: 0.017554
09/16 11:16:27 AM: macro_avg: validation: 0.762841
09/16 11:16:27 AM: micro_avg: validation: 0.000000
09/16 11:16:27 AM: edges-srl-ontonotes_mcc: training: 0.742274 validation: 0.761869
09/16 11:16:27 AM: edges-srl-ontonotes_acc: training: 0.662017 validation: 0.696251
09/16 11:16:27 AM: edges-srl-ontonotes_precision: training: 0.800694 validation: 0.827378
09/16 11:16:27 AM: edges-srl-ontonotes_recall: training: 0.694827 validation: 0.707644
09/16 11:16:27 AM: edges-srl-ontonotes_f1: training: 0.744013 validation: 0.762841
09/16 11:16:27 AM: Global learning rate: 2.5e-05
09/16 11:16:27 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:16:36 AM: Update 22177: task edges-srl-ontonotes, batch 177 (22177): mcc: 0.7630, acc: 0.6865, precision: 0.8178, recall: 0.7182, f1: 0.7648, edges-srl-ontonotes_loss: 0.0175
09/16 11:16:46 AM: Update 22369: task edges-srl-ontonotes, batch 369 (22369): mcc: 0.7672, acc: 0.6914, precision: 0.8217, recall: 0.7226, f1: 0.7689, edges-srl-ontonotes_loss: 0.0173
09/16 11:16:56 AM: Update 22607: task edges-srl-ontonotes, batch 607 (22607): mcc: 0.7548, acc: 0.6759, precision: 0.8118, recall: 0.7082, f1: 0.7565, edges-srl-ontonotes_loss: 0.0182
09/16 11:17:06 AM: Update 22788: task edges-srl-ontonotes, batch 788 (22788): mcc: 0.7474, acc: 0.6668, precision: 0.8054, recall: 0.7002, f1: 0.7491, edges-srl-ontonotes_loss: 0.0186
09/16 11:17:16 AM: ***** Step 23000 / Validation 23 *****
09/16 11:17:16 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:17:16 AM: Validating...
09/16 11:17:16 AM: Evaluate: task edges-srl-ontonotes, batch 13 (157): mcc: 0.7713, acc: 0.7027, precision: 0.8398, recall: 0.7142, f1: 0.7719, edges-srl-ontonotes_loss: 0.0167
09/16 11:17:20 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:17:20 AM: Best result seen so far for macro.
09/16 11:17:20 AM: Updating LR scheduler:
09/16 11:17:20 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:17:20 AM: 	# validation passes without improvement: 0
09/16 11:17:20 AM: edges-srl-ontonotes_loss: training: 0.018998 validation: 0.017259
09/16 11:17:20 AM: macro_avg: validation: 0.767059
09/16 11:17:20 AM: micro_avg: validation: 0.000000
09/16 11:17:20 AM: edges-srl-ontonotes_mcc: training: 0.741313 validation: 0.766015
09/16 11:17:20 AM: edges-srl-ontonotes_acc: training: 0.659835 validation: 0.702101
09/16 11:17:20 AM: edges-srl-ontonotes_precision: training: 0.800186 validation: 0.829944
09/16 11:17:20 AM: edges-srl-ontonotes_recall: training: 0.693497 validation: 0.713032
09/16 11:17:20 AM: edges-srl-ontonotes_f1: training: 0.743032 validation: 0.767059
09/16 11:17:20 AM: Global learning rate: 2.5e-05
09/16 11:17:20 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:17:26 AM: Update 23125: task edges-srl-ontonotes, batch 125 (23125): mcc: 0.7181, acc: 0.6285, precision: 0.7801, recall: 0.6683, f1: 0.7199, edges-srl-ontonotes_loss: 0.0200
09/16 11:17:36 AM: Update 23357: task edges-srl-ontonotes, batch 357 (23357): mcc: 0.7108, acc: 0.6228, precision: 0.7737, recall: 0.6604, f1: 0.7125, edges-srl-ontonotes_loss: 0.0205
09/16 11:17:46 AM: Update 23602: task edges-srl-ontonotes, batch 602 (23602): mcc: 0.7031, acc: 0.6141, precision: 0.7690, recall: 0.6504, f1: 0.7048, edges-srl-ontonotes_loss: 0.0210
09/16 11:17:56 AM: Update 23833: task edges-srl-ontonotes, batch 833 (23833): mcc: 0.7023, acc: 0.6123, precision: 0.7699, recall: 0.6482, f1: 0.7038, edges-srl-ontonotes_loss: 0.0211
09/16 11:18:06 AM: ***** Step 24000 / Validation 24 *****
09/16 11:18:06 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:18:06 AM: Validating...
09/16 11:18:06 AM: Evaluate: task edges-srl-ontonotes, batch 2 (157): mcc: 0.7457, acc: 0.6648, precision: 0.8369, recall: 0.6705, f1: 0.7445, edges-srl-ontonotes_loss: 0.0180
09/16 11:18:11 AM: Updating LR scheduler:
09/16 11:18:11 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:18:11 AM: 	# validation passes without improvement: 1
09/16 11:18:11 AM: edges-srl-ontonotes_loss: training: 0.021200 validation: 0.017684
09/16 11:18:11 AM: macro_avg: validation: 0.757254
09/16 11:18:11 AM: micro_avg: validation: 0.000000
09/16 11:18:11 AM: edges-srl-ontonotes_mcc: training: 0.701283 validation: 0.756425
09/16 11:18:11 AM: edges-srl-ontonotes_acc: training: 0.610917 validation: 0.688400
09/16 11:18:11 AM: edges-srl-ontonotes_precision: training: 0.769061 validation: 0.824569
09/16 11:18:11 AM: edges-srl-ontonotes_recall: training: 0.647005 validation: 0.700100
09/16 11:18:11 AM: edges-srl-ontonotes_f1: training: 0.702773 validation: 0.757254
09/16 11:18:11 AM: Global learning rate: 2.5e-05
09/16 11:18:11 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:18:16 AM: Update 24127: task edges-srl-ontonotes, batch 127 (24127): mcc: 0.7132, acc: 0.6269, precision: 0.7788, recall: 0.6604, f1: 0.7148, edges-srl-ontonotes_loss: 0.0204
09/16 11:18:26 AM: Update 24316: task edges-srl-ontonotes, batch 316 (24316): mcc: 0.7169, acc: 0.6305, precision: 0.7818, recall: 0.6647, f1: 0.7185, edges-srl-ontonotes_loss: 0.0202
09/16 11:18:36 AM: Update 24540: task edges-srl-ontonotes, batch 540 (24540): mcc: 0.7190, acc: 0.6316, precision: 0.7832, recall: 0.6672, f1: 0.7206, edges-srl-ontonotes_loss: 0.0201
09/16 11:18:46 AM: Update 24749: task edges-srl-ontonotes, batch 749 (24749): mcc: 0.7219, acc: 0.6357, precision: 0.7855, recall: 0.6705, f1: 0.7235, edges-srl-ontonotes_loss: 0.0200
09/16 11:18:56 AM: Update 24918: task edges-srl-ontonotes, batch 918 (24918): mcc: 0.7229, acc: 0.6369, precision: 0.7862, recall: 0.6718, f1: 0.7245, edges-srl-ontonotes_loss: 0.0200
09/16 11:19:00 AM: ***** Step 25000 / Validation 25 *****
09/16 11:19:00 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:19:00 AM: Validating...
09/16 11:19:05 AM: Updating LR scheduler:
09/16 11:19:05 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:19:05 AM: 	# validation passes without improvement: 2
09/16 11:19:05 AM: edges-srl-ontonotes_loss: training: 0.020086 validation: 0.017994
09/16 11:19:05 AM: macro_avg: validation: 0.751259
09/16 11:19:05 AM: micro_avg: validation: 0.000000
09/16 11:19:05 AM: edges-srl-ontonotes_mcc: training: 0.721712 validation: 0.750301
09/16 11:19:05 AM: edges-srl-ontonotes_acc: training: 0.635702 validation: 0.685552
09/16 11:19:05 AM: edges-srl-ontonotes_precision: training: 0.785226 validation: 0.817721
09/16 11:19:05 AM: edges-srl-ontonotes_recall: training: 0.670458 validation: 0.694789
09/16 11:19:05 AM: edges-srl-ontonotes_f1: training: 0.723318 validation: 0.751259
09/16 11:19:05 AM: Global learning rate: 2.5e-05
09/16 11:19:05 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:19:06 AM: Update 25030: task edges-srl-ontonotes, batch 30 (25030): mcc: 0.7250, acc: 0.6505, precision: 0.7829, recall: 0.6785, f1: 0.7270, edges-srl-ontonotes_loss: 0.0201
09/16 11:19:16 AM: Update 25231: task edges-srl-ontonotes, batch 231 (25231): mcc: 0.7155, acc: 0.6278, precision: 0.7782, recall: 0.6651, f1: 0.7172, edges-srl-ontonotes_loss: 0.0204
09/16 11:19:26 AM: Update 25471: task edges-srl-ontonotes, batch 471 (25471): mcc: 0.7173, acc: 0.6301, precision: 0.7803, recall: 0.6666, f1: 0.7190, edges-srl-ontonotes_loss: 0.0204
09/16 11:19:36 AM: Update 25675: task edges-srl-ontonotes, batch 675 (25675): mcc: 0.7163, acc: 0.6293, precision: 0.7804, recall: 0.6648, f1: 0.7180, edges-srl-ontonotes_loss: 0.0204
09/16 11:19:46 AM: Update 25859: task edges-srl-ontonotes, batch 859 (25859): mcc: 0.7168, acc: 0.6296, precision: 0.7812, recall: 0.6649, f1: 0.7184, edges-srl-ontonotes_loss: 0.0204
09/16 11:19:52 AM: ***** Step 26000 / Validation 26 *****
09/16 11:19:52 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:19:52 AM: Validating...
09/16 11:19:56 AM: Evaluate: task edges-srl-ontonotes, batch 115 (157): mcc: 0.7478, acc: 0.6843, precision: 0.8127, recall: 0.6945, f1: 0.7490, edges-srl-ontonotes_loss: 0.0182
09/16 11:19:58 AM: Updating LR scheduler:
09/16 11:19:58 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:19:58 AM: 	# validation passes without improvement: 3
09/16 11:19:58 AM: edges-srl-ontonotes_loss: training: 0.020306 validation: 0.018349
09/16 11:19:58 AM: macro_avg: validation: 0.747749
09/16 11:19:58 AM: micro_avg: validation: 0.000000
09/16 11:19:58 AM: edges-srl-ontonotes_mcc: training: 0.717830 validation: 0.746523
09/16 11:19:58 AM: edges-srl-ontonotes_acc: training: 0.630765 validation: 0.682780
09/16 11:19:58 AM: edges-srl-ontonotes_precision: training: 0.781677 validation: 0.811127
09/16 11:19:58 AM: edges-srl-ontonotes_recall: training: 0.666409 validation: 0.693557
09/16 11:19:58 AM: edges-srl-ontonotes_f1: training: 0.719455 validation: 0.747749
09/16 11:19:58 AM: Global learning rate: 2.5e-05
09/16 11:19:58 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:20:07 AM: Update 26167: task edges-srl-ontonotes, batch 167 (26167): mcc: 0.7210, acc: 0.6355, precision: 0.7814, recall: 0.6724, f1: 0.7229, edges-srl-ontonotes_loss: 0.0202
09/16 11:20:17 AM: Update 26382: task edges-srl-ontonotes, batch 382 (26382): mcc: 0.7224, acc: 0.6374, precision: 0.7830, recall: 0.6736, f1: 0.7242, edges-srl-ontonotes_loss: 0.0200
09/16 11:20:27 AM: Update 26588: task edges-srl-ontonotes, batch 588 (26588): mcc: 0.7178, acc: 0.6308, precision: 0.7812, recall: 0.6667, f1: 0.7194, edges-srl-ontonotes_loss: 0.0204
09/16 11:20:37 AM: Update 26795: task edges-srl-ontonotes, batch 795 (26795): mcc: 0.7185, acc: 0.6316, precision: 0.7825, recall: 0.6669, f1: 0.7201, edges-srl-ontonotes_loss: 0.0203
09/16 11:20:44 AM: ***** Step 27000 / Validation 27 *****
09/16 11:20:44 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:20:44 AM: Validating...
09/16 11:20:47 AM: Evaluate: task edges-srl-ontonotes, batch 78 (157): mcc: 0.7482, acc: 0.6821, precision: 0.8129, recall: 0.6951, f1: 0.7494, edges-srl-ontonotes_loss: 0.0183
09/16 11:20:49 AM: Updating LR scheduler:
09/16 11:20:49 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:20:49 AM: 	# validation passes without improvement: 0
09/16 11:20:49 AM: edges-srl-ontonotes_loss: training: 0.019676 validation: 0.018114
09/16 11:20:49 AM: macro_avg: validation: 0.754039
09/16 11:20:49 AM: micro_avg: validation: 0.000000
09/16 11:20:49 AM: edges-srl-ontonotes_mcc: training: 0.727686 validation: 0.752799
09/16 11:20:49 AM: edges-srl-ontonotes_acc: training: 0.642745 validation: 0.688246
09/16 11:20:49 AM: edges-srl-ontonotes_precision: training: 0.789504 validation: 0.816250
09/16 11:20:49 AM: edges-srl-ontonotes_recall: training: 0.677718 validation: 0.700639
09/16 11:20:49 AM: edges-srl-ontonotes_f1: training: 0.729352 validation: 0.754039
09/16 11:20:49 AM: Global learning rate: 1.25e-05
09/16 11:20:49 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:20:57 AM: Update 27125: task edges-srl-ontonotes, batch 125 (27125): mcc: 0.7751, acc: 0.7014, precision: 0.8224, recall: 0.7366, f1: 0.7771, edges-srl-ontonotes_loss: 0.0166
09/16 11:21:07 AM: Update 27324: task edges-srl-ontonotes, batch 324 (27324): mcc: 0.7801, acc: 0.7067, precision: 0.8269, recall: 0.7419, f1: 0.7821, edges-srl-ontonotes_loss: 0.0161
09/16 11:21:17 AM: Update 27527: task edges-srl-ontonotes, batch 527 (27527): mcc: 0.7793, acc: 0.7059, precision: 0.8261, recall: 0.7410, f1: 0.7813, edges-srl-ontonotes_loss: 0.0162
09/16 11:21:27 AM: Update 27748: task edges-srl-ontonotes, batch 748 (27748): mcc: 0.7819, acc: 0.7093, precision: 0.8288, recall: 0.7435, f1: 0.7838, edges-srl-ontonotes_loss: 0.0161
09/16 11:21:37 AM: Update 27942: task edges-srl-ontonotes, batch 942 (27942): mcc: 0.7856, acc: 0.7138, precision: 0.8316, recall: 0.7480, f1: 0.7876, edges-srl-ontonotes_loss: 0.0159
09/16 11:21:42 AM: ***** Step 28000 / Validation 28 *****
09/16 11:21:42 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:21:42 AM: Validating...
09/16 11:21:47 AM: Evaluate: task edges-srl-ontonotes, batch 141 (157): mcc: 0.7628, acc: 0.7011, precision: 0.8237, recall: 0.7127, f1: 0.7642, edges-srl-ontonotes_loss: 0.0176
09/16 11:21:47 AM: Updating LR scheduler:
09/16 11:21:47 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:21:47 AM: 	# validation passes without improvement: 1
09/16 11:21:47 AM: edges-srl-ontonotes_loss: training: 0.015863 validation: 0.017900
09/16 11:21:47 AM: macro_avg: validation: 0.759350
09/16 11:21:47 AM: micro_avg: validation: 0.000000
09/16 11:21:47 AM: edges-srl-ontonotes_mcc: training: 0.785651 validation: 0.758050
09/16 11:21:47 AM: edges-srl-ontonotes_acc: training: 0.714002 validation: 0.695405
09/16 11:21:47 AM: edges-srl-ontonotes_precision: training: 0.831675 validation: 0.819829
09/16 11:21:47 AM: edges-srl-ontonotes_recall: training: 0.747974 validation: 0.707182
09/16 11:21:47 AM: edges-srl-ontonotes_f1: training: 0.787607 validation: 0.759350
09/16 11:21:47 AM: Global learning rate: 1.25e-05
09/16 11:21:47 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:21:57 AM: Update 28237: task edges-srl-ontonotes, batch 237 (28237): mcc: 0.8212, acc: 0.7606, precision: 0.8626, recall: 0.7867, f1: 0.8229, edges-srl-ontonotes_loss: 0.0142
09/16 11:22:07 AM: Update 28454: task edges-srl-ontonotes, batch 454 (28454): mcc: 0.7985, acc: 0.7331, precision: 0.8448, recall: 0.7602, f1: 0.8003, edges-srl-ontonotes_loss: 0.0156
09/16 11:22:17 AM: Update 28670: task edges-srl-ontonotes, batch 670 (28670): mcc: 0.7783, acc: 0.7083, precision: 0.8278, recall: 0.7377, f1: 0.7802, edges-srl-ontonotes_loss: 0.0168
09/16 11:22:27 AM: Update 28910: task edges-srl-ontonotes, batch 910 (28910): mcc: 0.7621, acc: 0.6880, precision: 0.8153, recall: 0.7188, f1: 0.7640, edges-srl-ontonotes_loss: 0.0178
09/16 11:22:34 AM: ***** Step 29000 / Validation 29 *****
09/16 11:22:34 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:22:34 AM: Validating...
09/16 11:22:37 AM: Evaluate: task edges-srl-ontonotes, batch 106 (157): mcc: 0.7693, acc: 0.7077, precision: 0.8267, recall: 0.7219, f1: 0.7708, edges-srl-ontonotes_loss: 0.0172
09/16 11:22:38 AM: Updating LR scheduler:
09/16 11:22:38 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:22:38 AM: 	# validation passes without improvement: 2
09/16 11:22:38 AM: edges-srl-ontonotes_loss: training: 0.017996 validation: 0.017613
09/16 11:22:38 AM: macro_avg: validation: 0.763490
09/16 11:22:38 AM: micro_avg: validation: 0.000000
09/16 11:22:38 AM: edges-srl-ontonotes_mcc: training: 0.758921 validation: 0.762043
09/16 11:22:38 AM: edges-srl-ontonotes_acc: training: 0.683657 validation: 0.700100
09/16 11:22:38 AM: edges-srl-ontonotes_precision: training: 0.813420 validation: 0.821122
09/16 11:22:38 AM: edges-srl-ontonotes_recall: training: 0.714433 validation: 0.713417
09/16 11:22:38 AM: edges-srl-ontonotes_f1: training: 0.760720 validation: 0.763490
09/16 11:22:38 AM: Global learning rate: 1.25e-05
09/16 11:22:38 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:22:47 AM: Update 29200: task edges-srl-ontonotes, batch 200 (29200): mcc: 0.7598, acc: 0.6830, precision: 0.8158, recall: 0.7140, f1: 0.7615, edges-srl-ontonotes_loss: 0.0178
09/16 11:22:57 AM: Update 29448: task edges-srl-ontonotes, batch 448 (29448): mcc: 0.7635, acc: 0.6876, precision: 0.8182, recall: 0.7187, f1: 0.7652, edges-srl-ontonotes_loss: 0.0175
09/16 11:23:07 AM: Update 29670: task edges-srl-ontonotes, batch 670 (29670): mcc: 0.7606, acc: 0.6843, precision: 0.8158, recall: 0.7155, f1: 0.7624, edges-srl-ontonotes_loss: 0.0177
09/16 11:23:17 AM: Update 29918: task edges-srl-ontonotes, batch 918 (29918): mcc: 0.7544, acc: 0.6772, precision: 0.8107, recall: 0.7084, f1: 0.7561, edges-srl-ontonotes_loss: 0.0181
09/16 11:23:23 AM: ***** Step 30000 / Validation 30 *****
09/16 11:23:23 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:23:23 AM: Validating...
09/16 11:23:27 AM: Evaluate: task edges-srl-ontonotes, batch 140 (157): mcc: 0.7711, acc: 0.7086, precision: 0.8306, recall: 0.7219, f1: 0.7724, edges-srl-ontonotes_loss: 0.0169
09/16 11:23:27 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:23:27 AM: Best result seen so far for macro.
09/16 11:23:27 AM: Updating LR scheduler:
09/16 11:23:27 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:23:27 AM: 	# validation passes without improvement: 0
09/16 11:23:27 AM: edges-srl-ontonotes_loss: training: 0.018274 validation: 0.017242
09/16 11:23:27 AM: macro_avg: validation: 0.767524
09/16 11:23:27 AM: micro_avg: validation: 0.000000
09/16 11:23:27 AM: edges-srl-ontonotes_mcc: training: 0.751671 validation: 0.766215
09/16 11:23:27 AM: edges-srl-ontonotes_acc: training: 0.673947 validation: 0.703256
09/16 11:23:27 AM: edges-srl-ontonotes_precision: training: 0.808501 validation: 0.826481
09/16 11:23:27 AM: edges-srl-ontonotes_recall: training: 0.705339 validation: 0.716419
09/16 11:23:27 AM: edges-srl-ontonotes_f1: training: 0.753405 validation: 0.767524
09/16 11:23:27 AM: Global learning rate: 1.25e-05
09/16 11:23:27 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:23:37 AM: Update 30228: task edges-srl-ontonotes, batch 228 (30228): mcc: 0.7164, acc: 0.6299, precision: 0.7791, recall: 0.6660, f1: 0.7181, edges-srl-ontonotes_loss: 0.0204
09/16 11:23:47 AM: Update 30464: task edges-srl-ontonotes, batch 464 (30464): mcc: 0.7188, acc: 0.6315, precision: 0.7808, recall: 0.6689, f1: 0.7205, edges-srl-ontonotes_loss: 0.0201
09/16 11:23:57 AM: Update 30699: task edges-srl-ontonotes, batch 699 (30699): mcc: 0.7124, acc: 0.6244, precision: 0.7755, recall: 0.6618, f1: 0.7142, edges-srl-ontonotes_loss: 0.0205
09/16 11:24:07 AM: Update 30885: task edges-srl-ontonotes, batch 885 (30885): mcc: 0.7086, acc: 0.6199, precision: 0.7728, recall: 0.6571, f1: 0.7103, edges-srl-ontonotes_loss: 0.0207
09/16 11:24:11 AM: ***** Step 31000 / Validation 31 *****
09/16 11:24:11 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:24:11 AM: Validating...
09/16 11:24:16 AM: Updating LR scheduler:
09/16 11:24:16 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:24:16 AM: 	# validation passes without improvement: 1
09/16 11:24:16 AM: edges-srl-ontonotes_loss: training: 0.020842 validation: 0.017339
09/16 11:24:16 AM: macro_avg: validation: 0.762903
09/16 11:24:16 AM: micro_avg: validation: 0.000000
09/16 11:24:16 AM: edges-srl-ontonotes_mcc: training: 0.707502 validation: 0.762101
09/16 11:24:16 AM: edges-srl-ontonotes_acc: training: 0.618710 validation: 0.694943
09/16 11:24:16 AM: edges-srl-ontonotes_precision: training: 0.772256 validation: 0.829745
09/16 11:24:16 AM: edges-srl-ontonotes_recall: training: 0.655619 validation: 0.706027
09/16 11:24:16 AM: edges-srl-ontonotes_f1: training: 0.709174 validation: 0.762903
09/16 11:24:16 AM: Global learning rate: 1.25e-05
09/16 11:24:16 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:24:17 AM: Update 31017: task edges-srl-ontonotes, batch 17 (31017): mcc: 0.7043, acc: 0.6151, precision: 0.7802, recall: 0.6430, f1: 0.7050, edges-srl-ontonotes_loss: 0.0217
09/16 11:24:27 AM: Update 31210: task edges-srl-ontonotes, batch 210 (31210): mcc: 0.7063, acc: 0.6160, precision: 0.7750, recall: 0.6510, f1: 0.7076, edges-srl-ontonotes_loss: 0.0211
09/16 11:24:37 AM: Update 31426: task edges-srl-ontonotes, batch 426 (31426): mcc: 0.7140, acc: 0.6258, precision: 0.7797, recall: 0.6611, f1: 0.7155, edges-srl-ontonotes_loss: 0.0205
09/16 11:24:47 AM: Update 31650: task edges-srl-ontonotes, batch 650 (31650): mcc: 0.7156, acc: 0.6277, precision: 0.7800, recall: 0.6638, f1: 0.7172, edges-srl-ontonotes_loss: 0.0203
09/16 11:24:57 AM: Update 31848: task edges-srl-ontonotes, batch 848 (31848): mcc: 0.7180, acc: 0.6307, precision: 0.7813, recall: 0.6670, f1: 0.7196, edges-srl-ontonotes_loss: 0.0202
09/16 11:25:04 AM: ***** Step 32000 / Validation 32 *****
09/16 11:25:04 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:25:04 AM: Validating...
09/16 11:25:07 AM: Evaluate: task edges-srl-ontonotes, batch 86 (157): mcc: 0.7551, acc: 0.6914, precision: 0.8194, recall: 0.7021, f1: 0.7562, edges-srl-ontonotes_loss: 0.0178
09/16 11:25:10 AM: Updating LR scheduler:
09/16 11:25:10 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:25:10 AM: 	# validation passes without improvement: 2
09/16 11:25:10 AM: edges-srl-ontonotes_loss: training: 0.020108 validation: 0.017656
09/16 11:25:10 AM: macro_avg: validation: 0.756215
09/16 11:25:10 AM: micro_avg: validation: 0.000000
09/16 11:25:10 AM: edges-srl-ontonotes_mcc: training: 0.719302 validation: 0.755030
09/16 11:25:10 AM: edges-srl-ontonotes_acc: training: 0.632191 validation: 0.691633
09/16 11:25:10 AM: edges-srl-ontonotes_precision: training: 0.782202 validation: 0.818843
09/16 11:25:10 AM: edges-srl-ontonotes_recall: training: 0.668653 validation: 0.702486
09/16 11:25:10 AM: edges-srl-ontonotes_f1: training: 0.720984 validation: 0.756215
09/16 11:25:10 AM: Global learning rate: 1.25e-05
09/16 11:25:10 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:25:18 AM: Update 32115: task edges-srl-ontonotes, batch 115 (32115): mcc: 0.7316, acc: 0.6476, precision: 0.7922, recall: 0.6826, f1: 0.7333, edges-srl-ontonotes_loss: 0.0194
09/16 11:25:28 AM: Update 32355: task edges-srl-ontonotes, batch 355 (32355): mcc: 0.7185, acc: 0.6326, precision: 0.7812, recall: 0.6680, f1: 0.7202, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:38 AM: Update 32563: task edges-srl-ontonotes, batch 563 (32563): mcc: 0.7181, acc: 0.6322, precision: 0.7813, recall: 0.6673, f1: 0.7198, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:48 AM: Update 32752: task edges-srl-ontonotes, batch 752 (32752): mcc: 0.7191, acc: 0.6330, precision: 0.7818, recall: 0.6686, f1: 0.7208, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:58 AM: Update 32982: task edges-srl-ontonotes, batch 982 (32982): mcc: 0.7187, acc: 0.6326, precision: 0.7818, recall: 0.6678, f1: 0.7203, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:59 AM: ***** Step 33000 / Validation 33 *****
09/16 11:25:59 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:25:59 AM: Validating...
09/16 11:26:04 AM: Updating LR scheduler:
09/16 11:26:04 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:26:04 AM: 	# validation passes without improvement: 3
09/16 11:26:04 AM: edges-srl-ontonotes_loss: training: 0.020288 validation: 0.017919
09/16 11:26:04 AM: macro_avg: validation: 0.753545
09/16 11:26:04 AM: micro_avg: validation: 0.000000
09/16 11:26:04 AM: edges-srl-ontonotes_mcc: training: 0.718811 validation: 0.752365
09/16 11:26:04 AM: edges-srl-ontonotes_acc: training: 0.632719 validation: 0.690247
09/16 11:26:04 AM: edges-srl-ontonotes_precision: training: 0.782017 validation: 0.816662
09/16 11:26:04 AM: edges-srl-ontonotes_recall: training: 0.667912 validation: 0.699484
09/16 11:26:04 AM: edges-srl-ontonotes_f1: training: 0.720475 validation: 0.753545
09/16 11:26:04 AM: Global learning rate: 1.25e-05
09/16 11:26:04 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:26:09 AM: Update 33054: task edges-srl-ontonotes, batch 54 (33054): mcc: 0.7063, acc: 0.6186, precision: 0.7720, recall: 0.6536, f1: 0.7079, edges-srl-ontonotes_loss: 0.0209
09/16 11:26:19 AM: Update 33272: task edges-srl-ontonotes, batch 272 (33272): mcc: 0.7186, acc: 0.6328, precision: 0.7810, recall: 0.6685, f1: 0.7204, edges-srl-ontonotes_loss: 0.0201
09/16 11:26:29 AM: Update 33471: task edges-srl-ontonotes, batch 471 (33471): mcc: 0.7198, acc: 0.6344, precision: 0.7821, recall: 0.6696, f1: 0.7215, edges-srl-ontonotes_loss: 0.0201
09/16 11:26:39 AM: Update 33680: task edges-srl-ontonotes, batch 680 (33680): mcc: 0.7226, acc: 0.6377, precision: 0.7847, recall: 0.6726, f1: 0.7244, edges-srl-ontonotes_loss: 0.0199
09/16 11:26:49 AM: Update 33901: task edges-srl-ontonotes, batch 901 (33901): mcc: 0.7194, acc: 0.6340, precision: 0.7829, recall: 0.6682, f1: 0.7210, edges-srl-ontonotes_loss: 0.0202
09/16 11:26:55 AM: ***** Step 34000 / Validation 34 *****
09/16 11:26:55 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:26:55 AM: Validating...
09/16 11:27:00 AM: Evaluate: task edges-srl-ontonotes, batch 136 (157): mcc: 0.7519, acc: 0.6886, precision: 0.8180, recall: 0.6975, f1: 0.7530, edges-srl-ontonotes_loss: 0.0177
09/16 11:27:00 AM: Updating LR scheduler:
09/16 11:27:00 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:27:00 AM: 	# validation passes without improvement: 0
09/16 11:27:00 AM: edges-srl-ontonotes_loss: training: 0.020215 validation: 0.018060
09/16 11:27:00 AM: macro_avg: validation: 0.748980
09/16 11:27:00 AM: micro_avg: validation: 0.000000
09/16 11:27:00 AM: edges-srl-ontonotes_mcc: training: 0.719131 validation: 0.747979
09/16 11:27:00 AM: edges-srl-ontonotes_acc: training: 0.633782 validation: 0.684012
09/16 11:27:00 AM: edges-srl-ontonotes_precision: training: 0.783209 validation: 0.815201
09/16 11:27:00 AM: edges-srl-ontonotes_recall: training: 0.667470 validation: 0.692710
09/16 11:27:00 AM: edges-srl-ontonotes_f1: training: 0.720722 validation: 0.748980
09/16 11:27:00 AM: Global learning rate: 6.25e-06
09/16 11:27:00 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:27:10 AM: Update 34239: task edges-srl-ontonotes, batch 239 (34239): mcc: 0.7557, acc: 0.6772, precision: 0.8118, recall: 0.7099, f1: 0.7575, edges-srl-ontonotes_loss: 0.0176
09/16 11:27:20 AM: Update 34440: task edges-srl-ontonotes, batch 440 (34440): mcc: 0.7602, acc: 0.6832, precision: 0.8146, recall: 0.7159, f1: 0.7620, edges-srl-ontonotes_loss: 0.0173
09/16 11:27:30 AM: Update 34638: task edges-srl-ontonotes, batch 638 (34638): mcc: 0.7672, acc: 0.6919, precision: 0.8191, recall: 0.7248, f1: 0.7691, edges-srl-ontonotes_loss: 0.0169
09/16 11:27:40 AM: Update 34851: task edges-srl-ontonotes, batch 851 (34851): mcc: 0.7694, acc: 0.6947, precision: 0.8202, recall: 0.7279, f1: 0.7713, edges-srl-ontonotes_loss: 0.0167
09/16 11:27:48 AM: ***** Step 35000 / Validation 35 *****
09/16 11:27:48 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:27:48 AM: Validating...
09/16 11:27:50 AM: Evaluate: task edges-srl-ontonotes, batch 61 (157): mcc: 0.7547, acc: 0.6886, precision: 0.8211, recall: 0.7000, f1: 0.7557, edges-srl-ontonotes_loss: 0.0182
09/16 11:27:53 AM: Updating LR scheduler:
09/16 11:27:53 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:27:53 AM: 	# validation passes without improvement: 1
09/16 11:27:53 AM: edges-srl-ontonotes_loss: training: 0.016568 validation: 0.017768
09/16 11:27:53 AM: macro_avg: validation: 0.759869
09/16 11:27:53 AM: micro_avg: validation: 0.000000
09/16 11:27:53 AM: edges-srl-ontonotes_mcc: training: 0.772712 validation: 0.758594
09/16 11:27:53 AM: edges-srl-ontonotes_acc: training: 0.698647 validation: 0.695405
09/16 11:27:53 AM: edges-srl-ontonotes_precision: training: 0.822702 validation: 0.820625
09/16 11:27:53 AM: edges-srl-ontonotes_recall: training: 0.731838 validation: 0.707490
09/16 11:27:53 AM: edges-srl-ontonotes_f1: training: 0.774614 validation: 0.759869
09/16 11:27:53 AM: Global learning rate: 6.25e-06
09/16 11:27:53 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:28:00 AM: Update 35153: task edges-srl-ontonotes, batch 153 (35153): mcc: 0.7972, acc: 0.7298, precision: 0.8428, recall: 0.7596, f1: 0.7991, edges-srl-ontonotes_loss: 0.0153
09/16 11:28:10 AM: Update 35324: task edges-srl-ontonotes, batch 324 (35324): mcc: 0.8019, acc: 0.7364, precision: 0.8459, recall: 0.7656, f1: 0.8038, edges-srl-ontonotes_loss: 0.0150
09/16 11:28:20 AM: Update 35558: task edges-srl-ontonotes, batch 558 (35558): mcc: 0.8103, acc: 0.7465, precision: 0.8527, recall: 0.7752, f1: 0.8121, edges-srl-ontonotes_loss: 0.0148
09/16 11:28:30 AM: Update 35816: task edges-srl-ontonotes, batch 816 (35816): mcc: 0.7892, acc: 0.7214, precision: 0.8362, recall: 0.7506, f1: 0.7911, edges-srl-ontonotes_loss: 0.0160
09/16 11:28:38 AM: ***** Step 36000 / Validation 36 *****
09/16 11:28:38 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:28:38 AM: Validating...
09/16 11:28:40 AM: Evaluate: task edges-srl-ontonotes, batch 55 (157): mcc: 0.7582, acc: 0.6904, precision: 0.8279, recall: 0.7006, f1: 0.7589, edges-srl-ontonotes_loss: 0.0181
09/16 11:28:43 AM: Updating LR scheduler:
09/16 11:28:43 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:28:43 AM: 	# validation passes without improvement: 2
09/16 11:28:43 AM: edges-srl-ontonotes_loss: training: 0.016739 validation: 0.017490
09/16 11:28:43 AM: macro_avg: validation: 0.764496
09/16 11:28:43 AM: micro_avg: validation: 0.000000
09/16 11:28:43 AM: edges-srl-ontonotes_mcc: training: 0.778120 validation: 0.763432
09/16 11:28:43 AM: edges-srl-ontonotes_acc: training: 0.707563 validation: 0.699253
09/16 11:28:43 AM: edges-srl-ontonotes_precision: training: 0.827383 validation: 0.827490
09/16 11:28:43 AM: edges-srl-ontonotes_recall: training: 0.737739 validation: 0.710415
09/16 11:28:43 AM: edges-srl-ontonotes_f1: training: 0.779994 validation: 0.764496
09/16 11:28:43 AM: Global learning rate: 6.25e-06
09/16 11:28:43 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:28:50 AM: Update 36166: task edges-srl-ontonotes, batch 166 (36166): mcc: 0.7138, acc: 0.6289, precision: 0.7785, recall: 0.6618, f1: 0.7154, edges-srl-ontonotes_loss: 0.0210
09/16 11:29:00 AM: Update 36356: task edges-srl-ontonotes, batch 356 (36356): mcc: 0.7319, acc: 0.6489, precision: 0.7937, recall: 0.6819, f1: 0.7335, edges-srl-ontonotes_loss: 0.0197
09/16 11:29:12 AM: Update 36544: task edges-srl-ontonotes, batch 544 (36544): mcc: 0.7422, acc: 0.6612, precision: 0.8012, recall: 0.6943, f1: 0.7439, edges-srl-ontonotes_loss: 0.0190
09/16 11:29:22 AM: Update 36738: task edges-srl-ontonotes, batch 738 (36738): mcc: 0.7491, acc: 0.6697, precision: 0.8069, recall: 0.7019, f1: 0.7508, edges-srl-ontonotes_loss: 0.0186
09/16 11:29:32 AM: ***** Step 37000 / Validation 37 *****
09/16 11:29:32 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:29:32 AM: Validating...
09/16 11:29:32 AM: Evaluate: task edges-srl-ontonotes, batch 4 (157): mcc: 0.7736, acc: 0.7057, precision: 0.8547, recall: 0.7057, f1: 0.7731, edges-srl-ontonotes_loss: 0.0167
09/16 11:29:36 AM: Updating LR scheduler:
09/16 11:29:36 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:29:36 AM: 	# validation passes without improvement: 3
09/16 11:29:36 AM: edges-srl-ontonotes_loss: training: 0.018498 validation: 0.017238
09/16 11:29:36 AM: macro_avg: validation: 0.765762
09/16 11:29:36 AM: micro_avg: validation: 0.000000
09/16 11:29:36 AM: edges-srl-ontonotes_mcc: training: 0.749802 validation: 0.764640
09/16 11:29:36 AM: edges-srl-ontonotes_acc: training: 0.670892 validation: 0.700177
09/16 11:29:36 AM: edges-srl-ontonotes_precision: training: 0.807335 validation: 0.827743
09/16 11:29:36 AM: edges-srl-ontonotes_recall: training: 0.702907 validation: 0.712416
09/16 11:29:36 AM: edges-srl-ontonotes_f1: training: 0.751511 validation: 0.765762
09/16 11:29:36 AM: Global learning rate: 6.25e-06
09/16 11:29:36 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:29:42 AM: Update 37135: task edges-srl-ontonotes, batch 135 (37135): mcc: 0.7396, acc: 0.6588, precision: 0.8000, recall: 0.6906, f1: 0.7413, edges-srl-ontonotes_loss: 0.0190
09/16 11:29:52 AM: Update 37349: task edges-srl-ontonotes, batch 349 (37349): mcc: 0.7297, acc: 0.6456, precision: 0.7910, recall: 0.6801, f1: 0.7314, edges-srl-ontonotes_loss: 0.0195
09/16 11:30:02 AM: Update 37512: task edges-srl-ontonotes, batch 512 (37512): mcc: 0.7271, acc: 0.6433, precision: 0.7889, recall: 0.6771, f1: 0.7288, edges-srl-ontonotes_loss: 0.0199
09/16 11:30:12 AM: Update 37759: task edges-srl-ontonotes, batch 759 (37759): mcc: 0.7253, acc: 0.6403, precision: 0.7867, recall: 0.6758, f1: 0.7271, edges-srl-ontonotes_loss: 0.0198
09/16 11:30:22 AM: Update 37968: task edges-srl-ontonotes, batch 968 (37968): mcc: 0.7203, acc: 0.6338, precision: 0.7825, recall: 0.6702, f1: 0.7220, edges-srl-ontonotes_loss: 0.0201
09/16 11:30:23 AM: ***** Step 38000 / Validation 38 *****
09/16 11:30:23 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:30:23 AM: Validating...
09/16 11:30:28 AM: Updating LR scheduler:
09/16 11:30:28 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:30:28 AM: 	# validation passes without improvement: 0
09/16 11:30:28 AM: edges-srl-ontonotes_loss: training: 0.020180 validation: 0.017284
09/16 11:30:28 AM: macro_avg: validation: 0.764377
09/16 11:30:28 AM: micro_avg: validation: 0.000000
09/16 11:30:28 AM: edges-srl-ontonotes_mcc: training: 0.719279 validation: 0.763285
09/16 11:30:28 AM: edges-srl-ontonotes_acc: training: 0.632799 validation: 0.698868
09/16 11:30:28 AM: edges-srl-ontonotes_precision: training: 0.781746 validation: 0.827002
09/16 11:30:28 AM: edges-srl-ontonotes_recall: training: 0.669006 validation: 0.710569
09/16 11:30:28 AM: edges-srl-ontonotes_f1: training: 0.720996 validation: 0.764377
09/16 11:30:28 AM: Global learning rate: 3.125e-06
09/16 11:30:28 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:30:32 AM: Update 38107: task edges-srl-ontonotes, batch 107 (38107): mcc: 0.6942, acc: 0.6041, precision: 0.7659, recall: 0.6369, f1: 0.6955, edges-srl-ontonotes_loss: 0.0218
09/16 11:30:42 AM: Update 38330: task edges-srl-ontonotes, batch 330 (38330): mcc: 0.6960, acc: 0.6069, precision: 0.7641, recall: 0.6415, f1: 0.6975, edges-srl-ontonotes_loss: 0.0217
09/16 11:30:52 AM: Update 38475: task edges-srl-ontonotes, batch 475 (38475): mcc: 0.7015, acc: 0.6124, precision: 0.7701, recall: 0.6466, f1: 0.7029, edges-srl-ontonotes_loss: 0.0213
09/16 11:31:02 AM: Update 38707: task edges-srl-ontonotes, batch 707 (38707): mcc: 0.7084, acc: 0.6198, precision: 0.7750, recall: 0.6549, f1: 0.7099, edges-srl-ontonotes_loss: 0.0208
09/16 11:31:12 AM: Update 38954: task edges-srl-ontonotes, batch 954 (38954): mcc: 0.7114, acc: 0.6233, precision: 0.7763, recall: 0.6593, f1: 0.7131, edges-srl-ontonotes_loss: 0.0206
09/16 11:31:16 AM: ***** Step 39000 / Validation 39 *****
09/16 11:31:16 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:31:16 AM: Validating...
09/16 11:31:21 AM: Updating LR scheduler:
09/16 11:31:21 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:31:21 AM: 	# validation passes without improvement: 1
09/16 11:31:21 AM: edges-srl-ontonotes_loss: training: 0.020580 validation: 0.017370
09/16 11:31:21 AM: macro_avg: validation: 0.762652
09/16 11:31:21 AM: micro_avg: validation: 0.000000
09/16 11:31:21 AM: edges-srl-ontonotes_mcc: training: 0.712117 validation: 0.761329
09/16 11:31:21 AM: edges-srl-ontonotes_acc: training: 0.623963 validation: 0.697945
09/16 11:31:21 AM: edges-srl-ontonotes_precision: training: 0.776997 validation: 0.822252
09/16 11:31:21 AM: edges-srl-ontonotes_recall: training: 0.659982 validation: 0.711108
09/16 11:31:21 AM: edges-srl-ontonotes_f1: training: 0.713725 validation: 0.762652
09/16 11:31:21 AM: Global learning rate: 3.125e-06
09/16 11:31:21 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:31:22 AM: Update 39037: task edges-srl-ontonotes, batch 37 (39037): mcc: 0.7295, acc: 0.6445, precision: 0.7828, recall: 0.6869, f1: 0.7317, edges-srl-ontonotes_loss: 0.0197
09/16 11:31:32 AM: Update 39247: task edges-srl-ontonotes, batch 247 (39247): mcc: 0.7292, acc: 0.6443, precision: 0.7887, recall: 0.6812, f1: 0.7310, edges-srl-ontonotes_loss: 0.0196
09/16 11:31:42 AM: Update 39477: task edges-srl-ontonotes, batch 477 (39477): mcc: 0.7251, acc: 0.6394, precision: 0.7852, recall: 0.6767, f1: 0.7270, edges-srl-ontonotes_loss: 0.0197
09/16 11:31:55 AM: Update 39674: task edges-srl-ontonotes, batch 674 (39674): mcc: 0.7218, acc: 0.6355, precision: 0.7832, recall: 0.6724, f1: 0.7236, edges-srl-ontonotes_loss: 0.0200
09/16 11:32:05 AM: Update 39915: task edges-srl-ontonotes, batch 915 (39915): mcc: 0.7211, acc: 0.6343, precision: 0.7832, recall: 0.6712, f1: 0.7229, edges-srl-ontonotes_loss: 0.0201
09/16 11:32:10 AM: ***** Step 40000 / Validation 40 *****
09/16 11:32:10 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:32:10 AM: Validating...
09/16 11:32:15 AM: Evaluate: task edges-srl-ontonotes, batch 156 (157): mcc: 0.7593, acc: 0.6961, precision: 0.8227, recall: 0.7070, f1: 0.7605, edges-srl-ontonotes_loss: 0.0174
09/16 11:32:15 AM: Updating LR scheduler:
09/16 11:32:15 AM: 	Best result seen so far for macro_avg: 0.768
09/16 11:32:15 AM: 	# validation passes without improvement: 2
09/16 11:32:15 AM: Ran out of early stopping patience. Stopping training.
09/16 11:32:15 AM: edges-srl-ontonotes_loss: training: 0.020123 validation: 0.017456
09/16 11:32:15 AM: macro_avg: validation: 0.760556
09/16 11:32:15 AM: micro_avg: validation: 0.000000
09/16 11:32:15 AM: edges-srl-ontonotes_mcc: training: 0.720301 validation: 0.759390
09/16 11:32:15 AM: edges-srl-ontonotes_acc: training: 0.633429 validation: 0.696174
09/16 11:32:15 AM: edges-srl-ontonotes_precision: training: 0.782676 validation: 0.822750
09/16 11:32:15 AM: edges-srl-ontonotes_recall: training: 0.670076 validation: 0.707105
09/16 11:32:15 AM: edges-srl-ontonotes_f1: training: 0.722013 validation: 0.760556
09/16 11:32:15 AM: Global learning rate: 3.125e-06
09/16 11:32:15 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:32:15 AM: Stopped training after 40 validation checks
09/16 11:32:15 AM: Trained edges-srl-ontonotes for 40000 batches or 5.529 epochs
09/16 11:32:15 AM: ***** VALIDATION RESULTS *****
09/16 11:32:15 AM: edges-srl-ontonotes_f1 (for best val pass 30): edges-srl-ontonotes_loss: 0.01724, macro_avg: 0.76752, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.76621, edges-srl-ontonotes_acc: 0.70326, edges-srl-ontonotes_precision: 0.82648, edges-srl-ontonotes_recall: 0.71642, edges-srl-ontonotes_f1: 0.76752
09/16 11:32:15 AM: micro_avg (for best val pass 1): edges-srl-ontonotes_loss: 0.02754, macro_avg: 0.63871, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.64675, edges-srl-ontonotes_acc: 0.52028, edges-srl-ontonotes_precision: 0.79182, edges-srl-ontonotes_recall: 0.53522, edges-srl-ontonotes_f1: 0.63871
09/16 11:32:15 AM: macro_avg (for best val pass 30): edges-srl-ontonotes_loss: 0.01724, macro_avg: 0.76752, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.76621, edges-srl-ontonotes_acc: 0.70326, edges-srl-ontonotes_precision: 0.82648, edges-srl-ontonotes_recall: 0.71642, edges-srl-ontonotes_f1: 0.76752
09/16 11:32:15 AM: Evaluating...
09/16 11:32:15 AM: Loaded model state from ./experiments/srl-ontonotes-mrpc-only/run/edges-srl-ontonotes/model_state_target_train_val_30.best.th
09/16 11:32:15 AM: Evaluating on: edges-srl-ontonotes, split: val
09/16 11:32:45 AM: 	Task edges-srl-ontonotes: batch 901
09/16 11:32:49 AM: Task 'edges-srl-ontonotes': sorting predictions by 'idx'
09/16 11:32:49 AM: Finished evaluating on: edges-srl-ontonotes
09/16 11:32:49 AM: Task 'edges-srl-ontonotes': joining predictions with input split 'val'
09/16 11:32:56 AM: Task 'edges-srl-ontonotes': Wrote predictions to ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:32:56 AM: Wrote all preds for split 'val' to ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:32:56 AM: Evaluating on: edges-srl-ontonotes, split: test
09/16 11:33:21 AM: Task 'edges-srl-ontonotes': sorting predictions by 'idx'
09/16 11:33:21 AM: Finished evaluating on: edges-srl-ontonotes
09/16 11:33:21 AM: Task 'edges-srl-ontonotes': joining predictions with input split 'test'
09/16 11:33:25 AM: Task 'edges-srl-ontonotes': Wrote predictions to ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:33:25 AM: Wrote all preds for split 'test' to ./experiments/srl-ontonotes-mrpc-only/run
09/16 11:33:25 AM: Writing results for split 'val' to ./experiments/srl-ontonotes-mrpc-only/results.tsv
09/16 11:33:25 AM: micro_avg: 0.000, macro_avg: 0.750, edges-srl-ontonotes_mcc: 0.749, edges-srl-ontonotes_acc: 0.685, edges-srl-ontonotes_precision: 0.813, edges-srl-ontonotes_recall: 0.696, edges-srl-ontonotes_f1: 0.750
09/16 11:33:25 AM: Done!
09/16 11:33:25 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
