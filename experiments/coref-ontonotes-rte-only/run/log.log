09/17 03:01:27 AM: Git branch: master
09/17 03:01:27 AM: Git SHA: 4086cd8f278243816795989a620c769378a6ab56
09/17 03:01:27 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/coref-ontonotes-rte-only/",
  "exp_name": "experiments/coref-ontonotes-rte-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/coref-ontonotes-rte-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/rte",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/coref-ontonotes-rte-only__run",
  "run_dir": "./experiments/coref-ontonotes-rte-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-coref-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/17 03:01:27 AM: Saved config to ./experiments/coref-ontonotes-rte-only/run/params.conf
09/17 03:01:27 AM: Using random seed 1234
09/17 03:01:31 AM: Using GPU 0
09/17 03:01:31 AM: Loading tasks...
09/17 03:01:31 AM: Writing pre-preprocessed tasks to ./experiments/coref-ontonotes-rte-only/
09/17 03:01:31 AM: 	Creating task edges-coref-ontonotes from scratch.
09/17 03:01:33 AM: Read=41777, Skip=74035, Total=115812 from ./probing_data/edges/ontonotes/coref/train.json.retokenized.bert-base-uncased
09/17 03:01:33 AM: Read=5044, Skip=10636, Total=15680 from ./probing_data/edges/ontonotes/coref/development.json.retokenized.bert-base-uncased
09/17 03:01:33 AM: Read=5188, Skip=7029, Total=12217 from ./probing_data/edges/ontonotes/coref/test.json.retokenized.bert-base-uncased
09/17 03:01:34 AM: 	Task 'edges-coref-ontonotes': |train|=41777 |val|=5044 |test|=5188
09/17 03:01:34 AM: 	Finished loading tasks: edges-coref-ontonotes.
09/17 03:01:34 AM: 	Building vocab from scratch.
09/17 03:01:34 AM: 	Counting units for task edges-coref-ontonotes.
09/17 03:01:35 AM: 	Task 'edges-coref-ontonotes': adding vocab namespace 'edges-coref-ontonotes_labels'
09/17 03:01:36 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 03:01:36 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/17 03:01:36 AM: 	Saved vocab to ./experiments/coref-ontonotes-rte-only/vocab
09/17 03:01:36 AM: Loading token dictionary from ./experiments/coref-ontonotes-rte-only/vocab.
09/17 03:01:36 AM: 	Loaded vocab from ./experiments/coref-ontonotes-rte-only/vocab
09/17 03:01:36 AM: 	Vocab namespace tokens: size 20434
09/17 03:01:36 AM: 	Vocab namespace bert_uncased: size 30524
09/17 03:01:36 AM: 	Vocab namespace edges-coref-ontonotes_labels: size 2
09/17 03:01:36 AM: 	Vocab namespace chars: size 72
09/17 03:01:36 AM: 	Finished building vocab.
09/17 03:01:36 AM: 	Task edges-coref-ontonotes (train): Indexing from scratch.
09/17 03:01:46 AM: 	Task edges-coref-ontonotes (train): Saved 41777 instances to ./experiments/coref-ontonotes-rte-only/preproc/edges-coref-ontonotes__train_data
09/17 03:01:46 AM: 	Task edges-coref-ontonotes (val): Indexing from scratch.
09/17 03:01:47 AM: 	Task edges-coref-ontonotes (val): Saved 5044 instances to ./experiments/coref-ontonotes-rte-only/preproc/edges-coref-ontonotes__val_data
09/17 03:01:47 AM: 	Task edges-coref-ontonotes (test): Indexing from scratch.
09/17 03:01:48 AM: 	Task edges-coref-ontonotes (test): Saved 5188 instances to ./experiments/coref-ontonotes-rte-only/preproc/edges-coref-ontonotes__test_data
09/17 03:01:48 AM: 	Finished indexing tasks
09/17 03:01:48 AM: 	Creating trimmed target-only version of edges-coref-ontonotes train.
09/17 03:01:48 AM: 	  Training on 
09/17 03:01:48 AM: 	  Evaluating on edges-coref-ontonotes
09/17 03:01:48 AM: 	Finished loading tasks in 16.845s
09/17 03:01:48 AM: 	 Tasks: ['edges-coref-ontonotes']
09/17 03:01:48 AM: Building model...
09/17 03:01:48 AM: Using BERT model (bert-base-uncased).
09/17 03:01:48 AM: LOADING A FUNETUNED MODEL from: 
09/17 03:01:48 AM: models/rte
09/17 03:01:48 AM: loading configuration file models/rte/config.json
09/17 03:01:48 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/17 03:01:48 AM: loading weights file models/rte/pytorch_model.bin
09/17 03:01:52 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpqn5p3x8j
09/17 03:01:54 AM: copying /tmp/tmpqn5p3x8j to cache at ./experiments/coref-ontonotes-rte-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 03:01:54 AM: creating metadata file for ./experiments/coref-ontonotes-rte-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 03:01:54 AM: removing temp file /tmp/tmpqn5p3x8j
09/17 03:01:54 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/coref-ontonotes-rte-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 03:01:54 AM: Initializing parameters
09/17 03:01:54 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/17 03:01:54 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/17 03:01:54 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/17 03:01:54 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/17 03:01:54 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/17 03:01:55 AM:    _text_field_embedder.model.pooler.dense.bias
09/17 03:01:55 AM:    _text_field_embedder.model.pooler.dense.weight
09/17 03:01:55 AM: 	Task 'edges-coref-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-coref-ontonotes"
}
09/17 03:01:59 AM: Model specification:
09/17 03:01:59 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-coref-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=2, bias=True)
      )
    )
  )
)
09/17 03:01:59 AM: Model parameters:
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 03:01:59 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 512 with torch.Size([2, 256])
09/17 03:01:59 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
09/17 03:01:59 AM: Total number of parameters: 110139394 (1.10139e+08)
09/17 03:01:59 AM: Number of trainable parameters: 657154 (657154)
09/17 03:01:59 AM: Finished building model in 11.053s
09/17 03:01:59 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-coref-ontonotes 

09/17 03:02:04 AM: patience = 9
09/17 03:02:04 AM: val_interval = 1000
09/17 03:02:04 AM: max_vals = 250
09/17 03:02:04 AM: cuda_device = 0
09/17 03:02:04 AM: grad_norm = 5.0
09/17 03:02:04 AM: grad_clipping = None
09/17 03:02:04 AM: lr_decay = 0.99
09/17 03:02:04 AM: min_lr = 1e-06
09/17 03:02:04 AM: keep_all_checkpoints = 0
09/17 03:02:04 AM: val_data_limit = 5000
09/17 03:02:04 AM: max_epochs = -1
09/17 03:02:04 AM: dec_val_scale = 250
09/17 03:02:04 AM: training_data_fraction = 1
09/17 03:02:04 AM: type = adam
09/17 03:02:04 AM: parameter_groups = None
09/17 03:02:04 AM: Number of trainable parameters: 657154
09/17 03:02:04 AM: infer_type_and_cast = True
09/17 03:02:04 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 03:02:04 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 03:02:04 AM: lr = 0.0001
09/17 03:02:04 AM: amsgrad = True
09/17 03:02:04 AM: type = reduce_on_plateau
09/17 03:02:04 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 03:02:04 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 03:02:04 AM: mode = max
09/17 03:02:04 AM: factor = 0.5
09/17 03:02:04 AM: patience = 3
09/17 03:02:04 AM: threshold = 0.0001
09/17 03:02:04 AM: threshold_mode = abs
09/17 03:02:04 AM: verbose = True
09/17 03:02:04 AM: type = adam
09/17 03:02:04 AM: parameter_groups = None
09/17 03:02:04 AM: Number of trainable parameters: 657154
09/17 03:02:04 AM: infer_type_and_cast = True
09/17 03:02:04 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 03:02:04 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 03:02:04 AM: lr = 0.0001
09/17 03:02:04 AM: amsgrad = True
09/17 03:02:04 AM: type = reduce_on_plateau
09/17 03:02:04 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 03:02:04 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 03:02:04 AM: mode = max
09/17 03:02:04 AM: factor = 0.5
09/17 03:02:04 AM: patience = 3
09/17 03:02:04 AM: threshold = 0.0001
09/17 03:02:04 AM: threshold_mode = abs
09/17 03:02:04 AM: verbose = True
09/17 03:02:04 AM: Starting training without restoring from a checkpoint.
09/17 03:02:04 AM: Training examples per task, before any subsampling: {'edges-coref-ontonotes': 41777}
09/17 03:02:04 AM: Beginning training with stopping criteria based on metric: edges-coref-ontonotes_f1
09/17 03:02:14 AM: Update 292: task edges-coref-ontonotes, batch 292 (292): mcc: 0.5922, acc: 0.7600, precision: 0.7946, recall: 0.7986, f1: 0.7966, edges-coref-ontonotes_loss: 0.4259
09/17 03:02:24 AM: Update 558: task edges-coref-ontonotes, batch 558 (558): mcc: 0.6056, acc: 0.7703, precision: 0.8026, recall: 0.8032, f1: 0.8029, edges-coref-ontonotes_loss: 0.4192
09/17 03:02:34 AM: Update 771: task edges-coref-ontonotes, batch 771 (771): mcc: 0.6225, acc: 0.7823, precision: 0.8114, recall: 0.8110, f1: 0.8112, edges-coref-ontonotes_loss: 0.4098
09/17 03:02:44 AM: Update 986: task edges-coref-ontonotes, batch 986 (986): mcc: 0.6417, acc: 0.7947, precision: 0.8212, recall: 0.8203, f1: 0.8207, edges-coref-ontonotes_loss: 0.3971
09/17 03:02:45 AM: ***** Step 1000 / Validation 1 *****
09/17 03:02:45 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:02:45 AM: Validating...
09/17 03:02:51 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:02:51 AM: Best result seen so far for micro.
09/17 03:02:51 AM: Best result seen so far for macro.
09/17 03:02:51 AM: Updating LR scheduler:
09/17 03:02:51 AM: 	Best result seen so far for macro_avg: 0.855
09/17 03:02:51 AM: 	# validation passes without improvement: 0
09/17 03:02:51 AM: edges-coref-ontonotes_loss: training: 0.396216 validation: 0.376058
09/17 03:02:51 AM: macro_avg: validation: 0.854696
09/17 03:02:51 AM: micro_avg: validation: 0.000000
09/17 03:02:51 AM: edges-coref-ontonotes_mcc: training: 0.642728 validation: 0.710306
09/17 03:02:51 AM: edges-coref-ontonotes_acc: training: 0.795404 validation: 0.851202
09/17 03:02:51 AM: edges-coref-ontonotes_precision: training: 0.821735 validation: 0.857363
09/17 03:02:51 AM: edges-coref-ontonotes_recall: training: 0.820786 validation: 0.852045
09/17 03:02:51 AM: edges-coref-ontonotes_f1: training: 0.821261 validation: 0.854696
09/17 03:02:51 AM: Global learning rate: 0.0001
09/17 03:02:51 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:02:54 AM: Update 1124: task edges-coref-ontonotes, batch 124 (1124): mcc: 0.7638, acc: 0.8688, precision: 0.8834, recall: 0.8799, f1: 0.8817, edges-coref-ontonotes_loss: 0.2922
09/17 03:03:04 AM: Update 1363: task edges-coref-ontonotes, batch 363 (1363): mcc: 0.7417, acc: 0.8581, precision: 0.8718, recall: 0.8696, f1: 0.8707, edges-coref-ontonotes_loss: 0.3086
09/17 03:03:14 AM: Update 1636: task edges-coref-ontonotes, batch 636 (1636): mcc: 0.7469, acc: 0.8620, precision: 0.8743, recall: 0.8723, f1: 0.8733, edges-coref-ontonotes_loss: 0.3041
09/17 03:03:25 AM: Update 1935: task edges-coref-ontonotes, batch 935 (1935): mcc: 0.7313, acc: 0.8524, precision: 0.8661, recall: 0.8649, f1: 0.8655, edges-coref-ontonotes_loss: 0.3191
09/17 03:03:27 AM: ***** Step 2000 / Validation 2 *****
09/17 03:03:27 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:03:27 AM: Validating...
09/17 03:03:33 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:03:33 AM: Best result seen so far for macro.
09/17 03:03:33 AM: Updating LR scheduler:
09/17 03:03:33 AM: 	Best result seen so far for macro_avg: 0.867
09/17 03:03:33 AM: 	# validation passes without improvement: 0
09/17 03:03:33 AM: edges-coref-ontonotes_loss: training: 0.318195 validation: 0.346081
09/17 03:03:33 AM: macro_avg: validation: 0.867463
09/17 03:03:33 AM: micro_avg: validation: 0.000000
09/17 03:03:33 AM: edges-coref-ontonotes_mcc: training: 0.731501 validation: 0.735029
09/17 03:03:33 AM: edges-coref-ontonotes_acc: training: 0.852751 validation: 0.865485
09/17 03:03:33 AM: edges-coref-ontonotes_precision: training: 0.866268 validation: 0.867796
09/17 03:03:33 AM: edges-coref-ontonotes_recall: training: 0.865043 validation: 0.867131
09/17 03:03:33 AM: edges-coref-ontonotes_f1: training: 0.865655 validation: 0.867463
09/17 03:03:33 AM: Global learning rate: 0.0001
09/17 03:03:33 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:03:35 AM: Update 2062: task edges-coref-ontonotes, batch 62 (2062): mcc: 0.7499, acc: 0.8656, precision: 0.8750, recall: 0.8748, f1: 0.8749, edges-coref-ontonotes_loss: 0.3010
09/17 03:03:45 AM: Update 2327: task edges-coref-ontonotes, batch 327 (2327): mcc: 0.7610, acc: 0.8710, precision: 0.8808, recall: 0.8800, f1: 0.8804, edges-coref-ontonotes_loss: 0.2796
09/17 03:03:55 AM: Update 2617: task edges-coref-ontonotes, batch 617 (2617): mcc: 0.7707, acc: 0.8762, precision: 0.8854, recall: 0.8853, f1: 0.8853, edges-coref-ontonotes_loss: 0.2646
09/17 03:04:06 AM: Update 2930: task edges-coref-ontonotes, batch 930 (2930): mcc: 0.7711, acc: 0.8766, precision: 0.8856, recall: 0.8855, f1: 0.8856, edges-coref-ontonotes_loss: 0.2648
09/17 03:04:08 AM: ***** Step 3000 / Validation 3 *****
09/17 03:04:08 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:04:08 AM: Validating...
09/17 03:04:13 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:04:13 AM: Best result seen so far for macro.
09/17 03:04:13 AM: Updating LR scheduler:
09/17 03:04:13 AM: 	Best result seen so far for macro_avg: 0.874
09/17 03:04:13 AM: 	# validation passes without improvement: 0
09/17 03:04:13 AM: edges-coref-ontonotes_loss: training: 0.269213 validation: 0.317819
09/17 03:04:13 AM: macro_avg: validation: 0.874485
09/17 03:04:13 AM: micro_avg: validation: 0.000000
09/17 03:04:13 AM: edges-coref-ontonotes_mcc: training: 0.767493 validation: 0.749314
09/17 03:04:13 AM: edges-coref-ontonotes_acc: training: 0.874575 validation: 0.871956
09/17 03:04:13 AM: edges-coref-ontonotes_precision: training: 0.883783 validation: 0.875677
09/17 03:04:13 AM: edges-coref-ontonotes_recall: training: 0.883699 validation: 0.873296
09/17 03:04:13 AM: edges-coref-ontonotes_f1: training: 0.883741 validation: 0.874485
09/17 03:04:13 AM: Global learning rate: 0.0001
09/17 03:04:13 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:04:16 AM: Update 3082: task edges-coref-ontonotes, batch 82 (3082): mcc: 0.6980, acc: 0.8354, precision: 0.8490, recall: 0.8490, f1: 0.8490, edges-coref-ontonotes_loss: 0.3431
09/17 03:04:26 AM: Update 3341: task edges-coref-ontonotes, batch 341 (3341): mcc: 0.7317, acc: 0.8544, precision: 0.8659, recall: 0.8658, f1: 0.8659, edges-coref-ontonotes_loss: 0.3098
09/17 03:04:36 AM: Update 3640: task edges-coref-ontonotes, batch 640 (3640): mcc: 0.7488, acc: 0.8644, precision: 0.8746, recall: 0.8741, f1: 0.8744, edges-coref-ontonotes_loss: 0.2863
09/17 03:04:46 AM: Update 3925: task edges-coref-ontonotes, batch 925 (3925): mcc: 0.7619, acc: 0.8719, precision: 0.8811, recall: 0.8808, f1: 0.8809, edges-coref-ontonotes_loss: 0.2687
09/17 03:04:49 AM: ***** Step 4000 / Validation 4 *****
09/17 03:04:49 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:04:49 AM: Validating...
09/17 03:04:54 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:04:54 AM: Best result seen so far for macro.
09/17 03:04:54 AM: Updating LR scheduler:
09/17 03:04:54 AM: 	Best result seen so far for macro_avg: 0.876
09/17 03:04:54 AM: 	# validation passes without improvement: 0
09/17 03:04:54 AM: edges-coref-ontonotes_loss: training: 0.267399 validation: 0.313755
09/17 03:04:54 AM: macro_avg: validation: 0.876187
09/17 03:04:54 AM: micro_avg: validation: 0.000000
09/17 03:04:54 AM: edges-coref-ontonotes_mcc: training: 0.763284 validation: 0.752412
09/17 03:04:54 AM: edges-coref-ontonotes_acc: training: 0.872756 validation: 0.874521
09/17 03:04:54 AM: edges-coref-ontonotes_precision: training: 0.881802 validation: 0.876321
09/17 03:04:54 AM: edges-coref-ontonotes_recall: training: 0.881433 validation: 0.876053
09/17 03:04:54 AM: edges-coref-ontonotes_f1: training: 0.881617 validation: 0.876187
09/17 03:04:54 AM: Global learning rate: 0.0001
09/17 03:04:54 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:04:56 AM: Update 4084: task edges-coref-ontonotes, batch 84 (4084): mcc: 0.7977, acc: 0.8916, precision: 0.8984, recall: 0.8994, f1: 0.8989, edges-coref-ontonotes_loss: 0.2401
09/17 03:05:06 AM: Update 4348: task edges-coref-ontonotes, batch 348 (4348): mcc: 0.7651, acc: 0.8741, precision: 0.8821, recall: 0.8832, f1: 0.8826, edges-coref-ontonotes_loss: 0.2719
09/17 03:05:16 AM: Update 4601: task edges-coref-ontonotes, batch 601 (4601): mcc: 0.7522, acc: 0.8670, precision: 0.8759, recall: 0.8763, f1: 0.8761, edges-coref-ontonotes_loss: 0.2852
09/17 03:05:26 AM: Update 4864: task edges-coref-ontonotes, batch 864 (4864): mcc: 0.7579, acc: 0.8705, precision: 0.8790, recall: 0.8790, f1: 0.8790, edges-coref-ontonotes_loss: 0.2774
09/17 03:05:30 AM: ***** Step 5000 / Validation 5 *****
09/17 03:05:30 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:05:30 AM: Validating...
09/17 03:05:35 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:05:35 AM: Best result seen so far for macro.
09/17 03:05:35 AM: Updating LR scheduler:
09/17 03:05:35 AM: 	Best result seen so far for macro_avg: 0.877
09/17 03:05:35 AM: 	# validation passes without improvement: 0
09/17 03:05:35 AM: edges-coref-ontonotes_loss: training: 0.265963 validation: 0.302611
09/17 03:05:35 AM: macro_avg: validation: 0.876617
09/17 03:05:35 AM: micro_avg: validation: 0.000000
09/17 03:05:35 AM: edges-coref-ontonotes_mcc: training: 0.765336 validation: 0.753178
09/17 03:05:35 AM: edges-coref-ontonotes_acc: training: 0.874565 validation: 0.874598
09/17 03:05:35 AM: edges-coref-ontonotes_precision: training: 0.882658 validation: 0.876416
09/17 03:05:35 AM: edges-coref-ontonotes_recall: training: 0.882681 validation: 0.876819
09/17 03:05:35 AM: edges-coref-ontonotes_f1: training: 0.882670 validation: 0.876617
09/17 03:05:35 AM: Global learning rate: 0.0001
09/17 03:05:35 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:05:36 AM: Update 5033: task edges-coref-ontonotes, batch 33 (5033): mcc: 0.8450, acc: 0.9183, precision: 0.9228, recall: 0.9222, f1: 0.9225, edges-coref-ontonotes_loss: 0.1838
09/17 03:05:46 AM: Update 5285: task edges-coref-ontonotes, batch 285 (5285): mcc: 0.7872, acc: 0.8876, precision: 0.8935, recall: 0.8937, f1: 0.8936, edges-coref-ontonotes_loss: 0.2293
09/17 03:05:57 AM: Update 5546: task edges-coref-ontonotes, batch 546 (5546): mcc: 0.7882, acc: 0.8882, precision: 0.8941, recall: 0.8941, f1: 0.8941, edges-coref-ontonotes_loss: 0.2334
09/17 03:06:08 AM: Update 5859: task edges-coref-ontonotes, batch 859 (5859): mcc: 0.7681, acc: 0.8772, precision: 0.8840, recall: 0.8841, f1: 0.8841, edges-coref-ontonotes_loss: 0.2588
09/17 03:06:13 AM: ***** Step 6000 / Validation 6 *****
09/17 03:06:13 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:06:13 AM: Validating...
09/17 03:06:18 AM: Updating LR scheduler:
09/17 03:06:18 AM: 	Best result seen so far for macro_avg: 0.877
09/17 03:06:18 AM: 	# validation passes without improvement: 1
09/17 03:06:18 AM: edges-coref-ontonotes_loss: training: 0.259741 validation: 0.305716
09/17 03:06:18 AM: macro_avg: validation: 0.876559
09/17 03:06:18 AM: micro_avg: validation: 0.000000
09/17 03:06:18 AM: edges-coref-ontonotes_mcc: training: 0.767981 validation: 0.753371
09/17 03:06:18 AM: edges-coref-ontonotes_acc: training: 0.877130 validation: 0.875134
09/17 03:06:18 AM: edges-coref-ontonotes_precision: training: 0.884031 validation: 0.877451
09/17 03:06:18 AM: edges-coref-ontonotes_recall: training: 0.883938 validation: 0.875670
09/17 03:06:18 AM: edges-coref-ontonotes_f1: training: 0.883984 validation: 0.876559
09/17 03:06:18 AM: Global learning rate: 0.0001
09/17 03:06:18 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:06:18 AM: Update 6008: task edges-coref-ontonotes, batch 8 (6008): mcc: 0.7779, acc: 0.8846, precision: 0.8895, recall: 0.8882, f1: 0.8889, edges-coref-ontonotes_loss: 0.2777
09/17 03:06:28 AM: Update 6295: task edges-coref-ontonotes, batch 295 (6295): mcc: 0.7893, acc: 0.8886, precision: 0.8947, recall: 0.8946, f1: 0.8946, edges-coref-ontonotes_loss: 0.2282
09/17 03:06:38 AM: Update 6568: task edges-coref-ontonotes, batch 568 (6568): mcc: 0.7941, acc: 0.8913, precision: 0.8971, recall: 0.8969, f1: 0.8970, edges-coref-ontonotes_loss: 0.2203
09/17 03:06:49 AM: Update 6854: task edges-coref-ontonotes, batch 854 (6854): mcc: 0.7941, acc: 0.8915, precision: 0.8973, recall: 0.8968, f1: 0.8970, edges-coref-ontonotes_loss: 0.2244
09/17 03:06:53 AM: ***** Step 7000 / Validation 7 *****
09/17 03:06:53 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:06:53 AM: Validating...
09/17 03:06:59 AM: Updating LR scheduler:
09/17 03:06:59 AM: 	Best result seen so far for macro_avg: 0.877
09/17 03:06:59 AM: 	# validation passes without improvement: 2
09/17 03:06:59 AM: edges-coref-ontonotes_loss: training: 0.235211 validation: 0.304294
09/17 03:06:59 AM: macro_avg: validation: 0.872639
09/17 03:06:59 AM: micro_avg: validation: 0.000000
09/17 03:06:59 AM: edges-coref-ontonotes_mcc: training: 0.785661 validation: 0.745367
09/17 03:06:59 AM: edges-coref-ontonotes_acc: training: 0.886994 validation: 0.870845
09/17 03:06:59 AM: edges-coref-ontonotes_precision: training: 0.893020 validation: 0.872940
09/17 03:06:59 AM: edges-coref-ontonotes_recall: training: 0.892589 validation: 0.872339
09/17 03:06:59 AM: edges-coref-ontonotes_f1: training: 0.892805 validation: 0.872639
09/17 03:06:59 AM: Global learning rate: 0.0001
09/17 03:06:59 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:06:59 AM: Update 7005: task edges-coref-ontonotes, batch 5 (7005): mcc: 0.7419, acc: 0.8629, precision: 0.8713, recall: 0.8705, f1: 0.8709, edges-coref-ontonotes_loss: 0.2638
09/17 03:07:09 AM: Update 7266: task edges-coref-ontonotes, batch 266 (7266): mcc: 0.7509, acc: 0.8682, precision: 0.8752, recall: 0.8759, f1: 0.8755, edges-coref-ontonotes_loss: 0.2826
09/17 03:07:19 AM: Update 7547: task edges-coref-ontonotes, batch 547 (7547): mcc: 0.7665, acc: 0.8766, precision: 0.8832, recall: 0.8834, f1: 0.8833, edges-coref-ontonotes_loss: 0.2571
09/17 03:07:29 AM: Update 7849: task edges-coref-ontonotes, batch 849 (7849): mcc: 0.7791, acc: 0.8835, precision: 0.8896, recall: 0.8895, f1: 0.8895, edges-coref-ontonotes_loss: 0.2398
09/17 03:07:34 AM: ***** Step 8000 / Validation 8 *****
09/17 03:07:34 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:07:34 AM: Validating...
09/17 03:07:39 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:07:39 AM: Best result seen so far for macro.
09/17 03:07:39 AM: Updating LR scheduler:
09/17 03:07:39 AM: 	Best result seen so far for macro_avg: 0.883
09/17 03:07:39 AM: 	# validation passes without improvement: 0
09/17 03:07:39 AM: edges-coref-ontonotes_loss: training: 0.238793 validation: 0.290208
09/17 03:07:39 AM: macro_avg: validation: 0.882845
09/17 03:07:39 AM: micro_avg: validation: 0.000000
09/17 03:07:39 AM: edges-coref-ontonotes_mcc: training: 0.780521 validation: 0.765623
09/17 03:07:39 AM: edges-coref-ontonotes_acc: training: 0.884389 validation: 0.881031
09/17 03:07:39 AM: edges-coref-ontonotes_precision: training: 0.890346 validation: 0.882592
09/17 03:07:39 AM: edges-coref-ontonotes_recall: training: 0.890151 validation: 0.883098
09/17 03:07:39 AM: edges-coref-ontonotes_f1: training: 0.890249 validation: 0.882845
09/17 03:07:39 AM: Global learning rate: 0.0001
09/17 03:07:39 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:07:39 AM: Update 8007: task edges-coref-ontonotes, batch 7 (8007): mcc: 0.7681, acc: 0.8764, precision: 0.8855, recall: 0.8821, f1: 0.8838, edges-coref-ontonotes_loss: 0.2486
09/17 03:07:49 AM: Update 8272: task edges-coref-ontonotes, batch 272 (8272): mcc: 0.7786, acc: 0.8836, precision: 0.8894, recall: 0.8891, f1: 0.8893, edges-coref-ontonotes_loss: 0.2537
09/17 03:07:59 AM: Update 8534: task edges-coref-ontonotes, batch 534 (8534): mcc: 0.7660, acc: 0.8768, precision: 0.8832, recall: 0.8828, f1: 0.8830, edges-coref-ontonotes_loss: 0.2662
09/17 03:08:10 AM: Update 8796: task edges-coref-ontonotes, batch 796 (8796): mcc: 0.7695, acc: 0.8788, precision: 0.8849, recall: 0.8846, f1: 0.8848, edges-coref-ontonotes_loss: 0.2593
09/17 03:08:15 AM: ***** Step 9000 / Validation 9 *****
09/17 03:08:15 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:08:15 AM: Validating...
09/17 03:08:20 AM: Evaluate: task edges-coref-ontonotes, batch 144 (157): mcc: 0.7685, acc: 0.8832, precision: 0.8841, recall: 0.8844, f1: 0.8843, edges-coref-ontonotes_loss: 0.2872
09/17 03:08:20 AM: Updating LR scheduler:
09/17 03:08:20 AM: 	Best result seen so far for macro_avg: 0.883
09/17 03:08:20 AM: 	# validation passes without improvement: 1
09/17 03:08:20 AM: edges-coref-ontonotes_loss: training: 0.243065 validation: 0.292784
09/17 03:08:20 AM: macro_avg: validation: 0.881738
09/17 03:08:20 AM: micro_avg: validation: 0.000000
09/17 03:08:20 AM: edges-coref-ontonotes_mcc: training: 0.783375 validation: 0.763440
09/17 03:08:20 AM: edges-coref-ontonotes_acc: training: 0.886117 validation: 0.880724
09/17 03:08:20 AM: edges-coref-ontonotes_precision: training: 0.891823 validation: 0.881603
09/17 03:08:20 AM: edges-coref-ontonotes_recall: training: 0.891514 validation: 0.881873
09/17 03:08:20 AM: edges-coref-ontonotes_f1: training: 0.891669 validation: 0.881738
09/17 03:08:20 AM: Global learning rate: 0.0001
09/17 03:08:20 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:08:30 AM: Update 9248: task edges-coref-ontonotes, batch 248 (9248): mcc: 0.7806, acc: 0.8854, precision: 0.8906, recall: 0.8899, f1: 0.8903, edges-coref-ontonotes_loss: 0.2261
09/17 03:08:40 AM: Update 9516: task edges-coref-ontonotes, batch 516 (9516): mcc: 0.7874, acc: 0.8891, precision: 0.8938, recall: 0.8936, f1: 0.8937, edges-coref-ontonotes_loss: 0.2298
09/17 03:08:50 AM: Update 9783: task edges-coref-ontonotes, batch 783 (9783): mcc: 0.7749, acc: 0.8820, precision: 0.8875, recall: 0.8874, f1: 0.8875, edges-coref-ontonotes_loss: 0.2481
09/17 03:08:56 AM: ***** Step 10000 / Validation 10 *****
09/17 03:08:56 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:08:56 AM: Validating...
09/17 03:09:00 AM: Evaluate: task edges-coref-ontonotes, batch 97 (157): mcc: 0.7543, acc: 0.8749, precision: 0.8764, recall: 0.8782, f1: 0.8773, edges-coref-ontonotes_loss: 0.3114
09/17 03:09:02 AM: Updating LR scheduler:
09/17 03:09:02 AM: 	Best result seen so far for macro_avg: 0.883
09/17 03:09:02 AM: 	# validation passes without improvement: 2
09/17 03:09:02 AM: edges-coref-ontonotes_loss: training: 0.245988 validation: 0.293296
09/17 03:09:02 AM: macro_avg: validation: 0.881432
09/17 03:09:02 AM: micro_avg: validation: 0.000000
09/17 03:09:02 AM: edges-coref-ontonotes_mcc: training: 0.777525 validation: 0.762562
09/17 03:09:02 AM: edges-coref-ontonotes_acc: training: 0.883548 validation: 0.878810
09/17 03:09:02 AM: edges-coref-ontonotes_precision: training: 0.888738 validation: 0.880304
09/17 03:09:02 AM: edges-coref-ontonotes_recall: training: 0.888794 validation: 0.882562
09/17 03:09:02 AM: edges-coref-ontonotes_f1: training: 0.888766 validation: 0.881432
09/17 03:09:02 AM: Global learning rate: 0.0001
09/17 03:09:02 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:09:10 AM: Update 10200: task edges-coref-ontonotes, batch 200 (10200): mcc: 0.7963, acc: 0.8936, precision: 0.8987, recall: 0.8975, f1: 0.8981, edges-coref-ontonotes_loss: 0.2168
09/17 03:09:20 AM: Update 10482: task edges-coref-ontonotes, batch 482 (10482): mcc: 0.7979, acc: 0.8948, precision: 0.8993, recall: 0.8985, f1: 0.8989, edges-coref-ontonotes_loss: 0.2091
09/17 03:09:30 AM: Update 10778: task edges-coref-ontonotes, batch 778 (10778): mcc: 0.7996, acc: 0.8958, precision: 0.9001, recall: 0.8994, f1: 0.8997, edges-coref-ontonotes_loss: 0.2134
09/17 03:09:37 AM: ***** Step 11000 / Validation 11 *****
09/17 03:09:37 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:09:37 AM: Validating...
09/17 03:09:40 AM: Evaluate: task edges-coref-ontonotes, batch 89 (157): mcc: 0.7573, acc: 0.8773, precision: 0.8785, recall: 0.8788, f1: 0.8787, edges-coref-ontonotes_loss: 0.3019
09/17 03:09:43 AM: Updating LR scheduler:
09/17 03:09:43 AM: 	Best result seen so far for macro_avg: 0.883
09/17 03:09:43 AM: 	# validation passes without improvement: 3
09/17 03:09:43 AM: edges-coref-ontonotes_loss: training: 0.228730 validation: 0.290839
09/17 03:09:43 AM: macro_avg: validation: 0.880900
09/17 03:09:43 AM: micro_avg: validation: 0.000000
09/17 03:09:43 AM: edges-coref-ontonotes_mcc: training: 0.789398 validation: 0.761755
09/17 03:09:43 AM: edges-coref-ontonotes_acc: training: 0.890420 validation: 0.879691
09/17 03:09:43 AM: edges-coref-ontonotes_precision: training: 0.894937 validation: 0.880732
09/17 03:09:43 AM: edges-coref-ontonotes_recall: training: 0.894397 validation: 0.881069
09/17 03:09:43 AM: edges-coref-ontonotes_f1: training: 0.894667 validation: 0.880900
09/17 03:09:43 AM: Global learning rate: 0.0001
09/17 03:09:43 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:09:50 AM: Update 11193: task edges-coref-ontonotes, batch 193 (11193): mcc: 0.7729, acc: 0.8819, precision: 0.8865, recall: 0.8863, f1: 0.8864, edges-coref-ontonotes_loss: 0.2539
09/17 03:10:00 AM: Update 11460: task edges-coref-ontonotes, batch 460 (11460): mcc: 0.7822, acc: 0.8863, precision: 0.8913, recall: 0.8909, f1: 0.8911, edges-coref-ontonotes_loss: 0.2406
09/17 03:10:11 AM: Update 11773: task edges-coref-ontonotes, batch 773 (11773): mcc: 0.7933, acc: 0.8923, precision: 0.8968, recall: 0.8965, f1: 0.8967, edges-coref-ontonotes_loss: 0.2224
09/17 03:10:18 AM: ***** Step 12000 / Validation 12 *****
09/17 03:10:18 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:10:18 AM: Validating...
09/17 03:10:21 AM: Evaluate: task edges-coref-ontonotes, batch 100 (157): mcc: 0.7617, acc: 0.8796, precision: 0.8813, recall: 0.8802, f1: 0.8808, edges-coref-ontonotes_loss: 0.3094
09/17 03:10:24 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:10:24 AM: Best result seen so far for macro.
09/17 03:10:24 AM: Updating LR scheduler:
09/17 03:10:24 AM: 	Best result seen so far for macro_avg: 0.884
09/17 03:10:24 AM: 	# validation passes without improvement: 0
09/17 03:10:24 AM: edges-coref-ontonotes_loss: training: 0.218694 validation: 0.293336
09/17 03:10:24 AM: macro_avg: validation: 0.883606
09/17 03:10:24 AM: micro_avg: validation: 0.000000
09/17 03:10:24 AM: edges-coref-ontonotes_mcc: training: 0.796312 validation: 0.767346
09/17 03:10:24 AM: edges-coref-ontonotes_acc: training: 0.893945 validation: 0.882486
09/17 03:10:24 AM: edges-coref-ontonotes_precision: training: 0.898284 validation: 0.884114
09/17 03:10:24 AM: edges-coref-ontonotes_recall: training: 0.897995 validation: 0.883098
09/17 03:10:24 AM: edges-coref-ontonotes_f1: training: 0.898140 validation: 0.883606
09/17 03:10:24 AM: Global learning rate: 0.0001
09/17 03:10:24 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:10:31 AM: Update 12175: task edges-coref-ontonotes, batch 175 (12175): mcc: 0.7801, acc: 0.8856, precision: 0.8898, recall: 0.8904, f1: 0.8901, edges-coref-ontonotes_loss: 0.2524
09/17 03:10:41 AM: Update 12449: task edges-coref-ontonotes, batch 449 (12449): mcc: 0.7657, acc: 0.8780, precision: 0.8828, recall: 0.8828, f1: 0.8828, edges-coref-ontonotes_loss: 0.2652
09/17 03:10:51 AM: Update 12715: task edges-coref-ontonotes, batch 715 (12715): mcc: 0.7721, acc: 0.8815, precision: 0.8861, recall: 0.8860, f1: 0.8861, edges-coref-ontonotes_loss: 0.2553
09/17 03:10:59 AM: ***** Step 13000 / Validation 13 *****
09/17 03:10:59 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:10:59 AM: Validating...
09/17 03:11:02 AM: Evaluate: task edges-coref-ontonotes, batch 56 (157): mcc: 0.7860, acc: 0.8924, precision: 0.8933, recall: 0.8927, f1: 0.8930, edges-coref-ontonotes_loss: 0.2773
09/17 03:11:05 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:11:05 AM: Best result seen so far for macro.
09/17 03:11:05 AM: Updating LR scheduler:
09/17 03:11:05 AM: 	Best result seen so far for macro_avg: 0.886
09/17 03:11:05 AM: 	# validation passes without improvement: 0
09/17 03:11:05 AM: edges-coref-ontonotes_loss: training: 0.232252 validation: 0.287270
09/17 03:11:05 AM: macro_avg: validation: 0.886174
09/17 03:11:05 AM: micro_avg: validation: 0.000000
09/17 03:11:05 AM: edges-coref-ontonotes_mcc: training: 0.787895 validation: 0.772400
09/17 03:11:05 AM: edges-coref-ontonotes_acc: training: 0.889661 validation: 0.885511
09/17 03:11:05 AM: edges-coref-ontonotes_precision: training: 0.894023 validation: 0.886378
09/17 03:11:05 AM: edges-coref-ontonotes_recall: training: 0.893851 validation: 0.885970
09/17 03:11:05 AM: edges-coref-ontonotes_f1: training: 0.893937 validation: 0.886174
09/17 03:11:05 AM: Global learning rate: 0.0001
09/17 03:11:05 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:11:12 AM: Update 13166: task edges-coref-ontonotes, batch 166 (13166): mcc: 0.7840, acc: 0.8881, precision: 0.8920, recall: 0.8920, f1: 0.8920, edges-coref-ontonotes_loss: 0.2356
09/17 03:11:22 AM: Update 13452: task edges-coref-ontonotes, batch 452 (13452): mcc: 0.7908, acc: 0.8918, precision: 0.8956, recall: 0.8951, f1: 0.8953, edges-coref-ontonotes_loss: 0.2298
09/17 03:11:32 AM: Update 13708: task edges-coref-ontonotes, batch 708 (13708): mcc: 0.7803, acc: 0.8860, precision: 0.8903, recall: 0.8899, f1: 0.8901, edges-coref-ontonotes_loss: 0.2448
09/17 03:11:41 AM: ***** Step 14000 / Validation 14 *****
09/17 03:11:41 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:11:41 AM: Validating...
09/17 03:11:42 AM: Evaluate: task edges-coref-ontonotes, batch 33 (157): mcc: 0.7856, acc: 0.8918, precision: 0.8931, recall: 0.8924, f1: 0.8928, edges-coref-ontonotes_loss: 0.2720
09/17 03:11:46 AM: Updating LR scheduler:
09/17 03:11:46 AM: 	Best result seen so far for macro_avg: 0.886
09/17 03:11:46 AM: 	# validation passes without improvement: 1
09/17 03:11:46 AM: edges-coref-ontonotes_loss: training: 0.242322 validation: 0.291878
09/17 03:11:46 AM: macro_avg: validation: 0.879602
09/17 03:11:46 AM: micro_avg: validation: 0.000000
09/17 03:11:46 AM: edges-coref-ontonotes_mcc: training: 0.782864 validation: 0.759305
09/17 03:11:46 AM: edges-coref-ontonotes_acc: training: 0.887436 validation: 0.878274
09/17 03:11:46 AM: edges-coref-ontonotes_precision: training: 0.891631 validation: 0.879972
09/17 03:11:46 AM: edges-coref-ontonotes_recall: training: 0.891178 validation: 0.879231
09/17 03:11:46 AM: edges-coref-ontonotes_f1: training: 0.891404 validation: 0.879602
09/17 03:11:46 AM: Global learning rate: 0.0001
09/17 03:11:46 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:11:52 AM: Update 14136: task edges-coref-ontonotes, batch 136 (14136): mcc: 0.8124, acc: 0.9028, precision: 0.9063, recall: 0.9060, f1: 0.9062, edges-coref-ontonotes_loss: 0.1761
09/17 03:12:02 AM: Update 14429: task edges-coref-ontonotes, batch 429 (14429): mcc: 0.8087, acc: 0.9010, precision: 0.9044, recall: 0.9043, f1: 0.9043, edges-coref-ontonotes_loss: 0.1946
09/17 03:12:12 AM: Update 14704: task edges-coref-ontonotes, batch 704 (14704): mcc: 0.8085, acc: 0.9010, precision: 0.9044, recall: 0.9041, f1: 0.9042, edges-coref-ontonotes_loss: 0.2003
09/17 03:12:21 AM: ***** Step 15000 / Validation 15 *****
09/17 03:12:21 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:12:21 AM: Validating...
09/17 03:12:22 AM: Evaluate: task edges-coref-ontonotes, batch 28 (157): mcc: 0.7928, acc: 0.8952, precision: 0.8962, recall: 0.8967, f1: 0.8964, edges-coref-ontonotes_loss: 0.2632
09/17 03:12:26 AM: Updating LR scheduler:
09/17 03:12:26 AM: 	Best result seen so far for macro_avg: 0.886
09/17 03:12:26 AM: 	# validation passes without improvement: 2
09/17 03:12:26 AM: edges-coref-ontonotes_loss: training: 0.221079 validation: 0.289310
09/17 03:12:26 AM: macro_avg: validation: 0.881493
09/17 03:12:26 AM: micro_avg: validation: 0.000000
09/17 03:12:26 AM: edges-coref-ontonotes_mcc: training: 0.794510 validation: 0.763019
09/17 03:12:26 AM: edges-coref-ontonotes_acc: training: 0.893626 validation: 0.880648
09/17 03:12:26 AM: edges-coref-ontonotes_precision: training: 0.897359 validation: 0.881612
09/17 03:12:26 AM: edges-coref-ontonotes_recall: training: 0.897124 validation: 0.881375
09/17 03:12:26 AM: edges-coref-ontonotes_f1: training: 0.897242 validation: 0.881493
09/17 03:12:26 AM: Global learning rate: 0.0001
09/17 03:12:26 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:12:32 AM: Update 15120: task edges-coref-ontonotes, batch 120 (15120): mcc: 0.7788, acc: 0.8859, precision: 0.8892, recall: 0.8897, f1: 0.8894, edges-coref-ontonotes_loss: 0.2408
09/17 03:12:42 AM: Update 15383: task edges-coref-ontonotes, batch 383 (15383): mcc: 0.7868, acc: 0.8897, precision: 0.8934, recall: 0.8934, f1: 0.8934, edges-coref-ontonotes_loss: 0.2307
09/17 03:12:53 AM: Update 15697: task edges-coref-ontonotes, batch 697 (15697): mcc: 0.8017, acc: 0.8975, precision: 0.9009, recall: 0.9008, f1: 0.9008, edges-coref-ontonotes_loss: 0.2106
09/17 03:13:02 AM: ***** Step 16000 / Validation 16 *****
09/17 03:13:02 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:13:02 AM: Validating...
09/17 03:13:03 AM: Evaluate: task edges-coref-ontonotes, batch 42 (157): mcc: 0.7921, acc: 0.8950, precision: 0.8962, recall: 0.8959, f1: 0.8960, edges-coref-ontonotes_loss: 0.2637
09/17 03:13:07 AM: Updating LR scheduler:
09/17 03:13:07 AM: 	Best result seen so far for macro_avg: 0.886
09/17 03:13:07 AM: 	# validation passes without improvement: 3
09/17 03:13:07 AM: edges-coref-ontonotes_loss: training: 0.209242 validation: 0.292052
09/17 03:13:07 AM: macro_avg: validation: 0.885941
09/17 03:13:07 AM: micro_avg: validation: 0.000000
09/17 03:13:07 AM: edges-coref-ontonotes_mcc: training: 0.804268 validation: 0.771826
09/17 03:13:07 AM: edges-coref-ontonotes_acc: training: 0.898930 validation: 0.884936
09/17 03:13:07 AM: edges-coref-ontonotes_precision: training: 0.902225 validation: 0.885721
09/17 03:13:07 AM: edges-coref-ontonotes_recall: training: 0.902020 validation: 0.886162
09/17 03:13:07 AM: edges-coref-ontonotes_f1: training: 0.902123 validation: 0.885941
09/17 03:13:07 AM: Global learning rate: 0.0001
09/17 03:13:07 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:13:13 AM: Update 16137: task edges-coref-ontonotes, batch 137 (16137): mcc: 0.7686, acc: 0.8803, precision: 0.8847, recall: 0.8838, f1: 0.8842, edges-coref-ontonotes_loss: 0.2633
09/17 03:13:23 AM: Update 16400: task edges-coref-ontonotes, batch 400 (16400): mcc: 0.7696, acc: 0.8810, precision: 0.8850, recall: 0.8845, f1: 0.8847, edges-coref-ontonotes_loss: 0.2598
09/17 03:13:33 AM: Update 16696: task edges-coref-ontonotes, batch 696 (16696): mcc: 0.7806, acc: 0.8868, precision: 0.8904, recall: 0.8903, f1: 0.8903, edges-coref-ontonotes_loss: 0.2424
09/17 03:13:43 AM: ***** Step 17000 / Validation 17 *****
09/17 03:13:43 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:13:43 AM: Validating...
09/17 03:13:43 AM: Evaluate: task edges-coref-ontonotes, batch 23 (157): mcc: 0.8235, acc: 0.9101, precision: 0.9123, recall: 0.9110, f1: 0.9117, edges-coref-ontonotes_loss: 0.2364
09/17 03:13:48 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:13:48 AM: Best result seen so far for macro.
09/17 03:13:48 AM: Updating LR scheduler:
09/17 03:13:48 AM: 	Best result seen so far for macro_avg: 0.887
09/17 03:13:48 AM: 	# validation passes without improvement: 0
09/17 03:13:48 AM: edges-coref-ontonotes_loss: training: 0.224449 validation: 0.277483
09/17 03:13:48 AM: macro_avg: validation: 0.887204
09/17 03:13:48 AM: micro_avg: validation: 0.000000
09/17 03:13:48 AM: edges-coref-ontonotes_mcc: training: 0.792666 validation: 0.774468
09/17 03:13:48 AM: edges-coref-ontonotes_acc: training: 0.893035 validation: 0.886277
09/17 03:13:48 AM: edges-coref-ontonotes_precision: training: 0.896421 validation: 0.887442
09/17 03:13:48 AM: edges-coref-ontonotes_recall: training: 0.896222 validation: 0.886966
09/17 03:13:48 AM: edges-coref-ontonotes_f1: training: 0.896321 validation: 0.887204
09/17 03:13:48 AM: Global learning rate: 0.0001
09/17 03:13:48 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:13:53 AM: Update 17132: task edges-coref-ontonotes, batch 132 (17132): mcc: 0.8101, acc: 0.9023, precision: 0.9047, recall: 0.9054, f1: 0.9051, edges-coref-ontonotes_loss: 0.1970
09/17 03:14:03 AM: Update 17396: task edges-coref-ontonotes, batch 396 (17396): mcc: 0.7998, acc: 0.8969, precision: 0.9000, recall: 0.8999, f1: 0.8999, edges-coref-ontonotes_loss: 0.2183
09/17 03:14:13 AM: Update 17659: task edges-coref-ontonotes, batch 659 (17659): mcc: 0.7852, acc: 0.8894, precision: 0.8926, recall: 0.8926, f1: 0.8926, edges-coref-ontonotes_loss: 0.2380
09/17 03:14:24 AM: Update 17944: task edges-coref-ontonotes, batch 944 (17944): mcc: 0.7868, acc: 0.8902, precision: 0.8935, recall: 0.8933, f1: 0.8934, edges-coref-ontonotes_loss: 0.2352
09/17 03:14:26 AM: ***** Step 18000 / Validation 18 *****
09/17 03:14:26 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:14:26 AM: Validating...
09/17 03:14:31 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:14:31 AM: Best result seen so far for macro.
09/17 03:14:31 AM: Updating LR scheduler:
09/17 03:14:31 AM: 	Best result seen so far for macro_avg: 0.888
09/17 03:14:31 AM: 	# validation passes without improvement: 0
09/17 03:14:31 AM: edges-coref-ontonotes_loss: training: 0.232576 validation: 0.283113
09/17 03:14:31 AM: macro_avg: validation: 0.888442
09/17 03:14:31 AM: micro_avg: validation: 0.000000
09/17 03:14:31 AM: edges-coref-ontonotes_mcc: training: 0.787539 validation: 0.776918
09/17 03:14:31 AM: edges-coref-ontonotes_acc: training: 0.890581 validation: 0.887732
09/17 03:14:31 AM: edges-coref-ontonotes_precision: training: 0.893823 validation: 0.888578
09/17 03:14:31 AM: edges-coref-ontonotes_recall: training: 0.893702 validation: 0.888306
09/17 03:14:31 AM: edges-coref-ontonotes_f1: training: 0.893762 validation: 0.888442
09/17 03:14:31 AM: Global learning rate: 0.0001
09/17 03:14:31 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:14:34 AM: Update 18105: task edges-coref-ontonotes, batch 105 (18105): mcc: 0.8523, acc: 0.9241, precision: 0.9262, recall: 0.9261, f1: 0.9261, edges-coref-ontonotes_loss: 0.1605
09/17 03:14:44 AM: Update 18392: task edges-coref-ontonotes, batch 392 (18392): mcc: 0.8235, acc: 0.9094, precision: 0.9117, recall: 0.9118, f1: 0.9118, edges-coref-ontonotes_loss: 0.1859
09/17 03:14:54 AM: Update 18677: task edges-coref-ontonotes, batch 677 (18677): mcc: 0.8160, acc: 0.9057, precision: 0.9079, recall: 0.9080, f1: 0.9080, edges-coref-ontonotes_loss: 0.1962
09/17 03:15:04 AM: Update 18945: task edges-coref-ontonotes, batch 945 (18945): mcc: 0.8008, acc: 0.8977, precision: 0.9003, recall: 0.9005, f1: 0.9004, edges-coref-ontonotes_loss: 0.2157
09/17 03:15:05 AM: ***** Step 19000 / Validation 19 *****
09/17 03:15:05 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:15:05 AM: Validating...
09/17 03:15:11 AM: Updating LR scheduler:
09/17 03:15:11 AM: 	Best result seen so far for macro_avg: 0.888
09/17 03:15:11 AM: 	# validation passes without improvement: 1
09/17 03:15:11 AM: edges-coref-ontonotes_loss: training: 0.214984 validation: 0.295125
09/17 03:15:11 AM: macro_avg: validation: 0.883516
09/17 03:15:11 AM: micro_avg: validation: 0.000000
09/17 03:15:11 AM: edges-coref-ontonotes_mcc: training: 0.801800 validation: 0.767001
09/17 03:15:11 AM: edges-coref-ontonotes_acc: training: 0.898240 validation: 0.882754
09/17 03:15:11 AM: edges-coref-ontonotes_precision: training: 0.900800 validation: 0.883398
09/17 03:15:11 AM: edges-coref-ontonotes_recall: training: 0.901026 validation: 0.883635
09/17 03:15:11 AM: edges-coref-ontonotes_f1: training: 0.900913 validation: 0.883516
09/17 03:15:11 AM: Global learning rate: 0.0001
09/17 03:15:11 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:15:14 AM: Update 19112: task edges-coref-ontonotes, batch 112 (19112): mcc: 0.7863, acc: 0.8901, precision: 0.8926, recall: 0.8938, f1: 0.8932, edges-coref-ontonotes_loss: 0.2365
09/17 03:15:24 AM: Update 19403: task edges-coref-ontonotes, batch 403 (19403): mcc: 0.8060, acc: 0.9001, precision: 0.9029, recall: 0.9031, f1: 0.9030, edges-coref-ontonotes_loss: 0.2074
09/17 03:15:34 AM: Update 19674: task edges-coref-ontonotes, batch 674 (19674): mcc: 0.8071, acc: 0.9008, precision: 0.9036, recall: 0.9035, f1: 0.9035, edges-coref-ontonotes_loss: 0.2039
09/17 03:15:44 AM: Update 19968: task edges-coref-ontonotes, batch 968 (19968): mcc: 0.8081, acc: 0.9014, precision: 0.9041, recall: 0.9040, f1: 0.9041, edges-coref-ontonotes_loss: 0.2055
09/17 03:15:45 AM: ***** Step 20000 / Validation 20 *****
09/17 03:15:45 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:15:45 AM: Validating...
09/17 03:15:50 AM: Updating LR scheduler:
09/17 03:15:50 AM: 	Best result seen so far for macro_avg: 0.888
09/17 03:15:50 AM: 	# validation passes without improvement: 2
09/17 03:15:50 AM: edges-coref-ontonotes_loss: training: 0.206606 validation: 0.280077
09/17 03:15:50 AM: macro_avg: validation: 0.887058
09/17 03:15:50 AM: micro_avg: validation: 0.000000
09/17 03:15:50 AM: edges-coref-ontonotes_mcc: training: 0.807541 validation: 0.774316
09/17 03:15:50 AM: edges-coref-ontonotes_acc: training: 0.901063 validation: 0.885702
09/17 03:15:50 AM: edges-coref-ontonotes_precision: training: 0.903778 validation: 0.887840
09/17 03:15:50 AM: edges-coref-ontonotes_recall: training: 0.903761 validation: 0.886277
09/17 03:15:50 AM: edges-coref-ontonotes_f1: training: 0.903770 validation: 0.887058
09/17 03:15:50 AM: Global learning rate: 0.0001
09/17 03:15:50 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:15:54 AM: Update 20130: task edges-coref-ontonotes, batch 130 (20130): mcc: 0.7652, acc: 0.8791, precision: 0.8823, recall: 0.8830, f1: 0.8826, edges-coref-ontonotes_loss: 0.2665
09/17 03:16:04 AM: Update 20398: task edges-coref-ontonotes, batch 398 (20398): mcc: 0.7775, acc: 0.8856, precision: 0.8887, recall: 0.8888, f1: 0.8888, edges-coref-ontonotes_loss: 0.2481
09/17 03:16:14 AM: Update 20712: task edges-coref-ontonotes, batch 712 (20712): mcc: 0.7931, acc: 0.8936, precision: 0.8964, recall: 0.8966, f1: 0.8965, edges-coref-ontonotes_loss: 0.2268
09/17 03:16:24 AM: Update 20990: task edges-coref-ontonotes, batch 990 (20990): mcc: 0.7983, acc: 0.8964, precision: 0.8990, recall: 0.8993, f1: 0.8992, edges-coref-ontonotes_loss: 0.2170
09/17 03:16:24 AM: ***** Step 21000 / Validation 21 *****
09/17 03:16:24 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:16:24 AM: Validating...
09/17 03:16:30 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:16:30 AM: Best result seen so far for macro.
09/17 03:16:30 AM: Updating LR scheduler:
09/17 03:16:30 AM: 	Best result seen so far for macro_avg: 0.889
09/17 03:16:30 AM: 	# validation passes without improvement: 0
09/17 03:16:30 AM: edges-coref-ontonotes_loss: training: 0.216540 validation: 0.280961
09/17 03:16:30 AM: macro_avg: validation: 0.889102
09/17 03:16:30 AM: micro_avg: validation: 0.000000
09/17 03:16:30 AM: edges-coref-ontonotes_mcc: training: 0.798449 validation: 0.778220
09/17 03:16:30 AM: edges-coref-ontonotes_acc: training: 0.896436 validation: 0.888497
09/17 03:16:30 AM: edges-coref-ontonotes_precision: training: 0.899101 validation: 0.889170
09/17 03:16:30 AM: edges-coref-ontonotes_recall: training: 0.899379 validation: 0.889034
09/17 03:16:30 AM: edges-coref-ontonotes_f1: training: 0.899240 validation: 0.889102
09/17 03:16:30 AM: Global learning rate: 0.0001
09/17 03:16:30 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:16:34 AM: Update 21156: task edges-coref-ontonotes, batch 156 (21156): mcc: 0.8220, acc: 0.9091, precision: 0.9109, recall: 0.9112, f1: 0.9110, edges-coref-ontonotes_loss: 0.1896
09/17 03:16:44 AM: Update 21427: task edges-coref-ontonotes, batch 427 (21427): mcc: 0.7959, acc: 0.8956, precision: 0.8981, recall: 0.8977, f1: 0.8979, edges-coref-ontonotes_loss: 0.2238
09/17 03:16:54 AM: Update 21698: task edges-coref-ontonotes, batch 698 (21698): mcc: 0.7922, acc: 0.8935, precision: 0.8963, recall: 0.8959, f1: 0.8961, edges-coref-ontonotes_loss: 0.2310
09/17 03:17:04 AM: ***** Step 22000 / Validation 22 *****
09/17 03:17:04 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:17:04 AM: Validating...
09/17 03:17:04 AM: Evaluate: task edges-coref-ontonotes, batch 2 (157): mcc: 0.7414, acc: 0.8707, precision: 0.8707, recall: 0.8707, f1: 0.8707, edges-coref-ontonotes_loss: 0.3603
09/17 03:17:10 AM: Updating LR scheduler:
09/17 03:17:10 AM: 	Best result seen so far for macro_avg: 0.889
09/17 03:17:10 AM: 	# validation passes without improvement: 1
09/17 03:17:10 AM: edges-coref-ontonotes_loss: training: 0.220528 validation: 0.278547
09/17 03:17:10 AM: macro_avg: validation: 0.887953
09/17 03:17:10 AM: micro_avg: validation: 0.000000
09/17 03:17:10 AM: edges-coref-ontonotes_mcc: training: 0.798280 validation: 0.775846
09/17 03:17:10 AM: edges-coref-ontonotes_acc: training: 0.896577 validation: 0.887157
09/17 03:17:10 AM: edges-coref-ontonotes_precision: training: 0.899265 validation: 0.887715
09/17 03:17:10 AM: edges-coref-ontonotes_recall: training: 0.898984 validation: 0.888191
09/17 03:17:10 AM: edges-coref-ontonotes_f1: training: 0.899124 validation: 0.887953
09/17 03:17:10 AM: Global learning rate: 0.0001
09/17 03:17:10 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:17:14 AM: Update 22150: task edges-coref-ontonotes, batch 150 (22150): mcc: 0.8372, acc: 0.9167, precision: 0.9186, recall: 0.9186, f1: 0.9186, edges-coref-ontonotes_loss: 0.1671
09/17 03:17:24 AM: Update 22428: task edges-coref-ontonotes, batch 428 (22428): mcc: 0.8170, acc: 0.9064, precision: 0.9085, recall: 0.9084, f1: 0.9085, edges-coref-ontonotes_loss: 0.1935
09/17 03:17:34 AM: Update 22685: task edges-coref-ontonotes, batch 685 (22685): mcc: 0.8067, acc: 0.9010, precision: 0.9034, recall: 0.9033, f1: 0.9033, edges-coref-ontonotes_loss: 0.2070
09/17 03:17:44 AM: Update 22949: task edges-coref-ontonotes, batch 949 (22949): mcc: 0.8001, acc: 0.8977, precision: 0.9000, recall: 0.9001, f1: 0.9000, edges-coref-ontonotes_loss: 0.2179
09/17 03:17:46 AM: ***** Step 23000 / Validation 23 *****
09/17 03:17:46 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:17:46 AM: Validating...
09/17 03:17:51 AM: Updating LR scheduler:
09/17 03:17:51 AM: 	Best result seen so far for macro_avg: 0.889
09/17 03:17:51 AM: 	# validation passes without improvement: 2
09/17 03:17:51 AM: edges-coref-ontonotes_loss: training: 0.218235 validation: 0.282272
09/17 03:17:51 AM: macro_avg: validation: 0.886370
09/17 03:17:51 AM: micro_avg: validation: 0.000000
09/17 03:17:51 AM: edges-coref-ontonotes_mcc: training: 0.800113 validation: 0.772745
09/17 03:17:51 AM: edges-coref-ontonotes_acc: training: 0.897673 validation: 0.885549
09/17 03:17:51 AM: edges-coref-ontonotes_precision: training: 0.899986 validation: 0.886387
09/17 03:17:51 AM: edges-coref-ontonotes_recall: training: 0.900144 validation: 0.886353
09/17 03:17:51 AM: edges-coref-ontonotes_f1: training: 0.900065 validation: 0.886370
09/17 03:17:51 AM: Global learning rate: 0.0001
09/17 03:17:51 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:17:54 AM: Update 23099: task edges-coref-ontonotes, batch 99 (23099): mcc: 0.7967, acc: 0.8955, precision: 0.8983, recall: 0.8984, f1: 0.8984, edges-coref-ontonotes_loss: 0.2240
09/17 03:18:04 AM: Update 23387: task edges-coref-ontonotes, batch 387 (23387): mcc: 0.8177, acc: 0.9063, precision: 0.9089, recall: 0.9088, f1: 0.9088, edges-coref-ontonotes_loss: 0.1916
09/17 03:18:14 AM: Update 23653: task edges-coref-ontonotes, batch 653 (23653): mcc: 0.8140, acc: 0.9047, precision: 0.9070, recall: 0.9069, f1: 0.9070, edges-coref-ontonotes_loss: 0.1939
09/17 03:18:24 AM: Update 23916: task edges-coref-ontonotes, batch 916 (23916): mcc: 0.8126, acc: 0.9041, precision: 0.9063, recall: 0.9063, f1: 0.9063, edges-coref-ontonotes_loss: 0.1987
09/17 03:18:27 AM: ***** Step 24000 / Validation 24 *****
09/17 03:18:27 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:18:27 AM: Validating...
09/17 03:18:33 AM: Updating LR scheduler:
09/17 03:18:33 AM: 	Best result seen so far for macro_avg: 0.889
09/17 03:18:33 AM: 	# validation passes without improvement: 3
09/17 03:18:33 AM: edges-coref-ontonotes_loss: training: 0.203621 validation: 0.281477
09/17 03:18:33 AM: macro_avg: validation: 0.886255
09/17 03:18:33 AM: micro_avg: validation: 0.000000
09/17 03:18:33 AM: edges-coref-ontonotes_mcc: training: 0.809836 validation: 0.772553
09/17 03:18:33 AM: edges-coref-ontonotes_acc: training: 0.902746 validation: 0.885473
09/17 03:18:33 AM: edges-coref-ontonotes_precision: training: 0.904941 validation: 0.886425
09/17 03:18:33 AM: edges-coref-ontonotes_recall: training: 0.904890 validation: 0.886085
09/17 03:18:33 AM: edges-coref-ontonotes_f1: training: 0.904916 validation: 0.886255
09/17 03:18:33 AM: Global learning rate: 0.0001
09/17 03:18:33 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:18:34 AM: Update 24031: task edges-coref-ontonotes, batch 31 (24031): mcc: 0.7940, acc: 0.8942, precision: 0.8969, recall: 0.8971, f1: 0.8970, edges-coref-ontonotes_loss: 0.2343
09/17 03:18:44 AM: Update 24275: task edges-coref-ontonotes, batch 275 (24275): mcc: 0.7845, acc: 0.8897, precision: 0.8923, recall: 0.8923, f1: 0.8923, edges-coref-ontonotes_loss: 0.2433
09/17 03:18:54 AM: Update 24572: task edges-coref-ontonotes, batch 572 (24572): mcc: 0.7951, acc: 0.8950, precision: 0.8974, recall: 0.8977, f1: 0.8976, edges-coref-ontonotes_loss: 0.2225
09/17 03:19:05 AM: Update 24853: task edges-coref-ontonotes, batch 853 (24853): mcc: 0.8038, acc: 0.8995, precision: 0.9018, recall: 0.9020, f1: 0.9019, edges-coref-ontonotes_loss: 0.2091
09/17 03:19:09 AM: ***** Step 25000 / Validation 25 *****
09/17 03:19:09 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:19:09 AM: Validating...
09/17 03:19:15 AM: Evaluate: task edges-coref-ontonotes, batch 144 (157): mcc: 0.7857, acc: 0.8924, precision: 0.8929, recall: 0.8928, f1: 0.8929, edges-coref-ontonotes_loss: 0.2758
09/17 03:19:15 AM: Best result seen so far for edges-coref-ontonotes.
09/17 03:19:15 AM: Best result seen so far for macro.
09/17 03:19:15 AM: Updating LR scheduler:
09/17 03:19:15 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:19:15 AM: 	# validation passes without improvement: 0
09/17 03:19:15 AM: edges-coref-ontonotes_loss: training: 0.206913 validation: 0.279440
09/17 03:19:15 AM: macro_avg: validation: 0.890285
09/17 03:19:15 AM: micro_avg: validation: 0.000000
09/17 03:19:15 AM: edges-coref-ontonotes_mcc: training: 0.805430 validation: 0.780594
09/17 03:19:15 AM: edges-coref-ontonotes_acc: training: 0.900459 validation: 0.889723
09/17 03:19:15 AM: edges-coref-ontonotes_precision: training: 0.902684 validation: 0.890387
09/17 03:19:15 AM: edges-coref-ontonotes_recall: training: 0.902753 validation: 0.890182
09/17 03:19:15 AM: edges-coref-ontonotes_f1: training: 0.902719 validation: 0.890285
09/17 03:19:15 AM: Global learning rate: 0.0001
09/17 03:19:15 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:19:25 AM: Update 25234: task edges-coref-ontonotes, batch 234 (25234): mcc: 0.8063, acc: 0.9011, precision: 0.9030, recall: 0.9034, f1: 0.9032, edges-coref-ontonotes_loss: 0.2127
09/17 03:19:36 AM: Update 25479: task edges-coref-ontonotes, batch 479 (25479): mcc: 0.7929, acc: 0.8943, precision: 0.8965, recall: 0.8964, f1: 0.8965, edges-coref-ontonotes_loss: 0.2314
09/17 03:19:46 AM: Update 25745: task edges-coref-ontonotes, batch 745 (25745): mcc: 0.7941, acc: 0.8948, precision: 0.8970, recall: 0.8970, f1: 0.8970, edges-coref-ontonotes_loss: 0.2286
09/17 03:19:56 AM: Update 25998: task edges-coref-ontonotes, batch 998 (25998): mcc: 0.8016, acc: 0.8987, precision: 0.9008, recall: 0.9008, f1: 0.9008, edges-coref-ontonotes_loss: 0.2136
09/17 03:19:56 AM: ***** Step 26000 / Validation 26 *****
09/17 03:19:56 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:19:56 AM: Validating...
09/17 03:20:03 AM: Updating LR scheduler:
09/17 03:20:03 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:20:03 AM: 	# validation passes without improvement: 1
09/17 03:20:03 AM: edges-coref-ontonotes_loss: training: 0.213445 validation: 0.281232
09/17 03:20:03 AM: macro_avg: validation: 0.890174
09/17 03:20:03 AM: micro_avg: validation: 0.000000
09/17 03:20:03 AM: edges-coref-ontonotes_mcc: training: 0.801656 validation: 0.780326
09/17 03:20:03 AM: edges-coref-ontonotes_acc: training: 0.898709 validation: 0.889723
09/17 03:20:03 AM: edges-coref-ontonotes_precision: training: 0.900828 validation: 0.890088
09/17 03:20:03 AM: edges-coref-ontonotes_recall: training: 0.900828 validation: 0.890259
09/17 03:20:03 AM: edges-coref-ontonotes_f1: training: 0.900828 validation: 0.890174
09/17 03:20:03 AM: Global learning rate: 0.0001
09/17 03:20:03 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:20:06 AM: Update 26082: task edges-coref-ontonotes, batch 82 (26082): mcc: 0.8603, acc: 0.9287, precision: 0.9298, recall: 0.9306, f1: 0.9302, edges-coref-ontonotes_loss: 0.1607
09/17 03:20:16 AM: Update 26307: task edges-coref-ontonotes, batch 307 (26307): mcc: 0.8200, acc: 0.9083, precision: 0.9099, recall: 0.9101, f1: 0.9100, edges-coref-ontonotes_loss: 0.1934
09/17 03:20:26 AM: Update 26518: task edges-coref-ontonotes, batch 518 (26518): mcc: 0.8141, acc: 0.9053, precision: 0.9070, recall: 0.9071, f1: 0.9071, edges-coref-ontonotes_loss: 0.1996
09/17 03:20:36 AM: Update 26781: task edges-coref-ontonotes, batch 781 (26781): mcc: 0.8029, acc: 0.8994, precision: 0.9014, recall: 0.9014, f1: 0.9014, edges-coref-ontonotes_loss: 0.2157
09/17 03:20:46 AM: Update 26986: task edges-coref-ontonotes, batch 986 (26986): mcc: 0.8009, acc: 0.8984, precision: 0.9004, recall: 0.9005, f1: 0.9004, edges-coref-ontonotes_loss: 0.2176
09/17 03:20:46 AM: ***** Step 27000 / Validation 27 *****
09/17 03:20:46 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:20:46 AM: Validating...
09/17 03:20:53 AM: Updating LR scheduler:
09/17 03:20:53 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:20:53 AM: 	# validation passes without improvement: 2
09/17 03:20:53 AM: edges-coref-ontonotes_loss: training: 0.217275 validation: 0.284112
09/17 03:20:53 AM: macro_avg: validation: 0.884693
09/17 03:20:53 AM: micro_avg: validation: 0.000000
09/17 03:20:53 AM: edges-coref-ontonotes_mcc: training: 0.801099 validation: 0.769413
09/17 03:20:53 AM: edges-coref-ontonotes_acc: training: 0.898548 validation: 0.883864
09/17 03:20:53 AM: edges-coref-ontonotes_precision: training: 0.900520 validation: 0.884795
09/17 03:20:53 AM: edges-coref-ontonotes_recall: training: 0.900587 validation: 0.884592
09/17 03:20:53 AM: edges-coref-ontonotes_f1: training: 0.900553 validation: 0.884693
09/17 03:20:53 AM: Global learning rate: 0.0001
09/17 03:20:53 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:20:56 AM: Update 27078: task edges-coref-ontonotes, batch 78 (27078): mcc: 0.8023, acc: 0.8991, precision: 0.9013, recall: 0.9010, f1: 0.9012, edges-coref-ontonotes_loss: 0.2230
09/17 03:21:06 AM: Update 27307: task edges-coref-ontonotes, batch 307 (27307): mcc: 0.8264, acc: 0.9116, precision: 0.9132, recall: 0.9131, f1: 0.9132, edges-coref-ontonotes_loss: 0.1788
09/17 03:21:16 AM: Update 27540: task edges-coref-ontonotes, batch 540 (27540): mcc: 0.8219, acc: 0.9093, precision: 0.9110, recall: 0.9109, f1: 0.9110, edges-coref-ontonotes_loss: 0.1847
09/17 03:21:26 AM: Update 27782: task edges-coref-ontonotes, batch 782 (27782): mcc: 0.8214, acc: 0.9090, precision: 0.9107, recall: 0.9106, f1: 0.9107, edges-coref-ontonotes_loss: 0.1864
09/17 03:21:33 AM: ***** Step 28000 / Validation 28 *****
09/17 03:21:33 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:21:33 AM: Validating...
09/17 03:21:36 AM: Evaluate: task edges-coref-ontonotes, batch 84 (157): mcc: 0.7704, acc: 0.8847, precision: 0.8854, recall: 0.8850, f1: 0.8852, edges-coref-ontonotes_loss: 0.2901
09/17 03:21:39 AM: Updating LR scheduler:
09/17 03:21:39 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:21:39 AM: 	# validation passes without improvement: 3
09/17 03:21:39 AM: edges-coref-ontonotes_loss: training: 0.200170 validation: 0.281568
09/17 03:21:39 AM: macro_avg: validation: 0.885719
09/17 03:21:39 AM: micro_avg: validation: 0.000000
09/17 03:21:39 AM: edges-coref-ontonotes_mcc: training: 0.811635 validation: 0.771481
09/17 03:21:39 AM: edges-coref-ontonotes_acc: training: 0.903981 validation: 0.885243
09/17 03:21:39 AM: edges-coref-ontonotes_precision: training: 0.905841 validation: 0.885888
09/17 03:21:39 AM: edges-coref-ontonotes_recall: training: 0.905789 validation: 0.885549
09/17 03:21:39 AM: edges-coref-ontonotes_f1: training: 0.905815 validation: 0.885719
09/17 03:21:39 AM: Global learning rate: 0.0001
09/17 03:21:39 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:21:46 AM: Update 28172: task edges-coref-ontonotes, batch 172 (28172): mcc: 0.7903, acc: 0.8930, precision: 0.8952, recall: 0.8950, f1: 0.8951, edges-coref-ontonotes_loss: 0.2323
09/17 03:21:56 AM: Update 28419: task edges-coref-ontonotes, batch 419 (28419): mcc: 0.7947, acc: 0.8954, precision: 0.8973, recall: 0.8974, f1: 0.8974, edges-coref-ontonotes_loss: 0.2243
09/17 03:22:06 AM: Update 28760: task edges-coref-ontonotes, batch 760 (28760): mcc: 0.8132, acc: 0.9049, precision: 0.9065, recall: 0.9067, f1: 0.9066, edges-coref-ontonotes_loss: 0.1983
09/17 03:22:15 AM: ***** Step 29000 / Validation 29 *****
09/17 03:22:15 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:22:15 AM: Validating...
09/17 03:22:16 AM: Evaluate: task edges-coref-ontonotes, batch 35 (157): mcc: 0.8127, acc: 0.9059, precision: 0.9066, recall: 0.9061, f1: 0.9063, edges-coref-ontonotes_loss: 0.2422
09/17 03:22:21 AM: Updating LR scheduler:
09/17 03:22:21 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:22:21 AM: 	# validation passes without improvement: 0
09/17 03:22:21 AM: edges-coref-ontonotes_loss: training: 0.197218 validation: 0.281485
09/17 03:22:21 AM: macro_avg: validation: 0.888982
09/17 03:22:21 AM: micro_avg: validation: 0.000000
09/17 03:22:21 AM: edges-coref-ontonotes_mcc: training: 0.813800 validation: 0.777952
09/17 03:22:21 AM: edges-coref-ontonotes_acc: training: 0.905190 validation: 0.888536
09/17 03:22:21 AM: edges-coref-ontonotes_precision: training: 0.906806 validation: 0.888931
09/17 03:22:21 AM: edges-coref-ontonotes_recall: training: 0.907015 validation: 0.889034
09/17 03:22:21 AM: edges-coref-ontonotes_f1: training: 0.906911 validation: 0.888982
09/17 03:22:21 AM: Global learning rate: 5e-05
09/17 03:22:21 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:22:26 AM: Update 29096: task edges-coref-ontonotes, batch 96 (29096): mcc: 0.8098, acc: 0.9032, precision: 0.9049, recall: 0.9049, f1: 0.9049, edges-coref-ontonotes_loss: 0.1963
09/17 03:22:38 AM: Update 29403: task edges-coref-ontonotes, batch 403 (29403): mcc: 0.7914, acc: 0.8938, precision: 0.8958, recall: 0.8956, f1: 0.8957, edges-coref-ontonotes_loss: 0.2329
09/17 03:22:48 AM: Update 29654: task edges-coref-ontonotes, batch 654 (29654): mcc: 0.7954, acc: 0.8959, precision: 0.8978, recall: 0.8976, f1: 0.8977, edges-coref-ontonotes_loss: 0.2270
09/17 03:22:58 AM: Update 29881: task edges-coref-ontonotes, batch 881 (29881): mcc: 0.8025, acc: 0.8995, precision: 0.9013, recall: 0.9012, f1: 0.9012, edges-coref-ontonotes_loss: 0.2147
09/17 03:23:02 AM: ***** Step 30000 / Validation 30 *****
09/17 03:23:02 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:23:02 AM: Validating...
09/17 03:23:08 AM: Evaluate: task edges-coref-ontonotes, batch 146 (157): mcc: 0.7804, acc: 0.8897, precision: 0.8903, recall: 0.8900, f1: 0.8902, edges-coref-ontonotes_loss: 0.2790
09/17 03:23:08 AM: Updating LR scheduler:
09/17 03:23:08 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:23:08 AM: 	# validation passes without improvement: 1
09/17 03:23:08 AM: edges-coref-ontonotes_loss: training: 0.206827 validation: 0.282016
09/17 03:23:08 AM: macro_avg: validation: 0.889340
09/17 03:23:08 AM: micro_avg: validation: 0.000000
09/17 03:23:08 AM: edges-coref-ontonotes_mcc: training: 0.806912 validation: 0.778718
09/17 03:23:08 AM: edges-coref-ontonotes_acc: training: 0.901711 validation: 0.888919
09/17 03:23:08 AM: edges-coref-ontonotes_precision: training: 0.903494 validation: 0.889493
09/17 03:23:08 AM: edges-coref-ontonotes_recall: training: 0.903409 validation: 0.889187
09/17 03:23:08 AM: edges-coref-ontonotes_f1: training: 0.903451 validation: 0.889340
09/17 03:23:08 AM: Global learning rate: 5e-05
09/17 03:23:08 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:23:18 AM: Update 30202: task edges-coref-ontonotes, batch 202 (30202): mcc: 0.8052, acc: 0.9007, precision: 0.9026, recall: 0.9026, f1: 0.9026, edges-coref-ontonotes_loss: 0.2051
09/17 03:23:28 AM: Update 30431: task edges-coref-ontonotes, batch 431 (30431): mcc: 0.8123, acc: 0.9044, precision: 0.9062, recall: 0.9061, f1: 0.9061, edges-coref-ontonotes_loss: 0.1970
09/17 03:23:38 AM: Update 30695: task edges-coref-ontonotes, batch 695 (30695): mcc: 0.8030, acc: 0.8996, precision: 0.9015, recall: 0.9014, f1: 0.9015, edges-coref-ontonotes_loss: 0.2150
09/17 03:23:48 AM: Update 30901: task edges-coref-ontonotes, batch 901 (30901): mcc: 0.8014, acc: 0.8988, precision: 0.9007, recall: 0.9006, f1: 0.9007, edges-coref-ontonotes_loss: 0.2174
09/17 03:23:51 AM: ***** Step 31000 / Validation 31 *****
09/17 03:23:51 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:23:51 AM: Validating...
09/17 03:23:58 AM: Updating LR scheduler:
09/17 03:23:58 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:23:58 AM: 	# validation passes without improvement: 2
09/17 03:23:58 AM: edges-coref-ontonotes_loss: training: 0.216692 validation: 0.282071
09/17 03:23:58 AM: macro_avg: validation: 0.886105
09/17 03:23:58 AM: micro_avg: validation: 0.000000
09/17 03:23:58 AM: edges-coref-ontonotes_mcc: training: 0.801853 validation: 0.772324
09/17 03:23:58 AM: edges-coref-ontonotes_acc: training: 0.899074 validation: 0.885549
09/17 03:23:58 AM: edges-coref-ontonotes_precision: training: 0.900959 validation: 0.886547
09/17 03:23:58 AM: edges-coref-ontonotes_recall: training: 0.900886 validation: 0.885664
09/17 03:23:58 AM: edges-coref-ontonotes_f1: training: 0.900923 validation: 0.886105
09/17 03:23:58 AM: Global learning rate: 5e-05
09/17 03:23:58 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:23:58 AM: Update 31001: task edges-coref-ontonotes, batch 1 (31001): mcc: 0.7352, acc: 0.8617, precision: 0.8633, recall: 0.8735, f1: 0.8684, edges-coref-ontonotes_loss: 0.3549
09/17 03:24:08 AM: Update 31228: task edges-coref-ontonotes, batch 228 (31228): mcc: 0.8397, acc: 0.9184, precision: 0.9200, recall: 0.9197, f1: 0.9199, edges-coref-ontonotes_loss: 0.1595
09/17 03:24:18 AM: Update 31454: task edges-coref-ontonotes, batch 454 (31454): mcc: 0.8266, acc: 0.9119, precision: 0.9133, recall: 0.9133, f1: 0.9133, edges-coref-ontonotes_loss: 0.1739
09/17 03:24:29 AM: Update 31706: task edges-coref-ontonotes, batch 706 (31706): mcc: 0.8255, acc: 0.9114, precision: 0.9127, recall: 0.9128, f1: 0.9128, edges-coref-ontonotes_loss: 0.1792
09/17 03:24:39 AM: Update 31967: task edges-coref-ontonotes, batch 967 (31967): mcc: 0.8135, acc: 0.9052, precision: 0.9067, recall: 0.9068, f1: 0.9068, edges-coref-ontonotes_loss: 0.1965
09/17 03:24:40 AM: ***** Step 32000 / Validation 32 *****
09/17 03:24:40 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:24:40 AM: Validating...
09/17 03:24:47 AM: Updating LR scheduler:
09/17 03:24:47 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:24:47 AM: 	# validation passes without improvement: 3
09/17 03:24:47 AM: edges-coref-ontonotes_loss: training: 0.197245 validation: 0.279104
09/17 03:24:47 AM: macro_avg: validation: 0.887208
09/17 03:24:47 AM: micro_avg: validation: 0.000000
09/17 03:24:47 AM: edges-coref-ontonotes_mcc: training: 0.813406 validation: 0.774429
09/17 03:24:47 AM: edges-coref-ontonotes_acc: training: 0.905126 validation: 0.886966
09/17 03:24:47 AM: edges-coref-ontonotes_precision: training: 0.906672 validation: 0.887259
09/17 03:24:47 AM: edges-coref-ontonotes_recall: training: 0.906741 validation: 0.887157
09/17 03:24:47 AM: edges-coref-ontonotes_f1: training: 0.906707 validation: 0.887208
09/17 03:24:47 AM: Global learning rate: 5e-05
09/17 03:24:47 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:24:50 AM: Update 32019: task edges-coref-ontonotes, batch 19 (32019): mcc: 0.7686, acc: 0.8825, precision: 0.8846, recall: 0.8839, f1: 0.8842, edges-coref-ontonotes_loss: 0.2541
09/17 03:25:00 AM: Update 32288: task edges-coref-ontonotes, batch 288 (32288): mcc: 0.8008, acc: 0.8984, precision: 0.9003, recall: 0.9005, f1: 0.9004, edges-coref-ontonotes_loss: 0.2171
09/17 03:25:10 AM: Update 32531: task edges-coref-ontonotes, batch 531 (32531): mcc: 0.8125, acc: 0.9045, precision: 0.9062, recall: 0.9063, f1: 0.9063, edges-coref-ontonotes_loss: 0.1962
09/17 03:25:20 AM: Update 32729: task edges-coref-ontonotes, batch 729 (32729): mcc: 0.8148, acc: 0.9057, precision: 0.9073, recall: 0.9075, f1: 0.9074, edges-coref-ontonotes_loss: 0.1928
09/17 03:25:30 AM: ***** Step 33000 / Validation 33 *****
09/17 03:25:30 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:25:30 AM: Validating...
09/17 03:25:30 AM: Evaluate: task edges-coref-ontonotes, batch 11 (157): mcc: 0.8204, acc: 0.9102, precision: 0.9102, recall: 0.9102, f1: 0.9102, edges-coref-ontonotes_loss: 0.2472
09/17 03:25:37 AM: Updating LR scheduler:
09/17 03:25:37 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:25:37 AM: 	# validation passes without improvement: 0
09/17 03:25:37 AM: edges-coref-ontonotes_loss: training: 0.190063 validation: 0.280780
09/17 03:25:37 AM: macro_avg: validation: 0.888974
09/17 03:25:37 AM: micro_avg: validation: 0.000000
09/17 03:25:37 AM: edges-coref-ontonotes_mcc: training: 0.817757 validation: 0.777952
09/17 03:25:37 AM: edges-coref-ontonotes_acc: training: 0.907315 validation: 0.888612
09/17 03:25:37 AM: edges-coref-ontonotes_precision: training: 0.908809 validation: 0.888991
09/17 03:25:37 AM: edges-coref-ontonotes_recall: training: 0.908963 validation: 0.888957
09/17 03:25:37 AM: edges-coref-ontonotes_f1: training: 0.908886 validation: 0.888974
09/17 03:25:37 AM: Global learning rate: 2.5e-05
09/17 03:25:37 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:25:40 AM: Update 33048: task edges-coref-ontonotes, batch 48 (33048): mcc: 0.7868, acc: 0.8920, precision: 0.8938, recall: 0.8929, f1: 0.8933, edges-coref-ontonotes_loss: 0.2356
09/17 03:25:52 AM: Update 33327: task edges-coref-ontonotes, batch 327 (33327): mcc: 0.7875, acc: 0.8922, precision: 0.8938, recall: 0.8936, f1: 0.8937, edges-coref-ontonotes_loss: 0.2381
09/17 03:26:02 AM: Update 33638: task edges-coref-ontonotes, batch 638 (33638): mcc: 0.7954, acc: 0.8961, precision: 0.8978, recall: 0.8975, f1: 0.8977, edges-coref-ontonotes_loss: 0.2281
09/17 03:26:12 AM: Update 33909: task edges-coref-ontonotes, batch 909 (33909): mcc: 0.8078, acc: 0.9024, precision: 0.9040, recall: 0.9038, f1: 0.9039, edges-coref-ontonotes_loss: 0.2072
09/17 03:26:15 AM: ***** Step 34000 / Validation 34 *****
09/17 03:26:15 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:26:15 AM: Validating...
09/17 03:26:21 AM: Updating LR scheduler:
09/17 03:26:21 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:26:21 AM: 	# validation passes without improvement: 1
09/17 03:26:21 AM: edges-coref-ontonotes_loss: training: 0.206603 validation: 0.275645
09/17 03:26:21 AM: macro_avg: validation: 0.889548
09/17 03:26:21 AM: micro_avg: validation: 0.000000
09/17 03:26:21 AM: edges-coref-ontonotes_mcc: training: 0.807063 validation: 0.779063
09/17 03:26:21 AM: edges-coref-ontonotes_acc: training: 0.902065 validation: 0.889110
09/17 03:26:21 AM: edges-coref-ontonotes_precision: training: 0.903606 validation: 0.889412
09/17 03:26:21 AM: edges-coref-ontonotes_recall: training: 0.903440 validation: 0.889684
09/17 03:26:21 AM: edges-coref-ontonotes_f1: training: 0.903523 validation: 0.889548
09/17 03:26:21 AM: Global learning rate: 2.5e-05
09/17 03:26:21 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:26:23 AM: Update 34009: task edges-coref-ontonotes, batch 9 (34009): mcc: 0.7689, acc: 0.8828, precision: 0.8843, recall: 0.8847, f1: 0.8845, edges-coref-ontonotes_loss: 0.2329
09/17 03:26:34 AM: Update 34322: task edges-coref-ontonotes, batch 322 (34322): mcc: 0.8241, acc: 0.9106, precision: 0.9121, recall: 0.9120, f1: 0.9120, edges-coref-ontonotes_loss: 0.1849
09/17 03:26:44 AM: Update 34634: task edges-coref-ontonotes, batch 634 (34634): mcc: 0.8055, acc: 0.9010, precision: 0.9028, recall: 0.9026, f1: 0.9027, edges-coref-ontonotes_loss: 0.2114
09/17 03:26:54 AM: Update 34887: task edges-coref-ontonotes, batch 887 (34887): mcc: 0.8043, acc: 0.9005, precision: 0.9022, recall: 0.9020, f1: 0.9021, edges-coref-ontonotes_loss: 0.2126
09/17 03:27:00 AM: ***** Step 35000 / Validation 35 *****
09/17 03:27:00 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 03:27:00 AM: Validating...
09/17 03:27:04 AM: Evaluate: task edges-coref-ontonotes, batch 129 (157): mcc: 0.7741, acc: 0.8866, precision: 0.8872, recall: 0.8869, f1: 0.8870, edges-coref-ontonotes_loss: 0.2899
09/17 03:27:05 AM: Updating LR scheduler:
09/17 03:27:05 AM: 	Best result seen so far for macro_avg: 0.890
09/17 03:27:05 AM: 	# validation passes without improvement: 2
09/17 03:27:05 AM: Ran out of early stopping patience. Stopping training.
09/17 03:27:05 AM: edges-coref-ontonotes_loss: training: 0.210364 validation: 0.280072
09/17 03:27:05 AM: macro_avg: validation: 0.888561
09/17 03:27:05 AM: micro_avg: validation: 0.000000
09/17 03:27:05 AM: edges-coref-ontonotes_mcc: training: 0.804919 validation: 0.777187
09/17 03:27:05 AM: edges-coref-ontonotes_acc: training: 0.900773 validation: 0.888115
09/17 03:27:05 AM: edges-coref-ontonotes_precision: training: 0.902514 validation: 0.888817
09/17 03:27:05 AM: edges-coref-ontonotes_recall: training: 0.902392 validation: 0.888306
09/17 03:27:05 AM: edges-coref-ontonotes_f1: training: 0.902453 validation: 0.888561
09/17 03:27:05 AM: Global learning rate: 2.5e-05
09/17 03:27:05 AM: Saving checkpoints to: ./experiments/coref-ontonotes-rte-only/run
09/17 03:27:05 AM: Stopped training after 35 validation checks
09/17 03:27:05 AM: Trained edges-coref-ontonotes for 35000 batches or 26.799 epochs
09/17 03:27:05 AM: ***** VALIDATION RESULTS *****
09/17 03:27:05 AM: edges-coref-ontonotes_f1 (for best val pass 25): edges-coref-ontonotes_loss: 0.27944, macro_avg: 0.89028, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.78059, edges-coref-ontonotes_acc: 0.88972, edges-coref-ontonotes_precision: 0.89039, edges-coref-ontonotes_recall: 0.89018, edges-coref-ontonotes_f1: 0.89028
09/17 03:27:05 AM: micro_avg (for best val pass 1): edges-coref-ontonotes_loss: 0.37606, macro_avg: 0.85470, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.71031, edges-coref-ontonotes_acc: 0.85120, edges-coref-ontonotes_precision: 0.85736, edges-coref-ontonotes_recall: 0.85204, edges-coref-ontonotes_f1: 0.85470
09/17 03:27:05 AM: macro_avg (for best val pass 25): edges-coref-ontonotes_loss: 0.27944, macro_avg: 0.89028, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.78059, edges-coref-ontonotes_acc: 0.88972, edges-coref-ontonotes_precision: 0.89039, edges-coref-ontonotes_recall: 0.89018, edges-coref-ontonotes_f1: 0.89028
09/17 03:27:05 AM: Evaluating...
09/17 03:27:05 AM: Loaded model state from ./experiments/coref-ontonotes-rte-only/run/edges-coref-ontonotes/model_state_target_train_val_25.best.th
09/17 03:27:05 AM: Evaluating on: edges-coref-ontonotes, split: val
09/17 03:27:12 AM: Task 'edges-coref-ontonotes': sorting predictions by 'idx'
09/17 03:27:12 AM: Finished evaluating on: edges-coref-ontonotes
09/17 03:27:12 AM: Task 'edges-coref-ontonotes': joining predictions with input split 'val'
09/17 03:27:13 AM: Task 'edges-coref-ontonotes': Wrote predictions to ./experiments/coref-ontonotes-rte-only/run
09/17 03:27:13 AM: Wrote all preds for split 'val' to ./experiments/coref-ontonotes-rte-only/run
09/17 03:27:13 AM: Evaluating on: edges-coref-ontonotes, split: test
09/17 03:27:20 AM: Task 'edges-coref-ontonotes': sorting predictions by 'idx'
09/17 03:27:20 AM: Finished evaluating on: edges-coref-ontonotes
09/17 03:27:20 AM: Task 'edges-coref-ontonotes': joining predictions with input split 'test'
09/17 03:27:20 AM: Task 'edges-coref-ontonotes': Wrote predictions to ./experiments/coref-ontonotes-rte-only/run
09/17 03:27:20 AM: Wrote all preds for split 'test' to ./experiments/coref-ontonotes-rte-only/run
09/17 03:27:20 AM: Writing results for split 'val' to ./experiments/coref-ontonotes-rte-only/results.tsv
09/17 03:27:20 AM: micro_avg: 0.000, macro_avg: 0.891, edges-coref-ontonotes_mcc: 0.781, edges-coref-ontonotes_acc: 0.890, edges-coref-ontonotes_precision: 0.891, edges-coref-ontonotes_recall: 0.890, edges-coref-ontonotes_f1: 0.891
09/17 03:27:20 AM: Done!
09/17 03:27:20 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
