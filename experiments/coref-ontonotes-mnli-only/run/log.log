09/17 05:30:32 AM: Git branch: master
09/17 05:30:32 AM: Git SHA: 4086cd8f278243816795989a620c769378a6ab56
09/17 05:30:32 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/coref-ontonotes-mnli-only/",
  "exp_name": "experiments/coref-ontonotes-mnli-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/coref-ontonotes-mnli-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/mnli",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/coref-ontonotes-mnli-only__run",
  "run_dir": "./experiments/coref-ontonotes-mnli-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-coref-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/17 05:30:32 AM: Saved config to ./experiments/coref-ontonotes-mnli-only/run/params.conf
09/17 05:30:32 AM: Using random seed 1234
09/17 05:30:36 AM: Using GPU 0
09/17 05:30:36 AM: Loading tasks...
09/17 05:30:36 AM: Writing pre-preprocessed tasks to ./experiments/coref-ontonotes-mnli-only/
09/17 05:30:36 AM: 	Creating task edges-coref-ontonotes from scratch.
09/17 05:30:38 AM: Read=41777, Skip=74035, Total=115812 from ./probing_data/edges/ontonotes/coref/train.json.retokenized.bert-base-uncased
09/17 05:30:38 AM: Read=5044, Skip=10636, Total=15680 from ./probing_data/edges/ontonotes/coref/development.json.retokenized.bert-base-uncased
09/17 05:30:38 AM: Read=5188, Skip=7029, Total=12217 from ./probing_data/edges/ontonotes/coref/test.json.retokenized.bert-base-uncased
09/17 05:30:39 AM: 	Task 'edges-coref-ontonotes': |train|=41777 |val|=5044 |test|=5188
09/17 05:30:39 AM: 	Finished loading tasks: edges-coref-ontonotes.
09/17 05:30:39 AM: 	Building vocab from scratch.
09/17 05:30:39 AM: 	Counting units for task edges-coref-ontonotes.
09/17 05:30:40 AM: 	Task 'edges-coref-ontonotes': adding vocab namespace 'edges-coref-ontonotes_labels'
09/17 05:30:41 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 05:30:41 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/17 05:30:41 AM: 	Saved vocab to ./experiments/coref-ontonotes-mnli-only/vocab
09/17 05:30:41 AM: Loading token dictionary from ./experiments/coref-ontonotes-mnli-only/vocab.
09/17 05:30:41 AM: 	Loaded vocab from ./experiments/coref-ontonotes-mnli-only/vocab
09/17 05:30:41 AM: 	Vocab namespace tokens: size 20434
09/17 05:30:41 AM: 	Vocab namespace bert_uncased: size 30524
09/17 05:30:41 AM: 	Vocab namespace edges-coref-ontonotes_labels: size 2
09/17 05:30:41 AM: 	Vocab namespace chars: size 72
09/17 05:30:41 AM: 	Finished building vocab.
09/17 05:30:41 AM: 	Task edges-coref-ontonotes (train): Indexing from scratch.
09/17 05:30:52 AM: 	Task edges-coref-ontonotes (train): Saved 41777 instances to ./experiments/coref-ontonotes-mnli-only/preproc/edges-coref-ontonotes__train_data
09/17 05:30:52 AM: 	Task edges-coref-ontonotes (val): Indexing from scratch.
09/17 05:30:53 AM: 	Task edges-coref-ontonotes (val): Saved 5044 instances to ./experiments/coref-ontonotes-mnli-only/preproc/edges-coref-ontonotes__val_data
09/17 05:30:53 AM: 	Task edges-coref-ontonotes (test): Indexing from scratch.
09/17 05:30:55 AM: 	Task edges-coref-ontonotes (test): Saved 5188 instances to ./experiments/coref-ontonotes-mnli-only/preproc/edges-coref-ontonotes__test_data
09/17 05:30:55 AM: 	Finished indexing tasks
09/17 05:30:55 AM: 	Creating trimmed target-only version of edges-coref-ontonotes train.
09/17 05:30:55 AM: 	  Training on 
09/17 05:30:55 AM: 	  Evaluating on edges-coref-ontonotes
09/17 05:30:55 AM: 	Finished loading tasks in 19.098s
09/17 05:30:55 AM: 	 Tasks: ['edges-coref-ontonotes']
09/17 05:30:55 AM: Building model...
09/17 05:30:55 AM: Using BERT model (bert-base-uncased).
09/17 05:30:55 AM: LOADING A FUNETUNED MODEL from: 
09/17 05:30:55 AM: models/mnli
09/17 05:30:55 AM: loading configuration file models/mnli/config.json
09/17 05:30:55 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 3,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/17 05:30:55 AM: loading weights file models/mnli/pytorch_model.bin
09/17 05:30:59 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpusajwcpz
09/17 05:31:01 AM: copying /tmp/tmpusajwcpz to cache at ./experiments/coref-ontonotes-mnli-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 05:31:01 AM: creating metadata file for ./experiments/coref-ontonotes-mnli-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 05:31:01 AM: removing temp file /tmp/tmpusajwcpz
09/17 05:31:01 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/coref-ontonotes-mnli-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/17 05:31:01 AM: Initializing parameters
09/17 05:31:01 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/17 05:31:01 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/17 05:31:01 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/17 05:31:01 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/17 05:31:01 AM:    _text_field_embedder.model.pooler.dense.bias
09/17 05:31:01 AM:    _text_field_embedder.model.pooler.dense.weight
09/17 05:31:01 AM: 	Task 'edges-coref-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-coref-ontonotes"
}
09/17 05:31:06 AM: Model specification:
09/17 05:31:06 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-coref-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=2, bias=True)
      )
    )
  )
)
09/17 05:31:06 AM: Model parameters:
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/17 05:31:06 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 512 with torch.Size([2, 256])
09/17 05:31:06 AM: 	edges-coref-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 2 with torch.Size([2])
09/17 05:31:06 AM: Total number of parameters: 110139394 (1.10139e+08)
09/17 05:31:06 AM: Number of trainable parameters: 657154 (657154)
09/17 05:31:06 AM: Finished building model in 11.300s
09/17 05:31:06 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-coref-ontonotes 

09/17 05:31:12 AM: patience = 9
09/17 05:31:12 AM: val_interval = 1000
09/17 05:31:12 AM: max_vals = 250
09/17 05:31:12 AM: cuda_device = 0
09/17 05:31:12 AM: grad_norm = 5.0
09/17 05:31:12 AM: grad_clipping = None
09/17 05:31:12 AM: lr_decay = 0.99
09/17 05:31:12 AM: min_lr = 1e-06
09/17 05:31:12 AM: keep_all_checkpoints = 0
09/17 05:31:12 AM: val_data_limit = 5000
09/17 05:31:12 AM: max_epochs = -1
09/17 05:31:12 AM: dec_val_scale = 250
09/17 05:31:12 AM: training_data_fraction = 1
09/17 05:31:12 AM: type = adam
09/17 05:31:12 AM: parameter_groups = None
09/17 05:31:12 AM: Number of trainable parameters: 657154
09/17 05:31:12 AM: infer_type_and_cast = True
09/17 05:31:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 05:31:12 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 05:31:12 AM: lr = 0.0001
09/17 05:31:12 AM: amsgrad = True
09/17 05:31:12 AM: type = reduce_on_plateau
09/17 05:31:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 05:31:12 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 05:31:12 AM: mode = max
09/17 05:31:12 AM: factor = 0.5
09/17 05:31:12 AM: patience = 3
09/17 05:31:12 AM: threshold = 0.0001
09/17 05:31:12 AM: threshold_mode = abs
09/17 05:31:12 AM: verbose = True
09/17 05:31:12 AM: type = adam
09/17 05:31:12 AM: parameter_groups = None
09/17 05:31:12 AM: Number of trainable parameters: 657154
09/17 05:31:12 AM: infer_type_and_cast = True
09/17 05:31:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 05:31:12 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 05:31:12 AM: lr = 0.0001
09/17 05:31:12 AM: amsgrad = True
09/17 05:31:12 AM: type = reduce_on_plateau
09/17 05:31:12 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/17 05:31:12 AM: CURRENTLY DEFINED PARAMETERS: 
09/17 05:31:12 AM: mode = max
09/17 05:31:12 AM: factor = 0.5
09/17 05:31:12 AM: patience = 3
09/17 05:31:12 AM: threshold = 0.0001
09/17 05:31:12 AM: threshold_mode = abs
09/17 05:31:12 AM: verbose = True
09/17 05:31:12 AM: Starting training without restoring from a checkpoint.
09/17 05:31:12 AM: Training examples per task, before any subsampling: {'edges-coref-ontonotes': 41777}
09/17 05:31:12 AM: Beginning training with stopping criteria based on metric: edges-coref-ontonotes_f1
09/17 05:31:22 AM: Update 252: task edges-coref-ontonotes, batch 252 (252): mcc: 0.5877, acc: 0.7558, precision: 0.7923, recall: 0.7965, f1: 0.7944, edges-coref-ontonotes_loss: 0.4310
09/17 05:31:32 AM: Update 482: task edges-coref-ontonotes, batch 482 (482): mcc: 0.6034, acc: 0.7691, precision: 0.8013, recall: 0.8024, f1: 0.8018, edges-coref-ontonotes_loss: 0.4223
09/17 05:31:42 AM: Update 720: task edges-coref-ontonotes, batch 720 (720): mcc: 0.6191, acc: 0.7803, precision: 0.8096, recall: 0.8095, f1: 0.8096, edges-coref-ontonotes_loss: 0.4131
09/17 05:31:52 AM: Update 959: task edges-coref-ontonotes, batch 959 (959): mcc: 0.6398, acc: 0.7938, precision: 0.8202, recall: 0.8194, f1: 0.8198, edges-coref-ontonotes_loss: 0.3988
09/17 05:31:53 AM: ***** Step 1000 / Validation 1 *****
09/17 05:31:53 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:31:53 AM: Validating...
09/17 05:31:59 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:31:59 AM: Best result seen so far for micro.
09/17 05:31:59 AM: Best result seen so far for macro.
09/17 05:31:59 AM: Updating LR scheduler:
09/17 05:31:59 AM: 	Best result seen so far for macro_avg: 0.854
09/17 05:31:59 AM: 	# validation passes without improvement: 0
09/17 05:31:59 AM: edges-coref-ontonotes_loss: training: 0.396370 validation: 0.377808
09/17 05:31:59 AM: macro_avg: validation: 0.854469
09/17 05:31:59 AM: micro_avg: validation: 0.000000
09/17 05:31:59 AM: edges-coref-ontonotes_mcc: training: 0.642722 validation: 0.710044
09/17 05:31:59 AM: edges-coref-ontonotes_acc: training: 0.795613 validation: 0.850896
09/17 05:31:59 AM: edges-coref-ontonotes_precision: training: 0.821730 validation: 0.857683
09/17 05:31:59 AM: edges-coref-ontonotes_recall: training: 0.820786 validation: 0.851279
09/17 05:31:59 AM: edges-coref-ontonotes_f1: training: 0.821258 validation: 0.854469
09/17 05:31:59 AM: Global learning rate: 0.0001
09/17 05:31:59 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:32:02 AM: Update 1043: task edges-coref-ontonotes, batch 43 (1043): mcc: 0.7457, acc: 0.8561, precision: 0.8737, recall: 0.8717, f1: 0.8727, edges-coref-ontonotes_loss: 0.3180
09/17 05:32:13 AM: Update 1309: task edges-coref-ontonotes, batch 309 (1309): mcc: 0.7398, acc: 0.8560, precision: 0.8711, recall: 0.8683, f1: 0.8697, edges-coref-ontonotes_loss: 0.3075
09/17 05:32:23 AM: Update 1609: task edges-coref-ontonotes, batch 609 (1609): mcc: 0.7472, acc: 0.8616, precision: 0.8745, recall: 0.8725, f1: 0.8735, edges-coref-ontonotes_loss: 0.3040
09/17 05:32:33 AM: Update 1819: task edges-coref-ontonotes, batch 819 (1819): mcc: 0.7349, acc: 0.8543, precision: 0.8681, recall: 0.8666, f1: 0.8673, edges-coref-ontonotes_loss: 0.3160
09/17 05:32:42 AM: ***** Step 2000 / Validation 2 *****
09/17 05:32:42 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:32:42 AM: Validating...
09/17 05:32:43 AM: Evaluate: task edges-coref-ontonotes, batch 27 (157): mcc: 0.7732, acc: 0.8850, precision: 0.8878, recall: 0.8850, f1: 0.8864, edges-coref-ontonotes_loss: 0.3213
09/17 05:32:48 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:32:48 AM: Best result seen so far for macro.
09/17 05:32:48 AM: Updating LR scheduler:
09/17 05:32:48 AM: 	Best result seen so far for macro_avg: 0.868
09/17 05:32:48 AM: 	# validation passes without improvement: 0
09/17 05:32:48 AM: edges-coref-ontonotes_loss: training: 0.318679 validation: 0.347128
09/17 05:32:48 AM: macro_avg: validation: 0.868242
09/17 05:32:48 AM: micro_avg: validation: 0.000000
09/17 05:32:48 AM: edges-coref-ontonotes_mcc: training: 0.731392 validation: 0.736753
09/17 05:32:48 AM: edges-coref-ontonotes_acc: training: 0.852483 validation: 0.866442
09/17 05:32:48 AM: edges-coref-ontonotes_precision: training: 0.866249 validation: 0.869125
09/17 05:32:48 AM: edges-coref-ontonotes_recall: training: 0.864941 validation: 0.867361
09/17 05:32:48 AM: edges-coref-ontonotes_f1: training: 0.865594 validation: 0.868242
09/17 05:32:48 AM: Global learning rate: 0.0001
09/17 05:32:48 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:32:53 AM: Update 2129: task edges-coref-ontonotes, batch 129 (2129): mcc: 0.7523, acc: 0.8663, precision: 0.8770, recall: 0.8751, f1: 0.8760, edges-coref-ontonotes_loss: 0.2967
09/17 05:33:03 AM: Update 2380: task edges-coref-ontonotes, batch 380 (2380): mcc: 0.7680, acc: 0.8749, precision: 0.8845, recall: 0.8834, f1: 0.8839, edges-coref-ontonotes_loss: 0.2705
09/17 05:33:14 AM: Update 2617: task edges-coref-ontonotes, batch 617 (2617): mcc: 0.7712, acc: 0.8767, precision: 0.8857, recall: 0.8854, f1: 0.8856, edges-coref-ontonotes_loss: 0.2644
09/17 05:33:24 AM: Update 2924: task edges-coref-ontonotes, batch 924 (2924): mcc: 0.7723, acc: 0.8774, precision: 0.8862, recall: 0.8861, f1: 0.8861, edges-coref-ontonotes_loss: 0.2643
09/17 05:33:28 AM: ***** Step 3000 / Validation 3 *****
09/17 05:33:28 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:33:28 AM: Validating...
09/17 05:33:34 AM: Evaluate: task edges-coref-ontonotes, batch 139 (157): mcc: 0.7524, acc: 0.8735, precision: 0.8768, recall: 0.8754, f1: 0.8761, edges-coref-ontonotes_loss: 0.3148
09/17 05:33:34 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:33:34 AM: Best result seen so far for macro.
09/17 05:33:34 AM: Updating LR scheduler:
09/17 05:33:34 AM: 	Best result seen so far for macro_avg: 0.874
09/17 05:33:34 AM: 	# validation passes without improvement: 0
09/17 05:33:34 AM: edges-coref-ontonotes_loss: training: 0.268757 validation: 0.318244
09/17 05:33:34 AM: macro_avg: validation: 0.874246
09/17 05:33:34 AM: micro_avg: validation: 0.000000
09/17 05:33:34 AM: edges-coref-ontonotes_mcc: training: 0.768190 validation: 0.748699
09/17 05:33:34 AM: edges-coref-ontonotes_acc: training: 0.875056 validation: 0.871650
09/17 05:33:34 AM: edges-coref-ontonotes_precision: training: 0.884044 validation: 0.874966
09/17 05:33:34 AM: edges-coref-ontonotes_recall: training: 0.884161 validation: 0.873526
09/17 05:33:34 AM: edges-coref-ontonotes_f1: training: 0.884103 validation: 0.874246
09/17 05:33:34 AM: Global learning rate: 0.0001
09/17 05:33:34 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:33:45 AM: Update 3243: task edges-coref-ontonotes, batch 243 (3243): mcc: 0.7196, acc: 0.8470, precision: 0.8593, recall: 0.8604, f1: 0.8599, edges-coref-ontonotes_loss: 0.3269
09/17 05:33:55 AM: Update 3540: task edges-coref-ontonotes, batch 540 (3540): mcc: 0.7426, acc: 0.8611, precision: 0.8713, recall: 0.8713, f1: 0.8713, edges-coref-ontonotes_loss: 0.2982
09/17 05:34:05 AM: Update 3797: task edges-coref-ontonotes, batch 797 (3797): mcc: 0.7617, acc: 0.8716, precision: 0.8810, recall: 0.8807, f1: 0.8808, edges-coref-ontonotes_loss: 0.2707
09/17 05:34:14 AM: ***** Step 4000 / Validation 4 *****
09/17 05:34:14 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:34:14 AM: Validating...
09/17 05:34:15 AM: Evaluate: task edges-coref-ontonotes, batch 38 (157): mcc: 0.7711, acc: 0.8839, precision: 0.8858, recall: 0.8852, f1: 0.8855, edges-coref-ontonotes_loss: 0.2857
09/17 05:34:20 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:34:20 AM: Best result seen so far for macro.
09/17 05:34:20 AM: Updating LR scheduler:
09/17 05:34:20 AM: 	Best result seen so far for macro_avg: 0.876
09/17 05:34:20 AM: 	# validation passes without improvement: 0
09/17 05:34:20 AM: edges-coref-ontonotes_loss: training: 0.266994 validation: 0.314637
09/17 05:34:20 AM: macro_avg: validation: 0.876106
09/17 05:34:20 AM: micro_avg: validation: 0.000000
09/17 05:34:20 AM: edges-coref-ontonotes_mcc: training: 0.763515 validation: 0.752336
09/17 05:34:20 AM: edges-coref-ontonotes_acc: training: 0.872825 validation: 0.874521
09/17 05:34:20 AM: edges-coref-ontonotes_precision: training: 0.881896 validation: 0.876543
09/17 05:34:20 AM: edges-coref-ontonotes_recall: training: 0.881576 validation: 0.875670
09/17 05:34:20 AM: edges-coref-ontonotes_f1: training: 0.881736 validation: 0.876106
09/17 05:34:20 AM: Global learning rate: 0.0001
09/17 05:34:20 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:34:25 AM: Update 4181: task edges-coref-ontonotes, batch 181 (4181): mcc: 0.7967, acc: 0.8919, precision: 0.8977, recall: 0.8992, f1: 0.8985, edges-coref-ontonotes_loss: 0.2388
09/17 05:34:35 AM: Update 4414: task edges-coref-ontonotes, batch 414 (4414): mcc: 0.7596, acc: 0.8716, precision: 0.8794, recall: 0.8803, f1: 0.8799, edges-coref-ontonotes_loss: 0.2779
09/17 05:34:45 AM: Update 4637: task edges-coref-ontonotes, batch 637 (4637): mcc: 0.7535, acc: 0.8682, precision: 0.8766, recall: 0.8769, f1: 0.8768, edges-coref-ontonotes_loss: 0.2834
09/17 05:34:55 AM: Update 4867: task edges-coref-ontonotes, batch 867 (4867): mcc: 0.7580, acc: 0.8709, precision: 0.8791, recall: 0.8789, f1: 0.8790, edges-coref-ontonotes_loss: 0.2769
09/17 05:34:59 AM: ***** Step 5000 / Validation 5 *****
09/17 05:34:59 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:34:59 AM: Validating...
09/17 05:35:05 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:35:05 AM: Best result seen so far for macro.
09/17 05:35:05 AM: Updating LR scheduler:
09/17 05:35:05 AM: 	Best result seen so far for macro_avg: 0.877
09/17 05:35:05 AM: 	# validation passes without improvement: 0
09/17 05:35:05 AM: edges-coref-ontonotes_loss: training: 0.265590 validation: 0.303031
09/17 05:35:05 AM: macro_avg: validation: 0.877234
09/17 05:35:05 AM: micro_avg: validation: 0.000000
09/17 05:35:05 AM: edges-coref-ontonotes_mcc: training: 0.765664 validation: 0.754327
09/17 05:35:05 AM: edges-coref-ontonotes_acc: training: 0.874931 validation: 0.875402
09/17 05:35:05 AM: edges-coref-ontonotes_precision: training: 0.882871 validation: 0.876731
09/17 05:35:05 AM: edges-coref-ontonotes_recall: training: 0.882782 validation: 0.877738
09/17 05:35:05 AM: edges-coref-ontonotes_f1: training: 0.882826 validation: 0.877234
09/17 05:35:05 AM: Global learning rate: 0.0001
09/17 05:35:05 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:35:05 AM: Update 5010: task edges-coref-ontonotes, batch 10 (5010): mcc: 0.8256, acc: 0.9081, precision: 0.9120, recall: 0.9137, f1: 0.9129, edges-coref-ontonotes_loss: 0.2020
09/17 05:35:15 AM: Update 5252: task edges-coref-ontonotes, batch 252 (5252): mcc: 0.7876, acc: 0.8874, precision: 0.8940, recall: 0.8935, f1: 0.8938, edges-coref-ontonotes_loss: 0.2285
09/17 05:35:27 AM: Update 5546: task edges-coref-ontonotes, batch 546 (5546): mcc: 0.7885, acc: 0.8882, precision: 0.8944, recall: 0.8941, f1: 0.8943, edges-coref-ontonotes_loss: 0.2329
09/17 05:35:37 AM: Update 5833: task edges-coref-ontonotes, batch 833 (5833): mcc: 0.7698, acc: 0.8778, precision: 0.8849, recall: 0.8848, f1: 0.8849, edges-coref-ontonotes_loss: 0.2568
09/17 05:35:45 AM: ***** Step 6000 / Validation 6 *****
09/17 05:35:45 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:35:45 AM: Validating...
09/17 05:35:47 AM: Evaluate: task edges-coref-ontonotes, batch 36 (157): mcc: 0.7785, acc: 0.8881, precision: 0.8897, recall: 0.8886, f1: 0.8892, edges-coref-ontonotes_loss: 0.2863
09/17 05:35:52 AM: Updating LR scheduler:
09/17 05:35:52 AM: 	Best result seen so far for macro_avg: 0.877
09/17 05:35:52 AM: 	# validation passes without improvement: 1
09/17 05:35:52 AM: edges-coref-ontonotes_loss: training: 0.259415 validation: 0.306095
09/17 05:35:52 AM: macro_avg: validation: 0.876344
09/17 05:35:52 AM: micro_avg: validation: 0.000000
09/17 05:35:52 AM: edges-coref-ontonotes_mcc: training: 0.768801 validation: 0.752989
09/17 05:35:52 AM: edges-coref-ontonotes_acc: training: 0.877403 validation: 0.874521
09/17 05:35:52 AM: edges-coref-ontonotes_precision: training: 0.884489 validation: 0.877404
09/17 05:35:52 AM: edges-coref-ontonotes_recall: training: 0.884285 validation: 0.875287
09/17 05:35:52 AM: edges-coref-ontonotes_f1: training: 0.884387 validation: 0.876344
09/17 05:35:52 AM: Global learning rate: 0.0001
09/17 05:35:52 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:35:57 AM: Update 6169: task edges-coref-ontonotes, batch 169 (6169): mcc: 0.7809, acc: 0.8844, precision: 0.8905, recall: 0.8904, f1: 0.8905, edges-coref-ontonotes_loss: 0.2426
09/17 05:36:07 AM: Update 6414: task edges-coref-ontonotes, batch 414 (6414): mcc: 0.8024, acc: 0.8959, precision: 0.9012, recall: 0.9012, f1: 0.9012, edges-coref-ontonotes_loss: 0.2118
09/17 05:36:18 AM: Update 6640: task edges-coref-ontonotes, batch 640 (6640): mcc: 0.7945, acc: 0.8915, precision: 0.8974, recall: 0.8971, f1: 0.8972, edges-coref-ontonotes_loss: 0.2218
09/17 05:36:28 AM: Update 6872: task edges-coref-ontonotes, batch 872 (6872): mcc: 0.7933, acc: 0.8910, precision: 0.8968, recall: 0.8964, f1: 0.8966, edges-coref-ontonotes_loss: 0.2255
09/17 05:36:32 AM: ***** Step 7000 / Validation 7 *****
09/17 05:36:32 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:36:32 AM: Validating...
09/17 05:36:38 AM: Evaluate: task edges-coref-ontonotes, batch 137 (157): mcc: 0.7506, acc: 0.8734, precision: 0.8753, recall: 0.8752, f1: 0.8753, edges-coref-ontonotes_loss: 0.3041
09/17 05:36:38 AM: Updating LR scheduler:
09/17 05:36:38 AM: 	Best result seen so far for macro_avg: 0.877
09/17 05:36:38 AM: 	# validation passes without improvement: 2
09/17 05:36:38 AM: edges-coref-ontonotes_loss: training: 0.234734 validation: 0.304983
09/17 05:36:38 AM: macro_avg: validation: 0.874232
09/17 05:36:38 AM: micro_avg: validation: 0.000000
09/17 05:36:38 AM: edges-coref-ontonotes_mcc: training: 0.786284 validation: 0.748468
09/17 05:36:38 AM: edges-coref-ontonotes_acc: training: 0.887244 validation: 0.872301
09/17 05:36:38 AM: edges-coref-ontonotes_precision: training: 0.893369 validation: 0.874249
09/17 05:36:38 AM: edges-coref-ontonotes_recall: training: 0.892853 validation: 0.874215
09/17 05:36:38 AM: edges-coref-ontonotes_f1: training: 0.893111 validation: 0.874232
09/17 05:36:38 AM: Global learning rate: 0.0001
09/17 05:36:38 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:36:48 AM: Update 7187: task edges-coref-ontonotes, batch 187 (7187): mcc: 0.7391, acc: 0.8618, precision: 0.8693, recall: 0.8699, f1: 0.8696, edges-coref-ontonotes_loss: 0.2973
09/17 05:36:58 AM: Update 7474: task edges-coref-ontonotes, batch 474 (7474): mcc: 0.7623, acc: 0.8743, precision: 0.8810, recall: 0.8813, f1: 0.8812, edges-coref-ontonotes_loss: 0.2669
09/17 05:37:08 AM: Update 7717: task edges-coref-ontonotes, batch 717 (7717): mcc: 0.7803, acc: 0.8840, precision: 0.8901, recall: 0.8903, f1: 0.8902, edges-coref-ontonotes_loss: 0.2398
09/17 05:37:18 AM: Update 7951: task edges-coref-ontonotes, batch 951 (7951): mcc: 0.7805, acc: 0.8841, precision: 0.8902, recall: 0.8902, f1: 0.8902, edges-coref-ontonotes_loss: 0.2390
09/17 05:37:19 AM: ***** Step 8000 / Validation 8 *****
09/17 05:37:19 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:37:19 AM: Validating...
09/17 05:37:25 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:37:25 AM: Best result seen so far for macro.
09/17 05:37:25 AM: Updating LR scheduler:
09/17 05:37:25 AM: 	Best result seen so far for macro_avg: 0.883
09/17 05:37:25 AM: 	# validation passes without improvement: 0
09/17 05:37:25 AM: edges-coref-ontonotes_loss: training: 0.238921 validation: 0.291145
09/17 05:37:25 AM: macro_avg: validation: 0.882835
09/17 05:37:25 AM: micro_avg: validation: 0.000000
09/17 05:37:25 AM: edges-coref-ontonotes_mcc: training: 0.780640 validation: 0.765661
09/17 05:37:25 AM: edges-coref-ontonotes_acc: training: 0.884264 validation: 0.880878
09/17 05:37:25 AM: edges-coref-ontonotes_precision: training: 0.890349 validation: 0.882801
09/17 05:37:25 AM: edges-coref-ontonotes_recall: training: 0.890283 validation: 0.882869
09/17 05:37:25 AM: edges-coref-ontonotes_f1: training: 0.890316 validation: 0.882835
09/17 05:37:25 AM: Global learning rate: 0.0001
09/17 05:37:25 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:37:28 AM: Update 8068: task edges-coref-ontonotes, batch 68 (8068): mcc: 0.8116, acc: 0.9009, precision: 0.9055, recall: 0.9062, f1: 0.9059, edges-coref-ontonotes_loss: 0.2263
09/17 05:37:38 AM: Update 8292: task edges-coref-ontonotes, batch 292 (8292): mcc: 0.7737, acc: 0.8815, precision: 0.8869, recall: 0.8867, f1: 0.8868, edges-coref-ontonotes_loss: 0.2571
09/17 05:37:48 AM: Update 8518: task edges-coref-ontonotes, batch 518 (8518): mcc: 0.7655, acc: 0.8768, precision: 0.8829, recall: 0.8826, f1: 0.8827, edges-coref-ontonotes_loss: 0.2658
09/17 05:37:59 AM: Update 8788: task edges-coref-ontonotes, batch 788 (8788): mcc: 0.7701, acc: 0.8794, precision: 0.8851, recall: 0.8850, f1: 0.8851, edges-coref-ontonotes_loss: 0.2591
09/17 05:38:06 AM: ***** Step 9000 / Validation 9 *****
09/17 05:38:06 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:38:06 AM: Validating...
09/17 05:38:09 AM: Evaluate: task edges-coref-ontonotes, batch 72 (157): mcc: 0.7659, acc: 0.8822, precision: 0.8829, recall: 0.8829, f1: 0.8829, edges-coref-ontonotes_loss: 0.3006
09/17 05:38:12 AM: Updating LR scheduler:
09/17 05:38:12 AM: 	Best result seen so far for macro_avg: 0.883
09/17 05:38:12 AM: 	# validation passes without improvement: 1
09/17 05:38:12 AM: edges-coref-ontonotes_loss: training: 0.242820 validation: 0.293295
09/17 05:38:12 AM: macro_avg: validation: 0.882514
09/17 05:38:12 AM: micro_avg: validation: 0.000000
09/17 05:38:12 AM: edges-coref-ontonotes_mcc: training: 0.783199 validation: 0.765087
09/17 05:38:12 AM: edges-coref-ontonotes_acc: training: 0.886179 validation: 0.881605
09/17 05:38:12 AM: edges-coref-ontonotes_precision: training: 0.891656 validation: 0.882734
09/17 05:38:12 AM: edges-coref-ontonotes_recall: training: 0.891527 validation: 0.882294
09/17 05:38:12 AM: edges-coref-ontonotes_f1: training: 0.891591 validation: 0.882514
09/17 05:38:12 AM: Global learning rate: 0.0001
09/17 05:38:12 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:38:20 AM: Update 9157: task edges-coref-ontonotes, batch 157 (9157): mcc: 0.7741, acc: 0.8822, precision: 0.8874, recall: 0.8866, f1: 0.8870, edges-coref-ontonotes_loss: 0.2235
09/17 05:38:30 AM: Update 9453: task edges-coref-ontonotes, batch 453 (9453): mcc: 0.7930, acc: 0.8919, precision: 0.8966, recall: 0.8964, f1: 0.8965, edges-coref-ontonotes_loss: 0.2215
09/17 05:38:40 AM: Update 9671: task edges-coref-ontonotes, batch 671 (9671): mcc: 0.7790, acc: 0.8843, precision: 0.8894, recall: 0.8897, f1: 0.8895, edges-coref-ontonotes_loss: 0.2423
09/17 05:38:51 AM: Update 9923: task edges-coref-ontonotes, batch 923 (9923): mcc: 0.7779, acc: 0.8837, precision: 0.8889, recall: 0.8889, f1: 0.8889, edges-coref-ontonotes_loss: 0.2448
09/17 05:38:53 AM: ***** Step 10000 / Validation 10 *****
09/17 05:38:53 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:38:53 AM: Validating...
09/17 05:39:00 AM: Updating LR scheduler:
09/17 05:39:00 AM: 	Best result seen so far for macro_avg: 0.883
09/17 05:39:00 AM: 	# validation passes without improvement: 2
09/17 05:39:00 AM: edges-coref-ontonotes_loss: training: 0.245770 validation: 0.294682
09/17 05:39:00 AM: macro_avg: validation: 0.880271
09/17 05:39:00 AM: micro_avg: validation: 0.000000
09/17 05:39:00 AM: edges-coref-ontonotes_mcc: training: 0.777701 validation: 0.760607
09/17 05:39:00 AM: edges-coref-ontonotes_acc: training: 0.883705 validation: 0.879193
09/17 05:39:00 AM: edges-coref-ontonotes_precision: training: 0.888855 validation: 0.880507
09/17 05:39:00 AM: edges-coref-ontonotes_recall: training: 0.888844 validation: 0.880035
09/17 05:39:00 AM: edges-coref-ontonotes_f1: training: 0.888850 validation: 0.880271
09/17 05:39:00 AM: Global learning rate: 0.0001
09/17 05:39:00 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:39:01 AM: Update 10009: task edges-coref-ontonotes, batch 9 (10009): mcc: 0.8145, acc: 0.9047, precision: 0.9064, recall: 0.9084, f1: 0.9074, edges-coref-ontonotes_loss: 0.2134
09/17 05:39:11 AM: Update 10243: task edges-coref-ontonotes, batch 243 (10243): mcc: 0.8001, acc: 0.8961, precision: 0.9003, recall: 0.8998, f1: 0.9000, edges-coref-ontonotes_loss: 0.2103
09/17 05:39:21 AM: Update 10482: task edges-coref-ontonotes, batch 482 (10482): mcc: 0.7975, acc: 0.8947, precision: 0.8990, recall: 0.8984, f1: 0.8987, edges-coref-ontonotes_loss: 0.2087
09/17 05:39:32 AM: Update 10778: task edges-coref-ontonotes, batch 778 (10778): mcc: 0.7997, acc: 0.8958, precision: 0.9002, recall: 0.8993, f1: 0.8998, edges-coref-ontonotes_loss: 0.2132
09/17 05:39:41 AM: ***** Step 11000 / Validation 11 *****
09/17 05:39:41 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:39:41 AM: Validating...
09/17 05:39:42 AM: Evaluate: task edges-coref-ontonotes, batch 35 (157): mcc: 0.7867, acc: 0.8927, precision: 0.8935, recall: 0.8931, f1: 0.8933, edges-coref-ontonotes_loss: 0.2660
09/17 05:39:47 AM: Updating LR scheduler:
09/17 05:39:47 AM: 	Best result seen so far for macro_avg: 0.883
09/17 05:39:47 AM: 	# validation passes without improvement: 3
09/17 05:39:47 AM: edges-coref-ontonotes_loss: training: 0.228422 validation: 0.291651
09/17 05:39:47 AM: macro_avg: validation: 0.880550
09/17 05:39:47 AM: micro_avg: validation: 0.000000
09/17 05:39:47 AM: edges-coref-ontonotes_mcc: training: 0.789348 validation: 0.761104
09/17 05:39:47 AM: edges-coref-ontonotes_acc: training: 0.890326 validation: 0.879767
09/17 05:39:47 AM: edges-coref-ontonotes_precision: training: 0.894967 validation: 0.880567
09/17 05:39:47 AM: edges-coref-ontonotes_recall: training: 0.894303 validation: 0.880533
09/17 05:39:47 AM: edges-coref-ontonotes_f1: training: 0.894635 validation: 0.880550
09/17 05:39:47 AM: Global learning rate: 0.0001
09/17 05:39:47 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:39:53 AM: Update 11091: task edges-coref-ontonotes, batch 91 (11091): mcc: 0.7579, acc: 0.8729, precision: 0.8789, recall: 0.8791, f1: 0.8790, edges-coref-ontonotes_loss: 0.2751
09/17 05:40:03 AM: Update 11385: task edges-coref-ontonotes, batch 385 (11385): mcc: 0.7802, acc: 0.8851, precision: 0.8904, recall: 0.8897, f1: 0.8901, edges-coref-ontonotes_loss: 0.2476
09/17 05:40:13 AM: Update 11630: task edges-coref-ontonotes, batch 630 (11630): mcc: 0.7974, acc: 0.8942, precision: 0.8988, recall: 0.8985, f1: 0.8987, edges-coref-ontonotes_loss: 0.2213
09/17 05:40:23 AM: Update 11849: task edges-coref-ontonotes, batch 849 (11849): mcc: 0.7942, acc: 0.8927, precision: 0.8972, recall: 0.8969, f1: 0.8971, edges-coref-ontonotes_loss: 0.2215
09/17 05:40:28 AM: ***** Step 12000 / Validation 12 *****
09/17 05:40:28 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:40:28 AM: Validating...
09/17 05:40:33 AM: Evaluate: task edges-coref-ontonotes, batch 129 (157): mcc: 0.7665, acc: 0.8820, precision: 0.8833, recall: 0.8832, f1: 0.8832, edges-coref-ontonotes_loss: 0.3059
09/17 05:40:34 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:40:34 AM: Best result seen so far for macro.
09/17 05:40:34 AM: Updating LR scheduler:
09/17 05:40:34 AM: 	Best result seen so far for macro_avg: 0.884
09/17 05:40:34 AM: 	# validation passes without improvement: 0
09/17 05:40:34 AM: edges-coref-ontonotes_loss: training: 0.218466 validation: 0.294529
09/17 05:40:34 AM: macro_avg: validation: 0.884119
09/17 05:40:34 AM: micro_avg: validation: 0.000000
09/17 05:40:34 AM: edges-coref-ontonotes_mcc: training: 0.796634 validation: 0.768265
09/17 05:40:34 AM: edges-coref-ontonotes_acc: training: 0.894080 validation: 0.882945
09/17 05:40:34 AM: edges-coref-ontonotes_precision: training: 0.898440 validation: 0.884221
09/17 05:40:34 AM: edges-coref-ontonotes_recall: training: 0.898163 validation: 0.884017
09/17 05:40:34 AM: edges-coref-ontonotes_f1: training: 0.898301 validation: 0.884119
09/17 05:40:34 AM: Global learning rate: 0.0001
09/17 05:40:34 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:40:43 AM: Update 12185: task edges-coref-ontonotes, batch 185 (12185): mcc: 0.7786, acc: 0.8847, precision: 0.8889, recall: 0.8898, f1: 0.8894, edges-coref-ontonotes_loss: 0.2556
09/17 05:40:53 AM: Update 12404: task edges-coref-ontonotes, batch 404 (12404): mcc: 0.7651, acc: 0.8776, precision: 0.8823, recall: 0.8828, f1: 0.8826, edges-coref-ontonotes_loss: 0.2685
09/17 05:41:03 AM: Update 12693: task edges-coref-ontonotes, batch 693 (12693): mcc: 0.7731, acc: 0.8821, precision: 0.8864, recall: 0.8866, f1: 0.8865, edges-coref-ontonotes_loss: 0.2564
09/17 05:41:13 AM: Update 12943: task edges-coref-ontonotes, batch 943 (12943): mcc: 0.7857, acc: 0.8887, precision: 0.8929, recall: 0.8929, f1: 0.8929, edges-coref-ontonotes_loss: 0.2362
09/17 05:41:15 AM: ***** Step 13000 / Validation 13 *****
09/17 05:41:15 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:41:15 AM: Validating...
09/17 05:41:21 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:41:21 AM: Best result seen so far for macro.
09/17 05:41:21 AM: Updating LR scheduler:
09/17 05:41:21 AM: 	Best result seen so far for macro_avg: 0.886
09/17 05:41:21 AM: 	# validation passes without improvement: 0
09/17 05:41:21 AM: edges-coref-ontonotes_loss: training: 0.232358 validation: 0.288403
09/17 05:41:21 AM: macro_avg: validation: 0.886098
09/17 05:41:21 AM: micro_avg: validation: 0.000000
09/17 05:41:21 AM: edges-coref-ontonotes_mcc: training: 0.788025 validation: 0.772170
09/17 05:41:21 AM: edges-coref-ontonotes_acc: training: 0.889871 validation: 0.884936
09/17 05:41:21 AM: edges-coref-ontonotes_precision: training: 0.894007 validation: 0.885996
09/17 05:41:21 AM: edges-coref-ontonotes_recall: training: 0.894018 validation: 0.886200
09/17 05:41:21 AM: edges-coref-ontonotes_f1: training: 0.894013 validation: 0.886098
09/17 05:41:21 AM: Global learning rate: 0.0001
09/17 05:41:21 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:41:23 AM: Update 13062: task edges-coref-ontonotes, batch 62 (13062): mcc: 0.7669, acc: 0.8792, precision: 0.8829, recall: 0.8842, f1: 0.8835, edges-coref-ontonotes_loss: 0.2383
09/17 05:41:33 AM: Update 13316: task edges-coref-ontonotes, batch 316 (13316): mcc: 0.7972, acc: 0.8953, precision: 0.8987, recall: 0.8985, f1: 0.8986, edges-coref-ontonotes_loss: 0.2215
09/17 05:41:43 AM: Update 13552: task edges-coref-ontonotes, batch 552 (13552): mcc: 0.7861, acc: 0.8893, precision: 0.8932, recall: 0.8929, f1: 0.8931, edges-coref-ontonotes_loss: 0.2363
09/17 05:41:53 AM: Update 13769: task edges-coref-ontonotes, batch 769 (13769): mcc: 0.7789, acc: 0.8855, precision: 0.8896, recall: 0.8893, f1: 0.8894, edges-coref-ontonotes_loss: 0.2442
09/17 05:42:01 AM: ***** Step 14000 / Validation 14 *****
09/17 05:42:01 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:42:01 AM: Validating...
09/17 05:42:03 AM: Evaluate: task edges-coref-ontonotes, batch 54 (157): mcc: 0.7634, acc: 0.8811, precision: 0.8818, recall: 0.8816, f1: 0.8817, edges-coref-ontonotes_loss: 0.2922
09/17 05:42:08 AM: Updating LR scheduler:
09/17 05:42:08 AM: 	Best result seen so far for macro_avg: 0.886
09/17 05:42:08 AM: 	# validation passes without improvement: 1
09/17 05:42:08 AM: edges-coref-ontonotes_loss: training: 0.242028 validation: 0.292405
09/17 05:42:08 AM: macro_avg: validation: 0.880277
09/17 05:42:08 AM: micro_avg: validation: 0.000000
09/17 05:42:08 AM: edges-coref-ontonotes_mcc: training: 0.781879 validation: 0.760568
09/17 05:42:08 AM: edges-coref-ontonotes_acc: training: 0.887027 validation: 0.879116
09/17 05:42:08 AM: edges-coref-ontonotes_precision: training: 0.891039 validation: 0.880328
09/17 05:42:08 AM: edges-coref-ontonotes_recall: training: 0.890812 validation: 0.880227
09/17 05:42:08 AM: edges-coref-ontonotes_f1: training: 0.890925 validation: 0.880277
09/17 05:42:08 AM: Global learning rate: 0.0001
09/17 05:42:08 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:42:13 AM: Update 14103: task edges-coref-ontonotes, batch 103 (14103): mcc: 0.7987, acc: 0.8961, precision: 0.8993, recall: 0.8994, f1: 0.8994, edges-coref-ontonotes_loss: 0.1868
09/17 05:42:24 AM: Update 14389: task edges-coref-ontonotes, batch 389 (14389): mcc: 0.8121, acc: 0.9030, precision: 0.9060, recall: 0.9060, f1: 0.9060, edges-coref-ontonotes_loss: 0.1913
09/17 05:42:36 AM: Update 14702: task edges-coref-ontonotes, batch 702 (14702): mcc: 0.8095, acc: 0.9018, precision: 0.9048, recall: 0.9048, f1: 0.9048, edges-coref-ontonotes_loss: 0.1998
09/17 05:42:46 AM: Update 14992: task edges-coref-ontonotes, batch 992 (14992): mcc: 0.7947, acc: 0.8941, precision: 0.8974, recall: 0.8973, f1: 0.8973, edges-coref-ontonotes_loss: 0.2208
09/17 05:42:46 AM: ***** Step 15000 / Validation 15 *****
09/17 05:42:46 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:42:46 AM: Validating...
09/17 05:42:52 AM: Updating LR scheduler:
09/17 05:42:52 AM: 	Best result seen so far for macro_avg: 0.886
09/17 05:42:52 AM: 	# validation passes without improvement: 2
09/17 05:42:52 AM: edges-coref-ontonotes_loss: training: 0.220918 validation: 0.289431
09/17 05:42:52 AM: macro_avg: validation: 0.880929
09/17 05:42:52 AM: micro_avg: validation: 0.000000
09/17 05:42:52 AM: edges-coref-ontonotes_mcc: training: 0.794587 validation: 0.761985
09/17 05:42:52 AM: edges-coref-ontonotes_acc: training: 0.894073 validation: 0.879691
09/17 05:42:52 AM: edges-coref-ontonotes_precision: training: 0.897352 validation: 0.881401
09/17 05:42:52 AM: edges-coref-ontonotes_recall: training: 0.897220 validation: 0.880456
09/17 05:42:52 AM: edges-coref-ontonotes_f1: training: 0.897286 validation: 0.880929
09/17 05:42:52 AM: Global learning rate: 0.0001
09/17 05:42:52 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:42:56 AM: Update 15043: task edges-coref-ontonotes, batch 43 (15043): mcc: 0.7748, acc: 0.8829, precision: 0.8872, recall: 0.8877, f1: 0.8874, edges-coref-ontonotes_loss: 0.2470
09/17 05:43:07 AM: Update 15328: task edges-coref-ontonotes, batch 328 (15328): mcc: 0.7874, acc: 0.8900, precision: 0.8936, recall: 0.8939, f1: 0.8937, edges-coref-ontonotes_loss: 0.2366
09/17 05:43:18 AM: Update 15641: task edges-coref-ontonotes, batch 641 (15641): mcc: 0.8078, acc: 0.9006, precision: 0.9038, recall: 0.9040, f1: 0.9039, edges-coref-ontonotes_loss: 0.2040
09/17 05:43:28 AM: Update 15887: task edges-coref-ontonotes, batch 887 (15887): mcc: 0.8029, acc: 0.8981, precision: 0.9014, recall: 0.9015, f1: 0.9014, edges-coref-ontonotes_loss: 0.2114
09/17 05:43:31 AM: ***** Step 16000 / Validation 16 *****
09/17 05:43:31 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:43:31 AM: Validating...
09/17 05:43:37 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:43:37 AM: Best result seen so far for macro.
09/17 05:43:37 AM: Updating LR scheduler:
09/17 05:43:37 AM: 	Best result seen so far for macro_avg: 0.887
09/17 05:43:37 AM: 	# validation passes without improvement: 0
09/17 05:43:37 AM: edges-coref-ontonotes_loss: training: 0.209339 validation: 0.292854
09/17 05:43:37 AM: macro_avg: validation: 0.886502
09/17 05:43:37 AM: micro_avg: validation: 0.000000
09/17 05:43:37 AM: edges-coref-ontonotes_mcc: training: 0.804388 validation: 0.773051
09/17 05:43:37 AM: edges-coref-ontonotes_acc: training: 0.898943 validation: 0.885855
09/17 05:43:37 AM: edges-coref-ontonotes_precision: training: 0.902191 validation: 0.886688
09/17 05:43:37 AM: edges-coref-ontonotes_recall: training: 0.902197 validation: 0.886315
09/17 05:43:37 AM: edges-coref-ontonotes_f1: training: 0.902194 validation: 0.886502
09/17 05:43:37 AM: Global learning rate: 0.0001
09/17 05:43:37 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:43:40 AM: Update 16010: task edges-coref-ontonotes, batch 10 (16010): mcc: 0.7804, acc: 0.8874, precision: 0.8899, recall: 0.8906, f1: 0.8902, edges-coref-ontonotes_loss: 0.1876
09/17 05:43:50 AM: Update 16287: task edges-coref-ontonotes, batch 287 (16287): mcc: 0.7651, acc: 0.8787, precision: 0.8827, recall: 0.8824, f1: 0.8825, edges-coref-ontonotes_loss: 0.2653
09/17 05:44:00 AM: Update 16497: task edges-coref-ontonotes, batch 497 (16497): mcc: 0.7737, acc: 0.8832, precision: 0.8869, recall: 0.8868, f1: 0.8868, edges-coref-ontonotes_loss: 0.2536
09/17 05:44:10 AM: Update 16757: task edges-coref-ontonotes, batch 757 (16757): mcc: 0.7859, acc: 0.8895, precision: 0.8930, recall: 0.8929, f1: 0.8929, edges-coref-ontonotes_loss: 0.2349
09/17 05:44:19 AM: ***** Step 17000 / Validation 17 *****
09/17 05:44:19 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:44:19 AM: Validating...
09/17 05:44:20 AM: Evaluate: task edges-coref-ontonotes, batch 35 (157): mcc: 0.8118, acc: 0.9050, precision: 0.9060, recall: 0.9057, f1: 0.9059, edges-coref-ontonotes_loss: 0.2444
09/17 05:44:25 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:44:25 AM: Best result seen so far for macro.
09/17 05:44:25 AM: Updating LR scheduler:
09/17 05:44:25 AM: 	Best result seen so far for macro_avg: 0.887
09/17 05:44:25 AM: 	# validation passes without improvement: 0
09/17 05:44:25 AM: edges-coref-ontonotes_loss: training: 0.223834 validation: 0.278193
09/17 05:44:25 AM: macro_avg: validation: 0.887439
09/17 05:44:25 AM: micro_avg: validation: 0.000000
09/17 05:44:25 AM: edges-coref-ontonotes_mcc: training: 0.793482 validation: 0.774813
09/17 05:44:25 AM: edges-coref-ontonotes_acc: training: 0.893573 validation: 0.886430
09/17 05:44:25 AM: edges-coref-ontonotes_precision: training: 0.896736 validation: 0.887184
09/17 05:44:25 AM: edges-coref-ontonotes_recall: training: 0.896747 validation: 0.887693
09/17 05:44:25 AM: edges-coref-ontonotes_f1: training: 0.896741 validation: 0.887439
09/17 05:44:25 AM: Global learning rate: 0.0001
09/17 05:44:25 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:44:30 AM: Update 17115: task edges-coref-ontonotes, batch 115 (17115): mcc: 0.8109, acc: 0.9023, precision: 0.9055, recall: 0.9055, f1: 0.9055, edges-coref-ontonotes_loss: 0.1961
09/17 05:44:40 AM: Update 17340: task edges-coref-ontonotes, batch 340 (17340): mcc: 0.8024, acc: 0.8983, precision: 0.9014, recall: 0.9010, f1: 0.9012, edges-coref-ontonotes_loss: 0.2124
09/17 05:44:50 AM: Update 17617: task edges-coref-ontonotes, batch 617 (17617): mcc: 0.7868, acc: 0.8901, precision: 0.8937, recall: 0.8931, f1: 0.8934, edges-coref-ontonotes_loss: 0.2363
09/17 05:45:00 AM: Update 17838: task edges-coref-ontonotes, batch 838 (17838): mcc: 0.7873, acc: 0.8904, precision: 0.8939, recall: 0.8934, f1: 0.8936, edges-coref-ontonotes_loss: 0.2360
09/17 05:45:08 AM: ***** Step 18000 / Validation 18 *****
09/17 05:45:08 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:45:08 AM: Validating...
09/17 05:45:10 AM: Evaluate: task edges-coref-ontonotes, batch 53 (157): mcc: 0.7884, acc: 0.8933, precision: 0.8944, recall: 0.8940, f1: 0.8942, edges-coref-ontonotes_loss: 0.2747
09/17 05:45:14 AM: Updating LR scheduler:
09/17 05:45:14 AM: 	Best result seen so far for macro_avg: 0.887
09/17 05:45:14 AM: 	# validation passes without improvement: 1
09/17 05:45:14 AM: edges-coref-ontonotes_loss: training: 0.232852 validation: 0.283883
09/17 05:45:14 AM: macro_avg: validation: 0.886936
09/17 05:45:14 AM: micro_avg: validation: 0.000000
09/17 05:45:14 AM: edges-coref-ontonotes_mcc: training: 0.787863 validation: 0.773932
09/17 05:45:14 AM: edges-coref-ontonotes_acc: training: 0.890666 validation: 0.886162
09/17 05:45:14 AM: edges-coref-ontonotes_precision: training: 0.894088 validation: 0.887173
09/17 05:45:14 AM: edges-coref-ontonotes_recall: training: 0.893733 validation: 0.886698
09/17 05:45:14 AM: edges-coref-ontonotes_f1: training: 0.893910 validation: 0.886936
09/17 05:45:14 AM: Global learning rate: 0.0001
09/17 05:45:14 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:45:20 AM: Update 18195: task edges-coref-ontonotes, batch 195 (18195): mcc: 0.8571, acc: 0.9264, precision: 0.9288, recall: 0.9283, f1: 0.9285, edges-coref-ontonotes_loss: 0.1572
09/17 05:45:30 AM: Update 18451: task edges-coref-ontonotes, batch 451 (18451): mcc: 0.8222, acc: 0.9084, precision: 0.9111, recall: 0.9111, f1: 0.9111, edges-coref-ontonotes_loss: 0.1865
09/17 05:45:40 AM: Update 18678: task edges-coref-ontonotes, batch 678 (18678): mcc: 0.8158, acc: 0.9054, precision: 0.9079, recall: 0.9080, f1: 0.9079, edges-coref-ontonotes_loss: 0.1964
09/17 05:45:51 AM: Update 18939: task edges-coref-ontonotes, batch 939 (18939): mcc: 0.8018, acc: 0.8980, precision: 0.9008, recall: 0.9010, f1: 0.9009, edges-coref-ontonotes_loss: 0.2153
09/17 05:45:53 AM: ***** Step 19000 / Validation 19 *****
09/17 05:45:53 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:45:53 AM: Validating...
09/17 05:46:00 AM: Updating LR scheduler:
09/17 05:46:00 AM: 	Best result seen so far for macro_avg: 0.887
09/17 05:46:00 AM: 	# validation passes without improvement: 2
09/17 05:46:00 AM: edges-coref-ontonotes_loss: training: 0.214998 validation: 0.295686
09/17 05:46:00 AM: macro_avg: validation: 0.883189
09/17 05:46:00 AM: micro_avg: validation: 0.000000
09/17 05:46:00 AM: edges-coref-ontonotes_mcc: training: 0.801942 validation: 0.766312
09/17 05:46:00 AM: edges-coref-ontonotes_acc: training: 0.898085 validation: 0.882141
09/17 05:46:00 AM: edges-coref-ontonotes_precision: training: 0.900850 validation: 0.882936
09/17 05:46:00 AM: edges-coref-ontonotes_recall: training: 0.901122 validation: 0.883443
09/17 05:46:00 AM: edges-coref-ontonotes_f1: training: 0.900986 validation: 0.883189
09/17 05:46:00 AM: Global learning rate: 0.0001
09/17 05:46:00 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:46:01 AM: Update 19048: task edges-coref-ontonotes, batch 48 (19048): mcc: 0.7826, acc: 0.8885, precision: 0.8911, recall: 0.8915, f1: 0.8913, edges-coref-ontonotes_loss: 0.2402
09/17 05:46:11 AM: Update 19279: task edges-coref-ontonotes, batch 279 (19279): mcc: 0.7926, acc: 0.8932, precision: 0.8964, recall: 0.8962, f1: 0.8963, edges-coref-ontonotes_loss: 0.2264
09/17 05:46:21 AM: Update 19607: task edges-coref-ontonotes, batch 607 (19607): mcc: 0.8097, acc: 0.9022, precision: 0.9049, recall: 0.9049, f1: 0.9049, edges-coref-ontonotes_loss: 0.2006
09/17 05:46:31 AM: Update 19858: task edges-coref-ontonotes, batch 858 (19858): mcc: 0.8080, acc: 0.9014, precision: 0.9041, recall: 0.9039, f1: 0.9040, edges-coref-ontonotes_loss: 0.2019
09/17 05:46:38 AM: ***** Step 20000 / Validation 20 *****
09/17 05:46:38 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:46:38 AM: Validating...
09/17 05:46:41 AM: Evaluate: task edges-coref-ontonotes, batch 76 (157): mcc: 0.7844, acc: 0.8914, precision: 0.8920, recall: 0.8924, f1: 0.8922, edges-coref-ontonotes_loss: 0.2790
09/17 05:46:44 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:46:44 AM: Best result seen so far for macro.
09/17 05:46:44 AM: Updating LR scheduler:
09/17 05:46:44 AM: 	Best result seen so far for macro_avg: 0.888
09/17 05:46:44 AM: 	# validation passes without improvement: 0
09/17 05:46:44 AM: edges-coref-ontonotes_loss: training: 0.206526 validation: 0.279941
09/17 05:46:44 AM: macro_avg: validation: 0.888327
09/17 05:46:44 AM: micro_avg: validation: 0.000000
09/17 05:46:44 AM: edges-coref-ontonotes_mcc: training: 0.806885 validation: 0.776650
09/17 05:46:44 AM: edges-coref-ontonotes_acc: training: 0.900867 validation: 0.887693
09/17 05:46:44 AM: edges-coref-ontonotes_precision: training: 0.903486 validation: 0.888310
09/17 05:46:44 AM: edges-coref-ontonotes_recall: training: 0.903389 validation: 0.888344
09/17 05:46:44 AM: edges-coref-ontonotes_f1: training: 0.903438 validation: 0.888327
09/17 05:46:44 AM: Global learning rate: 0.0001
09/17 05:46:44 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:46:51 AM: Update 20196: task edges-coref-ontonotes, batch 196 (20196): mcc: 0.7696, acc: 0.8819, precision: 0.8849, recall: 0.8847, f1: 0.8848, edges-coref-ontonotes_loss: 0.2620
09/17 05:47:01 AM: Update 20401: task edges-coref-ontonotes, batch 401 (20401): mcc: 0.7768, acc: 0.8856, precision: 0.8884, recall: 0.8885, f1: 0.8884, edges-coref-ontonotes_loss: 0.2483
09/17 05:47:11 AM: Update 20624: task edges-coref-ontonotes, batch 624 (20624): mcc: 0.7868, acc: 0.8907, precision: 0.8934, recall: 0.8934, f1: 0.8934, edges-coref-ontonotes_loss: 0.2350
09/17 05:47:23 AM: Update 20929: task edges-coref-ontonotes, batch 929 (20929): mcc: 0.7974, acc: 0.8962, precision: 0.8987, recall: 0.8987, f1: 0.8987, edges-coref-ontonotes_loss: 0.2173
09/17 05:47:25 AM: ***** Step 21000 / Validation 21 *****
09/17 05:47:25 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:47:25 AM: Validating...
09/17 05:47:31 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:47:31 AM: Best result seen so far for macro.
09/17 05:47:31 AM: Updating LR scheduler:
09/17 05:47:31 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:47:31 AM: 	# validation passes without improvement: 0
09/17 05:47:31 AM: edges-coref-ontonotes_loss: training: 0.216446 validation: 0.281465
09/17 05:47:31 AM: macro_avg: validation: 0.889591
09/17 05:47:31 AM: micro_avg: validation: 0.000000
09/17 05:47:31 AM: edges-coref-ontonotes_mcc: training: 0.798020 validation: 0.779178
09/17 05:47:31 AM: edges-coref-ontonotes_acc: training: 0.896556 validation: 0.889378
09/17 05:47:31 AM: edges-coref-ontonotes_precision: training: 0.899008 validation: 0.889574
09/17 05:47:31 AM: edges-coref-ontonotes_recall: training: 0.899013 validation: 0.889608
09/17 05:47:31 AM: edges-coref-ontonotes_f1: training: 0.899010 validation: 0.889591
09/17 05:47:31 AM: Global learning rate: 0.0001
09/17 05:47:31 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:47:33 AM: Update 21059: task edges-coref-ontonotes, batch 59 (21059): mcc: 0.8182, acc: 0.9073, precision: 0.9091, recall: 0.9090, f1: 0.9091, edges-coref-ontonotes_loss: 0.1850
09/17 05:47:43 AM: Update 21299: task edges-coref-ontonotes, batch 299 (21299): mcc: 0.8051, acc: 0.9004, precision: 0.9025, recall: 0.9026, f1: 0.9026, edges-coref-ontonotes_loss: 0.2105
09/17 05:47:54 AM: Update 21555: task edges-coref-ontonotes, batch 555 (21555): mcc: 0.7914, acc: 0.8932, precision: 0.8957, recall: 0.8958, f1: 0.8957, edges-coref-ontonotes_loss: 0.2312
09/17 05:48:04 AM: Update 21856: task edges-coref-ontonotes, batch 856 (21856): mcc: 0.7927, acc: 0.8938, precision: 0.8963, recall: 0.8964, f1: 0.8963, edges-coref-ontonotes_loss: 0.2291
09/17 05:48:11 AM: ***** Step 22000 / Validation 22 *****
09/17 05:48:11 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:48:11 AM: Validating...
09/17 05:48:14 AM: Evaluate: task edges-coref-ontonotes, batch 90 (157): mcc: 0.7670, acc: 0.8828, precision: 0.8834, recall: 0.8835, f1: 0.8835, edges-coref-ontonotes_loss: 0.2983
09/17 05:48:17 AM: Updating LR scheduler:
09/17 05:48:17 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:48:17 AM: 	# validation passes without improvement: 1
09/17 05:48:17 AM: edges-coref-ontonotes_loss: training: 0.219938 validation: 0.278507
09/17 05:48:17 AM: macro_avg: validation: 0.888693
09/17 05:48:17 AM: micro_avg: validation: 0.000000
09/17 05:48:17 AM: edges-coref-ontonotes_mcc: training: 0.798124 validation: 0.777301
09/17 05:48:17 AM: edges-coref-ontonotes_acc: training: 0.896627 validation: 0.887808
09/17 05:48:17 AM: edges-coref-ontonotes_precision: training: 0.899004 validation: 0.888353
09/17 05:48:17 AM: edges-coref-ontonotes_recall: training: 0.899134 validation: 0.889034
09/17 05:48:17 AM: edges-coref-ontonotes_f1: training: 0.899069 validation: 0.888693
09/17 05:48:17 AM: Global learning rate: 0.0001
09/17 05:48:17 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:48:24 AM: Update 22232: task edges-coref-ontonotes, batch 232 (22232): mcc: 0.8211, acc: 0.9086, precision: 0.9107, recall: 0.9104, f1: 0.9106, edges-coref-ontonotes_loss: 0.1850
09/17 05:48:34 AM: Update 22493: task edges-coref-ontonotes, batch 493 (22493): mcc: 0.8166, acc: 0.9063, precision: 0.9085, recall: 0.9081, f1: 0.9083, edges-coref-ontonotes_loss: 0.1922
09/17 05:48:44 AM: Update 22725: task edges-coref-ontonotes, batch 725 (22725): mcc: 0.8050, acc: 0.9003, precision: 0.9026, recall: 0.9024, f1: 0.9025, edges-coref-ontonotes_loss: 0.2086
09/17 05:48:54 AM: Update 22960: task edges-coref-ontonotes, batch 960 (22960): mcc: 0.8006, acc: 0.8981, precision: 0.9003, recall: 0.9003, f1: 0.9003, edges-coref-ontonotes_loss: 0.2173
09/17 05:48:56 AM: ***** Step 23000 / Validation 23 *****
09/17 05:48:56 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:48:56 AM: Validating...
09/17 05:49:02 AM: Updating LR scheduler:
09/17 05:49:02 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:49:02 AM: 	# validation passes without improvement: 2
09/17 05:49:02 AM: edges-coref-ontonotes_loss: training: 0.217899 validation: 0.282726
09/17 05:49:02 AM: macro_avg: validation: 0.886827
09/17 05:49:02 AM: micro_avg: validation: 0.000000
09/17 05:49:02 AM: edges-coref-ontonotes_mcc: training: 0.800469 validation: 0.773435
09/17 05:49:02 AM: edges-coref-ontonotes_acc: training: 0.898029 validation: 0.884898
09/17 05:49:02 AM: edges-coref-ontonotes_precision: training: 0.900237 validation: 0.885963
09/17 05:49:02 AM: edges-coref-ontonotes_recall: training: 0.900231 validation: 0.887693
09/17 05:49:02 AM: edges-coref-ontonotes_f1: training: 0.900234 validation: 0.886827
09/17 05:49:02 AM: Global learning rate: 0.0001
09/17 05:49:02 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:49:04 AM: Update 23064: task edges-coref-ontonotes, batch 64 (23064): mcc: 0.7982, acc: 0.8964, precision: 0.8991, recall: 0.8991, f1: 0.8991, edges-coref-ontonotes_loss: 0.2231
09/17 05:49:14 AM: Update 23298: task edges-coref-ontonotes, batch 298 (23298): mcc: 0.8089, acc: 0.9020, precision: 0.9046, recall: 0.9043, f1: 0.9045, edges-coref-ontonotes_loss: 0.1992
09/17 05:49:25 AM: Update 23545: task edges-coref-ontonotes, batch 545 (23545): mcc: 0.8147, acc: 0.9052, precision: 0.9074, recall: 0.9073, f1: 0.9073, edges-coref-ontonotes_loss: 0.1921
09/17 05:49:35 AM: Update 23848: task edges-coref-ontonotes, batch 848 (23848): mcc: 0.8157, acc: 0.9058, precision: 0.9079, recall: 0.9077, f1: 0.9078, edges-coref-ontonotes_loss: 0.1939
09/17 05:49:43 AM: ***** Step 24000 / Validation 24 *****
09/17 05:49:43 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:49:43 AM: Validating...
09/17 05:49:45 AM: Evaluate: task edges-coref-ontonotes, batch 52 (157): mcc: 0.7863, acc: 0.8927, precision: 0.8933, recall: 0.8929, f1: 0.8931, edges-coref-ontonotes_loss: 0.2688
09/17 05:49:49 AM: Updating LR scheduler:
09/17 05:49:49 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:49:49 AM: 	# validation passes without improvement: 3
09/17 05:49:49 AM: edges-coref-ontonotes_loss: training: 0.203012 validation: 0.281694
09/17 05:49:49 AM: macro_avg: validation: 0.886170
09/17 05:49:49 AM: micro_avg: validation: 0.000000
09/17 05:49:49 AM: edges-coref-ontonotes_mcc: training: 0.809754 validation: 0.772400
09/17 05:49:49 AM: edges-coref-ontonotes_acc: training: 0.902759 validation: 0.885741
09/17 05:49:49 AM: edges-coref-ontonotes_precision: training: 0.904980 validation: 0.886407
09/17 05:49:49 AM: edges-coref-ontonotes_recall: training: 0.904750 validation: 0.885932
09/17 05:49:49 AM: edges-coref-ontonotes_f1: training: 0.904865 validation: 0.886170
09/17 05:49:49 AM: Global learning rate: 0.0001
09/17 05:49:49 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:49:57 AM: Update 24171: task edges-coref-ontonotes, batch 171 (24171): mcc: 0.7790, acc: 0.8870, precision: 0.8898, recall: 0.8892, f1: 0.8895, edges-coref-ontonotes_loss: 0.2480
09/17 05:50:07 AM: Update 24465: task edges-coref-ontonotes, batch 465 (24465): mcc: 0.7916, acc: 0.8933, precision: 0.8959, recall: 0.8957, f1: 0.8958, edges-coref-ontonotes_loss: 0.2324
09/17 05:50:17 AM: Update 24709: task edges-coref-ontonotes, batch 709 (24709): mcc: 0.8033, acc: 0.8994, precision: 0.9017, recall: 0.9015, f1: 0.9016, edges-coref-ontonotes_loss: 0.2107
09/17 05:50:27 AM: Update 24953: task edges-coref-ontonotes, batch 953 (24953): mcc: 0.8036, acc: 0.8996, precision: 0.9019, recall: 0.9017, f1: 0.9018, edges-coref-ontonotes_loss: 0.2076
09/17 05:50:29 AM: ***** Step 25000 / Validation 25 *****
09/17 05:50:29 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:50:29 AM: Validating...
09/17 05:50:35 AM: Best result seen so far for edges-coref-ontonotes.
09/17 05:50:35 AM: Best result seen so far for macro.
09/17 05:50:35 AM: Updating LR scheduler:
09/17 05:50:35 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:50:35 AM: 	# validation passes without improvement: 0
09/17 05:50:35 AM: edges-coref-ontonotes_loss: training: 0.206724 validation: 0.280816
09/17 05:50:35 AM: macro_avg: validation: 0.890225
09/17 05:50:35 AM: micro_avg: validation: 0.000000
09/17 05:50:35 AM: edges-coref-ontonotes_mcc: training: 0.804302 validation: 0.780441
09/17 05:50:35 AM: edges-coref-ontonotes_acc: training: 0.899908 validation: 0.889723
09/17 05:50:35 AM: edges-coref-ontonotes_precision: training: 0.902217 validation: 0.890191
09/17 05:50:35 AM: edges-coref-ontonotes_recall: training: 0.902069 validation: 0.890259
09/17 05:50:35 AM: edges-coref-ontonotes_f1: training: 0.902143 validation: 0.890225
09/17 05:50:35 AM: Global learning rate: 0.0001
09/17 05:50:35 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:50:37 AM: Update 25071: task edges-coref-ontonotes, batch 71 (25071): mcc: 0.8197, acc: 0.9082, precision: 0.9102, recall: 0.9094, f1: 0.9098, edges-coref-ontonotes_loss: 0.1861
09/17 05:50:47 AM: Update 25297: task edges-coref-ontonotes, batch 297 (25297): mcc: 0.8005, acc: 0.8984, precision: 0.9004, recall: 0.9001, f1: 0.9002, edges-coref-ontonotes_loss: 0.2215
09/17 05:50:57 AM: Update 25528: task edges-coref-ontonotes, batch 528 (25528): mcc: 0.7925, acc: 0.8940, precision: 0.8963, recall: 0.8961, f1: 0.8962, edges-coref-ontonotes_loss: 0.2327
09/17 05:51:08 AM: Update 25792: task edges-coref-ontonotes, batch 792 (25792): mcc: 0.7938, acc: 0.8948, precision: 0.8970, recall: 0.8968, f1: 0.8969, edges-coref-ontonotes_loss: 0.2283
09/17 05:51:14 AM: ***** Step 26000 / Validation 26 *****
09/17 05:51:14 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:51:14 AM: Validating...
09/17 05:51:18 AM: Evaluate: task edges-coref-ontonotes, batch 93 (157): mcc: 0.7743, acc: 0.8867, precision: 0.8871, recall: 0.8872, f1: 0.8872, edges-coref-ontonotes_loss: 0.2985
09/17 05:51:20 AM: Updating LR scheduler:
09/17 05:51:20 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:51:20 AM: 	# validation passes without improvement: 1
09/17 05:51:20 AM: edges-coref-ontonotes_loss: training: 0.213692 validation: 0.281961
09/17 05:51:20 AM: macro_avg: validation: 0.889557
09/17 05:51:20 AM: micro_avg: validation: 0.000000
09/17 05:51:20 AM: edges-coref-ontonotes_mcc: training: 0.801851 validation: 0.779101
09/17 05:51:20 AM: edges-coref-ontonotes_acc: training: 0.898829 validation: 0.889110
09/17 05:51:20 AM: edges-coref-ontonotes_precision: training: 0.900963 validation: 0.889506
09/17 05:51:20 AM: edges-coref-ontonotes_recall: training: 0.900878 validation: 0.889608
09/17 05:51:20 AM: edges-coref-ontonotes_f1: training: 0.900921 validation: 0.889557
09/17 05:51:21 AM: Global learning rate: 0.0001
09/17 05:51:21 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:51:28 AM: Update 26167: task edges-coref-ontonotes, batch 167 (26167): mcc: 0.8149, acc: 0.9056, precision: 0.9075, recall: 0.9074, f1: 0.9074, edges-coref-ontonotes_loss: 0.1969
09/17 05:51:38 AM: Update 26468: task edges-coref-ontonotes, batch 468 (26468): mcc: 0.8190, acc: 0.9078, precision: 0.9095, recall: 0.9094, f1: 0.9095, edges-coref-ontonotes_loss: 0.1938
09/17 05:51:48 AM: Update 26684: task edges-coref-ontonotes, batch 684 (26684): mcc: 0.8063, acc: 0.9012, precision: 0.9032, recall: 0.9030, f1: 0.9031, edges-coref-ontonotes_loss: 0.2105
09/17 05:51:58 AM: Update 26900: task edges-coref-ontonotes, batch 900 (26900): mcc: 0.8004, acc: 0.8981, precision: 0.9003, recall: 0.9001, f1: 0.9002, edges-coref-ontonotes_loss: 0.2172
09/17 05:52:02 AM: ***** Step 27000 / Validation 27 *****
09/17 05:52:02 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:52:02 AM: Validating...
09/17 05:52:08 AM: Updating LR scheduler:
09/17 05:52:08 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:52:08 AM: 	# validation passes without improvement: 2
09/17 05:52:08 AM: edges-coref-ontonotes_loss: training: 0.216844 validation: 0.284180
09/17 05:52:08 AM: macro_avg: validation: 0.885520
09/17 05:52:08 AM: micro_avg: validation: 0.000000
09/17 05:52:08 AM: edges-coref-ontonotes_mcc: training: 0.800658 validation: 0.770983
09/17 05:52:08 AM: edges-coref-ontonotes_acc: training: 0.898274 validation: 0.884745
09/17 05:52:08 AM: edges-coref-ontonotes_precision: training: 0.900441 validation: 0.885300
09/17 05:52:08 AM: edges-coref-ontonotes_recall: training: 0.900189 validation: 0.885741
09/17 05:52:08 AM: edges-coref-ontonotes_f1: training: 0.900315 validation: 0.885520
09/17 05:52:08 AM: Global learning rate: 0.0001
09/17 05:52:08 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:52:08 AM: Update 27003: task edges-coref-ontonotes, batch 3 (27003): mcc: 0.8031, acc: 0.9007, precision: 0.9009, recall: 0.9024, f1: 0.9016, edges-coref-ontonotes_loss: 0.2357
09/17 05:52:18 AM: Update 27237: task edges-coref-ontonotes, batch 237 (27237): mcc: 0.8198, acc: 0.9081, precision: 0.9098, recall: 0.9100, f1: 0.9099, edges-coref-ontonotes_loss: 0.1891
09/17 05:52:28 AM: Update 27470: task edges-coref-ontonotes, batch 470 (27470): mcc: 0.8209, acc: 0.9087, precision: 0.9104, recall: 0.9105, f1: 0.9104, edges-coref-ontonotes_loss: 0.1835
09/17 05:52:41 AM: Update 27782: task edges-coref-ontonotes, batch 782 (27782): mcc: 0.8216, acc: 0.9091, precision: 0.9108, recall: 0.9108, f1: 0.9108, edges-coref-ontonotes_loss: 0.1865
09/17 05:52:48 AM: ***** Step 28000 / Validation 28 *****
09/17 05:52:48 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:52:48 AM: Validating...
09/17 05:52:51 AM: Evaluate: task edges-coref-ontonotes, batch 70 (157): mcc: 0.7773, acc: 0.8880, precision: 0.8889, recall: 0.8883, f1: 0.8886, edges-coref-ontonotes_loss: 0.2841
09/17 05:52:54 AM: Updating LR scheduler:
09/17 05:52:54 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:52:54 AM: 	# validation passes without improvement: 3
09/17 05:52:54 AM: edges-coref-ontonotes_loss: training: 0.200362 validation: 0.281356
09/17 05:52:54 AM: macro_avg: validation: 0.886425
09/17 05:52:54 AM: micro_avg: validation: 0.000000
09/17 05:52:54 AM: edges-coref-ontonotes_mcc: training: 0.812092 validation: 0.772898
09/17 05:52:54 AM: edges-coref-ontonotes_acc: training: 0.904228 validation: 0.885970
09/17 05:52:54 AM: edges-coref-ontonotes_precision: training: 0.906043 validation: 0.886612
09/17 05:52:54 AM: edges-coref-ontonotes_recall: training: 0.906049 validation: 0.886238
09/17 05:52:54 AM: edges-coref-ontonotes_f1: training: 0.906046 validation: 0.886425
09/17 05:52:54 AM: Global learning rate: 0.0001
09/17 05:52:54 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:53:01 AM: Update 28138: task edges-coref-ontonotes, batch 138 (28138): mcc: 0.7869, acc: 0.8914, precision: 0.8937, recall: 0.8932, f1: 0.8934, edges-coref-ontonotes_loss: 0.2350
09/17 05:53:12 AM: Update 28408: task edges-coref-ontonotes, batch 408 (28408): mcc: 0.7959, acc: 0.8962, precision: 0.8980, recall: 0.8979, f1: 0.8980, edges-coref-ontonotes_loss: 0.2237
09/17 05:53:22 AM: Update 28720: task edges-coref-ontonotes, batch 720 (28720): mcc: 0.8150, acc: 0.9059, precision: 0.9076, recall: 0.9074, f1: 0.9075, edges-coref-ontonotes_loss: 0.1944
09/17 05:53:32 AM: Update 28970: task edges-coref-ontonotes, batch 970 (28970): mcc: 0.8126, acc: 0.9047, precision: 0.9063, recall: 0.9063, f1: 0.9063, edges-coref-ontonotes_loss: 0.1970
09/17 05:53:33 AM: ***** Step 29000 / Validation 29 *****
09/17 05:53:33 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:53:33 AM: Validating...
09/17 05:53:39 AM: Updating LR scheduler:
09/17 05:53:39 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:53:39 AM: 	# validation passes without improvement: 0
09/17 05:53:39 AM: edges-coref-ontonotes_loss: training: 0.196622 validation: 0.281147
09/17 05:53:39 AM: macro_avg: validation: 0.890156
09/17 05:53:39 AM: micro_avg: validation: 0.000000
09/17 05:53:39 AM: edges-coref-ontonotes_mcc: training: 0.813283 validation: 0.780250
09/17 05:53:39 AM: edges-coref-ontonotes_acc: training: 0.905075 validation: 0.889684
09/17 05:53:39 AM: edges-coref-ontonotes_precision: training: 0.906670 validation: 0.889901
09/17 05:53:39 AM: edges-coref-ontonotes_recall: training: 0.906606 validation: 0.890412
09/17 05:53:39 AM: edges-coref-ontonotes_f1: training: 0.906638 validation: 0.890156
09/17 05:53:39 AM: Global learning rate: 5e-05
09/17 05:53:39 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:53:42 AM: Update 29087: task edges-coref-ontonotes, batch 87 (29087): mcc: 0.8202, acc: 0.9085, precision: 0.9102, recall: 0.9100, f1: 0.9101, edges-coref-ontonotes_loss: 0.1895
09/17 05:53:52 AM: Update 29321: task edges-coref-ontonotes, batch 321 (29321): mcc: 0.7958, acc: 0.8960, precision: 0.8979, recall: 0.8979, f1: 0.8979, edges-coref-ontonotes_loss: 0.2290
09/17 05:54:02 AM: Update 29554: task edges-coref-ontonotes, batch 554 (29554): mcc: 0.7939, acc: 0.8950, precision: 0.8969, recall: 0.8969, f1: 0.8969, edges-coref-ontonotes_loss: 0.2303
09/17 05:54:12 AM: Update 29795: task edges-coref-ontonotes, batch 795 (29795): mcc: 0.7979, acc: 0.8971, precision: 0.8990, recall: 0.8989, f1: 0.8989, edges-coref-ontonotes_loss: 0.2212
09/17 05:54:19 AM: ***** Step 30000 / Validation 30 *****
09/17 05:54:19 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:54:19 AM: Validating...
09/17 05:54:22 AM: Evaluate: task edges-coref-ontonotes, batch 81 (157): mcc: 0.7811, acc: 0.8903, precision: 0.8906, recall: 0.8906, f1: 0.8906, edges-coref-ontonotes_loss: 0.2937
09/17 05:54:25 AM: Updating LR scheduler:
09/17 05:54:25 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:54:25 AM: 	# validation passes without improvement: 1
09/17 05:54:25 AM: edges-coref-ontonotes_loss: training: 0.206591 validation: 0.282441
09/17 05:54:25 AM: macro_avg: validation: 0.890093
09/17 05:54:25 AM: micro_avg: validation: 0.000000
09/17 05:54:25 AM: edges-coref-ontonotes_mcc: training: 0.806987 validation: 0.780211
09/17 05:54:25 AM: edges-coref-ontonotes_acc: training: 0.901717 validation: 0.889761
09/17 05:54:25 AM: edges-coref-ontonotes_precision: training: 0.903511 validation: 0.890195
09/17 05:54:25 AM: edges-coref-ontonotes_recall: training: 0.903472 validation: 0.889991
09/17 05:54:25 AM: edges-coref-ontonotes_f1: training: 0.903491 validation: 0.890093
09/17 05:54:25 AM: Global learning rate: 5e-05
09/17 05:54:25 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:54:32 AM: Update 30118: task edges-coref-ontonotes, batch 118 (30118): mcc: 0.7943, acc: 0.8952, precision: 0.8974, recall: 0.8967, f1: 0.8971, edges-coref-ontonotes_loss: 0.2147
09/17 05:54:43 AM: Update 30398: task edges-coref-ontonotes, batch 398 (30398): mcc: 0.8161, acc: 0.9065, precision: 0.9083, recall: 0.9078, f1: 0.9080, edges-coref-ontonotes_loss: 0.1927
09/17 05:54:54 AM: Update 30691: task edges-coref-ontonotes, batch 691 (30691): mcc: 0.8034, acc: 0.9000, precision: 0.9018, recall: 0.9016, f1: 0.9017, edges-coref-ontonotes_loss: 0.2142
09/17 05:55:04 AM: Update 30910: task edges-coref-ontonotes, batch 910 (30910): mcc: 0.8015, acc: 0.8991, precision: 0.9008, recall: 0.9007, f1: 0.9008, edges-coref-ontonotes_loss: 0.2169
09/17 05:55:06 AM: ***** Step 31000 / Validation 31 *****
09/17 05:55:06 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:55:06 AM: Validating...
09/17 05:55:13 AM: Updating LR scheduler:
09/17 05:55:13 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:55:13 AM: 	# validation passes without improvement: 2
09/17 05:55:13 AM: edges-coref-ontonotes_loss: training: 0.216193 validation: 0.281747
09/17 05:55:13 AM: macro_avg: validation: 0.887055
09/17 05:55:13 AM: micro_avg: validation: 0.000000
09/17 05:55:13 AM: edges-coref-ontonotes_mcc: training: 0.801940 validation: 0.774162
09/17 05:55:13 AM: edges-coref-ontonotes_acc: training: 0.899272 validation: 0.886621
09/17 05:55:13 AM: edges-coref-ontonotes_precision: training: 0.901007 validation: 0.887259
09/17 05:55:13 AM: edges-coref-ontonotes_recall: training: 0.900924 validation: 0.886851
09/17 05:55:13 AM: edges-coref-ontonotes_f1: training: 0.900965 validation: 0.887055
09/17 05:55:13 AM: Global learning rate: 5e-05
09/17 05:55:13 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:55:16 AM: Update 31024: task edges-coref-ontonotes, batch 24 (31024): mcc: 0.7998, acc: 0.8979, precision: 0.8996, recall: 0.9003, f1: 0.8999, edges-coref-ontonotes_loss: 0.2090
09/17 05:55:26 AM: Update 31337: task edges-coref-ontonotes, batch 337 (31337): mcc: 0.8390, acc: 0.9182, precision: 0.9195, recall: 0.9195, f1: 0.9195, edges-coref-ontonotes_loss: 0.1594
09/17 05:55:36 AM: Update 31589: task edges-coref-ontonotes, batch 589 (31589): mcc: 0.8267, acc: 0.9120, precision: 0.9134, recall: 0.9133, f1: 0.9134, edges-coref-ontonotes_loss: 0.1775
09/17 05:55:46 AM: Update 31822: task edges-coref-ontonotes, batch 822 (31822): mcc: 0.8196, acc: 0.9083, precision: 0.9098, recall: 0.9098, f1: 0.9098, edges-coref-ontonotes_loss: 0.1883
09/17 05:55:52 AM: ***** Step 32000 / Validation 32 *****
09/17 05:55:52 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:55:52 AM: Validating...
09/17 05:55:56 AM: Evaluate: task edges-coref-ontonotes, batch 100 (157): mcc: 0.7695, acc: 0.8844, precision: 0.8847, recall: 0.8848, f1: 0.8848, edges-coref-ontonotes_loss: 0.2924
09/17 05:55:59 AM: Updating LR scheduler:
09/17 05:55:59 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:55:59 AM: 	# validation passes without improvement: 3
09/17 05:55:59 AM: edges-coref-ontonotes_loss: training: 0.197024 validation: 0.279817
09/17 05:55:59 AM: macro_avg: validation: 0.887140
09/17 05:55:59 AM: micro_avg: validation: 0.000000
09/17 05:55:59 AM: edges-coref-ontonotes_mcc: training: 0.813853 validation: 0.774276
09/17 05:55:59 AM: edges-coref-ontonotes_acc: training: 0.905439 validation: 0.886774
09/17 05:55:59 AM: edges-coref-ontonotes_precision: training: 0.906942 validation: 0.887123
09/17 05:55:59 AM: edges-coref-ontonotes_recall: training: 0.906907 validation: 0.887157
09/17 05:55:59 AM: edges-coref-ontonotes_f1: training: 0.906925 validation: 0.887140
09/17 05:55:59 AM: Global learning rate: 5e-05
09/17 05:55:59 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:56:06 AM: Update 32150: task edges-coref-ontonotes, batch 150 (32150): mcc: 0.7914, acc: 0.8937, precision: 0.8954, recall: 0.8960, f1: 0.8957, edges-coref-ontonotes_loss: 0.2260
09/17 05:56:16 AM: Update 32382: task edges-coref-ontonotes, batch 382 (32382): mcc: 0.7979, acc: 0.8972, precision: 0.8988, recall: 0.8992, f1: 0.8990, edges-coref-ontonotes_loss: 0.2136
09/17 05:56:28 AM: Update 32701: task edges-coref-ontonotes, batch 701 (32701): mcc: 0.8152, acc: 0.9061, precision: 0.9076, recall: 0.9077, f1: 0.9076, edges-coref-ontonotes_loss: 0.1918
09/17 05:56:37 AM: ***** Step 33000 / Validation 33 *****
09/17 05:56:37 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:56:37 AM: Validating...
09/17 05:56:38 AM: Evaluate: task edges-coref-ontonotes, batch 20 (157): mcc: 0.8242, acc: 0.9121, precision: 0.9121, recall: 0.9121, f1: 0.9121, edges-coref-ontonotes_loss: 0.2433
09/17 05:56:43 AM: Updating LR scheduler:
09/17 05:56:43 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:56:43 AM: 	# validation passes without improvement: 0
09/17 05:56:43 AM: edges-coref-ontonotes_loss: training: 0.189735 validation: 0.281066
09/17 05:56:43 AM: macro_avg: validation: 0.890063
09/17 05:56:43 AM: micro_avg: validation: 0.000000
09/17 05:56:43 AM: edges-coref-ontonotes_mcc: training: 0.818040 validation: 0.780135
09/17 05:56:43 AM: edges-coref-ontonotes_acc: training: 0.907630 validation: 0.889838
09/17 05:56:43 AM: edges-coref-ontonotes_precision: training: 0.909030 validation: 0.890097
09/17 05:56:43 AM: edges-coref-ontonotes_recall: training: 0.909007 validation: 0.890029
09/17 05:56:43 AM: edges-coref-ontonotes_f1: training: 0.909019 validation: 0.890063
09/17 05:56:43 AM: Global learning rate: 2.5e-05
09/17 05:56:43 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:56:48 AM: Update 33074: task edges-coref-ontonotes, batch 74 (33074): mcc: 0.7963, acc: 0.8967, precision: 0.8985, recall: 0.8977, f1: 0.8981, edges-coref-ontonotes_loss: 0.2318
09/17 05:56:58 AM: Update 33327: task edges-coref-ontonotes, batch 327 (33327): mcc: 0.7864, acc: 0.8915, precision: 0.8931, recall: 0.8933, f1: 0.8932, edges-coref-ontonotes_loss: 0.2377
09/17 05:57:08 AM: Update 33630: task edges-coref-ontonotes, batch 630 (33630): mcc: 0.7946, acc: 0.8956, precision: 0.8973, recall: 0.8973, f1: 0.8973, edges-coref-ontonotes_loss: 0.2277
09/17 05:57:18 AM: Update 33891: task edges-coref-ontonotes, batch 891 (33891): mcc: 0.8067, acc: 0.9017, precision: 0.9034, recall: 0.9034, f1: 0.9034, edges-coref-ontonotes_loss: 0.2074
09/17 05:57:22 AM: ***** Step 34000 / Validation 34 *****
09/17 05:57:22 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:57:22 AM: Validating...
09/17 05:57:29 AM: Evaluate: task edges-coref-ontonotes, batch 145 (157): mcc: 0.7790, acc: 0.8891, precision: 0.8894, recall: 0.8897, f1: 0.8895, edges-coref-ontonotes_loss: 0.2738
09/17 05:57:29 AM: Updating LR scheduler:
09/17 05:57:29 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:57:29 AM: 	# validation passes without improvement: 1
09/17 05:57:29 AM: edges-coref-ontonotes_loss: training: 0.205960 validation: 0.276047
09/17 05:57:29 AM: macro_avg: validation: 0.888191
09/17 05:57:29 AM: micro_avg: validation: 0.000000
09/17 05:57:29 AM: edges-coref-ontonotes_mcc: training: 0.806480 validation: 0.776344
09/17 05:57:29 AM: edges-coref-ontonotes_acc: training: 0.901577 validation: 0.887770
09/17 05:57:29 AM: edges-coref-ontonotes_precision: training: 0.903243 validation: 0.888038
09/17 05:57:29 AM: edges-coref-ontonotes_recall: training: 0.903237 validation: 0.888344
09/17 05:57:29 AM: edges-coref-ontonotes_f1: training: 0.903240 validation: 0.888191
09/17 05:57:29 AM: Global learning rate: 2.5e-05
09/17 05:57:29 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:57:39 AM: Update 34238: task edges-coref-ontonotes, batch 238 (34238): mcc: 0.8218, acc: 0.9095, precision: 0.9107, recall: 0.9111, f1: 0.9109, edges-coref-ontonotes_loss: 0.1863
09/17 05:57:49 AM: Update 34465: task edges-coref-ontonotes, batch 465 (34465): mcc: 0.8132, acc: 0.9051, precision: 0.9065, recall: 0.9067, f1: 0.9066, edges-coref-ontonotes_loss: 0.2019
09/17 05:57:59 AM: Update 34673: task edges-coref-ontonotes, batch 673 (34673): mcc: 0.8059, acc: 0.9013, precision: 0.9029, recall: 0.9030, f1: 0.9029, edges-coref-ontonotes_loss: 0.2125
09/17 05:58:11 AM: Update 34948: task edges-coref-ontonotes, batch 948 (34948): mcc: 0.8051, acc: 0.9009, precision: 0.9025, recall: 0.9026, f1: 0.9026, edges-coref-ontonotes_loss: 0.2127
09/17 05:58:12 AM: ***** Step 35000 / Validation 35 *****
09/17 05:58:12 AM: edges-coref-ontonotes: trained on 1000 batches, 0.766 epochs
09/17 05:58:12 AM: Validating...
09/17 05:58:19 AM: Updating LR scheduler:
09/17 05:58:19 AM: 	Best result seen so far for macro_avg: 0.890
09/17 05:58:19 AM: 	# validation passes without improvement: 2
09/17 05:58:19 AM: Ran out of early stopping patience. Stopping training.
09/17 05:58:19 AM: edges-coref-ontonotes_loss: training: 0.210209 validation: 0.280076
09/17 05:58:19 AM: macro_avg: validation: 0.890097
09/17 05:58:19 AM: micro_avg: validation: 0.000000
09/17 05:58:19 AM: edges-coref-ontonotes_mcc: training: 0.805840 validation: 0.780211
09/17 05:58:19 AM: edges-coref-ontonotes_acc: training: 0.901329 validation: 0.889646
09/17 05:58:19 AM: edges-coref-ontonotes_precision: training: 0.902868 validation: 0.890165
09/17 05:58:19 AM: edges-coref-ontonotes_recall: training: 0.902985 validation: 0.890029
09/17 05:58:19 AM: edges-coref-ontonotes_f1: training: 0.902926 validation: 0.890097
09/17 05:58:19 AM: Global learning rate: 2.5e-05
09/17 05:58:19 AM: Saving checkpoints to: ./experiments/coref-ontonotes-mnli-only/run
09/17 05:58:19 AM: Stopped training after 35 validation checks
09/17 05:58:19 AM: Trained edges-coref-ontonotes for 35000 batches or 26.799 epochs
09/17 05:58:19 AM: ***** VALIDATION RESULTS *****
09/17 05:58:19 AM: edges-coref-ontonotes_f1 (for best val pass 25): edges-coref-ontonotes_loss: 0.28082, macro_avg: 0.89022, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.78044, edges-coref-ontonotes_acc: 0.88972, edges-coref-ontonotes_precision: 0.89019, edges-coref-ontonotes_recall: 0.89026, edges-coref-ontonotes_f1: 0.89022
09/17 05:58:19 AM: micro_avg (for best val pass 1): edges-coref-ontonotes_loss: 0.37781, macro_avg: 0.85447, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.71004, edges-coref-ontonotes_acc: 0.85090, edges-coref-ontonotes_precision: 0.85768, edges-coref-ontonotes_recall: 0.85128, edges-coref-ontonotes_f1: 0.85447
09/17 05:58:19 AM: macro_avg (for best val pass 25): edges-coref-ontonotes_loss: 0.28082, macro_avg: 0.89022, micro_avg: 0.00000, edges-coref-ontonotes_mcc: 0.78044, edges-coref-ontonotes_acc: 0.88972, edges-coref-ontonotes_precision: 0.89019, edges-coref-ontonotes_recall: 0.89026, edges-coref-ontonotes_f1: 0.89022
09/17 05:58:19 AM: Evaluating...
09/17 05:58:19 AM: Loaded model state from ./experiments/coref-ontonotes-mnli-only/run/edges-coref-ontonotes/model_state_target_train_val_25.best.th
09/17 05:58:19 AM: Evaluating on: edges-coref-ontonotes, split: val
09/17 05:58:25 AM: Task 'edges-coref-ontonotes': sorting predictions by 'idx'
09/17 05:58:25 AM: Finished evaluating on: edges-coref-ontonotes
09/17 05:58:25 AM: Task 'edges-coref-ontonotes': joining predictions with input split 'val'
09/17 05:58:26 AM: Task 'edges-coref-ontonotes': Wrote predictions to ./experiments/coref-ontonotes-mnli-only/run
09/17 05:58:26 AM: Wrote all preds for split 'val' to ./experiments/coref-ontonotes-mnli-only/run
09/17 05:58:26 AM: Evaluating on: edges-coref-ontonotes, split: test
09/17 05:58:33 AM: Task 'edges-coref-ontonotes': sorting predictions by 'idx'
09/17 05:58:33 AM: Finished evaluating on: edges-coref-ontonotes
09/17 05:58:33 AM: Task 'edges-coref-ontonotes': joining predictions with input split 'test'
09/17 05:58:33 AM: Task 'edges-coref-ontonotes': Wrote predictions to ./experiments/coref-ontonotes-mnli-only/run
09/17 05:58:33 AM: Wrote all preds for split 'test' to ./experiments/coref-ontonotes-mnli-only/run
09/17 05:58:33 AM: Writing results for split 'val' to ./experiments/coref-ontonotes-mnli-only/results.tsv
09/17 05:58:33 AM: micro_avg: 0.000, macro_avg: 0.891, edges-coref-ontonotes_mcc: 0.781, edges-coref-ontonotes_acc: 0.890, edges-coref-ontonotes_precision: 0.890, edges-coref-ontonotes_recall: 0.891, edges-coref-ontonotes_f1: 0.891
09/17 05:58:33 AM: Done!
09/17 05:58:33 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
