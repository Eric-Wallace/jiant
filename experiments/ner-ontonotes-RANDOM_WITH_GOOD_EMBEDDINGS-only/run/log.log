09/16 12:14:42 PM: Git branch: master
09/16 12:14:42 PM: Git SHA: 93c1dfd555f3458ddbb66d458dfeca984f2d8527
09/16 12:14:42 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/",
  "exp_name": "experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/RANDOM_WITH_GOOD_EMBEDDINGS",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only__run",
  "run_dir": "./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 12:14:42 PM: Saved config to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run/params.conf
09/16 12:14:42 PM: Using random seed 1234
09/16 12:15:22 PM: Using GPU 0
09/16 12:15:22 PM: Loading tasks...
09/16 12:15:22 PM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/
09/16 12:15:22 PM: 	Creating task edges-ner-ontonotes from scratch.
09/16 12:15:23 PM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/16 12:15:24 PM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/16 12:15:24 PM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/16 12:15:24 PM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/16 12:15:24 PM: 	Finished loading tasks: edges-ner-ontonotes.
09/16 12:15:24 PM: 	Building vocab from scratch.
09/16 12:15:24 PM: 	Counting units for task edges-ner-ontonotes.
09/16 12:15:27 PM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/16 12:15:28 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:15:28 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 12:15:28 PM: 	Saved vocab to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/vocab
09/16 12:15:28 PM: Loading token dictionary from ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/vocab.
09/16 12:15:28 PM: 	Loaded vocab from ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/vocab
09/16 12:15:28 PM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/16 12:15:28 PM: 	Vocab namespace tokens: size 22840
09/16 12:15:28 PM: 	Vocab namespace bert_uncased: size 30524
09/16 12:15:28 PM: 	Vocab namespace chars: size 77
09/16 12:15:28 PM: 	Finished building vocab.
09/16 12:15:28 PM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/16 12:15:43 PM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/preproc/edges-ner-ontonotes__train_data
09/16 12:15:43 PM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/16 12:15:44 PM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/preproc/edges-ner-ontonotes__val_data
09/16 12:15:44 PM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/16 12:15:45 PM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/preproc/edges-ner-ontonotes__test_data
09/16 12:15:45 PM: 	Finished indexing tasks
09/16 12:15:45 PM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/16 12:15:45 PM: 	  Training on 
09/16 12:15:45 PM: 	  Evaluating on edges-ner-ontonotes
09/16 12:15:45 PM: 	Finished loading tasks in 23.814s
09/16 12:15:45 PM: 	 Tasks: ['edges-ner-ontonotes']
09/16 12:15:45 PM: Building model...
09/16 12:15:45 PM: Using BERT model (bert-base-uncased).
09/16 12:15:45 PM: LOADING A RANDOMLY WEIGHTS BERT
09/16 12:15:49 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp3rur2txg
09/16 12:15:50 PM: copying /tmp/tmp3rur2txg to cache at ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:50 PM: creating metadata file for ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:50 PM: removing temp file /tmp/tmp3rur2txg
09/16 12:15:50 PM: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
09/16 12:15:50 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 12:15:54 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpd5u16i0d
09/16 12:26:34 PM: copying /tmp/tmpd5u16i0d to cache at ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:26:35 PM: creating metadata file for ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:26:35 PM: removing temp file /tmp/tmpd5u16i0d
09/16 12:26:35 PM: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
09/16 12:26:48 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpzh3e9_mc
09/16 12:26:50 PM: copying /tmp/tmpzh3e9_mc to cache at ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:26:50 PM: creating metadata file for ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:26:50 PM: removing temp file /tmp/tmpzh3e9_mc
09/16 12:26:50 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 12:26:50 PM: Initializing parameters
09/16 12:26:50 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 12:26:50 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 12:26:50 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 12:26:50 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 12:26:50 PM:    _text_field_embedder.model.pooler.dense.bias
09/16 12:26:50 PM:    _text_field_embedder.model.pooler.dense.weight
09/16 12:26:50 PM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/16 12:27:31 PM: Model specification:
09/16 12:27:31 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/16 12:27:31 PM: Model parameters:
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 12:27:31 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 12:27:31 PM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 12:27:31 PM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 12:27:31 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/16 12:27:31 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/16 12:27:31 PM: Total number of parameters: 109688338 (1.09688e+08)
09/16 12:27:31 PM: Number of trainable parameters: 206098 (206098)
09/16 12:27:31 PM: Finished building model in 705.716s
09/16 12:27:31 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/16 12:27:41 PM: patience = 9
09/16 12:27:41 PM: val_interval = 1000
09/16 12:27:41 PM: max_vals = 250
09/16 12:27:41 PM: cuda_device = 0
09/16 12:27:41 PM: grad_norm = 5.0
09/16 12:27:41 PM: grad_clipping = None
09/16 12:27:41 PM: lr_decay = 0.99
09/16 12:27:41 PM: min_lr = 1e-06
09/16 12:27:41 PM: keep_all_checkpoints = 0
09/16 12:27:41 PM: val_data_limit = 5000
09/16 12:27:41 PM: max_epochs = -1
09/16 12:27:41 PM: dec_val_scale = 250
09/16 12:27:41 PM: training_data_fraction = 1
09/16 12:27:43 PM: type = adam
09/16 12:27:43 PM: parameter_groups = None
09/16 12:27:43 PM: Number of trainable parameters: 206098
09/16 12:27:43 PM: infer_type_and_cast = True
09/16 12:27:43 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:27:43 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:27:43 PM: lr = 0.0001
09/16 12:27:43 PM: amsgrad = True
09/16 12:27:43 PM: type = reduce_on_plateau
09/16 12:27:43 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:27:43 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:27:43 PM: mode = max
09/16 12:27:43 PM: factor = 0.5
09/16 12:27:43 PM: patience = 3
09/16 12:27:43 PM: threshold = 0.0001
09/16 12:27:43 PM: threshold_mode = abs
09/16 12:27:43 PM: verbose = True
09/16 12:27:43 PM: type = adam
09/16 12:27:43 PM: parameter_groups = None
09/16 12:27:43 PM: Number of trainable parameters: 206098
09/16 12:27:43 PM: infer_type_and_cast = True
09/16 12:27:43 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:27:43 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:27:43 PM: lr = 0.0001
09/16 12:27:43 PM: amsgrad = True
09/16 12:27:43 PM: type = reduce_on_plateau
09/16 12:27:43 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 12:27:43 PM: CURRENTLY DEFINED PARAMETERS: 
09/16 12:27:43 PM: mode = max
09/16 12:27:43 PM: factor = 0.5
09/16 12:27:43 PM: patience = 3
09/16 12:27:43 PM: threshold = 0.0001
09/16 12:27:43 PM: threshold_mode = abs
09/16 12:27:43 PM: verbose = True
09/16 12:27:43 PM: Starting training without restoring from a checkpoint.
09/16 12:27:43 PM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/16 12:27:43 PM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/16 12:27:53 PM: Update 63: task edges-ner-ontonotes, batch 63 (63): mcc: 0.0945, acc: 0.0554, precision: 0.0841, recall: 0.5529, f1: 0.1460, edges-ner-ontonotes_loss: 0.6283
09/16 12:28:03 PM: Update 221: task edges-ner-ontonotes, batch 221 (221): mcc: 0.1636, acc: 0.2029, precision: 0.1249, recall: 0.5086, f1: 0.2005, edges-ner-ontonotes_loss: 0.4822
09/16 12:28:20 PM: Update 314: task edges-ner-ontonotes, batch 314 (314): mcc: 0.1822, acc: 0.2357, precision: 0.1407, recall: 0.4903, f1: 0.2187, edges-ner-ontonotes_loss: 0.4299
09/16 12:28:30 PM: Update 388: task edges-ner-ontonotes, batch 388 (388): mcc: 0.1832, acc: 0.2406, precision: 0.1449, recall: 0.4715, f1: 0.2216, edges-ner-ontonotes_loss: 0.4122
09/16 12:28:40 PM: Update 501: task edges-ner-ontonotes, batch 501 (501): mcc: 0.2019, acc: 0.2659, precision: 0.1617, recall: 0.4646, f1: 0.2399, edges-ner-ontonotes_loss: 0.3794
09/16 12:28:50 PM: Update 607: task edges-ner-ontonotes, batch 607 (607): mcc: 0.2231, acc: 0.2924, precision: 0.1801, recall: 0.4672, f1: 0.2600, edges-ner-ontonotes_loss: 0.3521
09/16 12:29:01 PM: Update 659: task edges-ner-ontonotes, batch 659 (659): mcc: 0.2286, acc: 0.2986, precision: 0.1857, recall: 0.4658, f1: 0.2655, edges-ner-ontonotes_loss: 0.3440
09/16 12:29:11 PM: Update 763: task edges-ner-ontonotes, batch 763 (763): mcc: 0.2411, acc: 0.3104, precision: 0.1978, recall: 0.4664, f1: 0.2777, edges-ner-ontonotes_loss: 0.3290
09/16 12:29:21 PM: Update 893: task edges-ner-ontonotes, batch 893 (893): mcc: 0.2597, acc: 0.3272, precision: 0.2160, recall: 0.4699, f1: 0.2960, edges-ner-ontonotes_loss: 0.3088
09/16 12:29:31 PM: Update 940: task edges-ner-ontonotes, batch 940 (940): mcc: 0.2671, acc: 0.3336, precision: 0.2234, recall: 0.4716, f1: 0.3032, edges-ner-ontonotes_loss: 0.3020
09/16 12:29:34 PM: ***** Step 1000 / Validation 1 *****
09/16 12:29:34 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:29:34 PM: Validating...
09/16 12:29:41 PM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.4375, acc: 0.4108, precision: 0.4730, recall: 0.4639, f1: 0.4684, edges-ner-ontonotes_loss: 0.1844
09/16 12:29:51 PM: Evaluate: task edges-ner-ontonotes, batch 126 (157): mcc: 0.4693, acc: 0.4445, precision: 0.5043, recall: 0.4925, f1: 0.4984, edges-ner-ontonotes_loss: 0.1712
09/16 12:29:55 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:29:55 PM: Best result seen so far for micro.
09/16 12:29:55 PM: Best result seen so far for macro.
09/16 12:29:55 PM: Updating LR scheduler:
09/16 12:29:55 PM: 	Best result seen so far for macro_avg: 0.509
09/16 12:29:55 PM: 	# validation passes without improvement: 0
09/16 12:29:55 PM: edges-ner-ontonotes_loss: training: 0.294418 validation: 0.166540
09/16 12:29:55 PM: macro_avg: validation: 0.509321
09/16 12:29:55 PM: micro_avg: validation: 0.000000
09/16 12:29:55 PM: edges-ner-ontonotes_mcc: training: 0.275094 validation: 0.481654
09/16 12:29:55 PM: edges-ner-ontonotes_acc: training: 0.340502 validation: 0.448135
09/16 12:29:55 PM: edges-ner-ontonotes_precision: training: 0.231728 validation: 0.525526
09/16 12:29:55 PM: edges-ner-ontonotes_recall: training: 0.473104 validation: 0.494086
09/16 12:29:55 PM: edges-ner-ontonotes_f1: training: 0.311086 validation: 0.509321
09/16 12:29:55 PM: Global learning rate: 0.0001
09/16 12:29:55 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:30:01 PM: Update 1066: task edges-ner-ontonotes, batch 66 (1066): mcc: 0.4867, acc: 0.4515, precision: 0.5272, recall: 0.5022, f1: 0.5144, edges-ner-ontonotes_loss: 0.1594
09/16 12:30:11 PM: Update 1171: task edges-ner-ontonotes, batch 171 (1171): mcc: 0.5176, acc: 0.4662, precision: 0.5774, recall: 0.5101, f1: 0.5417, edges-ner-ontonotes_loss: 0.1482
09/16 12:30:21 PM: Update 1347: task edges-ner-ontonotes, batch 347 (1347): mcc: 0.5151, acc: 0.4480, precision: 0.5957, recall: 0.4884, f1: 0.5367, edges-ner-ontonotes_loss: 0.1484
09/16 12:30:31 PM: Update 1495: task edges-ner-ontonotes, batch 495 (1495): mcc: 0.5269, acc: 0.4461, precision: 0.6283, recall: 0.4807, f1: 0.5447, edges-ner-ontonotes_loss: 0.1434
09/16 12:30:42 PM: Update 1638: task edges-ner-ontonotes, batch 638 (1638): mcc: 0.5432, acc: 0.4541, precision: 0.6576, recall: 0.4848, f1: 0.5581, edges-ner-ontonotes_loss: 0.1374
09/16 12:30:52 PM: Update 1839: task edges-ner-ontonotes, batch 839 (1839): mcc: 0.5712, acc: 0.4766, precision: 0.6953, recall: 0.5025, f1: 0.5834, edges-ner-ontonotes_loss: 0.1281
09/16 12:31:02 PM: Update 1892: task edges-ner-ontonotes, batch 892 (1892): mcc: 0.5774, acc: 0.4811, precision: 0.7040, recall: 0.5061, f1: 0.5888, edges-ner-ontonotes_loss: 0.1262
09/16 12:31:11 PM: ***** Step 2000 / Validation 2 *****
09/16 12:31:11 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:31:11 PM: Validating...
09/16 12:31:12 PM: Evaluate: task edges-ner-ontonotes, batch 14 (157): mcc: 0.6084, acc: 0.4904, precision: 0.7839, recall: 0.4983, f1: 0.6093, edges-ner-ontonotes_loss: 0.1185
09/16 12:31:20 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:31:20 PM: Best result seen so far for macro.
09/16 12:31:20 PM: Updating LR scheduler:
09/16 12:31:20 PM: 	Best result seen so far for macro_avg: 0.654
09/16 12:31:20 PM: 	# validation passes without improvement: 0
09/16 12:31:20 PM: edges-ner-ontonotes_loss: training: 0.123032 validation: 0.099782
09/16 12:31:20 PM: macro_avg: validation: 0.654109
09/16 12:31:20 PM: micro_avg: validation: 0.000000
09/16 12:31:20 PM: edges-ner-ontonotes_mcc: training: 0.587941 validation: 0.656731
09/16 12:31:20 PM: edges-ner-ontonotes_acc: training: 0.489410 validation: 0.527980
09/16 12:31:20 PM: edges-ner-ontonotes_precision: training: 0.718242 validation: 0.842678
09/16 12:31:20 PM: edges-ner-ontonotes_recall: training: 0.512770 validation: 0.534501
09/16 12:31:20 PM: edges-ner-ontonotes_f1: training: 0.598358 validation: 0.654109
09/16 12:31:20 PM: Global learning rate: 0.0001
09/16 12:31:20 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:31:22 PM: Update 2012: task edges-ner-ontonotes, batch 12 (2012): mcc: 0.6787, acc: 0.5574, precision: 0.8459, recall: 0.5672, f1: 0.6791, edges-ner-ontonotes_loss: 0.0927
09/16 12:31:32 PM: Update 2131: task edges-ner-ontonotes, batch 131 (2131): mcc: 0.7016, acc: 0.5826, precision: 0.8619, recall: 0.5927, f1: 0.7024, edges-ner-ontonotes_loss: 0.0890
09/16 12:31:44 PM: Update 2183: task edges-ner-ontonotes, batch 183 (2183): mcc: 0.7037, acc: 0.5865, precision: 0.8619, recall: 0.5961, f1: 0.7048, edges-ner-ontonotes_loss: 0.0882
09/16 12:31:54 PM: Update 2266: task edges-ner-ontonotes, batch 266 (2266): mcc: 0.6889, acc: 0.5680, precision: 0.8505, recall: 0.5804, f1: 0.6899, edges-ner-ontonotes_loss: 0.0907
09/16 12:32:04 PM: Update 2366: task edges-ner-ontonotes, batch 366 (2366): mcc: 0.6847, acc: 0.5617, precision: 0.8480, recall: 0.5753, f1: 0.6855, edges-ner-ontonotes_loss: 0.0910
09/16 12:32:14 PM: Update 2462: task edges-ner-ontonotes, batch 462 (2462): mcc: 0.6879, acc: 0.5649, precision: 0.8496, recall: 0.5794, f1: 0.6890, edges-ner-ontonotes_loss: 0.0898
09/16 12:32:24 PM: Update 2496: task edges-ner-ontonotes, batch 496 (2496): mcc: 0.6889, acc: 0.5668, precision: 0.8487, recall: 0.5817, f1: 0.6903, edges-ner-ontonotes_loss: 0.0894
09/16 12:32:34 PM: Update 2598: task edges-ner-ontonotes, batch 598 (2598): mcc: 0.6880, acc: 0.5662, precision: 0.8461, recall: 0.5820, f1: 0.6897, edges-ner-ontonotes_loss: 0.0893
09/16 12:32:44 PM: Update 2743: task edges-ner-ontonotes, batch 743 (2743): mcc: 0.6901, acc: 0.5688, precision: 0.8468, recall: 0.5850, f1: 0.6919, edges-ner-ontonotes_loss: 0.0883
09/16 12:32:56 PM: Update 2809: task edges-ner-ontonotes, batch 809 (2809): mcc: 0.6926, acc: 0.5719, precision: 0.8478, recall: 0.5883, f1: 0.6946, edges-ner-ontonotes_loss: 0.0875
09/16 12:33:06 PM: Update 2929: task edges-ner-ontonotes, batch 929 (2929): mcc: 0.6859, acc: 0.5650, precision: 0.8433, recall: 0.5808, f1: 0.6879, edges-ner-ontonotes_loss: 0.0894
09/16 12:33:11 PM: ***** Step 3000 / Validation 3 *****
09/16 12:33:11 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:33:11 PM: Validating...
09/16 12:33:16 PM: Evaluate: task edges-ner-ontonotes, batch 74 (157): mcc: 0.6808, acc: 0.5607, precision: 0.8554, recall: 0.5638, f1: 0.6797, edges-ner-ontonotes_loss: 0.0943
09/16 12:33:25 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:33:28 PM: Best result seen so far for macro.
09/16 12:33:28 PM: Updating LR scheduler:
09/16 12:33:28 PM: 	Best result seen so far for macro_avg: 0.689
09/16 12:33:28 PM: 	# validation passes without improvement: 0
09/16 12:33:28 PM: edges-ner-ontonotes_loss: training: 0.089862 validation: 0.088371
09/16 12:33:28 PM: macro_avg: validation: 0.689122
09/16 12:33:28 PM: micro_avg: validation: 0.000000
09/16 12:33:28 PM: edges-ner-ontonotes_mcc: training: 0.685629 validation: 0.691209
09/16 12:33:28 PM: edges-ner-ontonotes_acc: training: 0.564619 validation: 0.565211
09/16 12:33:28 PM: edges-ner-ontonotes_precision: training: 0.843586 validation: 0.869098
09/16 12:33:28 PM: edges-ner-ontonotes_recall: training: 0.580073 validation: 0.570898
09/16 12:33:28 PM: edges-ner-ontonotes_f1: training: 0.687442 validation: 0.689122
09/16 12:33:28 PM: Global learning rate: 0.0001
09/16 12:33:28 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:33:28 PM: Update 3001: task edges-ner-ontonotes, batch 1 (3001): mcc: 0.6058, acc: 0.4925, precision: 0.7447, recall: 0.5224, f1: 0.6140, edges-ner-ontonotes_loss: 0.1136
09/16 12:33:38 PM: Update 3108: task edges-ner-ontonotes, batch 108 (3108): mcc: 0.6736, acc: 0.5549, precision: 0.8366, recall: 0.5655, f1: 0.6749, edges-ner-ontonotes_loss: 0.0950
09/16 12:33:48 PM: Update 3150: task edges-ner-ontonotes, batch 150 (3150): mcc: 0.6840, acc: 0.5682, precision: 0.8411, recall: 0.5792, f1: 0.6860, edges-ner-ontonotes_loss: 0.0925
09/16 12:33:58 PM: Update 3275: task edges-ner-ontonotes, batch 275 (3275): mcc: 0.7059, acc: 0.5925, precision: 0.8573, recall: 0.6032, f1: 0.7081, edges-ner-ontonotes_loss: 0.0865
09/16 12:34:08 PM: Update 3383: task edges-ner-ontonotes, batch 383 (3383): mcc: 0.7175, acc: 0.6071, precision: 0.8646, recall: 0.6169, f1: 0.7200, edges-ner-ontonotes_loss: 0.0838
09/16 12:34:18 PM: Update 3490: task edges-ner-ontonotes, batch 490 (3490): mcc: 0.7198, acc: 0.6108, precision: 0.8639, recall: 0.6211, f1: 0.7227, edges-ner-ontonotes_loss: 0.0825
09/16 12:34:28 PM: Update 3594: task edges-ner-ontonotes, batch 594 (3594): mcc: 0.7208, acc: 0.6137, precision: 0.8624, recall: 0.6240, f1: 0.7241, edges-ner-ontonotes_loss: 0.0821
09/16 12:34:38 PM: Update 3707: task edges-ner-ontonotes, batch 707 (3707): mcc: 0.7244, acc: 0.6183, precision: 0.8639, recall: 0.6287, f1: 0.7278, edges-ner-ontonotes_loss: 0.0812
09/16 12:34:48 PM: Update 3751: task edges-ner-ontonotes, batch 751 (3751): mcc: 0.7245, acc: 0.6188, precision: 0.8632, recall: 0.6294, f1: 0.7280, edges-ner-ontonotes_loss: 0.0809
09/16 12:34:58 PM: Update 3851: task edges-ner-ontonotes, batch 851 (3851): mcc: 0.7219, acc: 0.6150, precision: 0.8607, recall: 0.6270, f1: 0.7255, edges-ner-ontonotes_loss: 0.0809
09/16 12:35:08 PM: Update 3949: task edges-ner-ontonotes, batch 949 (3949): mcc: 0.7219, acc: 0.6143, precision: 0.8605, recall: 0.6272, f1: 0.7256, edges-ner-ontonotes_loss: 0.0806
09/16 12:35:14 PM: ***** Step 4000 / Validation 4 *****
09/16 12:35:14 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:35:14 PM: Validating...
09/16 12:35:18 PM: Evaluate: task edges-ner-ontonotes, batch 75 (157): mcc: 0.6863, acc: 0.5924, precision: 0.8145, recall: 0.6031, f1: 0.6930, edges-ner-ontonotes_loss: 0.0910
09/16 12:35:25 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:35:25 PM: Best result seen so far for macro.
09/16 12:35:25 PM: Updating LR scheduler:
09/16 12:35:25 PM: 	Best result seen so far for macro_avg: 0.713
09/16 12:35:25 PM: 	# validation passes without improvement: 0
09/16 12:35:25 PM: edges-ner-ontonotes_loss: training: 0.080280 validation: 0.083919
09/16 12:35:25 PM: macro_avg: validation: 0.712659
09/16 12:35:25 PM: micro_avg: validation: 0.000000
09/16 12:35:25 PM: edges-ner-ontonotes_mcc: training: 0.722293 validation: 0.707365
09/16 12:35:25 PM: edges-ner-ontonotes_acc: training: 0.614734 validation: 0.607749
09/16 12:35:25 PM: edges-ner-ontonotes_precision: training: 0.860204 validation: 0.839346
09/16 12:35:25 PM: edges-ner-ontonotes_recall: training: 0.628071 validation: 0.619199
09/16 12:35:25 PM: edges-ner-ontonotes_f1: training: 0.726034 validation: 0.712659
09/16 12:35:25 PM: Global learning rate: 0.0001
09/16 12:35:25 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:35:28 PM: Update 4033: task edges-ner-ontonotes, batch 33 (4033): mcc: 0.7190, acc: 0.6084, precision: 0.8475, recall: 0.6324, f1: 0.7243, edges-ner-ontonotes_loss: 0.0768
09/16 12:35:38 PM: Update 4127: task edges-ner-ontonotes, batch 127 (4127): mcc: 0.7209, acc: 0.6153, precision: 0.8436, recall: 0.6388, f1: 0.7270, edges-ner-ontonotes_loss: 0.0766
09/16 12:35:48 PM: Update 4301: task edges-ner-ontonotes, batch 301 (4301): mcc: 0.7256, acc: 0.6193, precision: 0.8491, recall: 0.6424, f1: 0.7314, edges-ner-ontonotes_loss: 0.0756
09/16 12:35:58 PM: Update 4423: task edges-ner-ontonotes, batch 423 (4423): mcc: 0.7191, acc: 0.6130, precision: 0.8443, recall: 0.6351, f1: 0.7249, edges-ner-ontonotes_loss: 0.0783
09/16 12:36:08 PM: Update 4651: task edges-ner-ontonotes, batch 651 (4651): mcc: 0.7103, acc: 0.6035, precision: 0.8411, recall: 0.6229, f1: 0.7157, edges-ner-ontonotes_loss: 0.0824
09/16 12:36:18 PM: Update 4711: task edges-ner-ontonotes, batch 711 (4711): mcc: 0.7115, acc: 0.6049, precision: 0.8424, recall: 0.6238, f1: 0.7168, edges-ner-ontonotes_loss: 0.0822
09/16 12:36:28 PM: Update 4907: task edges-ner-ontonotes, batch 907 (4907): mcc: 0.7203, acc: 0.6158, precision: 0.8490, recall: 0.6334, f1: 0.7255, edges-ner-ontonotes_loss: 0.0801
09/16 12:36:39 PM: Update 4982: task edges-ner-ontonotes, batch 982 (4982): mcc: 0.7231, acc: 0.6194, precision: 0.8509, recall: 0.6366, f1: 0.7283, edges-ner-ontonotes_loss: 0.0793
09/16 12:36:39 PM: ***** Step 5000 / Validation 5 *****
09/16 12:36:39 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:36:39 PM: Validating...
09/16 12:36:48 PM: Updating LR scheduler:
09/16 12:36:48 PM: 	Best result seen so far for macro_avg: 0.713
09/16 12:36:48 PM: 	# validation passes without improvement: 1
09/16 12:36:48 PM: edges-ner-ontonotes_loss: training: 0.079338 validation: 0.082215
09/16 12:36:48 PM: macro_avg: validation: 0.708268
09/16 12:36:48 PM: micro_avg: validation: 0.000000
09/16 12:36:48 PM: edges-ner-ontonotes_mcc: training: 0.723080 validation: 0.706339
09/16 12:36:48 PM: edges-ner-ontonotes_acc: training: 0.619425 validation: 0.597589
09/16 12:36:48 PM: edges-ner-ontonotes_precision: training: 0.851035 validation: 0.859198
09/16 12:36:48 PM: edges-ner-ontonotes_recall: training: 0.636513 validation: 0.602442
09/16 12:36:48 PM: edges-ner-ontonotes_f1: training: 0.728306 validation: 0.708268
09/16 12:36:48 PM: Global learning rate: 0.0001
09/16 12:36:48 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:36:49 PM: Update 5010: task edges-ner-ontonotes, batch 10 (5010): mcc: 0.6738, acc: 0.5707, precision: 0.8074, recall: 0.5878, f1: 0.6803, edges-ner-ontonotes_loss: 0.0855
09/16 12:36:59 PM: Update 5106: task edges-ner-ontonotes, batch 106 (5106): mcc: 0.7232, acc: 0.6254, precision: 0.8443, recall: 0.6420, f1: 0.7294, edges-ner-ontonotes_loss: 0.0770
09/16 12:37:11 PM: Update 5295: task edges-ner-ontonotes, batch 295 (5295): mcc: 0.7454, acc: 0.6515, precision: 0.8611, recall: 0.6663, f1: 0.7513, edges-ner-ontonotes_loss: 0.0738
09/16 12:37:21 PM: Update 5435: task edges-ner-ontonotes, batch 435 (5435): mcc: 0.7365, acc: 0.6381, precision: 0.8567, recall: 0.6548, f1: 0.7423, edges-ner-ontonotes_loss: 0.0748
09/16 12:37:31 PM: Update 5547: task edges-ner-ontonotes, batch 547 (5547): mcc: 0.7368, acc: 0.6379, precision: 0.8559, recall: 0.6559, f1: 0.7427, edges-ner-ontonotes_loss: 0.0744
09/16 12:37:45 PM: Update 5608: task edges-ner-ontonotes, batch 608 (5608): mcc: 0.7373, acc: 0.6384, precision: 0.8552, recall: 0.6573, f1: 0.7433, edges-ner-ontonotes_loss: 0.0740
09/16 12:37:55 PM: Update 5732: task edges-ner-ontonotes, batch 732 (5732): mcc: 0.7357, acc: 0.6366, precision: 0.8520, recall: 0.6571, f1: 0.7420, edges-ner-ontonotes_loss: 0.0739
09/16 12:38:05 PM: Update 5841: task edges-ner-ontonotes, batch 841 (5841): mcc: 0.7373, acc: 0.6378, precision: 0.8526, recall: 0.6593, f1: 0.7436, edges-ner-ontonotes_loss: 0.0735
09/16 12:38:16 PM: Update 5921: task edges-ner-ontonotes, batch 921 (5921): mcc: 0.7376, acc: 0.6380, precision: 0.8527, recall: 0.6599, f1: 0.7440, edges-ner-ontonotes_loss: 0.0734
09/16 12:38:22 PM: ***** Step 6000 / Validation 6 *****
09/16 12:38:22 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:38:22 PM: Validating...
09/16 12:38:26 PM: Evaluate: task edges-ner-ontonotes, batch 47 (157): mcc: 0.6941, acc: 0.5989, precision: 0.8297, recall: 0.6043, f1: 0.6993, edges-ner-ontonotes_loss: 0.0886
09/16 12:38:34 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:38:34 PM: Best result seen so far for macro.
09/16 12:38:34 PM: Updating LR scheduler:
09/16 12:38:34 PM: 	Best result seen so far for macro_avg: 0.729
09/16 12:38:34 PM: 	# validation passes without improvement: 0
09/16 12:38:34 PM: edges-ner-ontonotes_loss: training: 0.074808 validation: 0.079796
09/16 12:38:34 PM: macro_avg: validation: 0.728570
09/16 12:38:34 PM: micro_avg: validation: 0.000000
09/16 12:38:34 PM: edges-ner-ontonotes_mcc: training: 0.733707 validation: 0.724730
09/16 12:38:34 PM: edges-ner-ontonotes_acc: training: 0.633602 validation: 0.623294
09/16 12:38:34 PM: edges-ner-ontonotes_precision: training: 0.850285 validation: 0.861355
09/16 12:38:34 PM: edges-ner-ontonotes_recall: training: 0.655159 validation: 0.631256
09/16 12:38:34 PM: edges-ner-ontonotes_f1: training: 0.740077 validation: 0.728570
09/16 12:38:34 PM: Global learning rate: 0.0001
09/16 12:38:34 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:38:36 PM: Update 6019: task edges-ner-ontonotes, batch 19 (6019): mcc: 0.6832, acc: 0.5759, precision: 0.8314, recall: 0.5851, f1: 0.6869, edges-ner-ontonotes_loss: 0.0852
09/16 12:38:46 PM: Update 6138: task edges-ner-ontonotes, batch 138 (6138): mcc: 0.6988, acc: 0.5968, precision: 0.8343, recall: 0.6088, f1: 0.7039, edges-ner-ontonotes_loss: 0.0873
09/16 12:39:00 PM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.7036, acc: 0.6024, precision: 0.8358, recall: 0.6157, f1: 0.7090, edges-ner-ontonotes_loss: 0.0862
09/16 12:39:10 PM: Update 6349: task edges-ner-ontonotes, batch 349 (6349): mcc: 0.7191, acc: 0.6191, precision: 0.8472, recall: 0.6329, f1: 0.7245, edges-ner-ontonotes_loss: 0.0814
09/16 12:39:24 PM: Update 6538: task edges-ner-ontonotes, batch 538 (6538): mcc: 0.7365, acc: 0.6396, precision: 0.8587, recall: 0.6533, f1: 0.7420, edges-ner-ontonotes_loss: 0.0765
09/16 12:39:34 PM: Update 6662: task edges-ner-ontonotes, batch 662 (6662): mcc: 0.7376, acc: 0.6411, precision: 0.8586, recall: 0.6551, f1: 0.7432, edges-ner-ontonotes_loss: 0.0761
09/16 12:39:44 PM: Update 6802: task edges-ner-ontonotes, batch 802 (6802): mcc: 0.7426, acc: 0.6476, precision: 0.8602, recall: 0.6623, f1: 0.7484, edges-ner-ontonotes_loss: 0.0748
09/16 12:39:54 PM: Update 6852: task edges-ner-ontonotes, batch 852 (6852): mcc: 0.7434, acc: 0.6492, precision: 0.8603, recall: 0.6637, f1: 0.7493, edges-ner-ontonotes_loss: 0.0747
09/16 12:40:05 PM: Update 6986: task edges-ner-ontonotes, batch 986 (6986): mcc: 0.7406, acc: 0.6450, precision: 0.8579, recall: 0.6607, f1: 0.7465, edges-ner-ontonotes_loss: 0.0747
09/16 12:40:06 PM: ***** Step 7000 / Validation 7 *****
09/16 12:40:06 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:40:06 PM: Validating...
09/16 12:40:15 PM: Evaluate: task edges-ner-ontonotes, batch 141 (157): mcc: 0.7256, acc: 0.6323, precision: 0.8426, recall: 0.6476, f1: 0.7323, edges-ner-ontonotes_loss: 0.0790
09/16 12:40:16 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:40:16 PM: Best result seen so far for macro.
09/16 12:40:16 PM: Updating LR scheduler:
09/16 12:40:16 PM: 	Best result seen so far for macro_avg: 0.733
09/16 12:40:16 PM: 	# validation passes without improvement: 0
09/16 12:40:16 PM: edges-ner-ontonotes_loss: training: 0.074630 validation: 0.078482
09/16 12:40:16 PM: macro_avg: validation: 0.733376
09/16 12:40:16 PM: micro_avg: validation: 0.000000
09/16 12:40:16 PM: edges-ner-ontonotes_mcc: training: 0.740596 validation: 0.726924
09/16 12:40:16 PM: edges-ner-ontonotes_acc: training: 0.644921 validation: 0.633076
09/16 12:40:16 PM: edges-ner-ontonotes_precision: training: 0.857913 validation: 0.845157
09/16 12:40:16 PM: edges-ner-ontonotes_recall: training: 0.660776 validation: 0.647710
09/16 12:40:16 PM: edges-ner-ontonotes_f1: training: 0.746550 validation: 0.733376
09/16 12:40:16 PM: Global learning rate: 0.0001
09/16 12:40:16 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:40:25 PM: Update 7080: task edges-ner-ontonotes, batch 80 (7080): mcc: 0.7459, acc: 0.6499, precision: 0.8522, recall: 0.6745, f1: 0.7530, edges-ner-ontonotes_loss: 0.0699
09/16 12:40:35 PM: Update 7183: task edges-ner-ontonotes, batch 183 (7183): mcc: 0.7450, acc: 0.6478, precision: 0.8531, recall: 0.6723, f1: 0.7520, edges-ner-ontonotes_loss: 0.0699
09/16 12:40:45 PM: Update 7311: task edges-ner-ontonotes, batch 311 (7311): mcc: 0.7444, acc: 0.6481, precision: 0.8516, recall: 0.6725, f1: 0.7515, edges-ner-ontonotes_loss: 0.0699
09/16 12:40:55 PM: Update 7474: task edges-ner-ontonotes, batch 474 (7474): mcc: 0.7451, acc: 0.6478, precision: 0.8513, recall: 0.6738, f1: 0.7523, edges-ner-ontonotes_loss: 0.0700
09/16 12:41:05 PM: Update 7611: task edges-ner-ontonotes, batch 611 (7611): mcc: 0.7351, acc: 0.6370, precision: 0.8449, recall: 0.6619, f1: 0.7423, edges-ner-ontonotes_loss: 0.0739
09/16 12:41:15 PM: Update 7782: task edges-ner-ontonotes, batch 782 (7782): mcc: 0.7315, acc: 0.6332, precision: 0.8440, recall: 0.6565, f1: 0.7385, edges-ner-ontonotes_loss: 0.0760
09/16 12:41:25 PM: Update 7952: task edges-ner-ontonotes, batch 952 (7952): mcc: 0.7362, acc: 0.6392, precision: 0.8482, recall: 0.6612, f1: 0.7431, edges-ner-ontonotes_loss: 0.0749
09/16 12:41:28 PM: ***** Step 8000 / Validation 8 *****
09/16 12:41:28 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:41:28 PM: Validating...
09/16 12:41:35 PM: Evaluate: task edges-ner-ontonotes, batch 102 (157): mcc: 0.7145, acc: 0.6148, precision: 0.8538, recall: 0.6200, f1: 0.7183, edges-ner-ontonotes_loss: 0.0811
09/16 12:41:39 PM: Updating LR scheduler:
09/16 12:41:39 PM: 	Best result seen so far for macro_avg: 0.733
09/16 12:41:39 PM: 	# validation passes without improvement: 1
09/16 12:41:39 PM: edges-ner-ontonotes_loss: training: 0.074370 validation: 0.079101
09/16 12:41:39 PM: macro_avg: validation: 0.718256
09/16 12:41:39 PM: micro_avg: validation: 0.000000
09/16 12:41:39 PM: edges-ner-ontonotes_mcc: training: 0.738202 validation: 0.714847
09/16 12:41:39 PM: edges-ner-ontonotes_acc: training: 0.641597 validation: 0.613436
09/16 12:41:39 PM: edges-ner-ontonotes_precision: training: 0.849935 validation: 0.856783
09/16 12:41:39 PM: edges-ner-ontonotes_recall: training: 0.663150 validation: 0.618289
09/16 12:41:39 PM: edges-ner-ontonotes_f1: training: 0.745013 validation: 0.718256
09/16 12:41:39 PM: Global learning rate: 0.0001
09/16 12:41:39 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:41:51 PM: Update 8094: task edges-ner-ontonotes, batch 94 (8094): mcc: 0.7839, acc: 0.7008, precision: 0.8820, recall: 0.7157, f1: 0.7902, edges-ner-ontonotes_loss: 0.0641
09/16 12:42:01 PM: Update 8233: task edges-ner-ontonotes, batch 233 (8233): mcc: 0.7614, acc: 0.6728, precision: 0.8652, recall: 0.6907, f1: 0.7681, edges-ner-ontonotes_loss: 0.0683
09/16 12:42:11 PM: Update 8365: task edges-ner-ontonotes, batch 365 (8365): mcc: 0.7599, acc: 0.6727, precision: 0.8618, recall: 0.6909, f1: 0.7670, edges-ner-ontonotes_loss: 0.0693
09/16 12:42:21 PM: Update 8407: task edges-ner-ontonotes, batch 407 (8407): mcc: 0.7610, acc: 0.6744, precision: 0.8621, recall: 0.6925, f1: 0.7680, edges-ner-ontonotes_loss: 0.0690
09/16 12:42:31 PM: Update 8558: task edges-ner-ontonotes, batch 558 (8558): mcc: 0.7524, acc: 0.6621, precision: 0.8563, recall: 0.6825, f1: 0.7596, edges-ner-ontonotes_loss: 0.0701
09/16 12:42:43 PM: Update 8720: task edges-ner-ontonotes, batch 720 (8720): mcc: 0.7521, acc: 0.6602, precision: 0.8561, recall: 0.6820, f1: 0.7592, edges-ner-ontonotes_loss: 0.0698
09/16 12:42:53 PM: Update 8825: task edges-ner-ontonotes, batch 825 (8825): mcc: 0.7512, acc: 0.6587, precision: 0.8547, recall: 0.6816, f1: 0.7584, edges-ner-ontonotes_loss: 0.0698
09/16 12:43:03 PM: Update 8939: task edges-ner-ontonotes, batch 939 (8939): mcc: 0.7514, acc: 0.6587, precision: 0.8544, recall: 0.6823, f1: 0.7587, edges-ner-ontonotes_loss: 0.0698
09/16 12:43:10 PM: ***** Step 9000 / Validation 9 *****
09/16 12:43:10 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:43:10 PM: Validating...
09/16 12:43:13 PM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.6962, acc: 0.6133, precision: 0.8098, recall: 0.6237, f1: 0.7047, edges-ner-ontonotes_loss: 0.0883
09/16 12:43:24 PM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.7285, acc: 0.6408, precision: 0.8363, recall: 0.6576, f1: 0.7363, edges-ner-ontonotes_loss: 0.0792
09/16 12:43:25 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:43:25 PM: Best result seen so far for macro.
09/16 12:43:25 PM: Updating LR scheduler:
09/16 12:43:25 PM: 	Best result seen so far for macro_avg: 0.740
09/16 12:43:25 PM: 	# validation passes without improvement: 0
09/16 12:43:25 PM: edges-ner-ontonotes_loss: training: 0.069742 validation: 0.077990
09/16 12:43:25 PM: macro_avg: validation: 0.740495
09/16 12:43:25 PM: micro_avg: validation: 0.000000
09/16 12:43:25 PM: edges-ner-ontonotes_mcc: training: 0.751631 validation: 0.733063
09/16 12:43:25 PM: edges-ner-ontonotes_acc: training: 0.658741 validation: 0.644904
09/16 12:43:25 PM: edges-ner-ontonotes_precision: training: 0.854514 validation: 0.841882
09/16 12:43:25 PM: edges-ner-ontonotes_recall: training: 0.682564 validation: 0.660904
09/16 12:43:25 PM: edges-ner-ontonotes_f1: training: 0.758921 validation: 0.740495
09/16 12:43:25 PM: Global learning rate: 0.0001
09/16 12:43:25 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:43:34 PM: Update 9033: task edges-ner-ontonotes, batch 33 (9033): mcc: 0.7553, acc: 0.6636, precision: 0.8469, recall: 0.6955, f1: 0.7638, edges-ner-ontonotes_loss: 0.0678
09/16 12:43:44 PM: Update 9152: task edges-ner-ontonotes, batch 152 (9152): mcc: 0.7191, acc: 0.6217, precision: 0.8287, recall: 0.6476, f1: 0.7271, edges-ner-ontonotes_loss: 0.0815
09/16 12:43:54 PM: Update 9278: task edges-ner-ontonotes, batch 278 (9278): mcc: 0.7180, acc: 0.6205, precision: 0.8324, recall: 0.6428, f1: 0.7254, edges-ner-ontonotes_loss: 0.0822
09/16 12:44:04 PM: Update 9380: task edges-ner-ontonotes, batch 380 (9380): mcc: 0.7209, acc: 0.6248, precision: 0.8348, recall: 0.6458, f1: 0.7282, edges-ner-ontonotes_loss: 0.0811
09/16 12:44:14 PM: Update 9540: task edges-ner-ontonotes, batch 540 (9540): mcc: 0.7357, acc: 0.6421, precision: 0.8469, recall: 0.6614, f1: 0.7427, edges-ner-ontonotes_loss: 0.0765
09/16 12:44:26 PM: Update 9650: task edges-ner-ontonotes, batch 650 (9650): mcc: 0.7437, acc: 0.6522, precision: 0.8521, recall: 0.6708, f1: 0.7507, edges-ner-ontonotes_loss: 0.0744
09/16 12:44:36 PM: Update 9788: task edges-ner-ontonotes, batch 788 (9788): mcc: 0.7425, acc: 0.6509, precision: 0.8508, recall: 0.6698, f1: 0.7495, edges-ner-ontonotes_loss: 0.0743
09/16 12:44:47 PM: Update 9918: task edges-ner-ontonotes, batch 918 (9918): mcc: 0.7474, acc: 0.6574, precision: 0.8535, recall: 0.6761, f1: 0.7545, edges-ner-ontonotes_loss: 0.0731
09/16 12:44:57 PM: Update 9986: task edges-ner-ontonotes, batch 986 (9986): mcc: 0.7484, acc: 0.6582, precision: 0.8543, recall: 0.6772, f1: 0.7555, edges-ner-ontonotes_loss: 0.0728
09/16 12:44:58 PM: ***** Step 10000 / Validation 10 *****
09/16 12:44:58 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:44:58 PM: Validating...
09/16 12:45:07 PM: Evaluate: task edges-ner-ontonotes, batch 127 (157): mcc: 0.7334, acc: 0.6462, precision: 0.8501, recall: 0.6547, f1: 0.7397, edges-ner-ontonotes_loss: 0.0779
09/16 12:45:09 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:45:09 PM: Best result seen so far for macro.
09/16 12:45:09 PM: Updating LR scheduler:
09/16 12:45:09 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:45:09 PM: 	# validation passes without improvement: 0
09/16 12:45:09 PM: edges-ner-ontonotes_loss: training: 0.072759 validation: 0.076398
09/16 12:45:09 PM: macro_avg: validation: 0.743131
09/16 12:45:09 PM: micro_avg: validation: 0.000000
09/16 12:45:09 PM: edges-ner-ontonotes_mcc: training: 0.748174 validation: 0.737057
09/16 12:45:09 PM: edges-ner-ontonotes_acc: training: 0.657717 validation: 0.648544
09/16 12:45:09 PM: edges-ner-ontonotes_precision: training: 0.854219 validation: 0.854678
09/16 12:45:09 PM: edges-ner-ontonotes_recall: training: 0.676820 validation: 0.657340
09/16 12:45:09 PM: edges-ner-ontonotes_f1: training: 0.755242 validation: 0.743131
09/16 12:45:09 PM: Global learning rate: 0.0001
09/16 12:45:09 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:45:17 PM: Update 10095: task edges-ner-ontonotes, batch 95 (10095): mcc: 0.7362, acc: 0.6377, precision: 0.8428, recall: 0.6657, f1: 0.7438, edges-ner-ontonotes_loss: 0.0723
09/16 12:45:28 PM: Update 10215: task edges-ner-ontonotes, batch 215 (10215): mcc: 0.7456, acc: 0.6487, precision: 0.8490, recall: 0.6767, f1: 0.7531, edges-ner-ontonotes_loss: 0.0697
09/16 12:45:38 PM: Update 10292: task edges-ner-ontonotes, batch 292 (10292): mcc: 0.7445, acc: 0.6484, precision: 0.8478, recall: 0.6758, f1: 0.7521, edges-ner-ontonotes_loss: 0.0699
09/16 12:45:48 PM: Update 10398: task edges-ner-ontonotes, batch 398 (10398): mcc: 0.7472, acc: 0.6514, precision: 0.8493, recall: 0.6793, f1: 0.7548, edges-ner-ontonotes_loss: 0.0692
09/16 12:45:58 PM: Update 10521: task edges-ner-ontonotes, batch 521 (10521): mcc: 0.7488, acc: 0.6528, precision: 0.8496, recall: 0.6818, f1: 0.7565, edges-ner-ontonotes_loss: 0.0692
09/16 12:46:08 PM: Update 10627: task edges-ner-ontonotes, batch 627 (10627): mcc: 0.7463, acc: 0.6503, precision: 0.8477, recall: 0.6790, f1: 0.7540, edges-ner-ontonotes_loss: 0.0701
09/16 12:46:18 PM: Update 10792: task edges-ner-ontonotes, batch 792 (10792): mcc: 0.7403, acc: 0.6439, precision: 0.8444, recall: 0.6714, f1: 0.7480, edges-ner-ontonotes_loss: 0.0728
09/16 12:46:28 PM: Update 10926: task edges-ner-ontonotes, batch 926 (10926): mcc: 0.7389, acc: 0.6432, precision: 0.8440, recall: 0.6693, f1: 0.7466, edges-ner-ontonotes_loss: 0.0738
09/16 12:46:32 PM: ***** Step 11000 / Validation 11 *****
09/16 12:46:33 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:46:33 PM: Validating...
09/16 12:46:38 PM: Evaluate: task edges-ner-ontonotes, batch 92 (157): mcc: 0.7326, acc: 0.6359, precision: 0.8646, recall: 0.6420, f1: 0.7369, edges-ner-ontonotes_loss: 0.0790
09/16 12:46:44 PM: Updating LR scheduler:
09/16 12:46:44 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:46:44 PM: 	# validation passes without improvement: 1
09/16 12:46:44 PM: edges-ner-ontonotes_loss: training: 0.073318 validation: 0.076993
09/16 12:46:44 PM: macro_avg: validation: 0.733048
09/16 12:46:44 PM: micro_avg: validation: 0.000000
09/16 12:46:44 PM: edges-ner-ontonotes_mcc: training: 0.741206 validation: 0.729242
09/16 12:46:44 PM: edges-ner-ontonotes_acc: training: 0.646045 validation: 0.630497
09/16 12:46:44 PM: edges-ner-ontonotes_precision: training: 0.846131 validation: 0.864845
09/16 12:46:44 PM: edges-ner-ontonotes_recall: training: 0.671466 validation: 0.636109
09/16 12:46:44 PM: edges-ner-ontonotes_f1: training: 0.748747 validation: 0.733048
09/16 12:46:44 PM: Global learning rate: 0.0001
09/16 12:46:44 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:46:48 PM: Update 11046: task edges-ner-ontonotes, batch 46 (11046): mcc: 0.7838, acc: 0.6951, precision: 0.8845, recall: 0.7133, f1: 0.7897, edges-ner-ontonotes_loss: 0.0632
09/16 12:46:58 PM: Update 11160: task edges-ner-ontonotes, batch 160 (11160): mcc: 0.7835, acc: 0.6977, precision: 0.8803, recall: 0.7164, f1: 0.7899, edges-ner-ontonotes_loss: 0.0635
09/16 12:47:08 PM: Update 11207: task edges-ner-ontonotes, batch 207 (11207): mcc: 0.7829, acc: 0.6981, precision: 0.8788, recall: 0.7166, f1: 0.7895, edges-ner-ontonotes_loss: 0.0634
09/16 12:47:18 PM: Update 11330: task edges-ner-ontonotes, batch 330 (11330): mcc: 0.7669, acc: 0.6805, precision: 0.8662, recall: 0.6994, f1: 0.7739, edges-ner-ontonotes_loss: 0.0661
09/16 12:47:28 PM: Update 11504: task edges-ner-ontonotes, batch 504 (11504): mcc: 0.7675, acc: 0.6814, precision: 0.8661, recall: 0.7005, f1: 0.7745, edges-ner-ontonotes_loss: 0.0665
09/16 12:47:38 PM: Update 11582: task edges-ner-ontonotes, batch 582 (11582): mcc: 0.7640, acc: 0.6762, precision: 0.8639, recall: 0.6962, f1: 0.7710, edges-ner-ontonotes_loss: 0.0670
09/16 12:47:48 PM: Update 11712: task edges-ner-ontonotes, batch 712 (11712): mcc: 0.7591, acc: 0.6689, precision: 0.8602, recall: 0.6907, f1: 0.7662, edges-ner-ontonotes_loss: 0.0677
09/16 12:48:01 PM: Update 11832: task edges-ner-ontonotes, batch 832 (11832): mcc: 0.7591, acc: 0.6690, precision: 0.8591, recall: 0.6917, f1: 0.7663, edges-ner-ontonotes_loss: 0.0675
09/16 12:48:11 PM: Update 11972: task edges-ner-ontonotes, batch 972 (11972): mcc: 0.7577, acc: 0.6669, precision: 0.8572, recall: 0.6909, f1: 0.7651, edges-ner-ontonotes_loss: 0.0676
09/16 12:48:13 PM: ***** Step 12000 / Validation 12 *****
09/16 12:48:16 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:48:16 PM: Validating...
09/16 12:48:21 PM: Evaluate: task edges-ner-ontonotes, batch 63 (157): mcc: 0.7259, acc: 0.6432, precision: 0.8355, recall: 0.6538, f1: 0.7336, edges-ner-ontonotes_loss: 0.0815
09/16 12:48:31 PM: Evaluate: task edges-ner-ontonotes, batch 138 (157): mcc: 0.7339, acc: 0.6476, precision: 0.8386, recall: 0.6650, f1: 0.7418, edges-ner-ontonotes_loss: 0.0778
09/16 12:48:33 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:48:33 PM: Best result seen so far for macro.
09/16 12:48:33 PM: Updating LR scheduler:
09/16 12:48:33 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:48:33 PM: 	# validation passes without improvement: 2
09/16 12:48:33 PM: edges-ner-ontonotes_loss: training: 0.067611 validation: 0.076713
09/16 12:48:33 PM: macro_avg: validation: 0.743172
09/16 12:48:33 PM: micro_avg: validation: 0.000000
09/16 12:48:33 PM: edges-ner-ontonotes_mcc: training: 0.757539 validation: 0.735502
09/16 12:48:33 PM: edges-ner-ontonotes_acc: training: 0.666849 validation: 0.648923
09/16 12:48:33 PM: edges-ner-ontonotes_precision: training: 0.856719 validation: 0.841565
09/16 12:48:33 PM: edges-ner-ontonotes_recall: training: 0.690991 validation: 0.665378
09/16 12:48:33 PM: edges-ner-ontonotes_f1: training: 0.764982 validation: 0.743172
09/16 12:48:33 PM: Global learning rate: 0.0001
09/16 12:48:33 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:48:41 PM: Update 12106: task edges-ner-ontonotes, batch 106 (12106): mcc: 0.7562, acc: 0.6646, precision: 0.8490, recall: 0.6952, f1: 0.7645, edges-ner-ontonotes_loss: 0.0663
09/16 12:48:52 PM: Update 12201: task edges-ner-ontonotes, batch 201 (12201): mcc: 0.7397, acc: 0.6466, precision: 0.8387, recall: 0.6751, f1: 0.7481, edges-ner-ontonotes_loss: 0.0729
09/16 12:49:02 PM: Update 12345: task edges-ner-ontonotes, batch 345 (12345): mcc: 0.7337, acc: 0.6403, precision: 0.8374, recall: 0.6658, f1: 0.7418, edges-ner-ontonotes_loss: 0.0761
09/16 12:49:13 PM: Update 12449: task edges-ner-ontonotes, batch 449 (12449): mcc: 0.7305, acc: 0.6372, precision: 0.8360, recall: 0.6612, f1: 0.7384, edges-ner-ontonotes_loss: 0.0773
09/16 12:49:23 PM: Update 12582: task edges-ner-ontonotes, batch 582 (12582): mcc: 0.7380, acc: 0.6458, precision: 0.8424, recall: 0.6690, f1: 0.7458, edges-ner-ontonotes_loss: 0.0748
09/16 12:49:33 PM: Update 12716: task edges-ner-ontonotes, batch 716 (12716): mcc: 0.7473, acc: 0.6561, precision: 0.8500, recall: 0.6787, f1: 0.7548, edges-ner-ontonotes_loss: 0.0725
09/16 12:49:43 PM: Update 12813: task edges-ner-ontonotes, batch 813 (12813): mcc: 0.7485, acc: 0.6577, precision: 0.8514, recall: 0.6798, f1: 0.7559, edges-ner-ontonotes_loss: 0.0721
09/16 12:49:53 PM: Update 12941: task edges-ner-ontonotes, batch 941 (12941): mcc: 0.7504, acc: 0.6603, precision: 0.8521, recall: 0.6825, f1: 0.7579, edges-ner-ontonotes_loss: 0.0716
09/16 12:49:59 PM: ***** Step 13000 / Validation 13 *****
09/16 12:49:59 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:49:59 PM: Validating...
09/16 12:50:03 PM: Evaluate: task edges-ner-ontonotes, batch 56 (157): mcc: 0.7589, acc: 0.6778, precision: 0.8677, recall: 0.6843, f1: 0.7651, edges-ner-ontonotes_loss: 0.0738
09/16 12:50:11 PM: Updating LR scheduler:
09/16 12:50:11 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:50:11 PM: 	# validation passes without improvement: 3
09/16 12:50:11 PM: edges-ner-ontonotes_loss: training: 0.071305 validation: 0.075564
09/16 12:50:11 PM: macro_avg: validation: 0.741586
09/16 12:50:11 PM: micro_avg: validation: 0.000000
09/16 12:50:11 PM: edges-ner-ontonotes_mcc: training: 0.751482 validation: 0.736008
09/16 12:50:11 PM: edges-ner-ontonotes_acc: training: 0.661732 validation: 0.645511
09/16 12:50:11 PM: edges-ner-ontonotes_precision: training: 0.852959 validation: 0.857555
09/16 12:50:11 PM: edges-ner-ontonotes_recall: training: 0.683605 validation: 0.653245
09/16 12:50:11 PM: edges-ner-ontonotes_f1: training: 0.758949 validation: 0.741586
09/16 12:50:11 PM: Global learning rate: 0.0001
09/16 12:50:11 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:50:13 PM: Update 13062: task edges-ner-ontonotes, batch 62 (13062): mcc: 0.7772, acc: 0.6925, precision: 0.8707, recall: 0.7135, f1: 0.7843, edges-ner-ontonotes_loss: 0.0652
09/16 12:50:23 PM: Update 13117: task edges-ner-ontonotes, batch 117 (13117): mcc: 0.7580, acc: 0.6677, precision: 0.8569, recall: 0.6917, f1: 0.7655, edges-ner-ontonotes_loss: 0.0677
09/16 12:50:33 PM: Update 13222: task edges-ner-ontonotes, batch 222 (13222): mcc: 0.7532, acc: 0.6601, precision: 0.8536, recall: 0.6860, f1: 0.7607, edges-ner-ontonotes_loss: 0.0682
09/16 12:50:43 PM: Update 13333: task edges-ner-ontonotes, batch 333 (13333): mcc: 0.7534, acc: 0.6601, precision: 0.8531, recall: 0.6867, f1: 0.7609, edges-ner-ontonotes_loss: 0.0682
09/16 12:50:53 PM: Update 13404: task edges-ner-ontonotes, batch 404 (13404): mcc: 0.7538, acc: 0.6601, precision: 0.8532, recall: 0.6875, f1: 0.7614, edges-ner-ontonotes_loss: 0.0680
09/16 12:51:04 PM: Update 13585: task edges-ner-ontonotes, batch 585 (13585): mcc: 0.7558, acc: 0.6626, precision: 0.8531, recall: 0.6910, f1: 0.7635, edges-ner-ontonotes_loss: 0.0676
09/16 12:51:15 PM: Update 13701: task edges-ner-ontonotes, batch 701 (13701): mcc: 0.7557, acc: 0.6625, precision: 0.8523, recall: 0.6914, f1: 0.7635, edges-ner-ontonotes_loss: 0.0675
09/16 12:51:25 PM: Update 13849: task edges-ner-ontonotes, batch 849 (13849): mcc: 0.7487, acc: 0.6548, precision: 0.8479, recall: 0.6830, f1: 0.7566, edges-ner-ontonotes_loss: 0.0703
09/16 12:51:35 PM: Update 13956: task edges-ner-ontonotes, batch 956 (13956): mcc: 0.7460, acc: 0.6522, precision: 0.8465, recall: 0.6794, f1: 0.7538, edges-ner-ontonotes_loss: 0.0713
09/16 12:51:39 PM: ***** Step 14000 / Validation 14 *****
09/16 12:51:39 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:51:39 PM: Validating...
09/16 12:51:45 PM: Evaluate: task edges-ner-ontonotes, batch 87 (157): mcc: 0.7322, acc: 0.6397, precision: 0.8579, recall: 0.6465, f1: 0.7374, edges-ner-ontonotes_loss: 0.0778
09/16 12:51:51 PM: Updating LR scheduler:
09/16 12:51:51 PM: 	Best result seen so far for macro_avg: 0.743
09/16 12:51:51 PM: 	# validation passes without improvement: 0
09/16 12:51:51 PM: edges-ner-ontonotes_loss: training: 0.071759 validation: 0.076403
09/16 12:51:51 PM: macro_avg: validation: 0.736401
09/16 12:51:51 PM: micro_avg: validation: 0.000000
09/16 12:51:51 PM: edges-ner-ontonotes_mcc: training: 0.744816 validation: 0.731068
09/16 12:51:51 PM: edges-ner-ontonotes_acc: training: 0.651130 validation: 0.638838
09/16 12:51:51 PM: edges-ner-ontonotes_precision: training: 0.845606 validation: 0.855880
09/16 12:51:51 PM: edges-ner-ontonotes_recall: training: 0.678171 validation: 0.646194
09/16 12:51:51 PM: edges-ner-ontonotes_f1: training: 0.752689 validation: 0.736401
09/16 12:51:51 PM: Global learning rate: 5e-05
09/16 12:51:51 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:51:58 PM: Update 14005: task edges-ner-ontonotes, batch 5 (14005): mcc: 0.7406, acc: 0.6254, precision: 0.8714, recall: 0.6502, f1: 0.7447, edges-ner-ontonotes_loss: 0.0729
09/16 12:52:08 PM: Update 14204: task edges-ner-ontonotes, batch 204 (14204): mcc: 0.7688, acc: 0.6798, precision: 0.8694, recall: 0.7000, f1: 0.7756, edges-ner-ontonotes_loss: 0.0671
09/16 12:52:21 PM: Update 14318: task edges-ner-ontonotes, batch 318 (14318): mcc: 0.7752, acc: 0.6885, precision: 0.8740, recall: 0.7073, f1: 0.7819, edges-ner-ontonotes_loss: 0.0655
09/16 12:52:31 PM: Update 14444: task edges-ner-ontonotes, batch 444 (14444): mcc: 0.7659, acc: 0.6777, precision: 0.8671, recall: 0.6969, f1: 0.7727, edges-ner-ontonotes_loss: 0.0668
09/16 12:52:41 PM: Update 14616: task edges-ner-ontonotes, batch 616 (14616): mcc: 0.7661, acc: 0.6787, precision: 0.8650, recall: 0.6989, f1: 0.7731, edges-ner-ontonotes_loss: 0.0669
09/16 12:52:51 PM: Update 14657: task edges-ner-ontonotes, batch 657 (14657): mcc: 0.7644, acc: 0.6763, precision: 0.8638, recall: 0.6970, f1: 0.7714, edges-ner-ontonotes_loss: 0.0670
09/16 12:53:01 PM: Update 14757: task edges-ner-ontonotes, batch 757 (14757): mcc: 0.7609, acc: 0.6712, precision: 0.8611, recall: 0.6931, f1: 0.7681, edges-ner-ontonotes_loss: 0.0674
09/16 12:53:11 PM: Update 14893: task edges-ner-ontonotes, batch 893 (14893): mcc: 0.7598, acc: 0.6690, precision: 0.8606, recall: 0.6917, f1: 0.7670, edges-ner-ontonotes_loss: 0.0674
09/16 12:53:21 PM: Update 14957: task edges-ner-ontonotes, batch 957 (14957): mcc: 0.7590, acc: 0.6678, precision: 0.8597, recall: 0.6911, f1: 0.7662, edges-ner-ontonotes_loss: 0.0675
09/16 12:53:25 PM: ***** Step 15000 / Validation 15 *****
09/16 12:53:25 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:53:25 PM: Validating...
09/16 12:53:31 PM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.7351, acc: 0.6483, precision: 0.8484, recall: 0.6591, f1: 0.7419, edges-ner-ontonotes_loss: 0.0792
09/16 12:53:34 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:53:34 PM: Best result seen so far for macro.
09/16 12:53:34 PM: Updating LR scheduler:
09/16 12:53:34 PM: 	Best result seen so far for macro_avg: 0.749
09/16 12:53:34 PM: 	# validation passes without improvement: 0
09/16 12:53:34 PM: edges-ner-ontonotes_loss: training: 0.067405 validation: 0.075027
09/16 12:53:34 PM: macro_avg: validation: 0.749427
09/16 12:53:34 PM: micro_avg: validation: 0.000000
09/16 12:53:34 PM: edges-ner-ontonotes_mcc: training: 0.758682 validation: 0.742458
09/16 12:53:34 PM: edges-ner-ontonotes_acc: training: 0.667198 validation: 0.655823
09/16 12:53:34 PM: edges-ner-ontonotes_precision: training: 0.859244 validation: 0.851437
09/16 12:53:34 PM: edges-ner-ontonotes_recall: training: 0.690862 validation: 0.669245
09/16 12:53:34 PM: edges-ner-ontonotes_f1: training: 0.765907 validation: 0.749427
09/16 12:53:34 PM: Global learning rate: 5e-05
09/16 12:53:34 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:53:41 PM: Update 15129: task edges-ner-ontonotes, batch 129 (15129): mcc: 0.7524, acc: 0.6577, precision: 0.8484, recall: 0.6891, f1: 0.7605, edges-ner-ontonotes_loss: 0.0678
09/16 12:53:51 PM: Update 15243: task edges-ner-ontonotes, batch 243 (15243): mcc: 0.7604, acc: 0.6670, precision: 0.8546, recall: 0.6979, f1: 0.7683, edges-ner-ontonotes_loss: 0.0663
09/16 12:54:01 PM: Update 15320: task edges-ner-ontonotes, batch 320 (15320): mcc: 0.7519, acc: 0.6587, precision: 0.8494, recall: 0.6874, f1: 0.7598, edges-ner-ontonotes_loss: 0.0694
09/16 12:54:11 PM: Update 15438: task edges-ner-ontonotes, batch 438 (15438): mcc: 0.7470, acc: 0.6545, precision: 0.8463, recall: 0.6813, f1: 0.7549, edges-ner-ontonotes_loss: 0.0724
09/16 12:54:21 PM: Update 15553: task edges-ner-ontonotes, batch 553 (15553): mcc: 0.7431, acc: 0.6498, precision: 0.8453, recall: 0.6754, f1: 0.7509, edges-ner-ontonotes_loss: 0.0738
09/16 12:54:31 PM: Update 15600: task edges-ner-ontonotes, batch 600 (15600): mcc: 0.7445, acc: 0.6516, precision: 0.8463, recall: 0.6770, f1: 0.7522, edges-ner-ontonotes_loss: 0.0733
09/16 12:54:42 PM: Update 15752: task edges-ner-ontonotes, batch 752 (15752): mcc: 0.7492, acc: 0.6574, precision: 0.8500, recall: 0.6821, f1: 0.7569, edges-ner-ontonotes_loss: 0.0717
09/16 12:54:53 PM: Update 15874: task edges-ner-ontonotes, batch 874 (15874): mcc: 0.7537, acc: 0.6631, precision: 0.8532, recall: 0.6874, f1: 0.7613, edges-ner-ontonotes_loss: 0.0705
09/16 12:54:59 PM: ***** Step 16000 / Validation 16 *****
09/16 12:54:59 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:54:59 PM: Validating...
09/16 12:55:03 PM: Evaluate: task edges-ner-ontonotes, batch 60 (157): mcc: 0.7487, acc: 0.6664, precision: 0.8587, recall: 0.6740, f1: 0.7553, edges-ner-ontonotes_loss: 0.0730
09/16 12:55:13 PM: Evaluate: task edges-ner-ontonotes, batch 155 (157): mcc: 0.7366, acc: 0.6451, precision: 0.8566, recall: 0.6550, f1: 0.7424, edges-ner-ontonotes_loss: 0.0748
09/16 12:55:13 PM: Updating LR scheduler:
09/16 12:55:15 PM: 	Best result seen so far for macro_avg: 0.749
09/16 12:55:15 PM: 	# validation passes without improvement: 1
09/16 12:55:15 PM: edges-ner-ontonotes_loss: training: 0.070262 validation: 0.074854
09/16 12:55:15 PM: macro_avg: validation: 0.742361
09/16 12:55:15 PM: micro_avg: validation: 0.000000
09/16 12:55:15 PM: edges-ner-ontonotes_mcc: training: 0.754307 validation: 0.736615
09/16 12:55:15 PM: edges-ner-ontonotes_acc: training: 0.663718 validation: 0.644980
09/16 12:55:15 PM: edges-ner-ontonotes_precision: training: 0.853922 validation: 0.856760
09/16 12:55:15 PM: edges-ner-ontonotes_recall: training: 0.687712 validation: 0.654914
09/16 12:55:15 PM: edges-ner-ontonotes_f1: training: 0.761858 validation: 0.742361
09/16 12:55:15 PM: Global learning rate: 5e-05
09/16 12:55:15 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:55:23 PM: Update 16092: task edges-ner-ontonotes, batch 92 (16092): mcc: 0.7591, acc: 0.6714, precision: 0.8538, recall: 0.6962, f1: 0.7670, edges-ner-ontonotes_loss: 0.0676
09/16 12:55:36 PM: Update 16187: task edges-ner-ontonotes, batch 187 (16187): mcc: 0.7628, acc: 0.6768, precision: 0.8558, recall: 0.7010, f1: 0.7707, edges-ner-ontonotes_loss: 0.0667
09/16 12:55:46 PM: Update 16292: task edges-ner-ontonotes, batch 292 (16292): mcc: 0.7535, acc: 0.6630, precision: 0.8511, recall: 0.6888, f1: 0.7614, edges-ner-ontonotes_loss: 0.0679
09/16 12:55:56 PM: Update 16413: task edges-ner-ontonotes, batch 413 (16413): mcc: 0.7531, acc: 0.6616, precision: 0.8508, recall: 0.6882, f1: 0.7609, edges-ner-ontonotes_loss: 0.0677
09/16 12:56:06 PM: Update 16520: task edges-ner-ontonotes, batch 520 (16520): mcc: 0.7539, acc: 0.6618, precision: 0.8519, recall: 0.6888, f1: 0.7617, edges-ner-ontonotes_loss: 0.0673
09/16 12:56:16 PM: Update 16700: task edges-ner-ontonotes, batch 700 (16700): mcc: 0.7531, acc: 0.6599, precision: 0.8507, recall: 0.6883, f1: 0.7609, edges-ner-ontonotes_loss: 0.0675
09/16 12:56:32 PM: Update 16813: task edges-ner-ontonotes, batch 813 (16813): mcc: 0.7545, acc: 0.6613, precision: 0.8516, recall: 0.6900, f1: 0.7623, edges-ner-ontonotes_loss: 0.0671
09/16 12:56:42 PM: Update 16920: task edges-ner-ontonotes, batch 920 (16920): mcc: 0.7502, acc: 0.6570, precision: 0.8479, recall: 0.6855, f1: 0.7581, edges-ner-ontonotes_loss: 0.0687
09/16 12:56:46 PM: ***** Step 17000 / Validation 17 *****
09/16 12:56:46 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:56:46 PM: Validating...
09/16 12:56:54 PM: Evaluate: task edges-ner-ontonotes, batch 99 (157): mcc: 0.7318, acc: 0.6393, precision: 0.8529, recall: 0.6499, f1: 0.7377, edges-ner-ontonotes_loss: 0.0793
09/16 12:56:58 PM: Updating LR scheduler:
09/16 12:56:58 PM: 	Best result seen so far for macro_avg: 0.749
09/16 12:56:58 PM: 	# validation passes without improvement: 2
09/16 12:56:58 PM: edges-ner-ontonotes_loss: training: 0.069632 validation: 0.074521
09/16 12:56:58 PM: macro_avg: validation: 0.748049
09/16 12:56:58 PM: micro_avg: validation: 0.000000
09/16 12:56:58 PM: edges-ner-ontonotes_mcc: training: 0.748595 validation: 0.742328
09/16 12:56:58 PM: edges-ner-ontonotes_acc: training: 0.655265 validation: 0.649075
09/16 12:56:58 PM: edges-ner-ontonotes_precision: training: 0.847138 validation: 0.860766
09/16 12:56:58 PM: edges-ner-ontonotes_recall: training: 0.683465 validation: 0.661435
09/16 12:56:58 PM: edges-ner-ontonotes_f1: training: 0.756551 validation: 0.748049
09/16 12:56:58 PM: Global learning rate: 5e-05
09/16 12:56:58 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:57:04 PM: Update 17068: task edges-ner-ontonotes, batch 68 (17068): mcc: 0.7211, acc: 0.6251, precision: 0.8336, recall: 0.6471, f1: 0.7286, edges-ner-ontonotes_loss: 0.0805
09/16 12:57:14 PM: Update 17125: task edges-ner-ontonotes, batch 125 (17125): mcc: 0.7299, acc: 0.6329, precision: 0.8408, recall: 0.6563, f1: 0.7372, edges-ner-ontonotes_loss: 0.0780
09/16 12:57:24 PM: Update 17274: task edges-ner-ontonotes, batch 274 (17274): mcc: 0.7568, acc: 0.6675, precision: 0.8578, recall: 0.6888, f1: 0.7640, edges-ner-ontonotes_loss: 0.0706
09/16 12:57:40 PM: Update 17430: task edges-ner-ontonotes, batch 430 (17430): mcc: 0.7643, acc: 0.6775, precision: 0.8620, recall: 0.6984, f1: 0.7716, edges-ner-ontonotes_loss: 0.0681
09/16 12:57:50 PM: Update 17558: task edges-ner-ontonotes, batch 558 (17558): mcc: 0.7614, acc: 0.6742, precision: 0.8598, recall: 0.6952, f1: 0.7688, edges-ner-ontonotes_loss: 0.0682
09/16 12:58:00 PM: Update 17674: task edges-ner-ontonotes, batch 674 (17674): mcc: 0.7622, acc: 0.6758, precision: 0.8590, recall: 0.6971, f1: 0.7697, edges-ner-ontonotes_loss: 0.0679
09/16 12:58:10 PM: Update 17759: task edges-ner-ontonotes, batch 759 (17759): mcc: 0.7627, acc: 0.6765, precision: 0.8587, recall: 0.6983, f1: 0.7702, edges-ner-ontonotes_loss: 0.0677
09/16 12:58:20 PM: Update 17906: task edges-ner-ontonotes, batch 906 (17906): mcc: 0.7600, acc: 0.6721, precision: 0.8566, recall: 0.6954, f1: 0.7676, edges-ner-ontonotes_loss: 0.0679
09/16 12:58:28 PM: ***** Step 18000 / Validation 18 *****
09/16 12:58:28 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 12:58:28 PM: Validating...
09/16 12:58:30 PM: Evaluate: task edges-ner-ontonotes, batch 27 (157): mcc: 0.6746, acc: 0.5940, precision: 0.7904, recall: 0.6024, f1: 0.6837, edges-ner-ontonotes_loss: 0.0945
09/16 12:58:40 PM: Evaluate: task edges-ner-ontonotes, batch 156 (157): mcc: 0.7443, acc: 0.6582, precision: 0.8509, recall: 0.6729, f1: 0.7515, edges-ner-ontonotes_loss: 0.0744
09/16 12:58:40 PM: Best result seen so far for edges-ner-ontonotes.
09/16 12:58:40 PM: Best result seen so far for macro.
09/16 12:58:40 PM: Updating LR scheduler:
09/16 12:58:40 PM: 	Best result seen so far for macro_avg: 0.751
09/16 12:58:40 PM: 	# validation passes without improvement: 0
09/16 12:58:40 PM: edges-ner-ontonotes_loss: training: 0.067680 validation: 0.074656
09/16 12:58:40 PM: macro_avg: validation: 0.751270
09/16 12:58:40 PM: micro_avg: validation: 0.000000
09/16 12:58:40 PM: edges-ner-ontonotes_mcc: training: 0.759605 validation: 0.744078
09/16 12:58:40 PM: edges-ner-ontonotes_acc: training: 0.670962 validation: 0.658022
09/16 12:58:40 PM: edges-ner-ontonotes_precision: training: 0.856275 validation: 0.850690
09/16 12:58:40 PM: edges-ner-ontonotes_recall: training: 0.694972 validation: 0.672657
09/16 12:58:40 PM: edges-ner-ontonotes_f1: training: 0.767237 validation: 0.751270
09/16 12:58:40 PM: Global learning rate: 5e-05
09/16 12:58:40 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 12:58:51 PM: Update 18056: task edges-ner-ontonotes, batch 56 (18056): mcc: 0.7581, acc: 0.6686, precision: 0.8503, recall: 0.6974, f1: 0.7663, edges-ner-ontonotes_loss: 0.0669
09/16 12:59:01 PM: Update 18172: task edges-ner-ontonotes, batch 172 (18172): mcc: 0.7509, acc: 0.6581, precision: 0.8453, recall: 0.6890, f1: 0.7592, edges-ner-ontonotes_loss: 0.0670
09/16 12:59:11 PM: Update 18284: task edges-ner-ontonotes, batch 284 (18284): mcc: 0.7534, acc: 0.6605, precision: 0.8469, recall: 0.6921, f1: 0.7617, edges-ner-ontonotes_loss: 0.0668
09/16 12:59:21 PM: Update 18371: task edges-ner-ontonotes, batch 371 (18371): mcc: 0.7538, acc: 0.6604, precision: 0.8471, recall: 0.6926, f1: 0.7621, edges-ner-ontonotes_loss: 0.0667
09/16 12:59:31 PM: Update 18497: task edges-ner-ontonotes, batch 497 (18497): mcc: 0.7456, acc: 0.6519, precision: 0.8427, recall: 0.6819, f1: 0.7538, edges-ner-ontonotes_loss: 0.0707
09/16 12:59:41 PM: Update 18614: task edges-ner-ontonotes, batch 614 (18614): mcc: 0.7438, acc: 0.6508, precision: 0.8426, recall: 0.6790, f1: 0.7520, edges-ner-ontonotes_loss: 0.0720
09/16 12:59:51 PM: Update 18720: task edges-ner-ontonotes, batch 720 (18720): mcc: 0.7443, acc: 0.6516, precision: 0.8431, recall: 0.6793, f1: 0.7524, edges-ner-ontonotes_loss: 0.0721
09/16 01:00:01 PM: Update 18887: task edges-ner-ontonotes, batch 887 (18887): mcc: 0.7505, acc: 0.6594, precision: 0.8485, recall: 0.6855, f1: 0.7584, edges-ner-ontonotes_loss: 0.0704
09/16 01:00:15 PM: Update 18986: task edges-ner-ontonotes, batch 986 (18986): mcc: 0.7538, acc: 0.6636, precision: 0.8512, recall: 0.6891, f1: 0.7616, edges-ner-ontonotes_loss: 0.0697
09/16 01:00:16 PM: ***** Step 19000 / Validation 19 *****
09/16 01:00:16 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:00:16 PM: Validating...
09/16 01:00:25 PM: Evaluate: task edges-ner-ontonotes, batch 117 (157): mcc: 0.7278, acc: 0.6361, precision: 0.8508, recall: 0.6448, f1: 0.7336, edges-ner-ontonotes_loss: 0.0771
09/16 01:00:28 PM: Updating LR scheduler:
09/16 01:00:28 PM: 	Best result seen so far for macro_avg: 0.751
09/16 01:00:28 PM: 	# validation passes without improvement: 1
09/16 01:00:28 PM: edges-ner-ontonotes_loss: training: 0.069724 validation: 0.074959
09/16 01:00:28 PM: macro_avg: validation: 0.740111
09/16 01:00:28 PM: micro_avg: validation: 0.000000
09/16 01:00:28 PM: edges-ner-ontonotes_mcc: training: 0.753724 validation: 0.734764
09/16 01:00:28 PM: edges-ner-ontonotes_acc: training: 0.663372 validation: 0.642251
09/16 01:00:28 PM: edges-ner-ontonotes_precision: training: 0.851391 validation: 0.858329
09/16 01:00:28 PM: edges-ner-ontonotes_recall: training: 0.688827 validation: 0.650516
09/16 01:00:28 PM: edges-ner-ontonotes_f1: training: 0.761530 validation: 0.740111
09/16 01:00:28 PM: Global learning rate: 5e-05
09/16 01:00:28 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:00:35 PM: Update 19122: task edges-ner-ontonotes, batch 122 (19122): mcc: 0.7602, acc: 0.6726, precision: 0.8557, recall: 0.6966, f1: 0.7680, edges-ner-ontonotes_loss: 0.0670
09/16 01:00:45 PM: Update 19230: task edges-ner-ontonotes, batch 230 (19230): mcc: 0.7622, acc: 0.6752, precision: 0.8569, recall: 0.6990, f1: 0.7699, edges-ner-ontonotes_loss: 0.0670
09/16 01:00:56 PM: Update 19299: task edges-ner-ontonotes, batch 299 (19299): mcc: 0.7654, acc: 0.6792, precision: 0.8587, recall: 0.7031, f1: 0.7731, edges-ner-ontonotes_loss: 0.0663
09/16 01:01:07 PM: Update 19440: task edges-ner-ontonotes, batch 440 (19440): mcc: 0.7596, acc: 0.6698, precision: 0.8567, recall: 0.6946, f1: 0.7672, edges-ner-ontonotes_loss: 0.0671
09/16 01:01:19 PM: Update 19612: task edges-ner-ontonotes, batch 612 (19612): mcc: 0.7583, acc: 0.6680, precision: 0.8546, recall: 0.6941, f1: 0.7660, edges-ner-ontonotes_loss: 0.0669
09/16 01:01:29 PM: Update 19819: task edges-ner-ontonotes, batch 819 (19819): mcc: 0.7578, acc: 0.6664, precision: 0.8533, recall: 0.6943, f1: 0.7657, edges-ner-ontonotes_loss: 0.0669
09/16 01:01:41 PM: Update 19925: task edges-ner-ontonotes, batch 925 (19925): mcc: 0.7584, acc: 0.6668, precision: 0.8533, recall: 0.6954, f1: 0.7663, edges-ner-ontonotes_loss: 0.0667
09/16 01:01:43 PM: ***** Step 20000 / Validation 20 *****
09/16 01:01:43 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:01:43 PM: Validating...
09/16 01:01:51 PM: Evaluate: task edges-ner-ontonotes, batch 125 (157): mcc: 0.7339, acc: 0.6440, precision: 0.8487, recall: 0.6568, f1: 0.7405, edges-ner-ontonotes_loss: 0.0769
09/16 01:01:54 PM: Updating LR scheduler:
09/16 01:01:54 PM: 	Best result seen so far for macro_avg: 0.751
09/16 01:01:54 PM: 	# validation passes without improvement: 2
09/16 01:01:54 PM: edges-ner-ontonotes_loss: training: 0.068072 validation: 0.074297
09/16 01:01:54 PM: macro_avg: validation: 0.748569
09/16 01:01:54 PM: micro_avg: validation: 0.000000
09/16 01:01:54 PM: edges-ner-ontonotes_mcc: training: 0.755457 validation: 0.742313
09/16 01:01:54 PM: edges-ner-ontonotes_acc: training: 0.663656 validation: 0.652639
09/16 01:01:54 PM: edges-ner-ontonotes_precision: training: 0.851047 validation: 0.856794
09/16 01:01:54 PM: edges-ner-ontonotes_recall: training: 0.692148 validation: 0.664619
09/16 01:01:54 PM: edges-ner-ontonotes_f1: training: 0.763417 validation: 0.748569
09/16 01:01:54 PM: Global learning rate: 5e-05
09/16 01:01:54 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:02:01 PM: Update 20102: task edges-ner-ontonotes, batch 102 (20102): mcc: 0.7217, acc: 0.6257, precision: 0.8306, recall: 0.6506, f1: 0.7297, edges-ner-ontonotes_loss: 0.0789
09/16 01:02:16 PM: Update 20229: task edges-ner-ontonotes, batch 229 (20229): mcc: 0.7277, acc: 0.6343, precision: 0.8334, recall: 0.6586, f1: 0.7358, edges-ner-ontonotes_loss: 0.0784
09/16 01:02:26 PM: Update 20366: task edges-ner-ontonotes, batch 366 (20366): mcc: 0.7459, acc: 0.6550, precision: 0.8467, recall: 0.6791, f1: 0.7537, edges-ner-ontonotes_loss: 0.0729
09/16 01:02:36 PM: Update 20502: task edges-ner-ontonotes, batch 502 (20502): mcc: 0.7566, acc: 0.6676, precision: 0.8554, recall: 0.6906, f1: 0.7642, edges-ner-ontonotes_loss: 0.0701
09/16 01:02:46 PM: Update 20573: task edges-ner-ontonotes, batch 573 (20573): mcc: 0.7578, acc: 0.6694, precision: 0.8559, recall: 0.6921, f1: 0.7654, edges-ner-ontonotes_loss: 0.0695
09/16 01:02:56 PM: Update 20686: task edges-ner-ontonotes, batch 686 (20686): mcc: 0.7591, acc: 0.6705, precision: 0.8572, recall: 0.6933, f1: 0.7666, edges-ner-ontonotes_loss: 0.0688
09/16 01:03:06 PM: Update 20808: task edges-ner-ontonotes, batch 808 (20808): mcc: 0.7595, acc: 0.6718, precision: 0.8562, recall: 0.6948, f1: 0.7671, edges-ner-ontonotes_loss: 0.0689
09/16 01:03:16 PM: Update 20884: task edges-ner-ontonotes, batch 884 (20884): mcc: 0.7602, acc: 0.6725, precision: 0.8566, recall: 0.6957, f1: 0.7678, edges-ner-ontonotes_loss: 0.0685
09/16 01:03:26 PM: Update 20984: task edges-ner-ontonotes, batch 984 (20984): mcc: 0.7585, acc: 0.6697, precision: 0.8557, recall: 0.6936, f1: 0.7662, edges-ner-ontonotes_loss: 0.0685
09/16 01:03:27 PM: ***** Step 21000 / Validation 21 *****
09/16 01:03:28 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:03:28 PM: Validating...
09/16 01:03:36 PM: Evaluate: task edges-ner-ontonotes, batch 89 (157): mcc: 0.7496, acc: 0.6661, precision: 0.8586, recall: 0.6757, f1: 0.7562, edges-ner-ontonotes_loss: 0.0759
09/16 01:03:41 PM: Best result seen so far for edges-ner-ontonotes.
09/16 01:03:41 PM: Best result seen so far for macro.
09/16 01:03:41 PM: Updating LR scheduler:
09/16 01:03:41 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:03:41 PM: 	# validation passes without improvement: 0
09/16 01:03:41 PM: edges-ner-ontonotes_loss: training: 0.068488 validation: 0.073974
09/16 01:03:41 PM: macro_avg: validation: 0.753126
09/16 01:03:41 PM: micro_avg: validation: 0.000000
09/16 01:03:41 PM: edges-ner-ontonotes_mcc: training: 0.758046 validation: 0.746156
09/16 01:03:41 PM: edges-ner-ontonotes_acc: training: 0.669029 validation: 0.658857
09/16 01:03:41 PM: edges-ner-ontonotes_precision: training: 0.855444 validation: 0.853752
09/16 01:03:41 PM: edges-ner-ontonotes_recall: training: 0.692950 validation: 0.673719
09/16 01:03:41 PM: edges-ner-ontonotes_f1: training: 0.765671 validation: 0.753126
09/16 01:03:41 PM: Global learning rate: 5e-05
09/16 01:03:41 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:03:46 PM: Update 21069: task edges-ner-ontonotes, batch 69 (21069): mcc: 0.7551, acc: 0.6629, precision: 0.8489, recall: 0.6935, f1: 0.7633, edges-ner-ontonotes_loss: 0.0668
09/16 01:04:02 PM: Update 21168: task edges-ner-ontonotes, batch 168 (21168): mcc: 0.7550, acc: 0.6622, precision: 0.8504, recall: 0.6920, f1: 0.7630, edges-ner-ontonotes_loss: 0.0668
09/16 01:04:12 PM: Update 21275: task edges-ner-ontonotes, batch 275 (21275): mcc: 0.7549, acc: 0.6620, precision: 0.8481, recall: 0.6937, f1: 0.7631, edges-ner-ontonotes_loss: 0.0672
09/16 01:04:22 PM: Update 21418: task edges-ner-ontonotes, batch 418 (21418): mcc: 0.7558, acc: 0.6627, precision: 0.8485, recall: 0.6949, f1: 0.7641, edges-ner-ontonotes_loss: 0.0665
09/16 01:04:32 PM: Update 21551: task edges-ner-ontonotes, batch 551 (21551): mcc: 0.7518, acc: 0.6590, precision: 0.8452, recall: 0.6908, f1: 0.7602, edges-ner-ontonotes_loss: 0.0683
09/16 01:04:42 PM: Update 21739: task edges-ner-ontonotes, batch 739 (21739): mcc: 0.7455, acc: 0.6526, precision: 0.8419, recall: 0.6825, f1: 0.7539, edges-ner-ontonotes_loss: 0.0712
09/16 01:04:52 PM: Update 21794: task edges-ner-ontonotes, batch 794 (21794): mcc: 0.7452, acc: 0.6525, precision: 0.8419, recall: 0.6820, f1: 0.7535, edges-ner-ontonotes_loss: 0.0714
09/16 01:05:02 PM: Update 21936: task edges-ner-ontonotes, batch 936 (21936): mcc: 0.7482, acc: 0.6562, precision: 0.8446, recall: 0.6849, f1: 0.7564, edges-ner-ontonotes_loss: 0.0705
09/16 01:05:07 PM: ***** Step 22000 / Validation 22 *****
09/16 01:05:07 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:05:07 PM: Validating...
09/16 01:05:12 PM: Evaluate: task edges-ner-ontonotes, batch 62 (157): mcc: 0.7529, acc: 0.6675, precision: 0.8675, recall: 0.6740, f1: 0.7586, edges-ner-ontonotes_loss: 0.0726
09/16 01:05:21 PM: Updating LR scheduler:
09/16 01:05:21 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:05:21 PM: 	# validation passes without improvement: 1
09/16 01:05:21 PM: edges-ner-ontonotes_loss: training: 0.069900 validation: 0.074575
09/16 01:05:21 PM: macro_avg: validation: 0.742744
09/16 01:05:21 PM: micro_avg: validation: 0.000000
09/16 01:05:21 PM: edges-ner-ontonotes_mcc: training: 0.750699 validation: 0.737756
09/16 01:05:21 PM: edges-ner-ontonotes_acc: training: 0.659193 validation: 0.644449
09/16 01:05:21 PM: edges-ner-ontonotes_precision: training: 0.846821 validation: 0.862906
09/16 01:05:21 PM: edges-ner-ontonotes_recall: training: 0.687407 validation: 0.651956
09/16 01:05:21 PM: edges-ner-ontonotes_f1: training: 0.758832 validation: 0.742744
09/16 01:05:21 PM: Global learning rate: 5e-05
09/16 01:05:21 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:05:22 PM: Update 22017: task edges-ner-ontonotes, batch 17 (22017): mcc: 0.7947, acc: 0.7212, precision: 0.8736, recall: 0.7420, f1: 0.8024, edges-ner-ontonotes_loss: 0.0633
09/16 01:05:34 PM: Update 22106: task edges-ner-ontonotes, batch 106 (22106): mcc: 0.7850, acc: 0.7040, precision: 0.8707, recall: 0.7272, f1: 0.7925, edges-ner-ontonotes_loss: 0.0621
09/16 01:05:44 PM: Update 22225: task edges-ner-ontonotes, batch 225 (22225): mcc: 0.7744, acc: 0.6899, precision: 0.8659, recall: 0.7127, f1: 0.7819, edges-ner-ontonotes_loss: 0.0640
09/16 01:05:54 PM: Update 22354: task edges-ner-ontonotes, batch 354 (22354): mcc: 0.7722, acc: 0.6877, precision: 0.8636, recall: 0.7107, f1: 0.7797, edges-ner-ontonotes_loss: 0.0649
09/16 01:06:04 PM: Update 22470: task edges-ner-ontonotes, batch 470 (22470): mcc: 0.7659, acc: 0.6789, precision: 0.8596, recall: 0.7030, f1: 0.7735, edges-ner-ontonotes_loss: 0.0660
09/16 01:06:14 PM: Update 22607: task edges-ner-ontonotes, batch 607 (22607): mcc: 0.7631, acc: 0.6740, precision: 0.8583, recall: 0.6994, f1: 0.7708, edges-ner-ontonotes_loss: 0.0662
09/16 01:06:24 PM: Update 22753: task edges-ner-ontonotes, batch 753 (22753): mcc: 0.7621, acc: 0.6723, precision: 0.8565, recall: 0.6992, f1: 0.7699, edges-ner-ontonotes_loss: 0.0661
09/16 01:06:34 PM: Update 22925: task edges-ner-ontonotes, batch 925 (22925): mcc: 0.7619, acc: 0.6714, precision: 0.8557, recall: 0.6994, f1: 0.7697, edges-ner-ontonotes_loss: 0.0660
09/16 01:06:39 PM: ***** Step 23000 / Validation 23 *****
09/16 01:06:39 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:06:39 PM: Validating...
09/16 01:06:44 PM: Evaluate: task edges-ner-ontonotes, batch 80 (157): mcc: 0.7393, acc: 0.6503, precision: 0.8552, recall: 0.6607, f1: 0.7455, edges-ner-ontonotes_loss: 0.0790
09/16 01:06:50 PM: Updating LR scheduler:
09/16 01:06:50 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:06:50 PM: 	# validation passes without improvement: 2
09/16 01:06:50 PM: edges-ner-ontonotes_loss: training: 0.065817 validation: 0.074608
09/16 01:06:50 PM: macro_avg: validation: 0.751834
09/16 01:06:50 PM: micro_avg: validation: 0.000000
09/16 01:06:50 PM: edges-ner-ontonotes_mcc: training: 0.762198 validation: 0.744841
09/16 01:06:50 PM: edges-ner-ontonotes_acc: training: 0.671818 validation: 0.658629
09/16 01:06:50 PM: edges-ner-ontonotes_precision: training: 0.855937 validation: 0.852746
09/16 01:06:50 PM: edges-ner-ontonotes_recall: training: 0.699797 validation: 0.672278
09/16 01:06:50 PM: edges-ner-ontonotes_f1: training: 0.770032 validation: 0.751834
09/16 01:06:50 PM: Global learning rate: 5e-05
09/16 01:06:50 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:06:57 PM: Update 23037: task edges-ner-ontonotes, batch 37 (23037): mcc: 0.7498, acc: 0.6567, precision: 0.8453, recall: 0.6871, f1: 0.7581, edges-ner-ontonotes_loss: 0.0671
09/16 01:07:07 PM: Update 23168: task edges-ner-ontonotes, batch 168 (23168): mcc: 0.7258, acc: 0.6294, precision: 0.8305, recall: 0.6578, f1: 0.7341, edges-ner-ontonotes_loss: 0.0782
09/16 01:07:17 PM: Update 23284: task edges-ner-ontonotes, batch 284 (23284): mcc: 0.7278, acc: 0.6340, precision: 0.8307, recall: 0.6611, f1: 0.7363, edges-ner-ontonotes_loss: 0.0779
09/16 01:07:27 PM: Update 23363: task edges-ner-ontonotes, batch 363 (23363): mcc: 0.7294, acc: 0.6360, precision: 0.8328, recall: 0.6622, f1: 0.7378, edges-ner-ontonotes_loss: 0.0772
09/16 01:07:37 PM: Update 23502: task edges-ner-ontonotes, batch 502 (23502): mcc: 0.7427, acc: 0.6517, precision: 0.8426, recall: 0.6770, f1: 0.7508, edges-ner-ontonotes_loss: 0.0732
09/16 01:07:47 PM: Update 23638: task edges-ner-ontonotes, batch 638 (23638): mcc: 0.7515, acc: 0.6622, precision: 0.8492, recall: 0.6869, f1: 0.7595, edges-ner-ontonotes_loss: 0.0709
09/16 01:07:58 PM: Update 23701: task edges-ner-ontonotes, batch 701 (23701): mcc: 0.7521, acc: 0.6624, precision: 0.8502, recall: 0.6869, f1: 0.7599, edges-ner-ontonotes_loss: 0.0706
09/16 01:08:08 PM: Update 23874: task edges-ner-ontonotes, batch 874 (23874): mcc: 0.7550, acc: 0.6659, precision: 0.8522, recall: 0.6903, f1: 0.7628, edges-ner-ontonotes_loss: 0.0699
09/16 01:08:19 PM: Update 23967: task edges-ner-ontonotes, batch 967 (23967): mcc: 0.7570, acc: 0.6685, precision: 0.8536, recall: 0.6927, f1: 0.7648, edges-ner-ontonotes_loss: 0.0694
09/16 01:08:21 PM: ***** Step 24000 / Validation 24 *****
09/16 01:08:21 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:08:21 PM: Validating...
09/16 01:08:29 PM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.7372, acc: 0.6518, precision: 0.8491, recall: 0.6621, f1: 0.7440, edges-ner-ontonotes_loss: 0.0775
09/16 01:08:32 PM: Updating LR scheduler:
09/16 01:08:32 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:08:32 PM: 	# validation passes without improvement: 3
09/16 01:08:32 PM: edges-ner-ontonotes_loss: training: 0.069357 validation: 0.073966
09/16 01:08:32 PM: macro_avg: validation: 0.749574
09/16 01:08:32 PM: micro_avg: validation: 0.000000
09/16 01:08:32 PM: edges-ner-ontonotes_mcc: training: 0.756336 validation: 0.743160
09/16 01:08:32 PM: edges-ner-ontonotes_acc: training: 0.667489 validation: 0.656809
09/16 01:08:32 PM: edges-ner-ontonotes_precision: training: 0.853313 validation: 0.856155
09/16 01:08:32 PM: edges-ner-ontonotes_recall: training: 0.691766 validation: 0.666591
09/16 01:08:32 PM: edges-ner-ontonotes_f1: training: 0.764094 validation: 0.749574
09/16 01:08:32 PM: Global learning rate: 5e-05
09/16 01:08:32 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:08:39 PM: Update 24096: task edges-ner-ontonotes, batch 96 (24096): mcc: 0.7465, acc: 0.6507, precision: 0.8439, recall: 0.6825, f1: 0.7547, edges-ner-ontonotes_loss: 0.0691
09/16 01:08:49 PM: Update 24214: task edges-ner-ontonotes, batch 214 (24214): mcc: 0.7509, acc: 0.6563, precision: 0.8483, recall: 0.6865, f1: 0.7589, edges-ner-ontonotes_loss: 0.0678
09/16 01:09:00 PM: Update 24280: task edges-ner-ontonotes, batch 280 (24280): mcc: 0.7521, acc: 0.6582, precision: 0.8486, recall: 0.6884, f1: 0.7601, edges-ner-ontonotes_loss: 0.0673
09/16 01:09:10 PM: Update 24392: task edges-ner-ontonotes, batch 392 (24392): mcc: 0.7525, acc: 0.6585, precision: 0.8482, recall: 0.6895, f1: 0.7606, edges-ner-ontonotes_loss: 0.0671
09/16 01:09:20 PM: Update 24529: task edges-ner-ontonotes, batch 529 (24529): mcc: 0.7559, acc: 0.6627, precision: 0.8491, recall: 0.6947, f1: 0.7641, edges-ner-ontonotes_loss: 0.0664
09/16 01:09:30 PM: Update 24593: task edges-ner-ontonotes, batch 593 (24593): mcc: 0.7561, acc: 0.6630, precision: 0.8494, recall: 0.6947, f1: 0.7643, edges-ner-ontonotes_loss: 0.0664
09/16 01:09:40 PM: Update 24762: task edges-ner-ontonotes, batch 762 (24762): mcc: 0.7490, acc: 0.6554, precision: 0.8447, recall: 0.6863, f1: 0.7573, edges-ner-ontonotes_loss: 0.0695
09/16 01:09:54 PM: Update 24897: task edges-ner-ontonotes, batch 897 (24897): mcc: 0.7472, acc: 0.6539, precision: 0.8443, recall: 0.6834, f1: 0.7554, edges-ner-ontonotes_loss: 0.0706
09/16 01:10:02 PM: ***** Step 25000 / Validation 25 *****
09/16 01:10:02 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:10:02 PM: Validating...
09/16 01:10:04 PM: Evaluate: task edges-ner-ontonotes, batch 22 (157): mcc: 0.6942, acc: 0.6104, precision: 0.8149, recall: 0.6163, f1: 0.7018, edges-ner-ontonotes_loss: 0.0898
09/16 01:10:14 PM: Updating LR scheduler:
09/16 01:10:15 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:10:15 PM: 	# validation passes without improvement: 0
09/16 01:10:15 PM: edges-ner-ontonotes_loss: training: 0.070048 validation: 0.074380
09/16 01:10:15 PM: macro_avg: validation: 0.745430
09/16 01:10:15 PM: micro_avg: validation: 0.000000
09/16 01:10:15 PM: edges-ner-ontonotes_mcc: training: 0.749777 validation: 0.739983
09/16 01:10:15 PM: edges-ner-ontonotes_acc: training: 0.657498 validation: 0.650212
09/16 01:10:15 PM: edges-ner-ontonotes_precision: training: 0.846233 validation: 0.861175
09/16 01:10:15 PM: edges-ner-ontonotes_recall: training: 0.686293 validation: 0.657113
09/16 01:10:15 PM: edges-ner-ontonotes_f1: training: 0.757917 validation: 0.745430
09/16 01:10:15 PM: Global learning rate: 2.5e-05
09/16 01:10:15 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:10:15 PM: Update 25012: task edges-ner-ontonotes, batch 12 (25012): mcc: 0.7852, acc: 0.7017, precision: 0.8587, recall: 0.7382, f1: 0.7939, edges-ner-ontonotes_loss: 0.0615
09/16 01:10:25 PM: Update 25136: task edges-ner-ontonotes, batch 136 (25136): mcc: 0.7856, acc: 0.7021, precision: 0.8757, recall: 0.7239, f1: 0.7926, edges-ner-ontonotes_loss: 0.0624
09/16 01:10:35 PM: Update 25211: task edges-ner-ontonotes, batch 211 (25211): mcc: 0.7857, acc: 0.7029, precision: 0.8749, recall: 0.7248, f1: 0.7928, edges-ner-ontonotes_loss: 0.0620
09/16 01:10:45 PM: Update 25344: task edges-ner-ontonotes, batch 344 (25344): mcc: 0.7761, acc: 0.6922, precision: 0.8679, recall: 0.7141, f1: 0.7835, edges-ner-ontonotes_loss: 0.0641
09/16 01:10:57 PM: Update 25497: task edges-ner-ontonotes, batch 497 (25497): mcc: 0.7731, acc: 0.6890, precision: 0.8651, recall: 0.7111, f1: 0.7806, edges-ner-ontonotes_loss: 0.0647
09/16 01:11:07 PM: Update 25524: task edges-ner-ontonotes, batch 524 (25524): mcc: 0.7723, acc: 0.6883, precision: 0.8644, recall: 0.7104, f1: 0.7799, edges-ner-ontonotes_loss: 0.0648
09/16 01:11:17 PM: Update 25635: task edges-ner-ontonotes, batch 635 (25635): mcc: 0.7649, acc: 0.6782, precision: 0.8599, recall: 0.7011, f1: 0.7724, edges-ner-ontonotes_loss: 0.0657
09/16 01:11:27 PM: Update 25811: task edges-ner-ontonotes, batch 811 (25811): mcc: 0.7634, acc: 0.6750, precision: 0.8589, recall: 0.6993, f1: 0.7709, edges-ner-ontonotes_loss: 0.0658
09/16 01:11:37 PM: Update 25925: task edges-ner-ontonotes, batch 925 (25925): mcc: 0.7625, acc: 0.6732, precision: 0.8574, recall: 0.6992, f1: 0.7702, edges-ner-ontonotes_loss: 0.0657
09/16 01:11:41 PM: ***** Step 26000 / Validation 26 *****
09/16 01:11:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:11:41 PM: Validating...
09/16 01:11:47 PM: Evaluate: task edges-ner-ontonotes, batch 135 (157): mcc: 0.7430, acc: 0.6575, precision: 0.8506, recall: 0.6709, f1: 0.7501, edges-ner-ontonotes_loss: 0.0748
09/16 01:11:48 PM: Updating LR scheduler:
09/16 01:11:48 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:11:48 PM: 	# validation passes without improvement: 1
09/16 01:11:48 PM: edges-ner-ontonotes_loss: training: 0.065781 validation: 0.073940
09/16 01:11:48 PM: macro_avg: validation: 0.751243
09/16 01:11:48 PM: micro_avg: validation: 0.000000
09/16 01:11:48 PM: edges-ner-ontonotes_mcc: training: 0.761834 validation: 0.744521
09/16 01:11:48 PM: edges-ner-ontonotes_acc: training: 0.672142 validation: 0.657416
09/16 01:11:48 PM: edges-ner-ontonotes_precision: training: 0.856636 validation: 0.854656
09/16 01:11:48 PM: edges-ner-ontonotes_recall: training: 0.698565 validation: 0.670155
09/16 01:11:48 PM: edges-ner-ontonotes_f1: training: 0.769567 validation: 0.751243
09/16 01:11:48 PM: Global learning rate: 2.5e-05
09/16 01:11:48 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:11:57 PM: Update 26143: task edges-ner-ontonotes, batch 143 (26143): mcc: 0.7618, acc: 0.6695, precision: 0.8532, recall: 0.7014, f1: 0.7699, edges-ner-ontonotes_loss: 0.0658
09/16 01:12:07 PM: Update 26190: task edges-ner-ontonotes, batch 190 (26190): mcc: 0.7523, acc: 0.6601, precision: 0.8457, recall: 0.6910, f1: 0.7606, edges-ner-ontonotes_loss: 0.0689
09/16 01:12:17 PM: Update 26293: task edges-ner-ontonotes, batch 293 (26293): mcc: 0.7438, acc: 0.6519, precision: 0.8390, recall: 0.6820, f1: 0.7524, edges-ner-ontonotes_loss: 0.0725
09/16 01:12:27 PM: Update 26448: task edges-ner-ontonotes, batch 448 (26448): mcc: 0.7408, acc: 0.6490, precision: 0.8391, recall: 0.6767, f1: 0.7492, edges-ner-ontonotes_loss: 0.0742
09/16 01:12:37 PM: Update 26510: task edges-ner-ontonotes, batch 510 (26510): mcc: 0.7438, acc: 0.6524, precision: 0.8417, recall: 0.6796, f1: 0.7520, edges-ner-ontonotes_loss: 0.0732
09/16 01:12:47 PM: Update 26706: task edges-ner-ontonotes, batch 706 (26706): mcc: 0.7533, acc: 0.6639, precision: 0.8491, recall: 0.6900, f1: 0.7613, edges-ner-ontonotes_loss: 0.0702
09/16 01:12:57 PM: Update 26805: task edges-ner-ontonotes, batch 805 (26805): mcc: 0.7565, acc: 0.6676, precision: 0.8522, recall: 0.6930, f1: 0.7644, edges-ner-ontonotes_loss: 0.0693
09/16 01:13:07 PM: Update 26930: task edges-ner-ontonotes, batch 930 (26930): mcc: 0.7574, acc: 0.6688, precision: 0.8532, recall: 0.6938, f1: 0.7653, edges-ner-ontonotes_loss: 0.0690
09/16 01:13:13 PM: ***** Step 27000 / Validation 27 *****
09/16 01:13:13 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:13:13 PM: Validating...
09/16 01:13:17 PM: Evaluate: task edges-ner-ontonotes, batch 77 (157): mcc: 0.7489, acc: 0.6614, precision: 0.8615, recall: 0.6720, f1: 0.7551, edges-ner-ontonotes_loss: 0.0758
09/16 01:13:23 PM: Updating LR scheduler:
09/16 01:13:23 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:13:23 PM: 	# validation passes without improvement: 2
09/16 01:13:23 PM: edges-ner-ontonotes_loss: training: 0.068675 validation: 0.073802
09/16 01:13:23 PM: macro_avg: validation: 0.752918
09/16 01:13:23 PM: micro_avg: validation: 0.000000
09/16 01:13:23 PM: edges-ner-ontonotes_mcc: training: 0.758723 validation: 0.747094
09/16 01:13:23 PM: edges-ner-ontonotes_acc: training: 0.670283 validation: 0.657264
09/16 01:13:23 PM: edges-ner-ontonotes_precision: training: 0.854296 validation: 0.863151
09/16 01:13:23 PM: edges-ner-ontonotes_recall: training: 0.695105 validation: 0.667652
09/16 01:13:23 PM: edges-ner-ontonotes_f1: training: 0.766522 validation: 0.752918
09/16 01:13:23 PM: Global learning rate: 2.5e-05
09/16 01:13:23 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:13:27 PM: Update 27048: task edges-ner-ontonotes, batch 48 (27048): mcc: 0.7653, acc: 0.6828, precision: 0.8559, recall: 0.7052, f1: 0.7733, edges-ner-ontonotes_loss: 0.0679
09/16 01:13:38 PM: Update 27124: task edges-ner-ontonotes, batch 124 (27124): mcc: 0.7647, acc: 0.6785, precision: 0.8581, recall: 0.7023, f1: 0.7724, edges-ner-ontonotes_loss: 0.0661
09/16 01:13:48 PM: Update 27253: task edges-ner-ontonotes, batch 253 (27253): mcc: 0.7572, acc: 0.6658, precision: 0.8557, recall: 0.6913, f1: 0.7648, edges-ner-ontonotes_loss: 0.0668
09/16 01:14:01 PM: Update 27392: task edges-ner-ontonotes, batch 392 (27392): mcc: 0.7564, acc: 0.6640, precision: 0.8544, recall: 0.6910, f1: 0.7640, edges-ner-ontonotes_loss: 0.0668
09/16 01:14:12 PM: Update 27553: task edges-ner-ontonotes, batch 553 (27553): mcc: 0.7577, acc: 0.6644, precision: 0.8543, recall: 0.6932, f1: 0.7654, edges-ner-ontonotes_loss: 0.0662
09/16 01:14:22 PM: Update 27683: task edges-ner-ontonotes, batch 683 (27683): mcc: 0.7563, acc: 0.6632, precision: 0.8522, recall: 0.6926, f1: 0.7641, edges-ner-ontonotes_loss: 0.0664
09/16 01:14:32 PM: Update 27746: task edges-ner-ontonotes, batch 746 (27746): mcc: 0.7549, acc: 0.6620, precision: 0.8513, recall: 0.6910, f1: 0.7628, edges-ner-ontonotes_loss: 0.0669
09/16 01:14:42 PM: Update 27873: task edges-ner-ontonotes, batch 873 (27873): mcc: 0.7517, acc: 0.6590, precision: 0.8490, recall: 0.6873, f1: 0.7597, edges-ner-ontonotes_loss: 0.0687
09/16 01:14:51 PM: ***** Step 28000 / Validation 28 *****
09/16 01:14:51 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:14:51 PM: Validating...
09/16 01:14:52 PM: Evaluate: task edges-ner-ontonotes, batch 7 (157): mcc: 0.6709, acc: 0.5927, precision: 0.7948, recall: 0.5927, f1: 0.6790, edges-ner-ontonotes_loss: 0.1030
09/16 01:15:02 PM: Evaluate: task edges-ner-ontonotes, batch 114 (157): mcc: 0.7347, acc: 0.6420, precision: 0.8523, recall: 0.6551, f1: 0.7408, edges-ner-ontonotes_loss: 0.0771
09/16 01:15:05 PM: Updating LR scheduler:
09/16 01:15:08 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:15:08 PM: 	# validation passes without improvement: 3
09/16 01:15:08 PM: edges-ner-ontonotes_loss: training: 0.069996 validation: 0.073878
09/16 01:15:08 PM: macro_avg: validation: 0.749496
09/16 01:15:08 PM: micro_avg: validation: 0.000000
09/16 01:15:08 PM: edges-ner-ontonotes_mcc: training: 0.749142 validation: 0.743797
09/16 01:15:08 PM: edges-ner-ontonotes_acc: training: 0.656401 validation: 0.650819
09/16 01:15:08 PM: edges-ner-ontonotes_precision: training: 0.847349 validation: 0.861902
09/16 01:15:08 PM: edges-ner-ontonotes_recall: training: 0.684242 validation: 0.663027
09/16 01:15:08 PM: edges-ner-ontonotes_f1: training: 0.757110 validation: 0.749496
09/16 01:15:08 PM: Global learning rate: 2.5e-05
09/16 01:15:08 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:15:14 PM: Update 28009: task edges-ner-ontonotes, batch 9 (28009): mcc: 0.7359, acc: 0.6320, precision: 0.8368, recall: 0.6699, f1: 0.7441, edges-ner-ontonotes_loss: 0.0771
09/16 01:15:24 PM: Update 28128: task edges-ner-ontonotes, batch 128 (28128): mcc: 0.7747, acc: 0.6899, precision: 0.8676, recall: 0.7118, f1: 0.7820, edges-ner-ontonotes_loss: 0.0645
09/16 01:15:34 PM: Update 28307: task edges-ner-ontonotes, batch 307 (28307): mcc: 0.7830, acc: 0.6999, precision: 0.8724, recall: 0.7222, f1: 0.7902, edges-ner-ontonotes_loss: 0.0630
09/16 01:15:44 PM: Update 28373: task edges-ner-ontonotes, batch 373 (28373): mcc: 0.7776, acc: 0.6930, precision: 0.8698, recall: 0.7150, f1: 0.7849, edges-ner-ontonotes_loss: 0.0636
09/16 01:15:54 PM: Update 28484: task edges-ner-ontonotes, batch 484 (28484): mcc: 0.7751, acc: 0.6900, precision: 0.8669, recall: 0.7132, f1: 0.7825, edges-ner-ontonotes_loss: 0.0640
09/16 01:16:04 PM: Update 28605: task edges-ner-ontonotes, batch 605 (28605): mcc: 0.7744, acc: 0.6889, precision: 0.8666, recall: 0.7121, f1: 0.7818, edges-ner-ontonotes_loss: 0.0643
09/16 01:16:14 PM: Update 28681: task edges-ner-ontonotes, batch 681 (28681): mcc: 0.7710, acc: 0.6847, precision: 0.8640, recall: 0.7084, f1: 0.7785, edges-ner-ontonotes_loss: 0.0649
09/16 01:16:24 PM: Update 28825: task edges-ner-ontonotes, batch 825 (28825): mcc: 0.7672, acc: 0.6792, precision: 0.8617, recall: 0.7036, f1: 0.7747, edges-ner-ontonotes_loss: 0.0652
09/16 01:16:35 PM: Update 28948: task edges-ner-ontonotes, batch 948 (28948): mcc: 0.7658, acc: 0.6769, precision: 0.8607, recall: 0.7021, f1: 0.7733, edges-ner-ontonotes_loss: 0.0652
09/16 01:16:37 PM: ***** Step 29000 / Validation 29 *****
09/16 01:16:37 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:16:37 PM: Validating...
09/16 01:16:45 PM: Evaluate: task edges-ner-ontonotes, batch 131 (157): mcc: 0.7375, acc: 0.6527, precision: 0.8445, recall: 0.6663, f1: 0.7449, edges-ner-ontonotes_loss: 0.0758
09/16 01:16:46 PM: Updating LR scheduler:
09/16 01:16:46 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:16:46 PM: 	# validation passes without improvement: 0
09/16 01:16:46 PM: edges-ner-ontonotes_loss: training: 0.065334 validation: 0.073802
09/16 01:16:46 PM: macro_avg: validation: 0.751949
09/16 01:16:46 PM: micro_avg: validation: 0.000000
09/16 01:16:46 PM: edges-ner-ontonotes_mcc: training: 0.764942 validation: 0.744883
09/16 01:16:46 PM: edges-ner-ontonotes_acc: training: 0.675678 validation: 0.659539
09/16 01:16:46 PM: edges-ner-ontonotes_precision: training: 0.859973 validation: 0.852190
09/16 01:16:46 PM: edges-ner-ontonotes_recall: training: 0.701170 validation: 0.672809
09/16 01:16:46 PM: edges-ner-ontonotes_f1: training: 0.772495 validation: 0.751949
09/16 01:16:46 PM: Global learning rate: 1.25e-05
09/16 01:16:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:16:55 PM: Update 29144: task edges-ner-ontonotes, batch 144 (29144): mcc: 0.7513, acc: 0.6550, precision: 0.8477, recall: 0.6876, f1: 0.7593, edges-ner-ontonotes_loss: 0.0665
09/16 01:17:05 PM: Update 29257: task edges-ner-ontonotes, batch 257 (29257): mcc: 0.7566, acc: 0.6633, precision: 0.8491, recall: 0.6957, f1: 0.7648, edges-ner-ontonotes_loss: 0.0659
09/16 01:17:15 PM: Update 29313: task edges-ner-ontonotes, batch 313 (29313): mcc: 0.7507, acc: 0.6566, precision: 0.8453, recall: 0.6887, f1: 0.7590, edges-ner-ontonotes_loss: 0.0680
09/16 01:17:25 PM: Update 29426: task edges-ner-ontonotes, batch 426 (29426): mcc: 0.7461, acc: 0.6525, precision: 0.8425, recall: 0.6831, f1: 0.7544, edges-ner-ontonotes_loss: 0.0709
09/16 01:17:36 PM: Update 29565: task edges-ner-ontonotes, batch 565 (29565): mcc: 0.7425, acc: 0.6491, precision: 0.8397, recall: 0.6792, f1: 0.7509, edges-ner-ontonotes_loss: 0.0729
09/16 01:17:46 PM: Update 29724: task edges-ner-ontonotes, batch 724 (29724): mcc: 0.7505, acc: 0.6595, precision: 0.8454, recall: 0.6883, f1: 0.7588, edges-ner-ontonotes_loss: 0.0707
09/16 01:17:56 PM: Update 29862: task edges-ner-ontonotes, batch 862 (29862): mcc: 0.7554, acc: 0.6658, precision: 0.8493, recall: 0.6935, f1: 0.7636, edges-ner-ontonotes_loss: 0.0693
09/16 01:18:06 PM: Update 29908: task edges-ner-ontonotes, batch 908 (29908): mcc: 0.7559, acc: 0.6663, precision: 0.8498, recall: 0.6940, f1: 0.7640, edges-ner-ontonotes_loss: 0.0692
09/16 01:18:14 PM: ***** Step 30000 / Validation 30 *****
09/16 01:18:14 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:18:14 PM: Validating...
09/16 01:18:16 PM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.7338, acc: 0.6514, precision: 0.8448, recall: 0.6598, f1: 0.7409, edges-ner-ontonotes_loss: 0.0818
09/16 01:18:25 PM: Updating LR scheduler:
09/16 01:18:25 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:18:25 PM: 	# validation passes without improvement: 1
09/16 01:18:25 PM: edges-ner-ontonotes_loss: training: 0.068735 validation: 0.073437
09/16 01:18:25 PM: macro_avg: validation: 0.752673
09/16 01:18:25 PM: micro_avg: validation: 0.000000
09/16 01:18:25 PM: edges-ner-ontonotes_mcc: training: 0.757999 validation: 0.746851
09/16 01:18:25 PM: edges-ner-ontonotes_acc: training: 0.668814 validation: 0.656582
09/16 01:18:25 PM: edges-ner-ontonotes_precision: training: 0.851533 validation: 0.863012
09/16 01:18:25 PM: edges-ner-ontonotes_recall: training: 0.696187 validation: 0.667349
09/16 01:18:25 PM: edges-ner-ontonotes_f1: training: 0.766064 validation: 0.752673
09/16 01:18:25 PM: Global learning rate: 1.25e-05
09/16 01:18:25 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:18:27 PM: Update 30019: task edges-ner-ontonotes, batch 19 (30019): mcc: 0.7795, acc: 0.6928, precision: 0.8714, recall: 0.7170, f1: 0.7867, edges-ner-ontonotes_loss: 0.0652
09/16 01:18:37 PM: Update 30130: task edges-ner-ontonotes, batch 130 (30130): mcc: 0.7704, acc: 0.6838, precision: 0.8643, recall: 0.7071, f1: 0.7778, edges-ner-ontonotes_loss: 0.0658
09/16 01:18:49 PM: Update 30191: task edges-ner-ontonotes, batch 191 (30191): mcc: 0.7702, acc: 0.6843, precision: 0.8623, recall: 0.7084, f1: 0.7778, edges-ner-ontonotes_loss: 0.0656
09/16 01:18:59 PM: Update 30328: task edges-ner-ontonotes, batch 328 (30328): mcc: 0.7639, acc: 0.6731, precision: 0.8600, recall: 0.6993, f1: 0.7714, edges-ner-ontonotes_loss: 0.0660
09/16 01:19:09 PM: Update 30467: task edges-ner-ontonotes, batch 467 (30467): mcc: 0.7611, acc: 0.6697, precision: 0.8577, recall: 0.6964, f1: 0.7687, edges-ner-ontonotes_loss: 0.0660
09/16 01:19:19 PM: Update 30506: task edges-ner-ontonotes, batch 506 (30506): mcc: 0.7603, acc: 0.6687, precision: 0.8566, recall: 0.6959, f1: 0.7679, edges-ner-ontonotes_loss: 0.0660
09/16 01:19:29 PM: Update 30636: task edges-ner-ontonotes, batch 636 (30636): mcc: 0.7592, acc: 0.6668, precision: 0.8551, recall: 0.6952, f1: 0.7669, edges-ner-ontonotes_loss: 0.0664
09/16 01:19:39 PM: Update 30786: task edges-ner-ontonotes, batch 786 (30786): mcc: 0.7596, acc: 0.6668, precision: 0.8550, recall: 0.6961, f1: 0.7674, edges-ner-ontonotes_loss: 0.0661
09/16 01:19:49 PM: Update 30856: task edges-ner-ontonotes, batch 856 (30856): mcc: 0.7583, acc: 0.6656, precision: 0.8535, recall: 0.6950, f1: 0.7662, edges-ner-ontonotes_loss: 0.0667
09/16 01:20:00 PM: Update 30967: task edges-ner-ontonotes, batch 967 (30967): mcc: 0.7555, acc: 0.6631, precision: 0.8512, recall: 0.6921, f1: 0.7634, edges-ner-ontonotes_loss: 0.0680
09/16 01:20:03 PM: ***** Step 31000 / Validation 31 *****
09/16 01:20:03 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/16 01:20:03 PM: Validating...
09/16 01:20:10 PM: Evaluate: task edges-ner-ontonotes, batch 87 (157): mcc: 0.7481, acc: 0.6587, precision: 0.8647, recall: 0.6682, f1: 0.7539, edges-ner-ontonotes_loss: 0.0756
09/16 01:20:13 PM: Updating LR scheduler:
09/16 01:20:13 PM: 	Best result seen so far for macro_avg: 0.753
09/16 01:20:13 PM: 	# validation passes without improvement: 2
09/16 01:20:13 PM: Ran out of early stopping patience. Stopping training.
09/16 01:20:13 PM: edges-ner-ontonotes_loss: training: 0.068162 validation: 0.073535
09/16 01:20:13 PM: macro_avg: validation: 0.752441
09/16 01:20:13 PM: micro_avg: validation: 0.000000
09/16 01:20:13 PM: edges-ner-ontonotes_mcc: training: 0.755152 validation: 0.746241
09/16 01:20:13 PM: edges-ner-ontonotes_acc: training: 0.662928 validation: 0.656278
09/16 01:20:13 PM: edges-ner-ontonotes_precision: training: 0.850871 validation: 0.859760
09/16 01:20:13 PM: edges-ner-ontonotes_recall: training: 0.691763 validation: 0.668941
09/16 01:20:13 PM: edges-ner-ontonotes_f1: training: 0.763112 validation: 0.752441
09/16 01:20:13 PM: Global learning rate: 1.25e-05
09/16 01:20:13 PM: Saving checkpoints to: ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:20:13 PM: Stopped training after 31 validation checks
09/16 01:20:13 PM: Trained edges-ner-ontonotes for 31000 batches or 19.949 epochs
09/16 01:20:13 PM: ***** VALIDATION RESULTS *****
09/16 01:20:13 PM: edges-ner-ontonotes_f1 (for best val pass 21): edges-ner-ontonotes_loss: 0.07397, macro_avg: 0.75313, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.74616, edges-ner-ontonotes_acc: 0.65886, edges-ner-ontonotes_precision: 0.85375, edges-ner-ontonotes_recall: 0.67372, edges-ner-ontonotes_f1: 0.75313
09/16 01:20:13 PM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.16654, macro_avg: 0.50932, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.48165, edges-ner-ontonotes_acc: 0.44813, edges-ner-ontonotes_precision: 0.52553, edges-ner-ontonotes_recall: 0.49409, edges-ner-ontonotes_f1: 0.50932
09/16 01:20:13 PM: macro_avg (for best val pass 21): edges-ner-ontonotes_loss: 0.07397, macro_avg: 0.75313, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.74616, edges-ner-ontonotes_acc: 0.65886, edges-ner-ontonotes_precision: 0.85375, edges-ner-ontonotes_recall: 0.67372, edges-ner-ontonotes_f1: 0.75313
09/16 01:20:13 PM: Evaluating...
09/16 01:20:13 PM: Loaded model state from ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run/edges-ner-ontonotes/model_state_target_train_val_21.best.th
09/16 01:20:13 PM: Evaluating on: edges-ner-ontonotes, split: val
09/16 01:20:34 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 01:20:34 PM: Finished evaluating on: edges-ner-ontonotes
09/16 01:20:34 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
09/16 01:20:37 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:20:37 PM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:20:37 PM: Evaluating on: edges-ner-ontonotes, split: test
09/16 01:20:49 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/16 01:20:49 PM: Finished evaluating on: edges-ner-ontonotes
09/16 01:20:49 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
09/16 01:20:53 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:20:53 PM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/run
09/16 01:20:53 PM: Writing results for split 'val' to ./experiments/ner-ontonotes-RANDOM_WITH_GOOD_EMBEDDINGS-only/results.tsv
09/16 01:20:53 PM: micro_avg: 0.000, macro_avg: 0.752, edges-ner-ontonotes_mcc: 0.745, edges-ner-ontonotes_acc: 0.658, edges-ner-ontonotes_precision: 0.854, edges-ner-ontonotes_recall: 0.671, edges-ner-ontonotes_f1: 0.752
09/16 01:20:53 PM: Done!
09/16 01:20:53 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
