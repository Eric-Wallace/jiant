09/09 03:49:08 PM: Git branch: master
09/09 03:49:08 PM: Git SHA: e26b1963b48bdbc7f001ca620a782c5a262276eb
09/09 03:49:10 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes- models/hotpot-cat/",
  "exp_name": "experiments/ner-ontonotes- models/hotpot-cat",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes- models/hotpot-cat/run/log.log",
  "lr_patience": 5,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 20,
  "pretrain_tasks": "",
  "pretrained_dir": "models/hotpot",
  "pytorch_transformers_output_mode": "cat",
  "remote_log_name": "experiments/ner-ontonotes- models/hotpot-cat__run",
  "run_dir": "./experiments/ner-ontonotes- models/hotpot-cat/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/09 03:49:10 PM: Saved config to ./experiments/ner-ontonotes- models/hotpot-cat/run/params.conf
09/09 03:49:10 PM: Using random seed 1234
09/09 03:49:10 PM: Using GPU 0
09/09 03:49:10 PM: Loading tasks...
09/09 03:49:10 PM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes- models/hotpot-cat/
09/09 03:49:10 PM: 	Creating task edges-ner-ontonotes from scratch.
09/09 03:49:12 PM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/09 03:49:12 PM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/09 03:49:12 PM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/09 03:49:13 PM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/09 03:49:13 PM: 	Finished loading tasks: edges-ner-ontonotes.
09/09 03:49:13 PM: 	Building vocab from scratch.
09/09 03:49:13 PM: 	Counting units for task edges-ner-ontonotes.
09/09 03:49:15 PM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/09 03:49:15 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /cliphomes/ewallac2/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:16 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/09 03:49:16 PM: 	Saved vocab to ./experiments/ner-ontonotes- models/hotpot-cat/vocab
09/09 03:49:16 PM: Loading token dictionary from ./experiments/ner-ontonotes- models/hotpot-cat/vocab.
09/09 03:49:16 PM: 	Loaded vocab from ./experiments/ner-ontonotes- models/hotpot-cat/vocab
09/09 03:49:16 PM: 	Vocab namespace tokens: size 22840
09/09 03:49:16 PM: 	Vocab namespace chars: size 77
09/09 03:49:16 PM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/09 03:49:16 PM: 	Vocab namespace bert_uncased: size 30524
09/09 03:49:16 PM: 	Finished building vocab.
09/09 03:49:16 PM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/09 03:49:29 PM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes- models/hotpot-cat/preproc/edges-ner-ontonotes__train_data
09/09 03:49:29 PM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/09 03:49:31 PM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes- models/hotpot-cat/preproc/edges-ner-ontonotes__val_data
09/09 03:49:31 PM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/09 03:49:32 PM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes- models/hotpot-cat/preproc/edges-ner-ontonotes__test_data
09/09 03:49:32 PM: 	Finished indexing tasks
09/09 03:49:32 PM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/09 03:49:32 PM: 	  Training on 
09/09 03:49:32 PM: 	  Evaluating on edges-ner-ontonotes
09/09 03:49:32 PM: 	Finished loading tasks in 22.661s
09/09 03:49:32 PM: 	 Tasks: ['edges-ner-ontonotes']
09/09 03:49:32 PM: Building model...
09/09 03:49:32 PM: Using BERT model (bert-base-uncased).
09/09 03:49:32 PM: LOADING A FUNETUNED MODEL from: 
09/09 03:49:32 PM: models/hotpot
09/09 03:49:32 PM: loading configuration file models/hotpot/config.json
09/09 03:49:32 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/09 03:49:32 PM: loading weights file models/hotpot/pytorch_model.bin
09/09 03:49:39 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmph35p8hac
09/09 03:49:39 PM: copying /tmp/tmph35p8hac to cache at ./experiments/ner-ontonotes- models/hotpot-cat/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:39 PM: creating metadata file for ./experiments/ner-ontonotes- models/hotpot-cat/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:39 PM: removing temp file /tmp/tmph35p8hac
09/09 03:49:39 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes- models/hotpot-cat/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:39 PM: Initializing parameters
09/09 03:49:39 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.pooler.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.pooler.dense.weight
09/09 03:49:39 PM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/09 03:49:45 PM: Model specification:
09/09 03:49:45 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(1536, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/09 03:49:45 PM: Model parameters:
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 393216 with torch.Size([256, 1536, 1])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/09 03:49:45 PM: Total number of parameters: 109884946 (1.09885e+08)
09/09 03:49:45 PM: Number of trainable parameters: 402706 (402706)
09/09 03:49:45 PM: Finished building model in 12.448s
09/09 03:49:45 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/09 03:49:49 PM: patience = 20
09/09 03:49:49 PM: val_interval = 1000
09/09 03:49:49 PM: max_vals = 250
09/09 03:49:49 PM: cuda_device = 0
09/09 03:49:49 PM: grad_norm = 5.0
09/09 03:49:49 PM: grad_clipping = None
09/09 03:49:49 PM: lr_decay = 0.99
09/09 03:49:49 PM: min_lr = 1e-06
09/09 03:49:49 PM: keep_all_checkpoints = 0
09/09 03:49:49 PM: val_data_limit = 5000
09/09 03:49:49 PM: max_epochs = -1
09/09 03:49:49 PM: dec_val_scale = 250
09/09 03:49:49 PM: training_data_fraction = 1
09/09 03:49:49 PM: type = adam
09/09 03:49:49 PM: parameter_groups = None
09/09 03:49:49 PM: Number of trainable parameters: 402706
09/09 03:49:49 PM: infer_type_and_cast = True
09/09 03:49:49 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:49 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:49 PM: lr = 0.0001
09/09 03:49:49 PM: amsgrad = True
09/09 03:49:49 PM: type = reduce_on_plateau
09/09 03:49:49 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:49 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:49 PM: mode = max
09/09 03:49:49 PM: factor = 0.5
09/09 03:49:49 PM: patience = 5
09/09 03:49:49 PM: threshold = 0.0001
09/09 03:49:49 PM: threshold_mode = abs
09/09 03:49:49 PM: verbose = True
09/09 03:49:49 PM: type = adam
09/09 03:49:49 PM: parameter_groups = None
09/09 03:49:49 PM: Number of trainable parameters: 402706
09/09 03:49:49 PM: infer_type_and_cast = True
09/09 03:49:49 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:49 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:49 PM: lr = 0.0001
09/09 03:49:49 PM: amsgrad = True
09/09 03:49:49 PM: type = reduce_on_plateau
09/09 03:49:49 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:49 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:49 PM: mode = max
09/09 03:49:49 PM: factor = 0.5
09/09 03:49:49 PM: patience = 5
09/09 03:49:49 PM: threshold = 0.0001
09/09 03:49:49 PM: threshold_mode = abs
09/09 03:49:49 PM: verbose = True
09/09 03:49:49 PM: Starting training without restoring from a checkpoint.
09/09 03:49:49 PM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/09 03:49:49 PM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/09 03:50:00 PM: Update 89: task edges-ner-ontonotes, batch 89 (89): mcc: 0.1298, acc: 0.1066, precision: 0.2064, recall: 0.1389, f1: 0.1661, edges-ner-ontonotes_loss: 0.2177
09/09 03:50:10 PM: Update 198: task edges-ner-ontonotes, batch 198 (198): mcc: 0.4229, acc: 0.3316, precision: 0.5765, recall: 0.3468, f1: 0.4331, edges-ner-ontonotes_loss: 0.1501
09/09 03:50:20 PM: Update 275: task edges-ner-ontonotes, batch 275 (275): mcc: 0.5244, acc: 0.4213, precision: 0.6858, recall: 0.4327, f1: 0.5306, edges-ner-ontonotes_loss: 0.1285
09/09 03:50:31 PM: Update 326: task edges-ner-ontonotes, batch 326 (326): mcc: 0.5704, acc: 0.4645, precision: 0.7314, recall: 0.4743, f1: 0.5754, edges-ner-ontonotes_loss: 0.1192
09/09 03:50:41 PM: Update 390: task edges-ner-ontonotes, batch 390 (390): mcc: 0.6133, acc: 0.5067, precision: 0.7704, recall: 0.5157, f1: 0.6178, edges-ner-ontonotes_loss: 0.1107
09/09 03:50:51 PM: Update 453: task edges-ner-ontonotes, batch 453 (453): mcc: 0.6460, acc: 0.5403, precision: 0.7975, recall: 0.5491, f1: 0.6504, edges-ner-ontonotes_loss: 0.1036
09/09 03:51:01 PM: Update 518: task edges-ner-ontonotes, batch 518 (518): mcc: 0.6741, acc: 0.5713, precision: 0.8187, recall: 0.5796, f1: 0.6787, edges-ner-ontonotes_loss: 0.0973
09/09 03:51:11 PM: Update 577: task edges-ner-ontonotes, batch 577 (577): mcc: 0.6949, acc: 0.5948, precision: 0.8333, recall: 0.6031, f1: 0.6997, edges-ner-ontonotes_loss: 0.0925
09/09 03:51:21 PM: Update 628: task edges-ner-ontonotes, batch 628 (628): mcc: 0.7094, acc: 0.6117, precision: 0.8430, recall: 0.6198, f1: 0.7144, edges-ner-ontonotes_loss: 0.0888
09/09 03:51:31 PM: Update 684: task edges-ner-ontonotes, batch 684 (684): mcc: 0.7220, acc: 0.6258, precision: 0.8506, recall: 0.6350, f1: 0.7271, edges-ner-ontonotes_loss: 0.0857
09/09 03:51:41 PM: Update 741: task edges-ner-ontonotes, batch 741 (741): mcc: 0.7346, acc: 0.6406, precision: 0.8581, recall: 0.6503, f1: 0.7399, edges-ner-ontonotes_loss: 0.0827
09/09 03:51:51 PM: Update 807: task edges-ner-ontonotes, batch 807 (807): mcc: 0.7461, acc: 0.6544, precision: 0.8646, recall: 0.6648, f1: 0.7517, edges-ner-ontonotes_loss: 0.0794
09/09 03:52:02 PM: Update 867: task edges-ner-ontonotes, batch 867 (867): mcc: 0.7555, acc: 0.6661, precision: 0.8695, recall: 0.6769, f1: 0.7612, edges-ner-ontonotes_loss: 0.0766
09/09 03:52:12 PM: Update 925: task edges-ner-ontonotes, batch 925 (925): mcc: 0.7652, acc: 0.6779, precision: 0.8751, recall: 0.6890, f1: 0.7710, edges-ner-ontonotes_loss: 0.0742
09/09 03:52:22 PM: Update 960: task edges-ner-ontonotes, batch 960 (960): mcc: 0.7691, acc: 0.6828, precision: 0.8771, recall: 0.6941, f1: 0.7749, edges-ner-ontonotes_loss: 0.0731
09/09 03:52:28 PM: ***** Step 1000 / Validation 1 *****
09/09 03:52:28 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:52:28 PM: Validating...
09/09 03:52:32 PM: Evaluate: task edges-ner-ontonotes, batch 17 (157): mcc: 0.7533, acc: 0.6843, precision: 0.8347, recall: 0.7025, f1: 0.7629, edges-ner-ontonotes_loss: 0.0710
09/09 03:52:42 PM: Evaluate: task edges-ner-ontonotes, batch 61 (157): mcc: 0.8364, acc: 0.7760, precision: 0.9007, recall: 0.7924, f1: 0.8431, edges-ner-ontonotes_loss: 0.0539
09/09 03:52:52 PM: Evaluate: task edges-ner-ontonotes, batch 105 (157): mcc: 0.8438, acc: 0.7848, precision: 0.9097, recall: 0.7976, f1: 0.8499, edges-ner-ontonotes_loss: 0.0517
09/09 03:53:02 PM: Evaluate: task edges-ner-ontonotes, batch 141 (157): mcc: 0.8585, acc: 0.8011, precision: 0.9192, recall: 0.8155, f1: 0.8642, edges-ner-ontonotes_loss: 0.0471
09/09 03:53:07 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:53:07 PM: Best result seen so far for micro.
09/09 03:53:07 PM: Best result seen so far for macro.
09/09 03:53:07 PM: Updating LR scheduler:
09/09 03:53:07 PM: 	Best result seen so far for macro_avg: 0.866
09/09 03:53:07 PM: 	# validation passes without improvement: 0
09/09 03:53:07 PM: edges-ner-ontonotes_loss: training: 0.071803 validation: 0.046301
09/09 03:53:07 PM: macro_avg: validation: 0.865681
09/09 03:53:07 PM: micro_avg: validation: 0.000000
09/09 03:53:07 PM: edges-ner-ontonotes_mcc: training: 0.773606 validation: 0.859969
09/09 03:53:07 PM: edges-ner-ontonotes_acc: training: 0.688357 validation: 0.802927
09/09 03:53:07 PM: edges-ner-ontonotes_precision: training: 0.879297 validation: 0.920396
09/09 03:53:07 PM: edges-ner-ontonotes_recall: training: 0.700040 validation: 0.817106
09/09 03:53:07 PM: edges-ner-ontonotes_f1: training: 0.779496 validation: 0.865681
09/09 03:53:07 PM: Global learning rate: 0.0001
09/09 03:53:07 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 03:53:12 PM: Update 1026: task edges-ner-ontonotes, batch 26 (1026): mcc: 0.8858, acc: 0.8316, precision: 0.9366, recall: 0.8491, f1: 0.8907, edges-ner-ontonotes_loss: 0.0357
09/09 03:53:22 PM: Update 1077: task edges-ner-ontonotes, batch 77 (1077): mcc: 0.8793, acc: 0.8254, precision: 0.9313, recall: 0.8422, f1: 0.8845, edges-ner-ontonotes_loss: 0.0368
09/09 03:53:33 PM: Update 1126: task edges-ner-ontonotes, batch 126 (1126): mcc: 0.8787, acc: 0.8253, precision: 0.9292, recall: 0.8430, f1: 0.8840, edges-ner-ontonotes_loss: 0.0372
09/09 03:53:43 PM: Update 1175: task edges-ner-ontonotes, batch 175 (1175): mcc: 0.8835, acc: 0.8314, precision: 0.9331, recall: 0.8481, f1: 0.8886, edges-ner-ontonotes_loss: 0.0361
09/09 03:53:53 PM: Update 1234: task edges-ner-ontonotes, batch 234 (1234): mcc: 0.8833, acc: 0.8320, precision: 0.9327, recall: 0.8483, f1: 0.8885, edges-ner-ontonotes_loss: 0.0360
09/09 03:54:03 PM: Update 1270: task edges-ner-ontonotes, batch 270 (1270): mcc: 0.8789, acc: 0.8258, precision: 0.9301, recall: 0.8425, f1: 0.8841, edges-ner-ontonotes_loss: 0.0371
09/09 03:54:13 PM: Update 1330: task edges-ner-ontonotes, batch 330 (1330): mcc: 0.8712, acc: 0.8158, precision: 0.9260, recall: 0.8322, f1: 0.8766, edges-ner-ontonotes_loss: 0.0403
09/09 03:54:23 PM: Update 1399: task edges-ner-ontonotes, batch 399 (1399): mcc: 0.8668, acc: 0.8105, precision: 0.9241, recall: 0.8261, f1: 0.8723, edges-ner-ontonotes_loss: 0.0422
09/09 03:54:33 PM: Update 1457: task edges-ner-ontonotes, batch 457 (1457): mcc: 0.8660, acc: 0.8096, precision: 0.9243, recall: 0.8245, f1: 0.8715, edges-ner-ontonotes_loss: 0.0427
09/09 03:54:43 PM: Update 1518: task edges-ner-ontonotes, batch 518 (1518): mcc: 0.8644, acc: 0.8079, precision: 0.9236, recall: 0.8222, f1: 0.8699, edges-ner-ontonotes_loss: 0.0434
09/09 03:54:53 PM: Update 1564: task edges-ner-ontonotes, batch 564 (1564): mcc: 0.8637, acc: 0.8073, precision: 0.9231, recall: 0.8214, f1: 0.8693, edges-ner-ontonotes_loss: 0.0436
09/09 03:55:03 PM: Update 1635: task edges-ner-ontonotes, batch 635 (1635): mcc: 0.8654, acc: 0.8098, precision: 0.9242, recall: 0.8233, f1: 0.8709, edges-ner-ontonotes_loss: 0.0430
09/09 03:55:13 PM: Update 1712: task edges-ner-ontonotes, batch 712 (1712): mcc: 0.8673, acc: 0.8126, precision: 0.9257, recall: 0.8255, f1: 0.8728, edges-ner-ontonotes_loss: 0.0424
09/09 03:55:23 PM: Update 1788: task edges-ner-ontonotes, batch 788 (1788): mcc: 0.8693, acc: 0.8155, precision: 0.9268, recall: 0.8282, f1: 0.8747, edges-ner-ontonotes_loss: 0.0418
09/09 03:55:33 PM: Update 1860: task edges-ner-ontonotes, batch 860 (1860): mcc: 0.8718, acc: 0.8189, precision: 0.9283, recall: 0.8312, f1: 0.8771, edges-ner-ontonotes_loss: 0.0410
09/09 03:55:44 PM: Update 1908: task edges-ner-ontonotes, batch 908 (1908): mcc: 0.8716, acc: 0.8192, precision: 0.9279, recall: 0.8314, f1: 0.8770, edges-ner-ontonotes_loss: 0.0408
09/09 03:55:54 PM: Update 1972: task edges-ner-ontonotes, batch 972 (1972): mcc: 0.8727, acc: 0.8208, precision: 0.9284, recall: 0.8329, f1: 0.8781, edges-ner-ontonotes_loss: 0.0404
09/09 03:55:58 PM: ***** Step 2000 / Validation 2 *****
09/09 03:55:58 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:55:58 PM: Validating...
09/09 03:56:04 PM: Evaluate: task edges-ner-ontonotes, batch 32 (157): mcc: 0.8547, acc: 0.8092, precision: 0.9112, recall: 0.8159, f1: 0.8609, edges-ner-ontonotes_loss: 0.0448
09/09 03:56:14 PM: Evaluate: task edges-ner-ontonotes, batch 74 (157): mcc: 0.8813, acc: 0.8338, precision: 0.9385, recall: 0.8393, f1: 0.8861, edges-ner-ontonotes_loss: 0.0400
09/09 03:56:25 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.8781, acc: 0.8293, precision: 0.9347, recall: 0.8368, f1: 0.8831, edges-ner-ontonotes_loss: 0.0386
09/09 03:56:35 PM: Evaluate: task edges-ner-ontonotes, batch 149 (157): mcc: 0.8830, acc: 0.8342, precision: 0.9386, recall: 0.8421, f1: 0.8878, edges-ner-ontonotes_loss: 0.0371
09/09 03:56:37 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:56:37 PM: Best result seen so far for macro.
09/09 03:56:37 PM: Updating LR scheduler:
09/09 03:56:37 PM: 	Best result seen so far for macro_avg: 0.888
09/09 03:56:37 PM: 	# validation passes without improvement: 0
09/09 03:56:37 PM: edges-ner-ontonotes_loss: training: 0.040219 validation: 0.036764
09/09 03:56:37 PM: macro_avg: validation: 0.887752
09/09 03:56:37 PM: micro_avg: validation: 0.000000
09/09 03:56:37 PM: edges-ner-ontonotes_mcc: training: 0.873259 validation: 0.882960
09/09 03:56:37 PM: edges-ner-ontonotes_acc: training: 0.821626 validation: 0.834167
09/09 03:56:37 PM: edges-ner-ontonotes_precision: training: 0.928497 validation: 0.938413
09/09 03:56:37 PM: edges-ner-ontonotes_recall: training: 0.833772 validation: 0.842281
09/09 03:56:37 PM: edges-ner-ontonotes_f1: training: 0.878589 validation: 0.887752
09/09 03:56:37 PM: Global learning rate: 0.0001
09/09 03:56:37 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 03:56:45 PM: Update 2034: task edges-ner-ontonotes, batch 34 (2034): mcc: 0.8983, acc: 0.8543, precision: 0.9447, recall: 0.8645, f1: 0.9028, edges-ner-ontonotes_loss: 0.0321
09/09 03:56:55 PM: Update 2081: task edges-ner-ontonotes, batch 81 (2081): mcc: 0.8985, acc: 0.8573, precision: 0.9422, recall: 0.8671, f1: 0.9031, edges-ner-ontonotes_loss: 0.0316
09/09 03:57:05 PM: Update 2130: task edges-ner-ontonotes, batch 130 (2130): mcc: 0.8962, acc: 0.8546, precision: 0.9398, recall: 0.8651, f1: 0.9009, edges-ner-ontonotes_loss: 0.0320
09/09 03:57:16 PM: Update 2183: task edges-ner-ontonotes, batch 183 (2183): mcc: 0.8958, acc: 0.8534, precision: 0.9400, recall: 0.8642, f1: 0.9005, edges-ner-ontonotes_loss: 0.0319
09/09 03:57:27 PM: Update 2243: task edges-ner-ontonotes, batch 243 (2243): mcc: 0.8957, acc: 0.8525, precision: 0.9385, recall: 0.8655, f1: 0.9005, edges-ner-ontonotes_loss: 0.0319
09/09 03:57:37 PM: Update 2304: task edges-ner-ontonotes, batch 304 (2304): mcc: 0.8970, acc: 0.8538, precision: 0.9383, recall: 0.8680, f1: 0.9018, edges-ner-ontonotes_loss: 0.0312
09/09 03:57:47 PM: Update 2363: task edges-ner-ontonotes, batch 363 (2363): mcc: 0.8975, acc: 0.8544, precision: 0.9379, recall: 0.8693, f1: 0.9023, edges-ner-ontonotes_loss: 0.0308
09/09 03:57:57 PM: Update 2420: task edges-ner-ontonotes, batch 420 (2420): mcc: 0.8985, acc: 0.8556, precision: 0.9381, recall: 0.8711, f1: 0.9033, edges-ner-ontonotes_loss: 0.0305
09/09 03:58:07 PM: Update 2475: task edges-ner-ontonotes, batch 475 (2475): mcc: 0.9005, acc: 0.8584, precision: 0.9390, recall: 0.8738, f1: 0.9052, edges-ner-ontonotes_loss: 0.0300
09/09 03:58:17 PM: Update 2516: task edges-ner-ontonotes, batch 516 (2516): mcc: 0.9003, acc: 0.8578, precision: 0.9390, recall: 0.8734, f1: 0.9050, edges-ner-ontonotes_loss: 0.0299
09/09 03:58:27 PM: Update 2577: task edges-ner-ontonotes, batch 577 (2577): mcc: 0.9004, acc: 0.8579, precision: 0.9387, recall: 0.8739, f1: 0.9051, edges-ner-ontonotes_loss: 0.0298
09/09 03:58:37 PM: Update 2637: task edges-ner-ontonotes, batch 637 (2637): mcc: 0.9007, acc: 0.8580, precision: 0.9385, recall: 0.8747, f1: 0.9055, edges-ner-ontonotes_loss: 0.0296
09/09 03:58:47 PM: Update 2697: task edges-ner-ontonotes, batch 697 (2697): mcc: 0.9015, acc: 0.8590, precision: 0.9387, recall: 0.8758, f1: 0.9062, edges-ner-ontonotes_loss: 0.0295
09/09 03:58:57 PM: Update 2754: task edges-ner-ontonotes, batch 754 (2754): mcc: 0.9021, acc: 0.8598, precision: 0.9389, recall: 0.8768, f1: 0.9068, edges-ner-ontonotes_loss: 0.0293
09/09 03:59:09 PM: Update 2809: task edges-ner-ontonotes, batch 809 (2809): mcc: 0.9032, acc: 0.8610, precision: 0.9394, recall: 0.8783, f1: 0.9078, edges-ner-ontonotes_loss: 0.0290
09/09 03:59:19 PM: Update 2863: task edges-ner-ontonotes, batch 863 (2863): mcc: 0.9006, acc: 0.8578, precision: 0.9378, recall: 0.8751, f1: 0.9054, edges-ner-ontonotes_loss: 0.0301
09/09 03:59:29 PM: Update 2927: task edges-ner-ontonotes, batch 927 (2927): mcc: 0.8988, acc: 0.8557, precision: 0.9367, recall: 0.8728, f1: 0.9036, edges-ner-ontonotes_loss: 0.0308
09/09 03:59:39 PM: Update 2986: task edges-ner-ontonotes, batch 986 (2986): mcc: 0.8972, acc: 0.8540, precision: 0.9357, recall: 0.8709, f1: 0.9021, edges-ner-ontonotes_loss: 0.0316
09/09 03:59:41 PM: ***** Step 3000 / Validation 3 *****
09/09 03:59:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:59:41 PM: Validating...
09/09 03:59:49 PM: Evaluate: task edges-ner-ontonotes, batch 41 (157): mcc: 0.8805, acc: 0.8386, precision: 0.9249, recall: 0.8503, f1: 0.8860, edges-ner-ontonotes_loss: 0.0397
09/09 03:59:59 PM: Evaluate: task edges-ner-ontonotes, batch 82 (157): mcc: 0.8950, acc: 0.8510, precision: 0.9438, recall: 0.8593, f1: 0.8996, edges-ner-ontonotes_loss: 0.0373
09/09 04:00:10 PM: Evaluate: task edges-ner-ontonotes, batch 119 (157): mcc: 0.8913, acc: 0.8472, precision: 0.9385, recall: 0.8574, f1: 0.8961, edges-ner-ontonotes_loss: 0.0358
09/09 04:00:20 PM: Evaluate: task edges-ner-ontonotes, batch 154 (157): mcc: 0.8980, acc: 0.8545, precision: 0.9437, recall: 0.8649, f1: 0.9025, edges-ner-ontonotes_loss: 0.0337
09/09 04:00:20 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:00:20 PM: Best result seen so far for macro.
09/09 04:00:20 PM: Updating LR scheduler:
09/09 04:00:20 PM: 	Best result seen so far for macro_avg: 0.903
09/09 04:00:20 PM: 	# validation passes without improvement: 0
09/09 04:00:20 PM: edges-ner-ontonotes_loss: training: 0.031654 validation: 0.033380
09/09 04:00:20 PM: macro_avg: validation: 0.903236
09/09 04:00:20 PM: micro_avg: validation: 0.000000
09/09 04:00:20 PM: edges-ner-ontonotes_mcc: training: 0.897182 validation: 0.898751
09/09 04:00:20 PM: edges-ner-ontonotes_acc: training: 0.853901 validation: 0.855399
09/09 04:00:20 PM: edges-ner-ontonotes_precision: training: 0.935762 validation: 0.944251
09/09 04:00:20 PM: edges-ner-ontonotes_recall: training: 0.870728 validation: 0.865635
09/09 04:00:20 PM: edges-ner-ontonotes_f1: training: 0.902074 validation: 0.903236
09/09 04:00:20 PM: Global learning rate: 0.0001
09/09 04:00:20 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:00:30 PM: Update 3046: task edges-ner-ontonotes, batch 46 (3046): mcc: 0.8804, acc: 0.8301, precision: 0.9283, recall: 0.8469, f1: 0.8857, edges-ner-ontonotes_loss: 0.0406
09/09 04:00:40 PM: Update 3096: task edges-ner-ontonotes, batch 96 (3096): mcc: 0.8761, acc: 0.8269, precision: 0.9247, recall: 0.8424, f1: 0.8816, edges-ner-ontonotes_loss: 0.0410
09/09 04:00:50 PM: Update 3144: task edges-ner-ontonotes, batch 144 (3144): mcc: 0.8790, acc: 0.8316, precision: 0.9263, recall: 0.8463, f1: 0.8845, edges-ner-ontonotes_loss: 0.0393
09/09 04:01:00 PM: Update 3214: task edges-ner-ontonotes, batch 214 (3214): mcc: 0.8853, acc: 0.8407, precision: 0.9297, recall: 0.8546, f1: 0.8906, edges-ner-ontonotes_loss: 0.0371
09/09 04:01:10 PM: Update 3295: task edges-ner-ontonotes, batch 295 (3295): mcc: 0.8909, acc: 0.8485, precision: 0.9328, recall: 0.8619, f1: 0.8959, edges-ner-ontonotes_loss: 0.0352
09/09 04:01:20 PM: Update 3365: task edges-ner-ontonotes, batch 365 (3365): mcc: 0.8949, acc: 0.8537, precision: 0.9357, recall: 0.8665, f1: 0.8998, edges-ner-ontonotes_loss: 0.0339
09/09 04:01:31 PM: Update 3426: task edges-ner-ontonotes, batch 426 (3426): mcc: 0.8961, acc: 0.8559, precision: 0.9359, recall: 0.8685, f1: 0.9010, edges-ner-ontonotes_loss: 0.0334
09/09 04:01:41 PM: Update 3491: task edges-ner-ontonotes, batch 491 (3491): mcc: 0.8952, acc: 0.8547, precision: 0.9351, recall: 0.8676, f1: 0.9001, edges-ner-ontonotes_loss: 0.0332
09/09 04:01:51 PM: Update 3561: task edges-ner-ontonotes, batch 561 (3561): mcc: 0.8959, acc: 0.8554, precision: 0.9355, recall: 0.8687, f1: 0.9009, edges-ner-ontonotes_loss: 0.0328
09/09 04:02:01 PM: Update 3624: task edges-ner-ontonotes, batch 624 (3624): mcc: 0.8976, acc: 0.8577, precision: 0.9364, recall: 0.8709, f1: 0.9025, edges-ner-ontonotes_loss: 0.0324
09/09 04:02:11 PM: Update 3685: task edges-ner-ontonotes, batch 685 (3685): mcc: 0.8988, acc: 0.8593, precision: 0.9371, recall: 0.8725, f1: 0.9036, edges-ner-ontonotes_loss: 0.0320
09/09 04:02:23 PM: Update 3739: task edges-ner-ontonotes, batch 739 (3739): mcc: 0.8999, acc: 0.8607, precision: 0.9378, recall: 0.8739, f1: 0.9047, edges-ner-ontonotes_loss: 0.0317
09/09 04:02:33 PM: Update 3794: task edges-ner-ontonotes, batch 794 (3794): mcc: 0.9003, acc: 0.8609, precision: 0.9374, recall: 0.8750, f1: 0.9051, edges-ner-ontonotes_loss: 0.0315
09/09 04:02:43 PM: Update 3857: task edges-ner-ontonotes, batch 857 (3857): mcc: 0.9015, acc: 0.8623, precision: 0.9377, recall: 0.8768, f1: 0.9062, edges-ner-ontonotes_loss: 0.0310
09/09 04:02:54 PM: Update 3916: task edges-ner-ontonotes, batch 916 (3916): mcc: 0.9022, acc: 0.8629, precision: 0.9381, recall: 0.8778, f1: 0.9069, edges-ner-ontonotes_loss: 0.0307
09/09 04:03:04 PM: Update 3975: task edges-ner-ontonotes, batch 975 (3975): mcc: 0.9034, acc: 0.8644, precision: 0.9387, recall: 0.8795, f1: 0.9081, edges-ner-ontonotes_loss: 0.0302
09/09 04:03:08 PM: ***** Step 4000 / Validation 4 *****
09/09 04:03:08 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:03:08 PM: Validating...
09/09 04:03:14 PM: Evaluate: task edges-ner-ontonotes, batch 29 (157): mcc: 0.8421, acc: 0.8024, precision: 0.8845, recall: 0.8178, f1: 0.8499, edges-ner-ontonotes_loss: 0.0498
09/09 04:03:24 PM: Evaluate: task edges-ner-ontonotes, batch 72 (157): mcc: 0.8851, acc: 0.8458, precision: 0.9260, recall: 0.8577, f1: 0.8905, edges-ner-ontonotes_loss: 0.0403
09/09 04:03:34 PM: Evaluate: task edges-ner-ontonotes, batch 111 (157): mcc: 0.8943, acc: 0.8581, precision: 0.9309, recall: 0.8701, f1: 0.8995, edges-ner-ontonotes_loss: 0.0357
09/09 04:03:44 PM: Evaluate: task edges-ner-ontonotes, batch 140 (157): mcc: 0.9029, acc: 0.8675, precision: 0.9369, recall: 0.8802, f1: 0.9077, edges-ner-ontonotes_loss: 0.0328
09/09 04:03:49 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:03:49 PM: Best result seen so far for macro.
09/09 04:03:49 PM: Updating LR scheduler:
09/09 04:03:49 PM: 	Best result seen so far for macro_avg: 0.908
09/09 04:03:49 PM: 	# validation passes without improvement: 0
09/09 04:03:49 PM: edges-ner-ontonotes_loss: training: 0.030024 validation: 0.032063
09/09 04:03:49 PM: macro_avg: validation: 0.907684
09/09 04:03:49 PM: micro_avg: validation: 0.000000
09/09 04:03:49 PM: edges-ner-ontonotes_mcc: training: 0.903971 validation: 0.902912
09/09 04:03:49 PM: edges-ner-ontonotes_acc: training: 0.865014 validation: 0.866924
09/09 04:03:49 PM: edges-ner-ontonotes_precision: training: 0.938950 validation: 0.936603
09/09 04:03:49 PM: edges-ner-ontonotes_recall: training: 0.880217 validation: 0.880497
09/09 04:03:49 PM: edges-ner-ontonotes_f1: training: 0.908635 validation: 0.907684
09/09 04:03:49 PM: Global learning rate: 0.0001
09/09 04:03:49 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:03:55 PM: Update 4024: task edges-ner-ontonotes, batch 24 (4024): mcc: 0.9128, acc: 0.8792, precision: 0.9384, recall: 0.8970, f1: 0.9172, edges-ner-ontonotes_loss: 0.0252
09/09 04:04:05 PM: Update 4060: task edges-ner-ontonotes, batch 60 (4060): mcc: 0.9140, acc: 0.8804, precision: 0.9397, recall: 0.8981, f1: 0.9184, edges-ner-ontonotes_loss: 0.0243
09/09 04:04:15 PM: Update 4114: task edges-ner-ontonotes, batch 114 (4114): mcc: 0.9159, acc: 0.8791, precision: 0.9424, recall: 0.8990, f1: 0.9202, edges-ner-ontonotes_loss: 0.0241
09/09 04:04:25 PM: Update 4176: task edges-ner-ontonotes, batch 176 (4176): mcc: 0.9165, acc: 0.8792, precision: 0.9429, recall: 0.8996, f1: 0.9207, edges-ner-ontonotes_loss: 0.0242
09/09 04:04:35 PM: Update 4231: task edges-ner-ontonotes, batch 231 (4231): mcc: 0.9170, acc: 0.8800, precision: 0.9434, recall: 0.9000, f1: 0.9212, edges-ner-ontonotes_loss: 0.0244
09/09 04:04:45 PM: Update 4289: task edges-ner-ontonotes, batch 289 (4289): mcc: 0.9173, acc: 0.8802, precision: 0.9446, recall: 0.8995, f1: 0.9215, edges-ner-ontonotes_loss: 0.0242
09/09 04:04:55 PM: Update 4351: task edges-ner-ontonotes, batch 351 (4351): mcc: 0.9169, acc: 0.8798, precision: 0.9439, recall: 0.8993, f1: 0.9211, edges-ner-ontonotes_loss: 0.0244
09/09 04:05:05 PM: Update 4398: task edges-ner-ontonotes, batch 398 (4398): mcc: 0.9126, acc: 0.8748, precision: 0.9410, recall: 0.8941, f1: 0.9170, edges-ner-ontonotes_loss: 0.0261
09/09 04:05:15 PM: Update 4464: task edges-ner-ontonotes, batch 464 (4464): mcc: 0.9095, acc: 0.8710, precision: 0.9394, recall: 0.8899, f1: 0.9140, edges-ner-ontonotes_loss: 0.0275
09/09 04:05:25 PM: Update 4526: task edges-ner-ontonotes, batch 526 (4526): mcc: 0.9060, acc: 0.8667, precision: 0.9377, recall: 0.8851, f1: 0.9107, edges-ner-ontonotes_loss: 0.0291
09/09 04:05:35 PM: Update 4583: task edges-ner-ontonotes, batch 583 (4583): mcc: 0.9029, acc: 0.8631, precision: 0.9357, recall: 0.8814, f1: 0.9077, edges-ner-ontonotes_loss: 0.0303
09/09 04:05:45 PM: Update 4643: task edges-ner-ontonotes, batch 643 (4643): mcc: 0.9015, acc: 0.8612, precision: 0.9350, recall: 0.8794, f1: 0.9064, edges-ner-ontonotes_loss: 0.0309
09/09 04:05:55 PM: Update 4693: task edges-ner-ontonotes, batch 693 (4693): mcc: 0.9008, acc: 0.8604, precision: 0.9344, recall: 0.8786, f1: 0.9057, edges-ner-ontonotes_loss: 0.0313
09/09 04:06:05 PM: Update 4771: task edges-ner-ontonotes, batch 771 (4771): mcc: 0.9017, acc: 0.8618, precision: 0.9350, recall: 0.8798, f1: 0.9065, edges-ner-ontonotes_loss: 0.0310
09/09 04:06:15 PM: Update 4844: task edges-ner-ontonotes, batch 844 (4844): mcc: 0.9022, acc: 0.8626, precision: 0.9357, recall: 0.8800, f1: 0.9070, edges-ner-ontonotes_loss: 0.0308
09/09 04:06:26 PM: Update 4916: task edges-ner-ontonotes, batch 916 (4916): mcc: 0.9032, acc: 0.8643, precision: 0.9364, recall: 0.8812, f1: 0.9080, edges-ner-ontonotes_loss: 0.0305
09/09 04:06:38 PM: Update 4982: task edges-ner-ontonotes, batch 982 (4982): mcc: 0.9035, acc: 0.8650, precision: 0.9366, recall: 0.8816, f1: 0.9083, edges-ner-ontonotes_loss: 0.0303
09/09 04:06:41 PM: ***** Step 5000 / Validation 5 *****
09/09 04:06:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:06:41 PM: Validating...
09/09 04:06:48 PM: Evaluate: task edges-ner-ontonotes, batch 42 (157): mcc: 0.8989, acc: 0.8642, precision: 0.9325, recall: 0.8770, f1: 0.9039, edges-ner-ontonotes_loss: 0.0342
09/09 04:06:58 PM: Evaluate: task edges-ner-ontonotes, batch 82 (157): mcc: 0.9070, acc: 0.8693, precision: 0.9464, recall: 0.8787, f1: 0.9113, edges-ner-ontonotes_loss: 0.0332
09/09 04:07:09 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.9002, acc: 0.8602, precision: 0.9416, recall: 0.8707, f1: 0.9048, edges-ner-ontonotes_loss: 0.0326
09/09 04:07:19 PM: Evaluate: task edges-ner-ontonotes, batch 148 (157): mcc: 0.9061, acc: 0.8664, precision: 0.9461, recall: 0.8774, f1: 0.9104, edges-ner-ontonotes_loss: 0.0307
09/09 04:07:21 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:07:21 PM: Best result seen so far for macro.
09/09 04:07:21 PM: Updating LR scheduler:
09/09 04:07:21 PM: 	Best result seen so far for macro_avg: 0.910
09/09 04:07:21 PM: 	# validation passes without improvement: 0
09/09 04:07:21 PM: edges-ner-ontonotes_loss: training: 0.030356 validation: 0.030414
09/09 04:07:21 PM: macro_avg: validation: 0.910235
09/09 04:07:21 PM: micro_avg: validation: 0.000000
09/09 04:07:21 PM: edges-ner-ontonotes_mcc: training: 0.903345 validation: 0.905877
09/09 04:07:21 PM: edges-ner-ontonotes_acc: training: 0.864736 validation: 0.865939
09/09 04:07:21 PM: edges-ner-ontonotes_precision: training: 0.936534 validation: 0.945725
09/09 04:07:21 PM: edges-ner-ontonotes_recall: training: 0.881365 validation: 0.877313
09/09 04:07:21 PM: edges-ner-ontonotes_f1: training: 0.908112 validation: 0.910235
09/09 04:07:21 PM: Global learning rate: 0.0001
09/09 04:07:21 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:07:29 PM: Update 5031: task edges-ner-ontonotes, batch 31 (5031): mcc: 0.8992, acc: 0.8588, precision: 0.9328, recall: 0.8773, f1: 0.9042, edges-ner-ontonotes_loss: 0.0304
09/09 04:07:39 PM: Update 5080: task edges-ner-ontonotes, batch 80 (5080): mcc: 0.9028, acc: 0.8641, precision: 0.9344, recall: 0.8824, f1: 0.9077, edges-ner-ontonotes_loss: 0.0306
09/09 04:07:49 PM: Update 5139: task edges-ner-ontonotes, batch 139 (5139): mcc: 0.9077, acc: 0.8709, precision: 0.9381, recall: 0.8880, f1: 0.9123, edges-ner-ontonotes_loss: 0.0292
09/09 04:07:59 PM: Update 5198: task edges-ner-ontonotes, batch 198 (5198): mcc: 0.9092, acc: 0.8737, precision: 0.9392, recall: 0.8896, f1: 0.9138, edges-ner-ontonotes_loss: 0.0283
09/09 04:08:09 PM: Update 5260: task edges-ner-ontonotes, batch 260 (5260): mcc: 0.9122, acc: 0.8774, precision: 0.9411, recall: 0.8934, f1: 0.9166, edges-ner-ontonotes_loss: 0.0274
09/09 04:08:19 PM: Update 5310: task edges-ner-ontonotes, batch 310 (5310): mcc: 0.9125, acc: 0.8770, precision: 0.9413, recall: 0.8938, f1: 0.9169, edges-ner-ontonotes_loss: 0.0271
09/09 04:08:29 PM: Update 5373: task edges-ner-ontonotes, batch 373 (5373): mcc: 0.9135, acc: 0.8778, precision: 0.9416, recall: 0.8953, f1: 0.9178, edges-ner-ontonotes_loss: 0.0266
09/09 04:08:39 PM: Update 5436: task edges-ner-ontonotes, batch 436 (5436): mcc: 0.9144, acc: 0.8789, precision: 0.9420, recall: 0.8967, f1: 0.9188, edges-ner-ontonotes_loss: 0.0262
09/09 04:08:49 PM: Update 5493: task edges-ner-ontonotes, batch 493 (5493): mcc: 0.9148, acc: 0.8789, precision: 0.9423, recall: 0.8971, f1: 0.9192, edges-ner-ontonotes_loss: 0.0259
09/09 04:09:00 PM: Update 5554: task edges-ner-ontonotes, batch 554 (5554): mcc: 0.9165, acc: 0.8810, precision: 0.9432, recall: 0.8992, f1: 0.9207, edges-ner-ontonotes_loss: 0.0254
09/09 04:09:13 PM: Update 5608: task edges-ner-ontonotes, batch 608 (5608): mcc: 0.9175, acc: 0.8820, precision: 0.9441, recall: 0.9004, f1: 0.9217, edges-ner-ontonotes_loss: 0.0250
09/09 04:09:23 PM: Update 5674: task edges-ner-ontonotes, batch 674 (5674): mcc: 0.9173, acc: 0.8816, precision: 0.9438, recall: 0.9004, f1: 0.9216, edges-ner-ontonotes_loss: 0.0250
09/09 04:09:33 PM: Update 5730: task edges-ner-ontonotes, batch 730 (5730): mcc: 0.9177, acc: 0.8820, precision: 0.9440, recall: 0.9008, f1: 0.9219, edges-ner-ontonotes_loss: 0.0248
09/09 04:09:43 PM: Update 5789: task edges-ner-ontonotes, batch 789 (5789): mcc: 0.9179, acc: 0.8825, precision: 0.9442, recall: 0.9011, f1: 0.9221, edges-ner-ontonotes_loss: 0.0247
09/09 04:09:53 PM: Update 5852: task edges-ner-ontonotes, batch 852 (5852): mcc: 0.9186, acc: 0.8831, precision: 0.9446, recall: 0.9018, f1: 0.9227, edges-ner-ontonotes_loss: 0.0245
09/09 04:10:03 PM: Update 5908: task edges-ner-ontonotes, batch 908 (5908): mcc: 0.9189, acc: 0.8835, precision: 0.9447, recall: 0.9023, f1: 0.9230, edges-ner-ontonotes_loss: 0.0244
09/09 04:10:13 PM: Update 5962: task edges-ner-ontonotes, batch 962 (5962): mcc: 0.9168, acc: 0.8808, precision: 0.9433, recall: 0.8998, f1: 0.9210, edges-ner-ontonotes_loss: 0.0251
09/09 04:10:20 PM: ***** Step 6000 / Validation 6 *****
09/09 04:10:20 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:10:20 PM: Validating...
09/09 04:10:23 PM: Evaluate: task edges-ner-ontonotes, batch 15 (157): mcc: 0.8408, acc: 0.7898, precision: 0.8880, recall: 0.8122, f1: 0.8484, edges-ner-ontonotes_loss: 0.0484
09/09 04:10:33 PM: Evaluate: task edges-ner-ontonotes, batch 58 (157): mcc: 0.9024, acc: 0.8691, precision: 0.9366, recall: 0.8796, f1: 0.9072, edges-ner-ontonotes_loss: 0.0335
09/09 04:10:43 PM: Evaluate: task edges-ner-ontonotes, batch 93 (157): mcc: 0.9036, acc: 0.8649, precision: 0.9424, recall: 0.8763, f1: 0.9081, edges-ner-ontonotes_loss: 0.0339
09/09 04:10:54 PM: Evaluate: task edges-ner-ontonotes, batch 121 (157): mcc: 0.8998, acc: 0.8630, precision: 0.9375, recall: 0.8739, f1: 0.9046, edges-ner-ontonotes_loss: 0.0328
09/09 04:11:04 PM: Evaluate: task edges-ner-ontonotes, batch 156 (157): mcc: 0.9060, acc: 0.8698, precision: 0.9426, recall: 0.8805, f1: 0.9105, edges-ner-ontonotes_loss: 0.0303
09/09 04:11:04 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:11:04 PM: Best result seen so far for macro.
09/09 04:11:04 PM: Updating LR scheduler:
09/09 04:11:04 PM: 	Best result seen so far for macro_avg: 0.910
09/09 04:11:04 PM: 	# validation passes without improvement: 0
09/09 04:11:04 PM: edges-ner-ontonotes_loss: training: 0.025652 validation: 0.030208
09/09 04:11:04 PM: macro_avg: validation: 0.910495
09/09 04:11:04 PM: micro_avg: validation: 0.000000
09/09 04:11:04 PM: edges-ner-ontonotes_mcc: training: 0.915692 validation: 0.906007
09/09 04:11:04 PM: edges-ner-ontonotes_acc: training: 0.879510 validation: 0.869806
09/09 04:11:04 PM: edges-ner-ontonotes_precision: training: 0.942620 validation: 0.942609
09/09 04:11:04 PM: edges-ner-ontonotes_recall: training: 0.898411 validation: 0.880497
09/09 04:11:04 PM: edges-ner-ontonotes_f1: training: 0.919985 validation: 0.910495
09/09 04:11:04 PM: Global learning rate: 0.0001
09/09 04:11:04 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:11:14 PM: Update 6049: task edges-ner-ontonotes, batch 49 (6049): mcc: 0.8907, acc: 0.8515, precision: 0.9258, recall: 0.8683, f1: 0.8961, edges-ner-ontonotes_loss: 0.0371
09/09 04:11:24 PM: Update 6109: task edges-ner-ontonotes, batch 109 (6109): mcc: 0.8905, acc: 0.8497, precision: 0.9275, recall: 0.8663, f1: 0.8958, edges-ner-ontonotes_loss: 0.0365
09/09 04:11:34 PM: Update 6169: task edges-ner-ontonotes, batch 169 (6169): mcc: 0.8864, acc: 0.8432, precision: 0.9247, recall: 0.8612, f1: 0.8919, edges-ner-ontonotes_loss: 0.0372
09/09 04:11:45 PM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.8873, acc: 0.8446, precision: 0.9254, recall: 0.8623, f1: 0.8927, edges-ner-ontonotes_loss: 0.0367
09/09 04:11:55 PM: Update 6304: task edges-ner-ontonotes, batch 304 (6304): mcc: 0.8932, acc: 0.8526, precision: 0.9294, recall: 0.8695, f1: 0.8984, edges-ner-ontonotes_loss: 0.0343
09/09 04:12:05 PM: Update 6378: task edges-ner-ontonotes, batch 378 (6378): mcc: 0.8963, acc: 0.8570, precision: 0.9311, recall: 0.8736, f1: 0.9014, edges-ner-ontonotes_loss: 0.0330
09/09 04:12:15 PM: Update 6449: task edges-ner-ontonotes, batch 449 (6449): mcc: 0.8990, acc: 0.8605, precision: 0.9327, recall: 0.8770, f1: 0.9040, edges-ner-ontonotes_loss: 0.0321
09/09 04:12:25 PM: Update 6522: task edges-ner-ontonotes, batch 522 (6522): mcc: 0.9021, acc: 0.8648, precision: 0.9346, recall: 0.8808, f1: 0.9069, edges-ner-ontonotes_loss: 0.0313
09/09 04:12:35 PM: Update 6575: task edges-ner-ontonotes, batch 575 (6575): mcc: 0.9024, acc: 0.8653, precision: 0.9348, recall: 0.8813, f1: 0.9073, edges-ner-ontonotes_loss: 0.0309
09/09 04:12:46 PM: Update 6637: task edges-ner-ontonotes, batch 637 (6637): mcc: 0.9040, acc: 0.8670, precision: 0.9359, recall: 0.8831, f1: 0.9087, edges-ner-ontonotes_loss: 0.0304
09/09 04:12:56 PM: Update 6699: task edges-ner-ontonotes, batch 699 (6699): mcc: 0.9042, acc: 0.8675, precision: 0.9361, recall: 0.8834, f1: 0.9090, edges-ner-ontonotes_loss: 0.0302
09/09 04:13:06 PM: Update 6762: task edges-ner-ontonotes, batch 762 (6762): mcc: 0.9060, acc: 0.8696, precision: 0.9372, recall: 0.8856, f1: 0.9107, edges-ner-ontonotes_loss: 0.0296
09/09 04:13:16 PM: Update 6816: task edges-ner-ontonotes, batch 816 (6816): mcc: 0.9069, acc: 0.8708, precision: 0.9375, recall: 0.8871, f1: 0.9116, edges-ner-ontonotes_loss: 0.0293
09/09 04:13:26 PM: Update 6866: task edges-ner-ontonotes, batch 866 (6866): mcc: 0.9068, acc: 0.8708, precision: 0.9372, recall: 0.8872, f1: 0.9115, edges-ner-ontonotes_loss: 0.0292
09/09 04:13:36 PM: Update 6927: task edges-ner-ontonotes, batch 927 (6927): mcc: 0.9080, acc: 0.8719, precision: 0.9378, recall: 0.8888, f1: 0.9126, edges-ner-ontonotes_loss: 0.0287
09/09 04:13:46 PM: Update 6982: task edges-ner-ontonotes, batch 982 (6982): mcc: 0.9090, acc: 0.8730, precision: 0.9383, recall: 0.8902, f1: 0.9136, edges-ner-ontonotes_loss: 0.0284
09/09 04:13:49 PM: ***** Step 7000 / Validation 7 *****
09/09 04:13:49 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:13:49 PM: Validating...
09/09 04:13:56 PM: Evaluate: task edges-ner-ontonotes, batch 28 (157): mcc: 0.8505, acc: 0.8111, precision: 0.8875, recall: 0.8306, f1: 0.8581, edges-ner-ontonotes_loss: 0.0482
09/09 04:14:06 PM: Evaluate: task edges-ner-ontonotes, batch 64 (157): mcc: 0.8982, acc: 0.8652, precision: 0.9297, recall: 0.8783, f1: 0.9033, edges-ner-ontonotes_loss: 0.0349
09/09 04:14:17 PM: Evaluate: task edges-ner-ontonotes, batch 100 (157): mcc: 0.8952, acc: 0.8591, precision: 0.9301, recall: 0.8724, f1: 0.9003, edges-ner-ontonotes_loss: 0.0354
09/09 04:14:27 PM: Evaluate: task edges-ner-ontonotes, batch 129 (157): mcc: 0.9034, acc: 0.8677, precision: 0.9375, recall: 0.8805, f1: 0.9081, edges-ner-ontonotes_loss: 0.0320
09/09 04:14:35 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:14:35 PM: Best result seen so far for macro.
09/09 04:14:35 PM: Updating LR scheduler:
09/09 04:14:35 PM: 	Best result seen so far for macro_avg: 0.913
09/09 04:14:35 PM: 	# validation passes without improvement: 0
09/09 04:14:35 PM: edges-ner-ontonotes_loss: training: 0.028214 validation: 0.029782
09/09 04:14:35 PM: macro_avg: validation: 0.913305
09/09 04:14:35 PM: micro_avg: validation: 0.000000
09/09 04:14:35 PM: edges-ner-ontonotes_mcc: training: 0.909446 validation: 0.908792
09/09 04:14:35 PM: edges-ner-ontonotes_acc: training: 0.873387 validation: 0.873901
09/09 04:14:35 PM: edges-ner-ontonotes_precision: training: 0.938673 validation: 0.940685
09/09 04:14:35 PM: edges-ner-ontonotes_recall: training: 0.890614 validation: 0.887473
09/09 04:14:35 PM: edges-ner-ontonotes_f1: training: 0.914012 validation: 0.913305
09/09 04:14:35 PM: Global learning rate: 0.0001
09/09 04:14:35 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:14:37 PM: Update 7012: task edges-ner-ontonotes, batch 12 (7012): mcc: 0.9131, acc: 0.8769, precision: 0.9322, recall: 0.9037, f1: 0.9177, edges-ner-ontonotes_loss: 0.0223
09/09 04:14:47 PM: Update 7076: task edges-ner-ontonotes, batch 76 (7076): mcc: 0.9261, acc: 0.8938, precision: 0.9463, recall: 0.9142, f1: 0.9300, edges-ner-ontonotes_loss: 0.0221
09/09 04:14:57 PM: Update 7129: task edges-ner-ontonotes, batch 129 (7129): mcc: 0.9274, acc: 0.8953, precision: 0.9475, recall: 0.9156, f1: 0.9313, edges-ner-ontonotes_loss: 0.0222
09/09 04:15:07 PM: Update 7172: task edges-ner-ontonotes, batch 172 (7172): mcc: 0.9268, acc: 0.8945, precision: 0.9475, recall: 0.9144, f1: 0.9307, edges-ner-ontonotes_loss: 0.0218
09/09 04:15:17 PM: Update 7230: task edges-ner-ontonotes, batch 230 (7230): mcc: 0.9242, acc: 0.8913, precision: 0.9456, recall: 0.9115, f1: 0.9282, edges-ner-ontonotes_loss: 0.0225
09/09 04:15:27 PM: Update 7292: task edges-ner-ontonotes, batch 292 (7292): mcc: 0.9250, acc: 0.8913, precision: 0.9469, recall: 0.9115, f1: 0.9289, edges-ner-ontonotes_loss: 0.0224
09/09 04:15:37 PM: Update 7347: task edges-ner-ontonotes, batch 347 (7347): mcc: 0.9256, acc: 0.8919, precision: 0.9480, recall: 0.9116, f1: 0.9295, edges-ner-ontonotes_loss: 0.0220
09/09 04:15:48 PM: Update 7406: task edges-ner-ontonotes, batch 406 (7406): mcc: 0.9258, acc: 0.8925, precision: 0.9480, recall: 0.9120, f1: 0.9296, edges-ner-ontonotes_loss: 0.0218
09/09 04:15:58 PM: Update 7466: task edges-ner-ontonotes, batch 466 (7466): mcc: 0.9253, acc: 0.8917, precision: 0.9476, recall: 0.9115, f1: 0.9292, edges-ner-ontonotes_loss: 0.0220
09/09 04:16:08 PM: Update 7521: task edges-ner-ontonotes, batch 521 (7521): mcc: 0.9213, acc: 0.8866, precision: 0.9450, recall: 0.9065, f1: 0.9254, edges-ner-ontonotes_loss: 0.0236
09/09 04:16:18 PM: Update 7574: task edges-ner-ontonotes, batch 574 (7574): mcc: 0.9182, acc: 0.8828, precision: 0.9434, recall: 0.9024, f1: 0.9224, edges-ner-ontonotes_loss: 0.0249
09/09 04:16:28 PM: Update 7638: task edges-ner-ontonotes, batch 638 (7638): mcc: 0.9154, acc: 0.8794, precision: 0.9414, recall: 0.8990, f1: 0.9197, edges-ner-ontonotes_loss: 0.0260
09/09 04:16:38 PM: Update 7697: task edges-ner-ontonotes, batch 697 (7697): mcc: 0.9132, acc: 0.8767, precision: 0.9399, recall: 0.8963, f1: 0.9176, edges-ner-ontonotes_loss: 0.0271
09/09 04:16:48 PM: Update 7763: task edges-ner-ontonotes, batch 763 (7763): mcc: 0.9118, acc: 0.8748, precision: 0.9393, recall: 0.8944, f1: 0.9163, edges-ner-ontonotes_loss: 0.0276
09/09 04:16:58 PM: Update 7823: task edges-ner-ontonotes, batch 823 (7823): mcc: 0.9109, acc: 0.8739, precision: 0.9386, recall: 0.8934, f1: 0.9154, edges-ner-ontonotes_loss: 0.0280
09/09 04:17:08 PM: Update 7895: task edges-ner-ontonotes, batch 895 (7895): mcc: 0.9115, acc: 0.8748, precision: 0.9393, recall: 0.8939, f1: 0.9160, edges-ner-ontonotes_loss: 0.0278
09/09 04:17:18 PM: Update 7970: task edges-ner-ontonotes, batch 970 (7970): mcc: 0.9122, acc: 0.8760, precision: 0.9397, recall: 0.8947, f1: 0.9166, edges-ner-ontonotes_loss: 0.0276
09/09 04:17:23 PM: ***** Step 8000 / Validation 8 *****
09/09 04:17:23 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:17:23 PM: Validating...
09/09 04:17:29 PM: Evaluate: task edges-ner-ontonotes, batch 22 (157): mcc: 0.8801, acc: 0.8407, precision: 0.9161, recall: 0.8578, f1: 0.8860, edges-ner-ontonotes_loss: 0.0389
09/09 04:17:39 PM: Evaluate: task edges-ner-ontonotes, batch 58 (157): mcc: 0.9152, acc: 0.8849, precision: 0.9461, recall: 0.8942, f1: 0.9194, edges-ner-ontonotes_loss: 0.0306
09/09 04:17:49 PM: Evaluate: task edges-ner-ontonotes, batch 92 (157): mcc: 0.9094, acc: 0.8712, precision: 0.9503, recall: 0.8794, f1: 0.9135, edges-ner-ontonotes_loss: 0.0318
09/09 04:17:59 PM: Evaluate: task edges-ner-ontonotes, batch 119 (157): mcc: 0.9000, acc: 0.8619, precision: 0.9400, recall: 0.8720, f1: 0.9047, edges-ner-ontonotes_loss: 0.0318
09/09 04:18:08 PM: Updating LR scheduler:
09/09 04:18:08 PM: 	Best result seen so far for macro_avg: 0.913
09/09 04:18:08 PM: 	# validation passes without improvement: 1
09/09 04:18:08 PM: edges-ner-ontonotes_loss: training: 0.027560 validation: 0.029631
09/09 04:18:08 PM: macro_avg: validation: 0.911083
09/09 04:18:08 PM: micro_avg: validation: 0.000000
09/09 04:18:08 PM: edges-ner-ontonotes_mcc: training: 0.912413 validation: 0.906704
09/09 04:18:08 PM: edges-ner-ontonotes_acc: training: 0.876439 validation: 0.869048
09/09 04:18:08 PM: edges-ner-ontonotes_precision: training: 0.939880 validation: 0.945006
09/09 04:18:08 PM: edges-ner-ontonotes_recall: training: 0.894962 validation: 0.879512
09/09 04:18:08 PM: edges-ner-ontonotes_f1: training: 0.916871 validation: 0.911083
09/09 04:18:08 PM: Global learning rate: 0.0001
09/09 04:18:08 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:18:09 PM: Update 8004: task edges-ner-ontonotes, batch 4 (8004): mcc: 0.9409, acc: 0.9097, precision: 0.9694, recall: 0.9194, f1: 0.9437, edges-ner-ontonotes_loss: 0.0213
09/09 04:18:19 PM: Update 8082: task edges-ner-ontonotes, batch 82 (8082): mcc: 0.9224, acc: 0.8921, precision: 0.9471, recall: 0.9066, f1: 0.9264, edges-ner-ontonotes_loss: 0.0249
09/09 04:18:29 PM: Update 8125: task edges-ner-ontonotes, batch 125 (8125): mcc: 0.9170, acc: 0.8851, precision: 0.9436, recall: 0.8999, f1: 0.9212, edges-ner-ontonotes_loss: 0.0256
09/09 04:18:39 PM: Update 8190: task edges-ner-ontonotes, batch 190 (8190): mcc: 0.9177, acc: 0.8857, precision: 0.9434, recall: 0.9013, f1: 0.9219, edges-ner-ontonotes_loss: 0.0255
09/09 04:18:49 PM: Update 8249: task edges-ner-ontonotes, batch 249 (8249): mcc: 0.9177, acc: 0.8854, precision: 0.9428, recall: 0.9019, f1: 0.9219, edges-ner-ontonotes_loss: 0.0254
09/09 04:18:59 PM: Update 8309: task edges-ner-ontonotes, batch 309 (8309): mcc: 0.9171, acc: 0.8844, precision: 0.9425, recall: 0.9011, f1: 0.9214, edges-ner-ontonotes_loss: 0.0254
09/09 04:19:09 PM: Update 8370: task edges-ner-ontonotes, batch 370 (8370): mcc: 0.9177, acc: 0.8854, precision: 0.9426, recall: 0.9022, f1: 0.9219, edges-ner-ontonotes_loss: 0.0253
09/09 04:19:20 PM: Update 8423: task edges-ner-ontonotes, batch 423 (8423): mcc: 0.9174, acc: 0.8850, precision: 0.9421, recall: 0.9021, f1: 0.9216, edges-ner-ontonotes_loss: 0.0253
09/09 04:19:30 PM: Update 8482: task edges-ner-ontonotes, batch 482 (8482): mcc: 0.9178, acc: 0.8846, precision: 0.9422, recall: 0.9027, f1: 0.9220, edges-ner-ontonotes_loss: 0.0249
09/09 04:19:40 PM: Update 8536: task edges-ner-ontonotes, batch 536 (8536): mcc: 0.9189, acc: 0.8856, precision: 0.9430, recall: 0.9039, f1: 0.9231, edges-ner-ontonotes_loss: 0.0246
09/09 04:19:50 PM: Update 8593: task edges-ner-ontonotes, batch 593 (8593): mcc: 0.9200, acc: 0.8869, precision: 0.9437, recall: 0.9054, f1: 0.9241, edges-ner-ontonotes_loss: 0.0242
09/09 04:20:00 PM: Update 8649: task edges-ner-ontonotes, batch 649 (8649): mcc: 0.9210, acc: 0.8881, precision: 0.9443, recall: 0.9067, f1: 0.9251, edges-ner-ontonotes_loss: 0.0239
09/09 04:20:10 PM: Update 8711: task edges-ner-ontonotes, batch 711 (8711): mcc: 0.9218, acc: 0.8892, precision: 0.9451, recall: 0.9073, f1: 0.9259, edges-ner-ontonotes_loss: 0.0236
09/09 04:20:20 PM: Update 8759: task edges-ner-ontonotes, batch 759 (8759): mcc: 0.9221, acc: 0.8894, precision: 0.9452, recall: 0.9078, f1: 0.9261, edges-ner-ontonotes_loss: 0.0234
09/09 04:20:30 PM: Update 8819: task edges-ner-ontonotes, batch 819 (8819): mcc: 0.9228, acc: 0.8903, precision: 0.9456, recall: 0.9088, f1: 0.9268, edges-ner-ontonotes_loss: 0.0233
09/09 04:20:40 PM: Update 8881: task edges-ner-ontonotes, batch 881 (8881): mcc: 0.9229, acc: 0.8904, precision: 0.9456, recall: 0.9090, f1: 0.9269, edges-ner-ontonotes_loss: 0.0232
09/09 04:20:51 PM: Update 8939: task edges-ner-ontonotes, batch 939 (8939): mcc: 0.9232, acc: 0.8905, precision: 0.9458, recall: 0.9093, f1: 0.9272, edges-ner-ontonotes_loss: 0.0230
09/09 04:21:01 PM: Update 8990: task edges-ner-ontonotes, batch 990 (8990): mcc: 0.9237, acc: 0.8912, precision: 0.9461, recall: 0.9100, f1: 0.9277, edges-ner-ontonotes_loss: 0.0229
09/09 04:21:03 PM: ***** Step 9000 / Validation 9 *****
09/09 04:21:03 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:21:03 PM: Validating...
09/09 04:21:11 PM: Evaluate: task edges-ner-ontonotes, batch 32 (157): mcc: 0.8731, acc: 0.8366, precision: 0.9111, recall: 0.8496, f1: 0.8793, edges-ner-ontonotes_loss: 0.0446
09/09 04:21:21 PM: Evaluate: task edges-ner-ontonotes, batch 67 (157): mcc: 0.8939, acc: 0.8564, precision: 0.9314, recall: 0.8688, f1: 0.8990, edges-ner-ontonotes_loss: 0.0386
09/09 04:21:31 PM: Evaluate: task edges-ner-ontonotes, batch 103 (157): mcc: 0.9004, acc: 0.8637, precision: 0.9353, recall: 0.8771, f1: 0.9053, edges-ner-ontonotes_loss: 0.0351
09/09 04:21:42 PM: Evaluate: task edges-ner-ontonotes, batch 134 (157): mcc: 0.9108, acc: 0.8766, precision: 0.9418, recall: 0.8901, f1: 0.9152, edges-ner-ontonotes_loss: 0.0311
09/09 04:21:46 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:21:46 PM: Best result seen so far for macro.
09/09 04:21:46 PM: Updating LR scheduler:
09/09 04:21:46 PM: 	Best result seen so far for macro_avg: 0.918
09/09 04:21:46 PM: 	# validation passes without improvement: 0
09/09 04:21:46 PM: edges-ner-ontonotes_loss: training: 0.022894 validation: 0.029634
09/09 04:21:46 PM: macro_avg: validation: 0.917569
09/09 04:21:46 PM: micro_avg: validation: 0.000000
09/09 04:21:46 PM: edges-ner-ontonotes_mcc: training: 0.923783 validation: 0.913247
09/09 04:21:46 PM: edges-ner-ontonotes_acc: training: 0.891312 validation: 0.879133
09/09 04:21:46 PM: edges-ner-ontonotes_precision: training: 0.946088 validation: 0.943519
09/09 04:21:46 PM: edges-ner-ontonotes_recall: training: 0.910112 validation: 0.893009
09/09 04:21:46 PM: edges-ner-ontonotes_f1: training: 0.927752 validation: 0.917569
09/09 04:21:46 PM: Global learning rate: 0.0001
09/09 04:21:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:21:52 PM: Update 9032: task edges-ner-ontonotes, batch 32 (9032): mcc: 0.9283, acc: 0.8970, precision: 0.9491, recall: 0.9156, f1: 0.9320, edges-ner-ontonotes_loss: 0.0210
09/09 04:22:02 PM: Update 9074: task edges-ner-ontonotes, batch 74 (9074): mcc: 0.8976, acc: 0.8592, precision: 0.9261, recall: 0.8808, f1: 0.9029, edges-ner-ontonotes_loss: 0.0307
09/09 04:22:12 PM: Update 9134: task edges-ner-ontonotes, batch 134 (9134): mcc: 0.8971, acc: 0.8584, precision: 0.9277, recall: 0.8783, f1: 0.9023, edges-ner-ontonotes_loss: 0.0327
09/09 04:22:22 PM: Update 9191: task edges-ner-ontonotes, batch 191 (9191): mcc: 0.8951, acc: 0.8549, precision: 0.9270, recall: 0.8752, f1: 0.9004, edges-ner-ontonotes_loss: 0.0333
09/09 04:22:32 PM: Update 9253: task edges-ner-ontonotes, batch 253 (9253): mcc: 0.8936, acc: 0.8526, precision: 0.9265, recall: 0.8729, f1: 0.8989, edges-ner-ontonotes_loss: 0.0342
09/09 04:22:42 PM: Update 9321: task edges-ner-ontonotes, batch 321 (9321): mcc: 0.8927, acc: 0.8514, precision: 0.9260, recall: 0.8718, f1: 0.8981, edges-ner-ontonotes_loss: 0.0342
09/09 04:22:52 PM: Update 9379: task edges-ner-ontonotes, batch 379 (9379): mcc: 0.8940, acc: 0.8536, precision: 0.9260, recall: 0.8741, f1: 0.8993, edges-ner-ontonotes_loss: 0.0337
09/09 04:23:02 PM: Update 9448: task edges-ner-ontonotes, batch 448 (9448): mcc: 0.8974, acc: 0.8580, precision: 0.9288, recall: 0.8777, f1: 0.9025, edges-ner-ontonotes_loss: 0.0325
09/09 04:23:13 PM: Update 9523: task edges-ner-ontonotes, batch 523 (9523): mcc: 0.9010, acc: 0.8631, precision: 0.9313, recall: 0.8821, f1: 0.9060, edges-ner-ontonotes_loss: 0.0312
09/09 04:23:23 PM: Update 9602: task edges-ner-ontonotes, batch 602 (9602): mcc: 0.9033, acc: 0.8665, precision: 0.9327, recall: 0.8849, f1: 0.9082, edges-ner-ontonotes_loss: 0.0306
09/09 04:23:33 PM: Update 9653: task edges-ner-ontonotes, batch 653 (9653): mcc: 0.9043, acc: 0.8677, precision: 0.9337, recall: 0.8859, f1: 0.9092, edges-ner-ontonotes_loss: 0.0302
09/09 04:23:43 PM: Update 9712: task edges-ner-ontonotes, batch 712 (9712): mcc: 0.9051, acc: 0.8687, precision: 0.9341, recall: 0.8869, f1: 0.9099, edges-ner-ontonotes_loss: 0.0298
09/09 04:23:53 PM: Update 9771: task edges-ner-ontonotes, batch 771 (9771): mcc: 0.9062, acc: 0.8701, precision: 0.9349, recall: 0.8882, f1: 0.9109, edges-ner-ontonotes_loss: 0.0295
09/09 04:24:03 PM: Update 9836: task edges-ner-ontonotes, batch 836 (9836): mcc: 0.9078, acc: 0.8721, precision: 0.9359, recall: 0.8901, f1: 0.9125, edges-ner-ontonotes_loss: 0.0290
09/09 04:24:13 PM: Update 9899: task edges-ner-ontonotes, batch 899 (9899): mcc: 0.9090, acc: 0.8737, precision: 0.9368, recall: 0.8916, f1: 0.9137, edges-ner-ontonotes_loss: 0.0287
09/09 04:24:23 PM: Update 9959: task edges-ner-ontonotes, batch 959 (9959): mcc: 0.9096, acc: 0.8746, precision: 0.9371, recall: 0.8925, f1: 0.9142, edges-ner-ontonotes_loss: 0.0285
09/09 04:24:33 PM: Update 9994: task edges-ner-ontonotes, batch 994 (9994): mcc: 0.9102, acc: 0.8753, precision: 0.9375, recall: 0.8932, f1: 0.9148, edges-ner-ontonotes_loss: 0.0282
09/09 04:24:35 PM: ***** Step 10000 / Validation 10 *****
09/09 04:24:35 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:24:35 PM: Validating...
09/09 04:24:43 PM: Evaluate: task edges-ner-ontonotes, batch 37 (157): mcc: 0.8816, acc: 0.8462, precision: 0.9160, recall: 0.8608, f1: 0.8875, edges-ner-ontonotes_loss: 0.0400
09/09 04:24:54 PM: Evaluate: task edges-ner-ontonotes, batch 71 (157): mcc: 0.8996, acc: 0.8622, precision: 0.9369, recall: 0.8741, f1: 0.9044, edges-ner-ontonotes_loss: 0.0361
09/09 04:25:04 PM: Evaluate: task edges-ner-ontonotes, batch 109 (157): mcc: 0.9050, acc: 0.8688, precision: 0.9391, recall: 0.8820, f1: 0.9097, edges-ner-ontonotes_loss: 0.0328
09/09 04:25:14 PM: Evaluate: task edges-ner-ontonotes, batch 145 (157): mcc: 0.9132, acc: 0.8786, precision: 0.9444, recall: 0.8921, f1: 0.9175, edges-ner-ontonotes_loss: 0.0296
09/09 04:25:17 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:25:17 PM: Best result seen so far for macro.
09/09 04:25:17 PM: Updating LR scheduler:
09/09 04:25:17 PM: 	Best result seen so far for macro_avg: 0.918
09/09 04:25:17 PM: 	# validation passes without improvement: 0
09/09 04:25:17 PM: edges-ner-ontonotes_loss: training: 0.028149 validation: 0.029029
09/09 04:25:17 PM: macro_avg: validation: 0.918112
09/09 04:25:17 PM: micro_avg: validation: 0.000000
09/09 04:25:17 PM: edges-ner-ontonotes_mcc: training: 0.910399 validation: 0.913839
09/09 04:25:17 PM: edges-ner-ontonotes_acc: training: 0.875434 validation: 0.879133
09/09 04:25:17 PM: edges-ner-ontonotes_precision: training: 0.937632 validation: 0.944583
09/09 04:25:17 PM: edges-ner-ontonotes_recall: training: 0.893388 validation: 0.893085
09/09 04:25:17 PM: edges-ner-ontonotes_f1: training: 0.914975 validation: 0.918112
09/09 04:25:17 PM: Global learning rate: 0.0001
09/09 04:25:17 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:25:24 PM: Update 10040: task edges-ner-ontonotes, batch 40 (10040): mcc: 0.9322, acc: 0.8969, precision: 0.9490, recall: 0.9231, f1: 0.9358, edges-ner-ontonotes_loss: 0.0202
09/09 04:25:34 PM: Update 10099: task edges-ner-ontonotes, batch 99 (10099): mcc: 0.9288, acc: 0.8939, precision: 0.9484, recall: 0.9172, f1: 0.9325, edges-ner-ontonotes_loss: 0.0213
09/09 04:25:44 PM: Update 10159: task edges-ner-ontonotes, batch 159 (10159): mcc: 0.9310, acc: 0.8978, precision: 0.9505, recall: 0.9192, f1: 0.9346, edges-ner-ontonotes_loss: 0.0206
09/09 04:25:54 PM: Update 10219: task edges-ner-ontonotes, batch 219 (10219): mcc: 0.9315, acc: 0.8996, precision: 0.9509, recall: 0.9198, f1: 0.9351, edges-ner-ontonotes_loss: 0.0203
09/09 04:26:04 PM: Update 10274: task edges-ner-ontonotes, batch 274 (10274): mcc: 0.9303, acc: 0.8984, precision: 0.9499, recall: 0.9186, f1: 0.9340, edges-ner-ontonotes_loss: 0.0204
09/09 04:26:14 PM: Update 10316: task edges-ner-ontonotes, batch 316 (10316): mcc: 0.9286, acc: 0.8963, precision: 0.9483, recall: 0.9169, f1: 0.9324, edges-ner-ontonotes_loss: 0.0208
09/09 04:26:24 PM: Update 10373: task edges-ner-ontonotes, batch 373 (10373): mcc: 0.9287, acc: 0.8964, precision: 0.9486, recall: 0.9167, f1: 0.9324, edges-ner-ontonotes_loss: 0.0209
09/09 04:26:35 PM: Update 10431: task edges-ner-ontonotes, batch 431 (10431): mcc: 0.9293, acc: 0.8971, precision: 0.9491, recall: 0.9174, f1: 0.9330, edges-ner-ontonotes_loss: 0.0206
09/09 04:26:45 PM: Update 10488: task edges-ner-ontonotes, batch 488 (10488): mcc: 0.9292, acc: 0.8971, precision: 0.9488, recall: 0.9176, f1: 0.9330, edges-ner-ontonotes_loss: 0.0207
09/09 04:26:55 PM: Update 10557: task edges-ner-ontonotes, batch 557 (10557): mcc: 0.9298, acc: 0.8981, precision: 0.9491, recall: 0.9185, f1: 0.9335, edges-ner-ontonotes_loss: 0.0208
09/09 04:27:05 PM: Update 10597: task edges-ner-ontonotes, batch 597 (10597): mcc: 0.9287, acc: 0.8963, precision: 0.9484, recall: 0.9170, f1: 0.9324, edges-ner-ontonotes_loss: 0.0210
09/09 04:27:15 PM: Update 10661: task edges-ner-ontonotes, batch 661 (10661): mcc: 0.9250, acc: 0.8916, precision: 0.9461, recall: 0.9124, f1: 0.9289, edges-ner-ontonotes_loss: 0.0224
09/09 04:27:25 PM: Update 10719: task edges-ner-ontonotes, batch 719 (10719): mcc: 0.9223, acc: 0.8883, precision: 0.9443, recall: 0.9092, f1: 0.9264, edges-ner-ontonotes_loss: 0.0234
09/09 04:27:35 PM: Update 10781: task edges-ner-ontonotes, batch 781 (10781): mcc: 0.9211, acc: 0.8867, precision: 0.9436, recall: 0.9075, f1: 0.9252, edges-ner-ontonotes_loss: 0.0240
09/09 04:27:45 PM: Update 10846: task edges-ner-ontonotes, batch 846 (10846): mcc: 0.9186, acc: 0.8839, precision: 0.9417, recall: 0.9047, f1: 0.9229, edges-ner-ontonotes_loss: 0.0250
09/09 04:27:55 PM: Update 10899: task edges-ner-ontonotes, batch 899 (10899): mcc: 0.9175, acc: 0.8825, precision: 0.9412, recall: 0.9032, f1: 0.9218, edges-ner-ontonotes_loss: 0.0256
09/09 04:28:05 PM: Update 10953: task edges-ner-ontonotes, batch 953 (10953): mcc: 0.9173, acc: 0.8824, precision: 0.9411, recall: 0.9029, f1: 0.9216, edges-ner-ontonotes_loss: 0.0258
09/09 04:28:14 PM: ***** Step 11000 / Validation 11 *****
09/09 04:28:14 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:28:14 PM: Validating...
09/09 04:28:15 PM: Evaluate: task edges-ner-ontonotes, batch 5 (157): mcc: 0.8188, acc: 0.7612, precision: 0.8517, recall: 0.8060, f1: 0.8282, edges-ner-ontonotes_loss: 0.0553
09/09 04:28:25 PM: Evaluate: task edges-ner-ontonotes, batch 45 (157): mcc: 0.9045, acc: 0.8697, precision: 0.9366, recall: 0.8835, f1: 0.9093, edges-ner-ontonotes_loss: 0.0331
09/09 04:28:36 PM: Evaluate: task edges-ner-ontonotes, batch 80 (157): mcc: 0.9102, acc: 0.8721, precision: 0.9475, recall: 0.8835, f1: 0.9144, edges-ner-ontonotes_loss: 0.0322
09/09 04:28:46 PM: Evaluate: task edges-ner-ontonotes, batch 119 (157): mcc: 0.9064, acc: 0.8659, precision: 0.9467, recall: 0.8775, f1: 0.9107, edges-ner-ontonotes_loss: 0.0314
09/09 04:28:54 PM: Updating LR scheduler:
09/09 04:28:54 PM: 	Best result seen so far for macro_avg: 0.918
09/09 04:28:54 PM: 	# validation passes without improvement: 1
09/09 04:28:54 PM: edges-ner-ontonotes_loss: training: 0.025781 validation: 0.029370
09/09 04:28:54 PM: macro_avg: validation: 0.915448
09/09 04:28:54 PM: micro_avg: validation: 0.000000
09/09 04:28:54 PM: edges-ner-ontonotes_mcc: training: 0.917409 validation: 0.911377
09/09 04:28:54 PM: edges-ner-ontonotes_acc: training: 0.882759 validation: 0.871550
09/09 04:28:54 PM: edges-ner-ontonotes_precision: training: 0.941038 validation: 0.950899
09/09 04:28:54 PM: edges-ner-ontonotes_recall: training: 0.903138 validation: 0.882545
09/09 04:28:54 PM: edges-ner-ontonotes_f1: training: 0.921699 validation: 0.915448
09/09 04:28:54 PM: Global learning rate: 0.0001
09/09 04:28:54 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-cat/run
09/09 04:28:56 PM: Update 11014: task edges-ner-ontonotes, batch 14 (11014): mcc: 0.9275, acc: 0.8914, precision: 0.9492, recall: 0.9141, f1: 0.9313, edges-ner-ontonotes_loss: 0.0239
