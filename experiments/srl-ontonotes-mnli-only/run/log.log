09/16 10:55:44 AM: Git branch: master
09/16 10:55:44 AM: Git SHA: 092d4f2e0b7152db74aa328af35fdb8b3f73d06a
09/16 10:55:44 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/srl-ontonotes-mnli-only/",
  "exp_name": "experiments/srl-ontonotes-mnli-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/srl-ontonotes-mnli-only/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/mnli",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/srl-ontonotes-mnli-only__run",
  "run_dir": "./experiments/srl-ontonotes-mnli-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-srl-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/16 10:55:44 AM: Saved config to ./experiments/srl-ontonotes-mnli-only/run/params.conf
09/16 10:55:44 AM: Using random seed 1234
09/16 10:55:45 AM: Using GPU 0
09/16 10:55:45 AM: Loading tasks...
09/16 10:55:45 AM: Writing pre-preprocessed tasks to ./experiments/srl-ontonotes-mnli-only/
09/16 10:55:45 AM: 	Creating task edges-srl-ontonotes from scratch.
09/16 10:55:51 AM: Read=231480, Skip=21590, Total=253070 from ./probing_data/edges/ontonotes/srl/train.json.retokenized.bert-base-uncased
09/16 10:55:51 AM: Read=32486, Skip=2811, Total=35297 from ./probing_data/edges/ontonotes/srl/development.json.retokenized.bert-base-uncased
09/16 10:55:52 AM: Read=23800, Skip=2915, Total=26715 from ./probing_data/edges/ontonotes/srl/test.json.retokenized.bert-base-uncased
09/16 10:55:54 AM: 	Task 'edges-srl-ontonotes': |train|=231480 |val|=32486 |test|=23800
09/16 10:55:54 AM: 	Finished loading tasks: edges-srl-ontonotes.
09/16 10:55:54 AM: 	Building vocab from scratch.
09/16 10:55:54 AM: 	Counting units for task edges-srl-ontonotes.
09/16 10:56:00 AM: 	Task 'edges-srl-ontonotes': adding vocab namespace 'edges-srl-ontonotes_labels'
09/16 10:56:01 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:01 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/16 10:56:01 AM: 	Saved vocab to ./experiments/srl-ontonotes-mnli-only/vocab
09/16 10:56:01 AM: Loading token dictionary from ./experiments/srl-ontonotes-mnli-only/vocab.
09/16 10:56:01 AM: 	Loaded vocab from ./experiments/srl-ontonotes-mnli-only/vocab
09/16 10:56:01 AM: 	Vocab namespace bert_uncased: size 30524
09/16 10:56:01 AM: 	Vocab namespace tokens: size 23662
09/16 10:56:01 AM: 	Vocab namespace edges-srl-ontonotes_labels: size 66
09/16 10:56:01 AM: 	Vocab namespace chars: size 76
09/16 10:56:01 AM: 	Finished building vocab.
09/16 10:56:01 AM: 	Task edges-srl-ontonotes (train): Indexing from scratch.
09/16 10:56:33 AM: 	Task edges-srl-ontonotes (train): Saved 231480 instances to ./experiments/srl-ontonotes-mnli-only/preproc/edges-srl-ontonotes__train_data
09/16 10:56:33 AM: 	Task edges-srl-ontonotes (val): Indexing from scratch.
09/16 10:56:38 AM: 	Task edges-srl-ontonotes (val): Saved 32486 instances to ./experiments/srl-ontonotes-mnli-only/preproc/edges-srl-ontonotes__val_data
09/16 10:56:38 AM: 	Task edges-srl-ontonotes (test): Indexing from scratch.
09/16 10:56:41 AM: 	Task edges-srl-ontonotes (test): Saved 23800 instances to ./experiments/srl-ontonotes-mnli-only/preproc/edges-srl-ontonotes__test_data
09/16 10:56:41 AM: 	Finished indexing tasks
09/16 10:56:41 AM: 	Creating trimmed target-only version of edges-srl-ontonotes train.
09/16 10:56:41 AM: 	  Training on 
09/16 10:56:41 AM: 	  Evaluating on edges-srl-ontonotes
09/16 10:56:41 AM: 	Finished loading tasks in 55.609s
09/16 10:56:41 AM: 	 Tasks: ['edges-srl-ontonotes']
09/16 10:56:41 AM: Building model...
09/16 10:56:41 AM: Using BERT model (bert-base-uncased).
09/16 10:56:41 AM: LOADING A FUNETUNED MODEL from: 
09/16 10:56:41 AM: models/mnli
09/16 10:56:41 AM: loading configuration file models/mnli/config.json
09/16 10:56:41 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 3,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/16 10:56:41 AM: loading weights file models/mnli/pytorch_model.bin
09/16 10:56:44 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmparc9ypsr
09/16 10:56:46 AM: copying /tmp/tmparc9ypsr to cache at ./experiments/srl-ontonotes-mnli-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:46 AM: creating metadata file for ./experiments/srl-ontonotes-mnli-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:46 AM: removing temp file /tmp/tmparc9ypsr
09/16 10:56:46 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/srl-ontonotes-mnli-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/16 10:56:46 AM: Initializing parameters
09/16 10:56:46 AM: Done initializing parameters; the following parameters are using their default initialization from their code
09/16 10:56:46 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/16 10:56:46 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/16 10:56:46 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/16 10:56:46 AM:    _text_field_embedder.model.pooler.dense.bias
09/16 10:56:46 AM:    _text_field_embedder.model.pooler.dense.weight
09/16 10:56:46 AM: 	Task 'edges-srl-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-srl-ontonotes"
}
09/16 10:56:50 AM: Model specification:
09/16 10:56:50 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-srl-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=66, bias=True)
      )
    )
  )
)
09/16 10:56:50 AM: Model parameters:
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/16 10:56:50 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 16896 with torch.Size([66, 256])
09/16 10:56:50 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 66 with torch.Size([66])
09/16 10:56:50 AM: Total number of parameters: 110155842 (1.10156e+08)
09/16 10:56:50 AM: Number of trainable parameters: 673602 (673602)
09/16 10:56:50 AM: Finished building model in 8.744s
09/16 10:56:50 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-srl-ontonotes 

09/16 10:57:03 AM: patience = 9
09/16 10:57:03 AM: val_interval = 1000
09/16 10:57:03 AM: max_vals = 250
09/16 10:57:03 AM: cuda_device = 0
09/16 10:57:03 AM: grad_norm = 5.0
09/16 10:57:03 AM: grad_clipping = None
09/16 10:57:03 AM: lr_decay = 0.99
09/16 10:57:03 AM: min_lr = 1e-06
09/16 10:57:03 AM: keep_all_checkpoints = 0
09/16 10:57:03 AM: val_data_limit = 5000
09/16 10:57:03 AM: max_epochs = -1
09/16 10:57:03 AM: dec_val_scale = 250
09/16 10:57:03 AM: training_data_fraction = 1
09/16 10:57:03 AM: type = adam
09/16 10:57:03 AM: parameter_groups = None
09/16 10:57:03 AM: Number of trainable parameters: 673602
09/16 10:57:03 AM: infer_type_and_cast = True
09/16 10:57:03 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:03 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:03 AM: lr = 0.0001
09/16 10:57:03 AM: amsgrad = True
09/16 10:57:03 AM: type = reduce_on_plateau
09/16 10:57:03 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:03 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:03 AM: mode = max
09/16 10:57:03 AM: factor = 0.5
09/16 10:57:03 AM: patience = 3
09/16 10:57:03 AM: threshold = 0.0001
09/16 10:57:03 AM: threshold_mode = abs
09/16 10:57:03 AM: verbose = True
09/16 10:57:03 AM: type = adam
09/16 10:57:03 AM: parameter_groups = None
09/16 10:57:03 AM: Number of trainable parameters: 673602
09/16 10:57:03 AM: infer_type_and_cast = True
09/16 10:57:03 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:03 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:03 AM: lr = 0.0001
09/16 10:57:03 AM: amsgrad = True
09/16 10:57:03 AM: type = reduce_on_plateau
09/16 10:57:03 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/16 10:57:03 AM: CURRENTLY DEFINED PARAMETERS: 
09/16 10:57:03 AM: mode = max
09/16 10:57:03 AM: factor = 0.5
09/16 10:57:03 AM: patience = 3
09/16 10:57:03 AM: threshold = 0.0001
09/16 10:57:03 AM: threshold_mode = abs
09/16 10:57:03 AM: verbose = True
09/16 10:57:03 AM: Starting training without restoring from a checkpoint.
09/16 10:57:03 AM: Training examples per task, before any subsampling: {'edges-srl-ontonotes': 231480}
09/16 10:57:03 AM: Beginning training with stopping criteria based on metric: edges-srl-ontonotes_f1
09/16 10:57:13 AM: Update 240: task edges-srl-ontonotes, batch 240 (240): mcc: 0.0665, acc: 0.0586, precision: 0.0692, recall: 0.1008, f1: 0.0820, edges-srl-ontonotes_loss: 0.1454
09/16 10:57:23 AM: Update 462: task edges-srl-ontonotes, batch 462 (462): mcc: 0.1804, acc: 0.1628, precision: 0.1987, recall: 0.1864, f1: 0.1923, edges-srl-ontonotes_loss: 0.0972
09/16 10:57:34 AM: Update 652: task edges-srl-ontonotes, batch 652 (652): mcc: 0.2767, acc: 0.2408, precision: 0.3145, recall: 0.2614, f1: 0.2855, edges-srl-ontonotes_loss: 0.0790
09/16 10:57:44 AM: Update 873: task edges-srl-ontonotes, batch 873 (873): mcc: 0.3427, acc: 0.2877, precision: 0.3980, recall: 0.3103, f1: 0.3487, edges-srl-ontonotes_loss: 0.0672
09/16 10:57:51 AM: ***** Step 1000 / Validation 1 *****
09/16 10:57:51 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 10:57:51 AM: Validating...
09/16 10:57:54 AM: Evaluate: task edges-srl-ontonotes, batch 96 (157): mcc: 0.6517, acc: 0.5258, precision: 0.7928, recall: 0.5426, f1: 0.6443, edges-srl-ontonotes_loss: 0.0276
09/16 10:57:56 AM: Best result seen so far for edges-srl-ontonotes.
09/16 10:57:56 AM: Best result seen so far for micro.
09/16 10:57:56 AM: Best result seen so far for macro.
09/16 10:57:56 AM: Updating LR scheduler:
09/16 10:57:56 AM: 	Best result seen so far for macro_avg: 0.638
09/16 10:57:56 AM: 	# validation passes without improvement: 0
09/16 10:57:56 AM: edges-srl-ontonotes_loss: training: 0.062529 validation: 0.027556
09/16 10:57:56 AM: macro_avg: validation: 0.638274
09/16 10:57:56 AM: micro_avg: validation: 0.000000
09/16 10:57:56 AM: edges-srl-ontonotes_mcc: training: 0.371243 validation: 0.646167
09/16 10:57:56 AM: edges-srl-ontonotes_acc: training: 0.308381 validation: 0.519437
09/16 10:57:56 AM: edges-srl-ontonotes_precision: training: 0.433551 validation: 0.790317
09/16 10:57:56 AM: edges-srl-ontonotes_recall: training: 0.332254 validation: 0.535294
09/16 10:57:56 AM: edges-srl-ontonotes_f1: training: 0.376203 validation: 0.638274
09/16 10:57:56 AM: Global learning rate: 0.0001
09/16 10:57:56 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 10:58:04 AM: Update 1217: task edges-srl-ontonotes, batch 217 (1217): mcc: 0.5963, acc: 0.4606, precision: 0.7305, recall: 0.4947, f1: 0.5899, edges-srl-ontonotes_loss: 0.0298
09/16 10:58:14 AM: Update 1436: task edges-srl-ontonotes, batch 436 (1436): mcc: 0.6074, acc: 0.4732, precision: 0.7335, recall: 0.5110, f1: 0.6024, edges-srl-ontonotes_loss: 0.0289
09/16 10:58:25 AM: Update 1630: task edges-srl-ontonotes, batch 630 (1630): mcc: 0.6113, acc: 0.4778, precision: 0.7325, recall: 0.5184, f1: 0.6071, edges-srl-ontonotes_loss: 0.0284
09/16 10:58:35 AM: Update 1841: task edges-srl-ontonotes, batch 841 (1841): mcc: 0.6117, acc: 0.4790, precision: 0.7318, recall: 0.5194, f1: 0.6076, edges-srl-ontonotes_loss: 0.0283
09/16 10:58:44 AM: ***** Step 2000 / Validation 2 *****
09/16 10:58:44 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 10:58:44 AM: Validating...
09/16 10:58:45 AM: Evaluate: task edges-srl-ontonotes, batch 10 (157): mcc: 0.6708, acc: 0.5587, precision: 0.7902, recall: 0.5765, f1: 0.6667, edges-srl-ontonotes_loss: 0.0241
09/16 10:58:49 AM: Best result seen so far for edges-srl-ontonotes.
09/16 10:58:49 AM: Best result seen so far for macro.
09/16 10:58:49 AM: Updating LR scheduler:
09/16 10:58:49 AM: 	Best result seen so far for macro_avg: 0.670
09/16 10:58:49 AM: 	# validation passes without improvement: 0
09/16 10:58:49 AM: edges-srl-ontonotes_loss: training: 0.028136 validation: 0.023920
09/16 10:58:49 AM: macro_avg: validation: 0.669527
09/16 10:58:49 AM: micro_avg: validation: 0.000000
09/16 10:58:49 AM: edges-srl-ontonotes_mcc: training: 0.613175 validation: 0.674310
09/16 10:58:49 AM: edges-srl-ontonotes_acc: training: 0.480961 validation: 0.561466
09/16 10:58:49 AM: edges-srl-ontonotes_precision: training: 0.732582 validation: 0.798104
09/16 10:58:49 AM: edges-srl-ontonotes_recall: training: 0.521347 validation: 0.576630
09/16 10:58:49 AM: edges-srl-ontonotes_f1: training: 0.609172 validation: 0.669527
09/16 10:58:49 AM: Global learning rate: 0.0001
09/16 10:58:49 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 10:58:55 AM: Update 2136: task edges-srl-ontonotes, batch 136 (2136): mcc: 0.6323, acc: 0.5043, precision: 0.7458, recall: 0.5441, f1: 0.6292, edges-srl-ontonotes_loss: 0.0269
09/16 10:59:05 AM: Update 2366: task edges-srl-ontonotes, batch 366 (2366): mcc: 0.6346, acc: 0.5092, precision: 0.7395, recall: 0.5527, f1: 0.6326, edges-srl-ontonotes_loss: 0.0263
09/16 10:59:15 AM: Update 2617: task edges-srl-ontonotes, batch 617 (2617): mcc: 0.6403, acc: 0.5180, precision: 0.7404, recall: 0.5618, f1: 0.6388, edges-srl-ontonotes_loss: 0.0259
09/16 10:59:26 AM: Update 2818: task edges-srl-ontonotes, batch 818 (2818): mcc: 0.6466, acc: 0.5263, precision: 0.7438, recall: 0.5702, f1: 0.6455, edges-srl-ontonotes_loss: 0.0255
09/16 10:59:33 AM: ***** Step 3000 / Validation 3 *****
09/16 10:59:33 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 10:59:33 AM: Validating...
09/16 10:59:36 AM: Evaluate: task edges-srl-ontonotes, batch 87 (157): mcc: 0.6852, acc: 0.5909, precision: 0.7853, recall: 0.6050, f1: 0.6835, edges-srl-ontonotes_loss: 0.0236
09/16 10:59:38 AM: Best result seen so far for edges-srl-ontonotes.
09/16 10:59:38 AM: Best result seen so far for macro.
09/16 10:59:38 AM: Updating LR scheduler:
09/16 10:59:38 AM: 	Best result seen so far for macro_avg: 0.684
09/16 10:59:38 AM: 	# validation passes without improvement: 0
09/16 10:59:38 AM: edges-srl-ontonotes_loss: training: 0.025256 validation: 0.023159
09/16 10:59:38 AM: macro_avg: validation: 0.684275
09/16 10:59:38 AM: micro_avg: validation: 0.000000
09/16 10:59:38 AM: edges-srl-ontonotes_mcc: training: 0.650660 validation: 0.685907
09/16 10:59:38 AM: edges-srl-ontonotes_acc: training: 0.531704 validation: 0.593565
09/16 10:59:38 AM: edges-srl-ontonotes_precision: training: 0.746110 validation: 0.785323
09/16 10:59:38 AM: edges-srl-ontonotes_recall: training: 0.575426 validation: 0.606266
09/16 10:59:38 AM: edges-srl-ontonotes_f1: training: 0.649746 validation: 0.684275
09/16 10:59:38 AM: Global learning rate: 0.0001
09/16 10:59:38 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 10:59:46 AM: Update 3150: task edges-srl-ontonotes, batch 150 (3150): mcc: 0.6787, acc: 0.5701, precision: 0.7628, recall: 0.6116, f1: 0.6789, edges-srl-ontonotes_loss: 0.0235
09/16 10:59:56 AM: Update 3403: task edges-srl-ontonotes, batch 403 (3403): mcc: 0.6649, acc: 0.5545, precision: 0.7528, recall: 0.5952, f1: 0.6648, edges-srl-ontonotes_loss: 0.0242
09/16 11:00:06 AM: Update 3627: task edges-srl-ontonotes, batch 627 (3627): mcc: 0.6659, acc: 0.5560, precision: 0.7546, recall: 0.5954, f1: 0.6656, edges-srl-ontonotes_loss: 0.0241
09/16 11:00:16 AM: Update 3862: task edges-srl-ontonotes, batch 862 (3862): mcc: 0.6667, acc: 0.5567, precision: 0.7553, recall: 0.5964, f1: 0.6665, edges-srl-ontonotes_loss: 0.0240
09/16 11:00:22 AM: ***** Step 4000 / Validation 4 *****
09/16 11:00:22 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:00:22 AM: Validating...
09/16 11:00:26 AM: Evaluate: task edges-srl-ontonotes, batch 80 (157): mcc: 0.6807, acc: 0.5890, precision: 0.7788, recall: 0.6023, f1: 0.6793, edges-srl-ontonotes_loss: 0.0234
09/16 11:00:29 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:00:29 AM: Best result seen so far for macro.
09/16 11:00:29 AM: Updating LR scheduler:
09/16 11:00:29 AM: 	Best result seen so far for macro_avg: 0.685
09/16 11:00:29 AM: 	# validation passes without improvement: 0
09/16 11:00:29 AM: edges-srl-ontonotes_loss: training: 0.024021 validation: 0.022884
09/16 11:00:29 AM: macro_avg: validation: 0.684916
09/16 11:00:29 AM: micro_avg: validation: 0.000000
09/16 11:00:29 AM: edges-srl-ontonotes_mcc: training: 0.666723 validation: 0.686079
09/16 11:00:29 AM: edges-srl-ontonotes_acc: training: 0.557472 validation: 0.596336
09/16 11:00:29 AM: edges-srl-ontonotes_precision: training: 0.754871 validation: 0.781506
09/16 11:00:29 AM: edges-srl-ontonotes_recall: training: 0.596714 validation: 0.609576
09/16 11:00:29 AM: edges-srl-ontonotes_f1: training: 0.666539 validation: 0.684916
09/16 11:00:29 AM: Global learning rate: 0.0001
09/16 11:00:29 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:00:36 AM: Update 4167: task edges-srl-ontonotes, batch 167 (4167): mcc: 0.6663, acc: 0.5618, precision: 0.7516, recall: 0.5987, f1: 0.6665, edges-srl-ontonotes_loss: 0.0238
09/16 11:00:47 AM: Update 4383: task edges-srl-ontonotes, batch 383 (4383): mcc: 0.6760, acc: 0.5728, precision: 0.7587, recall: 0.6101, f1: 0.6763, edges-srl-ontonotes_loss: 0.0234
09/16 11:00:57 AM: Update 4620: task edges-srl-ontonotes, batch 620 (4620): mcc: 0.6809, acc: 0.5784, precision: 0.7619, recall: 0.6162, f1: 0.6814, edges-srl-ontonotes_loss: 0.0231
09/16 11:01:07 AM: Update 4808: task edges-srl-ontonotes, batch 808 (4808): mcc: 0.6787, acc: 0.5759, precision: 0.7605, recall: 0.6134, f1: 0.6791, edges-srl-ontonotes_loss: 0.0233
09/16 11:01:15 AM: ***** Step 5000 / Validation 5 *****
09/16 11:01:15 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:01:15 AM: Validating...
09/16 11:01:17 AM: Evaluate: task edges-srl-ontonotes, batch 63 (157): mcc: 0.6745, acc: 0.5849, precision: 0.7585, recall: 0.6076, f1: 0.6747, edges-srl-ontonotes_loss: 0.0233
09/16 11:01:20 AM: Updating LR scheduler:
09/16 11:01:20 AM: 	Best result seen so far for macro_avg: 0.685
09/16 11:01:20 AM: 	# validation passes without improvement: 1
09/16 11:01:20 AM: edges-srl-ontonotes_loss: training: 0.023256 validation: 0.022605
09/16 11:01:20 AM: macro_avg: validation: 0.684786
09/16 11:01:20 AM: micro_avg: validation: 0.000000
09/16 11:01:20 AM: edges-srl-ontonotes_mcc: training: 0.679281 validation: 0.684081
09/16 11:01:20 AM: edges-srl-ontonotes_acc: training: 0.575808 validation: 0.599338
09/16 11:01:20 AM: edges-srl-ontonotes_precision: training: 0.761736 validation: 0.762296
09/16 11:01:20 AM: edges-srl-ontonotes_recall: training: 0.613456 validation: 0.621584
09/16 11:01:20 AM: edges-srl-ontonotes_f1: training: 0.679602 validation: 0.684786
09/16 11:01:20 AM: Global learning rate: 0.0001
09/16 11:01:20 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:01:27 AM: Update 5115: task edges-srl-ontonotes, batch 115 (5115): mcc: 0.6982, acc: 0.6006, precision: 0.7754, recall: 0.6361, f1: 0.6989, edges-srl-ontonotes_loss: 0.0218
09/16 11:01:37 AM: Update 5335: task edges-srl-ontonotes, batch 335 (5335): mcc: 0.7130, acc: 0.6182, precision: 0.7843, recall: 0.6553, f1: 0.7140, edges-srl-ontonotes_loss: 0.0209
09/16 11:01:47 AM: Update 5561: task edges-srl-ontonotes, batch 561 (5561): mcc: 0.7337, acc: 0.6436, precision: 0.7990, recall: 0.6804, f1: 0.7350, edges-srl-ontonotes_loss: 0.0197
09/16 11:01:57 AM: Update 5797: task edges-srl-ontonotes, batch 797 (5797): mcc: 0.7427, acc: 0.6549, precision: 0.8051, recall: 0.6917, f1: 0.7441, edges-srl-ontonotes_loss: 0.0191
09/16 11:02:07 AM: Update 5981: task edges-srl-ontonotes, batch 981 (5981): mcc: 0.7477, acc: 0.6614, precision: 0.8090, recall: 0.6976, f1: 0.7492, edges-srl-ontonotes_loss: 0.0188
09/16 11:02:08 AM: ***** Step 6000 / Validation 6 *****
09/16 11:02:08 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:02:08 AM: Validating...
09/16 11:02:13 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:02:13 AM: Best result seen so far for macro.
09/16 11:02:13 AM: Updating LR scheduler:
09/16 11:02:13 AM: 	Best result seen so far for macro_avg: 0.720
09/16 11:02:13 AM: 	# validation passes without improvement: 0
09/16 11:02:13 AM: edges-srl-ontonotes_loss: training: 0.018795 validation: 0.021226
09/16 11:02:13 AM: macro_avg: validation: 0.720227
09/16 11:02:13 AM: micro_avg: validation: 0.000000
09/16 11:02:13 AM: edges-srl-ontonotes_mcc: training: 0.748518 validation: 0.720192
09/16 11:02:13 AM: edges-srl-ontonotes_acc: training: 0.662352 validation: 0.645447
09/16 11:02:13 AM: edges-srl-ontonotes_precision: training: 0.809483 validation: 0.801623
09/16 11:02:13 AM: edges-srl-ontonotes_recall: training: 0.698652 validation: 0.653837
09/16 11:02:13 AM: edges-srl-ontonotes_f1: training: 0.749995 validation: 0.720227
09/16 11:02:13 AM: Global learning rate: 0.0001
09/16 11:02:13 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:02:17 AM: Update 6114: task edges-srl-ontonotes, batch 114 (6114): mcc: 0.7772, acc: 0.7009, precision: 0.8291, recall: 0.7344, f1: 0.7789, edges-srl-ontonotes_loss: 0.0172
09/16 11:02:27 AM: Update 6337: task edges-srl-ontonotes, batch 337 (6337): mcc: 0.7899, acc: 0.7180, precision: 0.8388, recall: 0.7494, f1: 0.7916, edges-srl-ontonotes_loss: 0.0165
09/16 11:02:38 AM: Update 6574: task edges-srl-ontonotes, batch 574 (6574): mcc: 0.8034, acc: 0.7366, precision: 0.8495, recall: 0.7651, f1: 0.8051, edges-srl-ontonotes_loss: 0.0159
09/16 11:02:48 AM: Update 6800: task edges-srl-ontonotes, batch 800 (6800): mcc: 0.7796, acc: 0.7073, precision: 0.8303, recall: 0.7379, f1: 0.7814, edges-srl-ontonotes_loss: 0.0173
09/16 11:02:58 AM: ***** Step 7000 / Validation 7 *****
09/16 11:02:58 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:02:58 AM: Validating...
09/16 11:02:58 AM: Evaluate: task edges-srl-ontonotes, batch 2 (157): mcc: 0.7448, acc: 0.6648, precision: 0.8079, recall: 0.6932, f1: 0.7462, edges-srl-ontonotes_loss: 0.0200
09/16 11:03:05 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:03:05 AM: Best result seen so far for macro.
09/16 11:03:05 AM: Updating LR scheduler:
09/16 11:03:05 AM: 	Best result seen so far for macro_avg: 0.732
09/16 11:03:05 AM: 	# validation passes without improvement: 0
09/16 11:03:05 AM: edges-srl-ontonotes_loss: training: 0.018232 validation: 0.020182
09/16 11:03:05 AM: macro_avg: validation: 0.731887
09/16 11:03:05 AM: micro_avg: validation: 0.000000
09/16 11:03:05 AM: edges-srl-ontonotes_mcc: training: 0.763489 validation: 0.731189
09/16 11:03:05 AM: edges-srl-ontonotes_acc: training: 0.686787 validation: 0.660072
09/16 11:03:05 AM: edges-srl-ontonotes_precision: training: 0.817946 validation: 0.804260
09/16 11:03:05 AM: edges-srl-ontonotes_recall: training: 0.718901 validation: 0.671465
09/16 11:03:05 AM: edges-srl-ontonotes_f1: training: 0.765232 validation: 0.731887
09/16 11:03:05 AM: Global learning rate: 0.0001
09/16 11:03:05 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:03:08 AM: Update 7053: task edges-srl-ontonotes, batch 53 (7053): mcc: 0.6928, acc: 0.6000, precision: 0.7600, recall: 0.6393, f1: 0.6944, edges-srl-ontonotes_loss: 0.0223
09/16 11:03:18 AM: Update 7238: task edges-srl-ontonotes, batch 238 (7238): mcc: 0.6964, acc: 0.6006, precision: 0.7701, recall: 0.6372, f1: 0.6974, edges-srl-ontonotes_loss: 0.0222
09/16 11:03:28 AM: Update 7445: task edges-srl-ontonotes, batch 445 (7445): mcc: 0.7125, acc: 0.6206, precision: 0.7826, recall: 0.6559, f1: 0.7137, edges-srl-ontonotes_loss: 0.0211
09/16 11:03:38 AM: Update 7632: task edges-srl-ontonotes, batch 632 (7632): mcc: 0.7230, acc: 0.6335, precision: 0.7899, recall: 0.6688, f1: 0.7243, edges-srl-ontonotes_loss: 0.0204
09/16 11:03:48 AM: Update 7820: task edges-srl-ontonotes, batch 820 (7820): mcc: 0.7301, acc: 0.6425, precision: 0.7955, recall: 0.6770, f1: 0.7315, edges-srl-ontonotes_loss: 0.0199
09/16 11:03:57 AM: ***** Step 8000 / Validation 8 *****
09/16 11:03:57 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:03:57 AM: Validating...
09/16 11:03:58 AM: Evaluate: task edges-srl-ontonotes, batch 30 (157): mcc: 0.7539, acc: 0.6854, precision: 0.8250, recall: 0.6952, f1: 0.7546, edges-srl-ontonotes_loss: 0.0188
09/16 11:04:02 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:04:02 AM: Best result seen so far for macro.
09/16 11:04:02 AM: Updating LR scheduler:
09/16 11:04:02 AM: 	Best result seen so far for macro_avg: 0.747
09/16 11:04:02 AM: 	# validation passes without improvement: 0
09/16 11:04:02 AM: edges-srl-ontonotes_loss: training: 0.019969 validation: 0.018787
09/16 11:04:02 AM: macro_avg: validation: 0.747094
09/16 11:04:02 AM: micro_avg: validation: 0.000000
09/16 11:04:02 AM: edges-srl-ontonotes_mcc: training: 0.729500 validation: 0.746419
09/16 11:04:02 AM: edges-srl-ontonotes_acc: training: 0.641920 validation: 0.677623
09/16 11:04:02 AM: edges-srl-ontonotes_precision: training: 0.794345 validation: 0.817816
09/16 11:04:02 AM: edges-srl-ontonotes_recall: training: 0.676854 validation: 0.687630
09/16 11:04:02 AM: edges-srl-ontonotes_f1: training: 0.730908 validation: 0.747094
09/16 11:04:02 AM: Global learning rate: 0.0001
09/16 11:04:02 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:04:08 AM: Update 8149: task edges-srl-ontonotes, batch 149 (8149): mcc: 0.7114, acc: 0.6223, precision: 0.7762, recall: 0.6594, f1: 0.7130, edges-srl-ontonotes_loss: 0.0209
09/16 11:04:18 AM: Update 8275: task edges-srl-ontonotes, batch 275 (8275): mcc: 0.7065, acc: 0.6157, precision: 0.7730, recall: 0.6532, f1: 0.7081, edges-srl-ontonotes_loss: 0.0213
09/16 11:04:29 AM: Update 8499: task edges-srl-ontonotes, batch 499 (8499): mcc: 0.7027, acc: 0.6109, precision: 0.7708, recall: 0.6481, f1: 0.7041, edges-srl-ontonotes_loss: 0.0215
09/16 11:04:39 AM: Update 8706: task edges-srl-ontonotes, batch 706 (8706): mcc: 0.7009, acc: 0.6077, precision: 0.7695, recall: 0.6459, f1: 0.7023, edges-srl-ontonotes_loss: 0.0215
09/16 11:04:49 AM: Update 8899: task edges-srl-ontonotes, batch 899 (8899): mcc: 0.6981, acc: 0.6042, precision: 0.7677, recall: 0.6424, f1: 0.6995, edges-srl-ontonotes_loss: 0.0216
09/16 11:04:54 AM: ***** Step 9000 / Validation 9 *****
09/16 11:04:54 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:04:54 AM: Validating...
09/16 11:04:58 AM: Updating LR scheduler:
09/16 11:04:58 AM: 	Best result seen so far for macro_avg: 0.747
09/16 11:04:58 AM: 	# validation passes without improvement: 1
09/16 11:04:58 AM: edges-srl-ontonotes_loss: training: 0.021836 validation: 0.018757
09/16 11:04:58 AM: macro_avg: validation: 0.741731
09/16 11:04:58 AM: micro_avg: validation: 0.000000
09/16 11:04:58 AM: edges-srl-ontonotes_mcc: training: 0.695237 validation: 0.741674
09/16 11:04:58 AM: edges-srl-ontonotes_acc: training: 0.600542 validation: 0.666384
09/16 11:04:58 AM: edges-srl-ontonotes_precision: training: 0.765675 validation: 0.820592
09/16 11:04:58 AM: edges-srl-ontonotes_recall: training: 0.638890 validation: 0.676699
09/16 11:04:58 AM: edges-srl-ontonotes_f1: training: 0.696560 validation: 0.741731
09/16 11:04:58 AM: Global learning rate: 0.0001
09/16 11:04:58 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:04:59 AM: Update 9014: task edges-srl-ontonotes, batch 14 (9014): mcc: 0.6585, acc: 0.5484, precision: 0.7508, recall: 0.5855, f1: 0.6579, edges-srl-ontonotes_loss: 0.0235
09/16 11:05:09 AM: Update 9200: task edges-srl-ontonotes, batch 200 (9200): mcc: 0.6764, acc: 0.5783, precision: 0.7560, recall: 0.6129, f1: 0.6770, edges-srl-ontonotes_loss: 0.0231
09/16 11:05:20 AM: Update 9438: task edges-srl-ontonotes, batch 438 (9438): mcc: 0.6808, acc: 0.5825, precision: 0.7606, recall: 0.6171, f1: 0.6814, edges-srl-ontonotes_loss: 0.0228
09/16 11:05:30 AM: Update 9685: task edges-srl-ontonotes, batch 685 (9685): mcc: 0.6847, acc: 0.5883, precision: 0.7611, recall: 0.6237, f1: 0.6856, edges-srl-ontonotes_loss: 0.0224
09/16 11:05:40 AM: Update 9891: task edges-srl-ontonotes, batch 891 (9891): mcc: 0.6881, acc: 0.5920, precision: 0.7628, recall: 0.6284, f1: 0.6891, edges-srl-ontonotes_loss: 0.0222
09/16 11:05:45 AM: ***** Step 10000 / Validation 10 *****
09/16 11:05:45 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:05:45 AM: Validating...
09/16 11:05:50 AM: Evaluate: task edges-srl-ontonotes, batch 136 (157): mcc: 0.7293, acc: 0.6596, precision: 0.8034, recall: 0.6688, f1: 0.7299, edges-srl-ontonotes_loss: 0.0194
09/16 11:05:51 AM: Updating LR scheduler:
09/16 11:05:51 AM: 	Best result seen so far for macro_avg: 0.747
09/16 11:05:51 AM: 	# validation passes without improvement: 2
09/16 11:05:51 AM: edges-srl-ontonotes_loss: training: 0.022106 validation: 0.019642
09/16 11:05:51 AM: macro_avg: validation: 0.725927
09/16 11:05:51 AM: micro_avg: validation: 0.000000
09/16 11:05:51 AM: edges-srl-ontonotes_mcc: training: 0.689354 validation: 0.725341
09/16 11:05:51 AM: edges-srl-ontonotes_acc: training: 0.593356 validation: 0.654145
09/16 11:05:51 AM: edges-srl-ontonotes_precision: training: 0.763342 validation: 0.800371
09/16 11:05:51 AM: edges-srl-ontonotes_recall: training: 0.630206 validation: 0.664152
09/16 11:05:51 AM: edges-srl-ontonotes_f1: training: 0.690414 validation: 0.725927
09/16 11:05:51 AM: Global learning rate: 0.0001
09/16 11:05:51 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:06:00 AM: Update 10150: task edges-srl-ontonotes, batch 150 (10150): mcc: 0.7144, acc: 0.6271, precision: 0.7785, recall: 0.6629, f1: 0.7161, edges-srl-ontonotes_loss: 0.0210
09/16 11:06:10 AM: Update 10383: task edges-srl-ontonotes, batch 383 (10383): mcc: 0.7127, acc: 0.6233, precision: 0.7794, recall: 0.6590, f1: 0.7142, edges-srl-ontonotes_loss: 0.0210
09/16 11:06:20 AM: Update 10633: task edges-srl-ontonotes, batch 633 (10633): mcc: 0.7072, acc: 0.6163, precision: 0.7756, recall: 0.6523, f1: 0.7086, edges-srl-ontonotes_loss: 0.0213
09/16 11:06:30 AM: Update 10841: task edges-srl-ontonotes, batch 841 (10841): mcc: 0.7052, acc: 0.6138, precision: 0.7736, recall: 0.6502, f1: 0.7066, edges-srl-ontonotes_loss: 0.0213
09/16 11:06:37 AM: ***** Step 11000 / Validation 11 *****
09/16 11:06:37 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:06:37 AM: Validating...
09/16 11:06:40 AM: Evaluate: task edges-srl-ontonotes, batch 90 (157): mcc: 0.7199, acc: 0.6488, precision: 0.7946, recall: 0.6592, f1: 0.7206, edges-srl-ontonotes_loss: 0.0202
09/16 11:06:42 AM: Updating LR scheduler:
09/16 11:06:42 AM: 	Best result seen so far for macro_avg: 0.747
09/16 11:06:42 AM: 	# validation passes without improvement: 3
09/16 11:06:42 AM: edges-srl-ontonotes_loss: training: 0.021319 validation: 0.020092
09/16 11:06:42 AM: macro_avg: validation: 0.722313
09/16 11:06:42 AM: micro_avg: validation: 0.000000
09/16 11:06:42 AM: edges-srl-ontonotes_mcc: training: 0.705467 validation: 0.721586
09/16 11:06:42 AM: edges-srl-ontonotes_acc: training: 0.614130 validation: 0.650758
09/16 11:06:42 AM: edges-srl-ontonotes_precision: training: 0.774351 validation: 0.795501
09/16 11:06:42 AM: edges-srl-ontonotes_recall: training: 0.650113 validation: 0.661458
09/16 11:06:42 AM: edges-srl-ontonotes_f1: training: 0.706814 validation: 0.722313
09/16 11:06:42 AM: Global learning rate: 0.0001
09/16 11:06:42 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:06:50 AM: Update 11179: task edges-srl-ontonotes, batch 179 (11179): mcc: 0.6999, acc: 0.6078, precision: 0.7699, recall: 0.6438, f1: 0.7013, edges-srl-ontonotes_loss: 0.0215
09/16 11:07:00 AM: Update 11343: task edges-srl-ontonotes, batch 343 (11343): mcc: 0.6999, acc: 0.6076, precision: 0.7698, recall: 0.6439, f1: 0.7012, edges-srl-ontonotes_loss: 0.0215
09/16 11:07:10 AM: Update 11591: task edges-srl-ontonotes, batch 591 (11591): mcc: 0.7051, acc: 0.6155, precision: 0.7725, recall: 0.6510, f1: 0.7065, edges-srl-ontonotes_loss: 0.0212
09/16 11:07:20 AM: Update 11817: task edges-srl-ontonotes, batch 817 (11817): mcc: 0.7070, acc: 0.6168, precision: 0.7745, recall: 0.6528, f1: 0.7084, edges-srl-ontonotes_loss: 0.0211
09/16 11:07:29 AM: ***** Step 12000 / Validation 12 *****
09/16 11:07:29 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:07:29 AM: Validating...
09/16 11:07:30 AM: Evaluate: task edges-srl-ontonotes, batch 47 (157): mcc: 0.7110, acc: 0.6367, precision: 0.7923, recall: 0.6451, f1: 0.7112, edges-srl-ontonotes_loss: 0.0208
09/16 11:07:33 AM: Updating LR scheduler:
09/16 11:07:33 AM: 	Best result seen so far for macro_avg: 0.747
09/16 11:07:33 AM: 	# validation passes without improvement: 0
09/16 11:07:33 AM: edges-srl-ontonotes_loss: training: 0.021170 validation: 0.020078
09/16 11:07:33 AM: macro_avg: validation: 0.716159
09/16 11:07:33 AM: micro_avg: validation: 0.000000
09/16 11:07:33 AM: edges-srl-ontonotes_mcc: training: 0.705734 validation: 0.715851
09/16 11:07:33 AM: edges-srl-ontonotes_acc: training: 0.615671 validation: 0.643676
09/16 11:07:33 AM: edges-srl-ontonotes_precision: training: 0.773453 validation: 0.794910
09/16 11:07:33 AM: edges-srl-ontonotes_recall: training: 0.651365 validation: 0.651605
09/16 11:07:33 AM: edges-srl-ontonotes_f1: training: 0.707178 validation: 0.716159
09/16 11:07:33 AM: Global learning rate: 5e-05
09/16 11:07:33 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:07:40 AM: Update 12168: task edges-srl-ontonotes, batch 168 (12168): mcc: 0.6950, acc: 0.6003, precision: 0.7694, recall: 0.6354, f1: 0.6960, edges-srl-ontonotes_loss: 0.0218
09/16 11:07:50 AM: Update 12354: task edges-srl-ontonotes, batch 354 (12354): mcc: 0.7065, acc: 0.6154, precision: 0.7755, recall: 0.6510, f1: 0.7078, edges-srl-ontonotes_loss: 0.0211
09/16 11:08:00 AM: Update 12559: task edges-srl-ontonotes, batch 559 (12559): mcc: 0.7208, acc: 0.6330, precision: 0.7856, recall: 0.6685, f1: 0.7223, edges-srl-ontonotes_loss: 0.0202
09/16 11:08:10 AM: Update 12768: task edges-srl-ontonotes, batch 768 (12768): mcc: 0.7354, acc: 0.6502, precision: 0.7959, recall: 0.6863, f1: 0.7370, edges-srl-ontonotes_loss: 0.0192
09/16 11:08:20 AM: Update 12992: task edges-srl-ontonotes, batch 992 (12992): mcc: 0.7460, acc: 0.6635, precision: 0.8035, recall: 0.6991, f1: 0.7477, edges-srl-ontonotes_loss: 0.0186
09/16 11:08:21 AM: ***** Step 13000 / Validation 13 *****
09/16 11:08:21 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:08:21 AM: Validating...
09/16 11:08:25 AM: Updating LR scheduler:
09/16 11:08:25 AM: 	Best result seen so far for macro_avg: 0.747
09/16 11:08:25 AM: 	# validation passes without improvement: 1
09/16 11:08:25 AM: edges-srl-ontonotes_loss: training: 0.018609 validation: 0.019422
09/16 11:08:25 AM: macro_avg: validation: 0.741520
09/16 11:08:25 AM: micro_avg: validation: 0.000000
09/16 11:08:25 AM: edges-srl-ontonotes_mcc: training: 0.745959 validation: 0.740310
09/16 11:08:25 AM: edges-srl-ontonotes_acc: training: 0.663489 validation: 0.673774
09/16 11:08:25 AM: edges-srl-ontonotes_precision: training: 0.803618 validation: 0.806055
09/16 11:08:25 AM: edges-srl-ontonotes_recall: training: 0.699070 validation: 0.686552
09/16 11:08:25 AM: edges-srl-ontonotes_f1: training: 0.747707 validation: 0.741520
09/16 11:08:25 AM: Global learning rate: 5e-05
09/16 11:08:25 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:08:30 AM: Update 13120: task edges-srl-ontonotes, batch 120 (13120): mcc: 0.7753, acc: 0.7015, precision: 0.8256, recall: 0.7341, f1: 0.7772, edges-srl-ontonotes_loss: 0.0167
09/16 11:08:40 AM: Update 13295: task edges-srl-ontonotes, batch 295 (13295): mcc: 0.7804, acc: 0.7078, precision: 0.8293, recall: 0.7403, f1: 0.7823, edges-srl-ontonotes_loss: 0.0164
09/16 11:08:51 AM: Update 13507: task edges-srl-ontonotes, batch 507 (13507): mcc: 0.7859, acc: 0.7153, precision: 0.8330, recall: 0.7472, f1: 0.7878, edges-srl-ontonotes_loss: 0.0160
09/16 11:09:01 AM: Update 13742: task edges-srl-ontonotes, batch 742 (13742): mcc: 0.7981, acc: 0.7312, precision: 0.8429, recall: 0.7612, f1: 0.8000, edges-srl-ontonotes_loss: 0.0155
09/16 11:09:11 AM: Update 13951: task edges-srl-ontonotes, batch 951 (13951): mcc: 0.7925, acc: 0.7243, precision: 0.8390, recall: 0.7543, f1: 0.7944, edges-srl-ontonotes_loss: 0.0160
09/16 11:09:13 AM: ***** Step 14000 / Validation 14 *****
09/16 11:09:13 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:09:13 AM: Validating...
09/16 11:09:18 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:09:18 AM: Best result seen so far for macro.
09/16 11:09:18 AM: Updating LR scheduler:
09/16 11:09:18 AM: 	Best result seen so far for macro_avg: 0.752
09/16 11:09:18 AM: 	# validation passes without improvement: 0
09/16 11:09:18 AM: edges-srl-ontonotes_loss: training: 0.016123 validation: 0.018531
09/16 11:09:18 AM: macro_avg: validation: 0.751777
09/16 11:09:18 AM: micro_avg: validation: 0.000000
09/16 11:09:18 AM: edges-srl-ontonotes_mcc: training: 0.789576 validation: 0.750726
09/16 11:09:18 AM: edges-srl-ontonotes_acc: training: 0.720422 validation: 0.685090
09/16 11:09:18 AM: edges-srl-ontonotes_precision: training: 0.836776 validation: 0.816926
09/16 11:09:18 AM: edges-srl-ontonotes_recall: training: 0.750717 validation: 0.696251
09/16 11:09:18 AM: edges-srl-ontonotes_f1: training: 0.791414 validation: 0.751777
09/16 11:09:18 AM: Global learning rate: 5e-05
09/16 11:09:18 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:09:21 AM: Update 14084: task edges-srl-ontonotes, batch 84 (14084): mcc: 0.7469, acc: 0.6679, precision: 0.8031, recall: 0.7012, f1: 0.7487, edges-srl-ontonotes_loss: 0.0187
09/16 11:09:31 AM: Update 14271: task edges-srl-ontonotes, batch 271 (14271): mcc: 0.7196, acc: 0.6356, precision: 0.7812, recall: 0.6700, f1: 0.7214, edges-srl-ontonotes_loss: 0.0204
09/16 11:09:41 AM: Update 14466: task edges-srl-ontonotes, batch 466 (14466): mcc: 0.7144, acc: 0.6282, precision: 0.7784, recall: 0.6629, f1: 0.7160, edges-srl-ontonotes_loss: 0.0207
09/16 11:09:51 AM: Update 14647: task edges-srl-ontonotes, batch 647 (14647): mcc: 0.7252, acc: 0.6409, precision: 0.7887, recall: 0.6739, f1: 0.7268, edges-srl-ontonotes_loss: 0.0200
09/16 11:10:01 AM: Update 14848: task edges-srl-ontonotes, batch 848 (14848): mcc: 0.7312, acc: 0.6479, precision: 0.7937, recall: 0.6805, f1: 0.7328, edges-srl-ontonotes_loss: 0.0196
09/16 11:10:08 AM: ***** Step 15000 / Validation 15 *****
09/16 11:10:08 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:10:08 AM: Validating...
09/16 11:10:11 AM: Evaluate: task edges-srl-ontonotes, batch 103 (157): mcc: 0.7686, acc: 0.7015, precision: 0.8358, recall: 0.7126, f1: 0.7693, edges-srl-ontonotes_loss: 0.0173
09/16 11:10:13 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:10:13 AM: Best result seen so far for macro.
09/16 11:10:13 AM: Updating LR scheduler:
09/16 11:10:13 AM: 	Best result seen so far for macro_avg: 0.761
09/16 11:10:13 AM: 	# validation passes without improvement: 0
09/16 11:10:13 AM: edges-srl-ontonotes_loss: training: 0.019251 validation: 0.017804
09/16 11:10:13 AM: macro_avg: validation: 0.760502
09/16 11:10:13 AM: micro_avg: validation: 0.000000
09/16 11:10:13 AM: edges-srl-ontonotes_mcc: training: 0.736582 validation: 0.759667
09/16 11:10:13 AM: edges-srl-ontonotes_acc: training: 0.654094 validation: 0.693018
09/16 11:10:13 AM: edges-srl-ontonotes_precision: training: 0.798163 validation: 0.827255
09/16 11:10:13 AM: edges-srl-ontonotes_recall: training: 0.686546 validation: 0.703718
09/16 11:10:13 AM: edges-srl-ontonotes_f1: training: 0.738159 validation: 0.760502
09/16 11:10:13 AM: Global learning rate: 5e-05
09/16 11:10:13 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:10:21 AM: Update 15178: task edges-srl-ontonotes, batch 178 (15178): mcc: 0.7523, acc: 0.6732, precision: 0.8083, recall: 0.7067, f1: 0.7541, edges-srl-ontonotes_loss: 0.0183
09/16 11:10:31 AM: Update 15409: task edges-srl-ontonotes, batch 409 (15409): mcc: 0.7370, acc: 0.6543, precision: 0.7961, recall: 0.6891, f1: 0.7388, edges-srl-ontonotes_loss: 0.0191
09/16 11:10:41 AM: Update 15574: task edges-srl-ontonotes, batch 574 (15574): mcc: 0.7302, acc: 0.6462, precision: 0.7906, recall: 0.6813, f1: 0.7319, edges-srl-ontonotes_loss: 0.0196
09/16 11:10:51 AM: Update 15802: task edges-srl-ontonotes, batch 802 (15802): mcc: 0.7259, acc: 0.6402, precision: 0.7875, recall: 0.6761, f1: 0.7276, edges-srl-ontonotes_loss: 0.0199
09/16 11:10:59 AM: ***** Step 16000 / Validation 16 *****
09/16 11:10:59 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:10:59 AM: Validating...
09/16 11:11:02 AM: Evaluate: task edges-srl-ontonotes, batch 69 (157): mcc: 0.7578, acc: 0.6941, precision: 0.8175, recall: 0.7087, f1: 0.7592, edges-srl-ontonotes_loss: 0.0180
09/16 11:11:04 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:11:04 AM: Best result seen so far for macro.
09/16 11:11:04 AM: Updating LR scheduler:
09/16 11:11:04 AM: 	Best result seen so far for macro_avg: 0.761
09/16 11:11:04 AM: 	# validation passes without improvement: 0
09/16 11:11:04 AM: edges-srl-ontonotes_loss: training: 0.019981 validation: 0.017728
09/16 11:11:04 AM: macro_avg: validation: 0.760956
09/16 11:11:04 AM: micro_avg: validation: 0.000000
09/16 11:11:04 AM: edges-srl-ontonotes_mcc: training: 0.723093 validation: 0.759408
09/16 11:11:04 AM: edges-srl-ontonotes_acc: training: 0.636933 validation: 0.696867
09/16 11:11:04 AM: edges-srl-ontonotes_precision: training: 0.784691 validation: 0.817507
09/16 11:11:04 AM: edges-srl-ontonotes_recall: training: 0.673457 validation: 0.711724
09/16 11:11:04 AM: edges-srl-ontonotes_f1: training: 0.724831 validation: 0.760956
09/16 11:11:04 AM: Global learning rate: 5e-05
09/16 11:11:04 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:11:12 AM: Update 16174: task edges-srl-ontonotes, batch 174 (16174): mcc: 0.6911, acc: 0.5994, precision: 0.7571, recall: 0.6387, f1: 0.6929, edges-srl-ontonotes_loss: 0.0219
09/16 11:11:22 AM: Update 16371: task edges-srl-ontonotes, batch 371 (16371): mcc: 0.6905, acc: 0.5973, precision: 0.7605, recall: 0.6347, f1: 0.6919, edges-srl-ontonotes_loss: 0.0219
09/16 11:11:32 AM: Update 16625: task edges-srl-ontonotes, batch 625 (16625): mcc: 0.6917, acc: 0.5982, precision: 0.7636, recall: 0.6343, f1: 0.6930, edges-srl-ontonotes_loss: 0.0219
09/16 11:11:42 AM: Update 16830: task edges-srl-ontonotes, batch 830 (16830): mcc: 0.6939, acc: 0.6009, precision: 0.7649, recall: 0.6370, f1: 0.6951, edges-srl-ontonotes_loss: 0.0217
09/16 11:11:51 AM: ***** Step 17000 / Validation 17 *****
09/16 11:11:51 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:11:51 AM: Validating...
09/16 11:11:52 AM: Evaluate: task edges-srl-ontonotes, batch 53 (157): mcc: 0.7367, acc: 0.6636, precision: 0.8157, recall: 0.6717, f1: 0.7368, edges-srl-ontonotes_loss: 0.0190
09/16 11:11:56 AM: Updating LR scheduler:
09/16 11:11:56 AM: 	Best result seen so far for macro_avg: 0.761
09/16 11:11:56 AM: 	# validation passes without improvement: 1
09/16 11:11:56 AM: edges-srl-ontonotes_loss: training: 0.021490 validation: 0.018360
09/16 11:11:56 AM: macro_avg: validation: 0.740900
09/16 11:11:56 AM: micro_avg: validation: 0.000000
09/16 11:11:56 AM: edges-srl-ontonotes_mcc: training: 0.696510 validation: 0.740525
09/16 11:11:56 AM: edges-srl-ontonotes_acc: training: 0.604362 validation: 0.669002
09/16 11:11:56 AM: edges-srl-ontonotes_precision: training: 0.765989 validation: 0.816094
09/16 11:11:56 AM: edges-srl-ontonotes_recall: training: 0.640935 validation: 0.678393
09/16 11:11:56 AM: edges-srl-ontonotes_f1: training: 0.697904 validation: 0.740900
09/16 11:11:56 AM: Global learning rate: 5e-05
09/16 11:11:56 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:12:03 AM: Update 17157: task edges-srl-ontonotes, batch 157 (17157): mcc: 0.7185, acc: 0.6326, precision: 0.7813, recall: 0.6681, f1: 0.7203, edges-srl-ontonotes_loss: 0.0203
09/16 11:12:13 AM: Update 17387: task edges-srl-ontonotes, batch 387 (17387): mcc: 0.7179, acc: 0.6323, precision: 0.7804, recall: 0.6676, f1: 0.7196, edges-srl-ontonotes_loss: 0.0203
09/16 11:12:23 AM: Update 17619: task edges-srl-ontonotes, batch 619 (17619): mcc: 0.7224, acc: 0.6368, precision: 0.7844, recall: 0.6723, f1: 0.7241, edges-srl-ontonotes_loss: 0.0201
09/16 11:12:33 AM: Update 17790: task edges-srl-ontonotes, batch 790 (17790): mcc: 0.7182, acc: 0.6319, precision: 0.7810, recall: 0.6677, f1: 0.7199, edges-srl-ontonotes_loss: 0.0204
09/16 11:12:42 AM: ***** Step 18000 / Validation 18 *****
09/16 11:12:42 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:12:42 AM: Validating...
09/16 11:12:43 AM: Evaluate: task edges-srl-ontonotes, batch 23 (157): mcc: 0.7472, acc: 0.6747, precision: 0.8301, recall: 0.6788, f1: 0.7469, edges-srl-ontonotes_loss: 0.0184
09/16 11:12:46 AM: Updating LR scheduler:
09/16 11:12:46 AM: 	Best result seen so far for macro_avg: 0.761
09/16 11:12:46 AM: 	# validation passes without improvement: 2
09/16 11:12:46 AM: edges-srl-ontonotes_loss: training: 0.020497 validation: 0.018643
09/16 11:12:46 AM: macro_avg: validation: 0.742278
09/16 11:12:46 AM: micro_avg: validation: 0.000000
09/16 11:12:46 AM: edges-srl-ontonotes_mcc: training: 0.716090 validation: 0.741696
09/16 11:12:46 AM: edges-srl-ontonotes_acc: training: 0.629024 validation: 0.672850
09/16 11:12:46 AM: edges-srl-ontonotes_precision: training: 0.779727 validation: 0.814777
09/16 11:12:46 AM: edges-srl-ontonotes_recall: training: 0.664904 validation: 0.681626
09/16 11:12:46 AM: edges-srl-ontonotes_f1: training: 0.717752 validation: 0.742278
09/16 11:12:46 AM: Global learning rate: 5e-05
09/16 11:12:46 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:12:53 AM: Update 18149: task edges-srl-ontonotes, batch 149 (18149): mcc: 0.7174, acc: 0.6311, precision: 0.7817, recall: 0.6655, f1: 0.7189, edges-srl-ontonotes_loss: 0.0205
09/16 11:13:03 AM: Update 18372: task edges-srl-ontonotes, batch 372 (18372): mcc: 0.7145, acc: 0.6262, precision: 0.7799, recall: 0.6618, f1: 0.7160, edges-srl-ontonotes_loss: 0.0206
09/16 11:13:13 AM: Update 18562: task edges-srl-ontonotes, batch 562 (18562): mcc: 0.7122, acc: 0.6235, precision: 0.7784, recall: 0.6590, f1: 0.7137, edges-srl-ontonotes_loss: 0.0206
09/16 11:13:23 AM: Update 18820: task edges-srl-ontonotes, batch 820 (18820): mcc: 0.7150, acc: 0.6275, precision: 0.7797, recall: 0.6630, f1: 0.7166, edges-srl-ontonotes_loss: 0.0205
09/16 11:13:30 AM: ***** Step 19000 / Validation 19 *****
09/16 11:13:30 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:13:30 AM: Validating...
09/16 11:13:33 AM: Evaluate: task edges-srl-ontonotes, batch 79 (157): mcc: 0.7340, acc: 0.6670, precision: 0.8018, recall: 0.6787, f1: 0.7351, edges-srl-ontonotes_loss: 0.0192
09/16 11:13:35 AM: Updating LR scheduler:
09/16 11:13:35 AM: 	Best result seen so far for macro_avg: 0.761
09/16 11:13:35 AM: 	# validation passes without improvement: 3
09/16 11:13:35 AM: edges-srl-ontonotes_loss: training: 0.020470 validation: 0.018903
09/16 11:13:35 AM: macro_avg: validation: 0.741459
09/16 11:13:35 AM: micro_avg: validation: 0.000000
09/16 11:13:35 AM: edges-srl-ontonotes_mcc: training: 0.716198 validation: 0.740065
09/16 11:13:35 AM: edges-srl-ontonotes_acc: training: 0.628903 validation: 0.676006
09/16 11:13:35 AM: edges-srl-ontonotes_precision: training: 0.780765 validation: 0.803486
09/16 11:13:35 AM: edges-srl-ontonotes_recall: training: 0.664206 validation: 0.688323
09/16 11:13:35 AM: edges-srl-ontonotes_f1: training: 0.717784 validation: 0.741459
09/16 11:13:35 AM: Global learning rate: 5e-05
09/16 11:13:35 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:13:44 AM: Update 19188: task edges-srl-ontonotes, batch 188 (19188): mcc: 0.7207, acc: 0.6365, precision: 0.7829, recall: 0.6706, f1: 0.7224, edges-srl-ontonotes_loss: 0.0202
09/16 11:13:54 AM: Update 19447: task edges-srl-ontonotes, batch 447 (19447): mcc: 0.7100, acc: 0.6219, precision: 0.7783, recall: 0.6551, f1: 0.7114, edges-srl-ontonotes_loss: 0.0210
09/16 11:14:04 AM: Update 19660: task edges-srl-ontonotes, batch 660 (19660): mcc: 0.7199, acc: 0.6336, precision: 0.7862, recall: 0.6663, f1: 0.7213, edges-srl-ontonotes_loss: 0.0203
09/16 11:14:14 AM: Update 19823: task edges-srl-ontonotes, batch 823 (19823): mcc: 0.7277, acc: 0.6427, precision: 0.7915, recall: 0.6760, f1: 0.7292, edges-srl-ontonotes_loss: 0.0198
09/16 11:14:21 AM: ***** Step 20000 / Validation 20 *****
09/16 11:14:21 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:14:21 AM: Validating...
09/16 11:14:24 AM: Evaluate: task edges-srl-ontonotes, batch 89 (157): mcc: 0.7494, acc: 0.6866, precision: 0.8093, recall: 0.7005, f1: 0.7510, edges-srl-ontonotes_loss: 0.0187
09/16 11:14:26 AM: Updating LR scheduler:
09/16 11:14:26 AM: 	Best result seen so far for macro_avg: 0.761
09/16 11:14:26 AM: 	# validation passes without improvement: 0
09/16 11:14:26 AM: edges-srl-ontonotes_loss: training: 0.019098 validation: 0.018656
09/16 11:14:26 AM: macro_avg: validation: 0.750062
09/16 11:14:26 AM: micro_avg: validation: 0.000000
09/16 11:14:26 AM: edges-srl-ontonotes_mcc: training: 0.738749 validation: 0.748328
09/16 11:14:26 AM: edges-srl-ontonotes_acc: training: 0.656061 validation: 0.686090
09/16 11:14:26 AM: edges-srl-ontonotes_precision: training: 0.799240 validation: 0.805764
09/16 11:14:26 AM: edges-srl-ontonotes_recall: training: 0.689596 validation: 0.701563
09/16 11:14:26 AM: edges-srl-ontonotes_f1: training: 0.740381 validation: 0.750062
09/16 11:14:26 AM: Global learning rate: 2.5e-05
09/16 11:14:26 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:14:34 AM: Update 20173: task edges-srl-ontonotes, batch 173 (20173): mcc: 0.7926, acc: 0.7215, precision: 0.8381, recall: 0.7553, f1: 0.7945, edges-srl-ontonotes_loss: 0.0155
09/16 11:14:44 AM: Update 20408: task edges-srl-ontonotes, batch 408 (20408): mcc: 0.7854, acc: 0.7134, precision: 0.8317, recall: 0.7474, f1: 0.7873, edges-srl-ontonotes_loss: 0.0159
09/16 11:14:54 AM: Update 20629: task edges-srl-ontonotes, batch 629 (20629): mcc: 0.7877, acc: 0.7168, precision: 0.8333, recall: 0.7503, f1: 0.7896, edges-srl-ontonotes_loss: 0.0158
09/16 11:15:04 AM: Update 20805: task edges-srl-ontonotes, batch 805 (20805): mcc: 0.7904, acc: 0.7205, precision: 0.8355, recall: 0.7534, f1: 0.7923, edges-srl-ontonotes_loss: 0.0157
09/16 11:15:12 AM: ***** Step 21000 / Validation 21 *****
09/16 11:15:12 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:15:12 AM: Validating...
09/16 11:15:14 AM: Evaluate: task edges-srl-ontonotes, batch 68 (157): mcc: 0.7511, acc: 0.6857, precision: 0.8164, recall: 0.6974, f1: 0.7522, edges-srl-ontonotes_loss: 0.0187
09/16 11:15:17 AM: Updating LR scheduler:
09/16 11:15:17 AM: 	Best result seen so far for macro_avg: 0.761
09/16 11:15:17 AM: 	# validation passes without improvement: 1
09/16 11:15:17 AM: edges-srl-ontonotes_loss: training: 0.015331 validation: 0.018375
09/16 11:15:17 AM: macro_avg: validation: 0.750238
09/16 11:15:17 AM: micro_avg: validation: 0.000000
09/16 11:15:17 AM: edges-srl-ontonotes_mcc: training: 0.798634 validation: 0.749033
09/16 11:15:17 AM: edges-srl-ontonotes_acc: training: 0.731208 validation: 0.684166
09/16 11:15:17 AM: edges-srl-ontonotes_precision: training: 0.842778 validation: 0.813512
09/16 11:15:17 AM: edges-srl-ontonotes_recall: training: 0.762283 validation: 0.696097
09/16 11:15:17 AM: edges-srl-ontonotes_f1: training: 0.800512 validation: 0.750238
09/16 11:15:17 AM: Global learning rate: 2.5e-05
09/16 11:15:17 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:15:24 AM: Update 21184: task edges-srl-ontonotes, batch 184 (21184): mcc: 0.7775, acc: 0.7080, precision: 0.8297, recall: 0.7345, f1: 0.7792, edges-srl-ontonotes_loss: 0.0170
09/16 11:15:34 AM: Update 21447: task edges-srl-ontonotes, batch 447 (21447): mcc: 0.7512, acc: 0.6753, precision: 0.8061, recall: 0.7066, f1: 0.7531, edges-srl-ontonotes_loss: 0.0185
09/16 11:15:44 AM: Update 21708: task edges-srl-ontonotes, batch 708 (21708): mcc: 0.7363, acc: 0.6562, precision: 0.7948, recall: 0.6890, f1: 0.7381, edges-srl-ontonotes_loss: 0.0193
09/16 11:15:54 AM: Update 21868: task edges-srl-ontonotes, batch 868 (21868): mcc: 0.7386, acc: 0.6584, precision: 0.7973, recall: 0.6910, f1: 0.7403, edges-srl-ontonotes_loss: 0.0192
09/16 11:15:59 AM: ***** Step 22000 / Validation 22 *****
09/16 11:15:59 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:15:59 AM: Validating...
09/16 11:16:04 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:16:04 AM: Best result seen so far for macro.
09/16 11:16:04 AM: Updating LR scheduler:
09/16 11:16:04 AM: 	Best result seen so far for macro_avg: 0.762
09/16 11:16:04 AM: 	# validation passes without improvement: 0
09/16 11:16:04 AM: edges-srl-ontonotes_loss: training: 0.018956 validation: 0.017570
09/16 11:16:04 AM: macro_avg: validation: 0.761770
09/16 11:16:04 AM: micro_avg: validation: 0.000000
09/16 11:16:04 AM: edges-srl-ontonotes_mcc: training: 0.741988 validation: 0.760765
09/16 11:16:04 AM: edges-srl-ontonotes_acc: training: 0.662077 validation: 0.695405
09/16 11:16:04 AM: edges-srl-ontonotes_precision: training: 0.800419 validation: 0.826017
09/16 11:16:04 AM: edges-srl-ontonotes_recall: training: 0.694540 validation: 0.706797
09/16 11:16:04 AM: edges-srl-ontonotes_f1: training: 0.743730 validation: 0.761770
09/16 11:16:04 AM: Global learning rate: 2.5e-05
09/16 11:16:04 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:16:04 AM: Update 22002: task edges-srl-ontonotes, batch 2 (22002): mcc: 0.7354, acc: 0.6569, precision: 0.7742, recall: 0.7059, f1: 0.7385, edges-srl-ontonotes_loss: 0.0198
09/16 11:16:14 AM: Update 22244: task edges-srl-ontonotes, batch 244 (22244): mcc: 0.7678, acc: 0.6909, precision: 0.8222, recall: 0.7231, f1: 0.7695, edges-srl-ontonotes_loss: 0.0173
09/16 11:16:24 AM: Update 22461: task edges-srl-ontonotes, batch 461 (22461): mcc: 0.7599, acc: 0.6818, precision: 0.8154, recall: 0.7146, f1: 0.7617, edges-srl-ontonotes_loss: 0.0178
09/16 11:16:35 AM: Update 22678: task edges-srl-ontonotes, batch 678 (22678): mcc: 0.7525, acc: 0.6732, precision: 0.8093, recall: 0.7061, f1: 0.7542, edges-srl-ontonotes_loss: 0.0183
09/16 11:16:45 AM: Update 22931: task edges-srl-ontonotes, batch 931 (22931): mcc: 0.7430, acc: 0.6617, precision: 0.8017, recall: 0.6953, f1: 0.7447, edges-srl-ontonotes_loss: 0.0189
09/16 11:16:48 AM: ***** Step 23000 / Validation 23 *****
09/16 11:16:48 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:16:48 AM: Validating...
09/16 11:16:53 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:16:53 AM: Best result seen so far for macro.
09/16 11:16:53 AM: Updating LR scheduler:
09/16 11:16:53 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:16:53 AM: 	# validation passes without improvement: 0
09/16 11:16:53 AM: edges-srl-ontonotes_loss: training: 0.019002 validation: 0.017272
09/16 11:16:53 AM: macro_avg: validation: 0.766582
09/16 11:16:53 AM: micro_avg: validation: 0.000000
09/16 11:16:53 AM: edges-srl-ontonotes_mcc: training: 0.741160 validation: 0.765489
09/16 11:16:53 AM: edges-srl-ontonotes_acc: training: 0.659529 validation: 0.701409
09/16 11:16:53 AM: edges-srl-ontonotes_precision: training: 0.799963 validation: 0.828830
09/16 11:16:53 AM: edges-srl-ontonotes_recall: training: 0.693412 validation: 0.713032
09/16 11:16:53 AM: edges-srl-ontonotes_f1: training: 0.742886 validation: 0.766582
09/16 11:16:53 AM: Global learning rate: 2.5e-05
09/16 11:16:53 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:16:55 AM: Update 23055: task edges-srl-ontonotes, batch 55 (23055): mcc: 0.7134, acc: 0.6248, precision: 0.7808, recall: 0.6591, f1: 0.7148, edges-srl-ontonotes_loss: 0.0201
09/16 11:17:05 AM: Update 23293: task edges-srl-ontonotes, batch 293 (23293): mcc: 0.7157, acc: 0.6278, precision: 0.7783, recall: 0.6655, f1: 0.7175, edges-srl-ontonotes_loss: 0.0201
09/16 11:17:15 AM: Update 23529: task edges-srl-ontonotes, batch 529 (23529): mcc: 0.7031, acc: 0.6136, precision: 0.7684, recall: 0.6509, f1: 0.7048, edges-srl-ontonotes_loss: 0.0210
09/16 11:17:25 AM: Update 23761: task edges-srl-ontonotes, batch 761 (23761): mcc: 0.7028, acc: 0.6128, precision: 0.7699, recall: 0.6490, f1: 0.7043, edges-srl-ontonotes_loss: 0.0211
09/16 11:17:35 AM: Update 23945: task edges-srl-ontonotes, batch 945 (23945): mcc: 0.7006, acc: 0.6106, precision: 0.7685, recall: 0.6463, f1: 0.7021, edges-srl-ontonotes_loss: 0.0212
09/16 11:17:37 AM: ***** Step 24000 / Validation 24 *****
09/16 11:17:37 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:17:37 AM: Validating...
09/16 11:17:41 AM: Updating LR scheduler:
09/16 11:17:41 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:17:41 AM: 	# validation passes without improvement: 1
09/16 11:17:41 AM: edges-srl-ontonotes_loss: training: 0.021209 validation: 0.017693
09/16 11:17:41 AM: macro_avg: validation: 0.756415
09/16 11:17:41 AM: micro_avg: validation: 0.000000
09/16 11:17:41 AM: edges-srl-ontonotes_mcc: training: 0.701006 validation: 0.755499
09/16 11:17:41 AM: edges-srl-ontonotes_acc: training: 0.610830 validation: 0.688169
09/16 11:17:41 AM: edges-srl-ontonotes_precision: training: 0.768526 validation: 0.822689
09/16 11:17:41 AM: edges-srl-ontonotes_recall: training: 0.646955 validation: 0.700023
09/16 11:17:41 AM: edges-srl-ontonotes_f1: training: 0.702520 validation: 0.756415
09/16 11:17:41 AM: Global learning rate: 2.5e-05
09/16 11:17:41 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:17:45 AM: Update 24083: task edges-srl-ontonotes, batch 83 (24083): mcc: 0.7168, acc: 0.6308, precision: 0.7825, recall: 0.6638, f1: 0.7183, edges-srl-ontonotes_loss: 0.0203
09/16 11:17:55 AM: Update 24305: task edges-srl-ontonotes, batch 305 (24305): mcc: 0.7155, acc: 0.6287, precision: 0.7799, recall: 0.6636, f1: 0.7171, edges-srl-ontonotes_loss: 0.0202
09/16 11:18:06 AM: Update 24556: task edges-srl-ontonotes, batch 556 (24556): mcc: 0.7185, acc: 0.6311, precision: 0.7821, recall: 0.6673, f1: 0.7201, edges-srl-ontonotes_loss: 0.0202
09/16 11:18:16 AM: Update 24774: task edges-srl-ontonotes, batch 774 (24774): mcc: 0.7213, acc: 0.6351, precision: 0.7843, recall: 0.6705, f1: 0.7230, edges-srl-ontonotes_loss: 0.0200
09/16 11:18:26 AM: Update 24955: task edges-srl-ontonotes, batch 955 (24955): mcc: 0.7217, acc: 0.6355, precision: 0.7846, recall: 0.6709, f1: 0.7233, edges-srl-ontonotes_loss: 0.0201
09/16 11:18:27 AM: ***** Step 25000 / Validation 25 *****
09/16 11:18:27 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:18:27 AM: Validating...
09/16 11:18:32 AM: Updating LR scheduler:
09/16 11:18:32 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:18:32 AM: 	# validation passes without improvement: 2
09/16 11:18:32 AM: edges-srl-ontonotes_loss: training: 0.020101 validation: 0.018009
09/16 11:18:32 AM: macro_avg: validation: 0.750729
09/16 11:18:32 AM: micro_avg: validation: 0.000000
09/16 11:18:32 AM: edges-srl-ontonotes_mcc: training: 0.721116 validation: 0.749881
09/16 11:18:32 AM: edges-srl-ontonotes_acc: training: 0.634944 validation: 0.684089
09/16 11:18:32 AM: edges-srl-ontonotes_precision: training: 0.784283 validation: 0.818711
09/16 11:18:32 AM: edges-srl-ontonotes_recall: training: 0.670181 validation: 0.693172
09/16 11:18:32 AM: edges-srl-ontonotes_f1: training: 0.722756 validation: 0.750729
09/16 11:18:32 AM: Global learning rate: 2.5e-05
09/16 11:18:32 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:18:36 AM: Update 25088: task edges-srl-ontonotes, batch 88 (25088): mcc: 0.7184, acc: 0.6341, precision: 0.7802, recall: 0.6687, f1: 0.7202, edges-srl-ontonotes_loss: 0.0202
09/16 11:18:46 AM: Update 25292: task edges-srl-ontonotes, batch 292 (25292): mcc: 0.7157, acc: 0.6277, precision: 0.7788, recall: 0.6650, f1: 0.7174, edges-srl-ontonotes_loss: 0.0204
09/16 11:18:56 AM: Update 25491: task edges-srl-ontonotes, batch 491 (25491): mcc: 0.7172, acc: 0.6304, precision: 0.7803, recall: 0.6664, f1: 0.7189, edges-srl-ontonotes_loss: 0.0204
09/16 11:19:06 AM: Update 25722: task edges-srl-ontonotes, batch 722 (25722): mcc: 0.7162, acc: 0.6294, precision: 0.7796, recall: 0.6652, f1: 0.7179, edges-srl-ontonotes_loss: 0.0204
09/16 11:19:16 AM: Update 25906: task edges-srl-ontonotes, batch 906 (25906): mcc: 0.7166, acc: 0.6296, precision: 0.7804, recall: 0.6654, f1: 0.7183, edges-srl-ontonotes_loss: 0.0204
09/16 11:19:20 AM: ***** Step 26000 / Validation 26 *****
09/16 11:19:20 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:19:20 AM: Validating...
09/16 11:19:25 AM: Updating LR scheduler:
09/16 11:19:25 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:19:25 AM: 	# validation passes without improvement: 3
09/16 11:19:25 AM: edges-srl-ontonotes_loss: training: 0.020323 validation: 0.018363
09/16 11:19:25 AM: macro_avg: validation: 0.747321
09/16 11:19:25 AM: micro_avg: validation: 0.000000
09/16 11:19:25 AM: edges-srl-ontonotes_mcc: training: 0.717337 validation: 0.746149
09/16 11:19:25 AM: edges-srl-ontonotes_acc: training: 0.630544 validation: 0.681780
09/16 11:19:25 AM: edges-srl-ontonotes_precision: training: 0.780815 validation: 0.811491
09/16 11:19:25 AM: edges-srl-ontonotes_recall: training: 0.666249 validation: 0.692556
09/16 11:19:25 AM: edges-srl-ontonotes_f1: training: 0.718997 validation: 0.747321
09/16 11:19:25 AM: Global learning rate: 2.5e-05
09/16 11:19:25 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:19:26 AM: Update 26021: task edges-srl-ontonotes, batch 21 (26021): mcc: 0.7198, acc: 0.6390, precision: 0.7798, recall: 0.6716, f1: 0.7217, edges-srl-ontonotes_loss: 0.0206
09/16 11:19:36 AM: Update 26241: task edges-srl-ontonotes, batch 241 (26241): mcc: 0.7219, acc: 0.6362, precision: 0.7840, recall: 0.6718, f1: 0.7236, edges-srl-ontonotes_loss: 0.0201
09/16 11:19:46 AM: Update 26459: task edges-srl-ontonotes, batch 459 (26459): mcc: 0.7208, acc: 0.6357, precision: 0.7824, recall: 0.6713, f1: 0.7226, edges-srl-ontonotes_loss: 0.0202
09/16 11:19:56 AM: Update 26667: task edges-srl-ontonotes, batch 667 (26667): mcc: 0.7171, acc: 0.6302, precision: 0.7813, recall: 0.6654, f1: 0.7187, edges-srl-ontonotes_loss: 0.0204
09/16 11:20:06 AM: Update 26876: task edges-srl-ontonotes, batch 876 (26876): mcc: 0.7226, acc: 0.6369, precision: 0.7862, recall: 0.6713, f1: 0.7242, edges-srl-ontonotes_loss: 0.0200
09/16 11:20:11 AM: ***** Step 27000 / Validation 27 *****
09/16 11:20:11 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:20:11 AM: Validating...
09/16 11:20:16 AM: Evaluate: task edges-srl-ontonotes, batch 154 (157): mcc: 0.7523, acc: 0.6882, precision: 0.8155, recall: 0.7004, f1: 0.7536, edges-srl-ontonotes_loss: 0.0181
09/16 11:20:16 AM: Updating LR scheduler:
09/16 11:20:16 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:20:16 AM: 	# validation passes without improvement: 0
09/16 11:20:16 AM: edges-srl-ontonotes_loss: training: 0.019686 validation: 0.018135
09/16 11:20:16 AM: macro_avg: validation: 0.753231
09/16 11:20:16 AM: micro_avg: validation: 0.000000
09/16 11:20:16 AM: edges-srl-ontonotes_mcc: training: 0.728303 validation: 0.751972
09/16 11:20:16 AM: edges-srl-ontonotes_acc: training: 0.643539 validation: 0.687707
09/16 11:20:16 AM: edges-srl-ontonotes_precision: training: 0.790317 validation: 0.815296
09/16 11:20:16 AM: edges-srl-ontonotes_recall: training: 0.678145 validation: 0.699946
09/16 11:20:16 AM: edges-srl-ontonotes_f1: training: 0.729947 validation: 0.753231
09/16 11:20:16 AM: Global learning rate: 1.25e-05
09/16 11:20:16 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:20:26 AM: Update 27183: task edges-srl-ontonotes, batch 183 (27183): mcc: 0.7776, acc: 0.7043, precision: 0.8256, recall: 0.7385, f1: 0.7796, edges-srl-ontonotes_loss: 0.0164
09/16 11:20:36 AM: Update 27376: task edges-srl-ontonotes, batch 376 (27376): mcc: 0.7821, acc: 0.7095, precision: 0.8274, recall: 0.7452, f1: 0.7841, edges-srl-ontonotes_loss: 0.0160
09/16 11:20:46 AM: Update 27632: task edges-srl-ontonotes, batch 632 (27632): mcc: 0.7813, acc: 0.7091, precision: 0.8280, recall: 0.7431, f1: 0.7833, edges-srl-ontonotes_loss: 0.0161
09/16 11:20:56 AM: Update 27854: task edges-srl-ontonotes, batch 854 (27854): mcc: 0.7838, acc: 0.7123, precision: 0.8304, recall: 0.7457, f1: 0.7858, edges-srl-ontonotes_loss: 0.0160
09/16 11:21:04 AM: ***** Step 28000 / Validation 28 *****
09/16 11:21:04 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:21:04 AM: Validating...
09/16 11:21:06 AM: Evaluate: task edges-srl-ontonotes, batch 48 (157): mcc: 0.7507, acc: 0.6867, precision: 0.8157, recall: 0.6972, f1: 0.7518, edges-srl-ontonotes_loss: 0.0187
09/16 11:21:09 AM: Updating LR scheduler:
09/16 11:21:09 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:21:09 AM: 	# validation passes without improvement: 1
09/16 11:21:09 AM: edges-srl-ontonotes_loss: training: 0.015860 validation: 0.017906
09/16 11:21:09 AM: macro_avg: validation: 0.758800
09/16 11:21:09 AM: micro_avg: validation: 0.000000
09/16 11:21:09 AM: edges-srl-ontonotes_mcc: training: 0.786036 validation: 0.757471
09/16 11:21:09 AM: edges-srl-ontonotes_acc: training: 0.715019 validation: 0.695251
09/16 11:21:09 AM: edges-srl-ontonotes_precision: training: 0.831994 validation: 0.818960
09/16 11:21:09 AM: edges-srl-ontonotes_recall: training: 0.748407 validation: 0.706874
09/16 11:21:09 AM: edges-srl-ontonotes_f1: training: 0.787990 validation: 0.758800
09/16 11:21:09 AM: Global learning rate: 1.25e-05
09/16 11:21:09 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:21:16 AM: Update 28152: task edges-srl-ontonotes, batch 152 (28152): mcc: 0.8163, acc: 0.7552, precision: 0.8585, recall: 0.7811, f1: 0.8180, edges-srl-ontonotes_loss: 0.0145
09/16 11:21:26 AM: Update 28355: task edges-srl-ontonotes, batch 355 (28355): mcc: 0.8119, acc: 0.7504, precision: 0.8547, recall: 0.7764, f1: 0.8137, edges-srl-ontonotes_loss: 0.0148
09/16 11:21:36 AM: Update 28593: task edges-srl-ontonotes, batch 593 (28593): mcc: 0.7848, acc: 0.7163, precision: 0.8338, recall: 0.7445, f1: 0.7866, edges-srl-ontonotes_loss: 0.0164
09/16 11:21:46 AM: Update 28771: task edges-srl-ontonotes, batch 771 (28771): mcc: 0.7691, acc: 0.6976, precision: 0.8208, recall: 0.7268, f1: 0.7710, edges-srl-ontonotes_loss: 0.0173
09/16 11:21:57 AM: Update 28985: task edges-srl-ontonotes, batch 985 (28985): mcc: 0.7581, acc: 0.6830, precision: 0.8130, recall: 0.7132, f1: 0.7599, edges-srl-ontonotes_loss: 0.0180
09/16 11:21:58 AM: ***** Step 29000 / Validation 29 *****
09/16 11:21:58 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:21:58 AM: Validating...
09/16 11:22:02 AM: Updating LR scheduler:
09/16 11:22:02 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:22:02 AM: 	# validation passes without improvement: 2
09/16 11:22:02 AM: edges-srl-ontonotes_loss: training: 0.018001 validation: 0.017626
09/16 11:22:02 AM: macro_avg: validation: 0.762791
09/16 11:22:02 AM: micro_avg: validation: 0.000000
09/16 11:22:02 AM: edges-srl-ontonotes_mcc: training: 0.758078 validation: 0.761344
09/16 11:22:02 AM: edges-srl-ontonotes_acc: training: 0.682975 validation: 0.700100
09/16 11:22:02 AM: edges-srl-ontonotes_precision: training: 0.812962 validation: 0.820526
09/16 11:22:02 AM: edges-srl-ontonotes_recall: training: 0.713273 validation: 0.712647
09/16 11:22:02 AM: edges-srl-ontonotes_f1: training: 0.759862 validation: 0.762791
09/16 11:22:02 AM: Global learning rate: 1.25e-05
09/16 11:22:02 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:22:07 AM: Update 29102: task edges-srl-ontonotes, batch 102 (29102): mcc: 0.7602, acc: 0.6860, precision: 0.8146, recall: 0.7157, f1: 0.7619, edges-srl-ontonotes_loss: 0.0180
09/16 11:22:17 AM: Update 29323: task edges-srl-ontonotes, batch 323 (29323): mcc: 0.7600, acc: 0.6843, precision: 0.8156, recall: 0.7144, f1: 0.7617, edges-srl-ontonotes_loss: 0.0178
09/16 11:22:27 AM: Update 29575: task edges-srl-ontonotes, batch 575 (29575): mcc: 0.7644, acc: 0.6893, precision: 0.8191, recall: 0.7196, f1: 0.7661, edges-srl-ontonotes_loss: 0.0174
09/16 11:22:38 AM: Update 29828: task edges-srl-ontonotes, batch 828 (29828): mcc: 0.7557, acc: 0.6788, precision: 0.8120, recall: 0.7097, f1: 0.7574, edges-srl-ontonotes_loss: 0.0180
09/16 11:22:47 AM: ***** Step 30000 / Validation 30 *****
09/16 11:22:47 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:22:47 AM: Validating...
09/16 11:22:48 AM: Evaluate: task edges-srl-ontonotes, batch 24 (157): mcc: 0.7691, acc: 0.7049, precision: 0.8335, recall: 0.7156, f1: 0.7701, edges-srl-ontonotes_loss: 0.0172
09/16 11:22:52 AM: Best result seen so far for edges-srl-ontonotes.
09/16 11:22:52 AM: Best result seen so far for macro.
09/16 11:22:52 AM: Updating LR scheduler:
09/16 11:22:52 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:22:52 AM: 	# validation passes without improvement: 0
09/16 11:22:52 AM: edges-srl-ontonotes_loss: training: 0.018281 validation: 0.017255
09/16 11:22:52 AM: macro_avg: validation: 0.766967
09/16 11:22:52 AM: micro_avg: validation: 0.000000
09/16 11:22:52 AM: edges-srl-ontonotes_mcc: training: 0.751278 validation: 0.765609
09/16 11:22:52 AM: edges-srl-ontonotes_acc: training: 0.673935 validation: 0.703025
09/16 11:22:52 AM: edges-srl-ontonotes_precision: training: 0.807920 validation: 0.825293
09/16 11:22:52 AM: edges-srl-ontonotes_recall: training: 0.705123 validation: 0.716342
09/16 11:22:52 AM: edges-srl-ontonotes_f1: training: 0.753029 validation: 0.766967
09/16 11:22:52 AM: Global learning rate: 1.25e-05
09/16 11:22:52 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:22:58 AM: Update 30129: task edges-srl-ontonotes, batch 129 (30129): mcc: 0.7157, acc: 0.6285, precision: 0.7784, recall: 0.6653, f1: 0.7174, edges-srl-ontonotes_loss: 0.0202
09/16 11:23:08 AM: Update 30342: task edges-srl-ontonotes, batch 342 (30342): mcc: 0.7169, acc: 0.6307, precision: 0.7793, recall: 0.6667, f1: 0.7186, edges-srl-ontonotes_loss: 0.0203
09/16 11:23:18 AM: Update 30585: task edges-srl-ontonotes, batch 585 (30585): mcc: 0.7158, acc: 0.6286, precision: 0.7779, recall: 0.6660, f1: 0.7176, edges-srl-ontonotes_loss: 0.0202
09/16 11:23:28 AM: Update 30829: task edges-srl-ontonotes, batch 829 (30829): mcc: 0.7094, acc: 0.6213, precision: 0.7735, recall: 0.6580, f1: 0.7111, edges-srl-ontonotes_loss: 0.0207
09/16 11:23:36 AM: ***** Step 31000 / Validation 31 *****
09/16 11:23:36 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:23:36 AM: Validating...
09/16 11:23:38 AM: Evaluate: task edges-srl-ontonotes, batch 71 (157): mcc: 0.7624, acc: 0.6948, precision: 0.8314, recall: 0.7052, f1: 0.7631, edges-srl-ontonotes_loss: 0.0175
09/16 11:23:40 AM: Updating LR scheduler:
09/16 11:23:40 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:23:40 AM: 	# validation passes without improvement: 1
09/16 11:23:40 AM: edges-srl-ontonotes_loss: training: 0.020845 validation: 0.017350
09/16 11:23:40 AM: macro_avg: validation: 0.763319
09/16 11:23:40 AM: micro_avg: validation: 0.000000
09/16 11:23:40 AM: edges-srl-ontonotes_mcc: training: 0.707599 validation: 0.762524
09/16 11:23:40 AM: edges-srl-ontonotes_acc: training: 0.619009 validation: 0.695558
09/16 11:23:40 AM: edges-srl-ontonotes_precision: training: 0.772332 validation: 0.830197
09/16 11:23:40 AM: edges-srl-ontonotes_recall: training: 0.655732 validation: 0.706412
09/16 11:23:40 AM: edges-srl-ontonotes_f1: training: 0.709272 validation: 0.763319
09/16 11:23:40 AM: Global learning rate: 1.25e-05
09/16 11:23:40 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:23:51 AM: Update 31176: task edges-srl-ontonotes, batch 176 (31176): mcc: 0.7037, acc: 0.6123, precision: 0.7729, recall: 0.6482, f1: 0.7051, edges-srl-ontonotes_loss: 0.0213
09/16 11:24:01 AM: Update 31432: task edges-srl-ontonotes, batch 432 (31432): mcc: 0.7130, acc: 0.6245, precision: 0.7783, recall: 0.6605, f1: 0.7145, edges-srl-ontonotes_loss: 0.0205
09/16 11:24:11 AM: Update 31642: task edges-srl-ontonotes, batch 642 (31642): mcc: 0.7148, acc: 0.6266, precision: 0.7790, recall: 0.6631, f1: 0.7164, edges-srl-ontonotes_loss: 0.0204
09/16 11:24:21 AM: Update 31850: task edges-srl-ontonotes, batch 850 (31850): mcc: 0.7173, acc: 0.6298, precision: 0.7804, recall: 0.6666, f1: 0.7190, edges-srl-ontonotes_loss: 0.0202
09/16 11:24:28 AM: ***** Step 32000 / Validation 32 *****
09/16 11:24:28 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:24:28 AM: Validating...
09/16 11:24:31 AM: Evaluate: task edges-srl-ontonotes, batch 49 (157): mcc: 0.7469, acc: 0.6800, precision: 0.8160, recall: 0.6900, f1: 0.7477, edges-srl-ontonotes_loss: 0.0185
09/16 11:24:34 AM: Updating LR scheduler:
09/16 11:24:34 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:24:34 AM: 	# validation passes without improvement: 2
09/16 11:24:34 AM: edges-srl-ontonotes_loss: training: 0.020125 validation: 0.017666
09/16 11:24:34 AM: macro_avg: validation: 0.756047
09/16 11:24:34 AM: micro_avg: validation: 0.000000
09/16 11:24:34 AM: edges-srl-ontonotes_mcc: training: 0.718950 validation: 0.754837
09/16 11:24:34 AM: edges-srl-ontonotes_acc: training: 0.631501 validation: 0.691633
09/16 11:24:34 AM: edges-srl-ontonotes_precision: training: 0.781696 validation: 0.818345
09/16 11:24:34 AM: edges-srl-ontonotes_recall: training: 0.668447 validation: 0.702563
09/16 11:24:34 AM: edges-srl-ontonotes_f1: training: 0.720649 validation: 0.756047
09/16 11:24:34 AM: Global learning rate: 1.25e-05
09/16 11:24:34 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:24:42 AM: Update 32115: task edges-srl-ontonotes, batch 115 (32115): mcc: 0.7306, acc: 0.6476, precision: 0.7905, recall: 0.6822, f1: 0.7324, edges-srl-ontonotes_loss: 0.0195
09/16 11:24:52 AM: Update 32331: task edges-srl-ontonotes, batch 331 (32331): mcc: 0.7192, acc: 0.6337, precision: 0.7813, recall: 0.6693, f1: 0.7210, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:02 AM: Update 32538: task edges-srl-ontonotes, batch 538 (32538): mcc: 0.7179, acc: 0.6318, precision: 0.7808, recall: 0.6672, f1: 0.7196, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:13 AM: Update 32741: task edges-srl-ontonotes, batch 741 (32741): mcc: 0.7187, acc: 0.6328, precision: 0.7812, recall: 0.6685, f1: 0.7204, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:23 AM: Update 32965: task edges-srl-ontonotes, batch 965 (32965): mcc: 0.7181, acc: 0.6318, precision: 0.7811, recall: 0.6674, f1: 0.7198, edges-srl-ontonotes_loss: 0.0203
09/16 11:25:24 AM: ***** Step 33000 / Validation 33 *****
09/16 11:25:24 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:25:24 AM: Validating...
09/16 11:25:30 AM: Updating LR scheduler:
09/16 11:25:30 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:25:30 AM: 	# validation passes without improvement: 3
09/16 11:25:30 AM: edges-srl-ontonotes_loss: training: 0.020303 validation: 0.017925
09/16 11:25:30 AM: macro_avg: validation: 0.752063
09/16 11:25:30 AM: micro_avg: validation: 0.000000
09/16 11:25:30 AM: edges-srl-ontonotes_mcc: training: 0.718138 validation: 0.750866
09/16 11:25:30 AM: edges-srl-ontonotes_acc: training: 0.631812 validation: 0.688400
09/16 11:25:30 AM: edges-srl-ontonotes_precision: training: 0.781171 validation: 0.815174
09/16 11:25:30 AM: edges-srl-ontonotes_recall: training: 0.667410 validation: 0.698022
09/16 11:25:30 AM: edges-srl-ontonotes_f1: training: 0.719823 validation: 0.752063
09/16 11:25:30 AM: Global learning rate: 1.25e-05
09/16 11:25:30 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:25:33 AM: Update 33052: task edges-srl-ontonotes, batch 52 (33052): mcc: 0.7049, acc: 0.6171, precision: 0.7707, recall: 0.6522, f1: 0.7065, edges-srl-ontonotes_loss: 0.0209
09/16 11:25:43 AM: Update 33202: task edges-srl-ontonotes, batch 202 (33202): mcc: 0.7164, acc: 0.6307, precision: 0.7802, recall: 0.6650, f1: 0.7180, edges-srl-ontonotes_loss: 0.0202
09/16 11:25:53 AM: Update 33403: task edges-srl-ontonotes, batch 403 (33403): mcc: 0.7188, acc: 0.6335, precision: 0.7821, recall: 0.6678, f1: 0.7205, edges-srl-ontonotes_loss: 0.0201
09/16 11:26:03 AM: Update 33642: task edges-srl-ontonotes, batch 642 (33642): mcc: 0.7227, acc: 0.6373, precision: 0.7856, recall: 0.6719, f1: 0.7243, edges-srl-ontonotes_loss: 0.0199
09/16 11:26:13 AM: Update 33843: task edges-srl-ontonotes, batch 843 (33843): mcc: 0.7203, acc: 0.6351, precision: 0.7834, recall: 0.6694, f1: 0.7219, edges-srl-ontonotes_loss: 0.0202
09/16 11:26:21 AM: ***** Step 34000 / Validation 34 *****
09/16 11:26:21 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:26:21 AM: Validating...
09/16 11:26:23 AM: Evaluate: task edges-srl-ontonotes, batch 49 (157): mcc: 0.7395, acc: 0.6702, precision: 0.8121, recall: 0.6800, f1: 0.7402, edges-srl-ontonotes_loss: 0.0189
09/16 11:26:29 AM: Updating LR scheduler:
09/16 11:26:29 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:26:29 AM: 	# validation passes without improvement: 0
09/16 11:26:29 AM: edges-srl-ontonotes_loss: training: 0.020220 validation: 0.018073
09/16 11:26:29 AM: macro_avg: validation: 0.747605
09/16 11:26:29 AM: micro_avg: validation: 0.000000
09/16 11:26:29 AM: edges-srl-ontonotes_mcc: training: 0.719294 validation: 0.746640
09/16 11:26:29 AM: edges-srl-ontonotes_acc: training: 0.633720 validation: 0.682396
09/16 11:26:29 AM: edges-srl-ontonotes_precision: training: 0.783298 validation: 0.814502
09/16 11:26:29 AM: edges-srl-ontonotes_recall: training: 0.667691 validation: 0.690863
09/16 11:26:29 AM: edges-srl-ontonotes_f1: training: 0.720889 validation: 0.747605
09/16 11:26:29 AM: Global learning rate: 6.25e-06
09/16 11:26:29 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:26:33 AM: Update 34074: task edges-srl-ontonotes, batch 74 (34074): mcc: 0.7555, acc: 0.6777, precision: 0.8108, recall: 0.7105, f1: 0.7573, edges-srl-ontonotes_loss: 0.0177
09/16 11:26:43 AM: Update 34288: task edges-srl-ontonotes, batch 288 (34288): mcc: 0.7561, acc: 0.6784, precision: 0.8122, recall: 0.7103, f1: 0.7578, edges-srl-ontonotes_loss: 0.0176
09/16 11:26:53 AM: Update 34459: task edges-srl-ontonotes, batch 459 (34459): mcc: 0.7624, acc: 0.6860, precision: 0.8163, recall: 0.7182, f1: 0.7641, edges-srl-ontonotes_loss: 0.0172
09/16 11:27:03 AM: Update 34676: task edges-srl-ontonotes, batch 676 (34676): mcc: 0.7674, acc: 0.6926, precision: 0.8192, recall: 0.7252, f1: 0.7693, edges-srl-ontonotes_loss: 0.0169
09/16 11:27:13 AM: Update 34926: task edges-srl-ontonotes, batch 926 (34926): mcc: 0.7712, acc: 0.6971, precision: 0.8216, recall: 0.7300, f1: 0.7731, edges-srl-ontonotes_loss: 0.0167
09/16 11:27:17 AM: ***** Step 35000 / Validation 35 *****
09/16 11:27:17 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:27:17 AM: Validating...
09/16 11:27:22 AM: Updating LR scheduler:
09/16 11:27:22 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:27:22 AM: 	# validation passes without improvement: 1
09/16 11:27:22 AM: edges-srl-ontonotes_loss: training: 0.016566 validation: 0.017779
09/16 11:27:22 AM: macro_avg: validation: 0.759282
09/16 11:27:22 AM: micro_avg: validation: 0.000000
09/16 11:27:22 AM: edges-srl-ontonotes_mcc: training: 0.773140 validation: 0.757931
09/16 11:27:22 AM: edges-srl-ontonotes_acc: training: 0.699460 validation: 0.696713
09/16 11:27:22 AM: edges-srl-ontonotes_precision: training: 0.823068 validation: 0.819048
09/16 11:27:22 AM: edges-srl-ontonotes_recall: training: 0.732309 validation: 0.707644
09/16 11:27:22 AM: edges-srl-ontonotes_f1: training: 0.775041 validation: 0.759282
09/16 11:27:22 AM: Global learning rate: 6.25e-06
09/16 11:27:22 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:27:23 AM: Update 35019: task edges-srl-ontonotes, batch 19 (35019): mcc: 0.8027, acc: 0.7360, precision: 0.8458, recall: 0.7672, f1: 0.8046, edges-srl-ontonotes_loss: 0.0149
09/16 11:27:33 AM: Update 35243: task edges-srl-ontonotes, batch 243 (35243): mcc: 0.7955, acc: 0.7285, precision: 0.8408, recall: 0.7581, f1: 0.7973, edges-srl-ontonotes_loss: 0.0153
09/16 11:27:43 AM: Update 35416: task edges-srl-ontonotes, batch 416 (35416): mcc: 0.8059, acc: 0.7412, precision: 0.8488, recall: 0.7704, f1: 0.8077, edges-srl-ontonotes_loss: 0.0149
09/16 11:27:53 AM: Update 35629: task edges-srl-ontonotes, batch 629 (35629): mcc: 0.8028, acc: 0.7380, precision: 0.8470, recall: 0.7662, f1: 0.8046, edges-srl-ontonotes_loss: 0.0152
09/16 11:28:03 AM: Update 35850: task edges-srl-ontonotes, batch 850 (35850): mcc: 0.7876, acc: 0.7196, precision: 0.8347, recall: 0.7489, f1: 0.7895, edges-srl-ontonotes_loss: 0.0161
09/16 11:28:10 AM: ***** Step 36000 / Validation 36 *****
09/16 11:28:10 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:28:10 AM: Validating...
09/16 11:28:13 AM: Evaluate: task edges-srl-ontonotes, batch 121 (157): mcc: 0.7691, acc: 0.7059, precision: 0.8307, recall: 0.7179, f1: 0.7702, edges-srl-ontonotes_loss: 0.0171
09/16 11:28:15 AM: Updating LR scheduler:
09/16 11:28:15 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:28:15 AM: 	# validation passes without improvement: 2
09/16 11:28:15 AM: edges-srl-ontonotes_loss: training: 0.016741 validation: 0.017501
09/16 11:28:15 AM: macro_avg: validation: 0.762823
09/16 11:28:15 AM: micro_avg: validation: 0.000000
09/16 11:28:15 AM: edges-srl-ontonotes_mcc: training: 0.778012 validation: 0.761639
09/16 11:28:15 AM: edges-srl-ontonotes_acc: training: 0.707754 validation: 0.698176
09/16 11:28:15 AM: edges-srl-ontonotes_precision: training: 0.827264 validation: 0.824408
09/16 11:28:15 AM: edges-srl-ontonotes_recall: training: 0.737643 validation: 0.709799
09/16 11:28:15 AM: edges-srl-ontonotes_f1: training: 0.779888 validation: 0.762823
09/16 11:28:15 AM: Global learning rate: 6.25e-06
09/16 11:28:15 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:28:23 AM: Update 36208: task edges-srl-ontonotes, batch 208 (36208): mcc: 0.7140, acc: 0.6278, precision: 0.7800, recall: 0.6609, f1: 0.7155, edges-srl-ontonotes_loss: 0.0209
09/16 11:28:33 AM: Update 36430: task edges-srl-ontonotes, batch 430 (36430): mcc: 0.7368, acc: 0.6551, precision: 0.7970, recall: 0.6880, f1: 0.7385, edges-srl-ontonotes_loss: 0.0194
09/16 11:28:43 AM: Update 36631: task edges-srl-ontonotes, batch 631 (36631): mcc: 0.7455, acc: 0.6657, precision: 0.8040, recall: 0.6979, f1: 0.7472, edges-srl-ontonotes_loss: 0.0188
09/16 11:28:54 AM: Update 36857: task edges-srl-ontonotes, batch 857 (36857): mcc: 0.7518, acc: 0.6740, precision: 0.8090, recall: 0.7051, f1: 0.7535, edges-srl-ontonotes_loss: 0.0184
09/16 11:28:59 AM: ***** Step 37000 / Validation 37 *****
09/16 11:28:59 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:28:59 AM: Validating...
09/16 11:29:04 AM: Evaluate: task edges-srl-ontonotes, batch 146 (157): mcc: 0.7671, acc: 0.7037, precision: 0.8295, recall: 0.7154, f1: 0.7683, edges-srl-ontonotes_loss: 0.0170
09/16 11:29:04 AM: Updating LR scheduler:
09/16 11:29:04 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:29:04 AM: 	# validation passes without improvement: 3
09/16 11:29:04 AM: edges-srl-ontonotes_loss: training: 0.018501 validation: 0.017248
09/16 11:29:04 AM: macro_avg: validation: 0.765521
09/16 11:29:04 AM: micro_avg: validation: 0.000000
09/16 11:29:04 AM: edges-srl-ontonotes_mcc: training: 0.749436 validation: 0.764381
09/16 11:29:04 AM: edges-srl-ontonotes_acc: training: 0.671081 validation: 0.700639
09/16 11:29:04 AM: edges-srl-ontonotes_precision: training: 0.807063 validation: 0.827284
09/16 11:29:04 AM: edges-srl-ontonotes_recall: training: 0.702469 validation: 0.712339
09/16 11:29:04 AM: edges-srl-ontonotes_f1: training: 0.751142 validation: 0.765521
09/16 11:29:04 AM: Global learning rate: 6.25e-06
09/16 11:29:04 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:29:14 AM: Update 37216: task edges-srl-ontonotes, batch 216 (37216): mcc: 0.7348, acc: 0.6516, precision: 0.7947, recall: 0.6864, f1: 0.7365, edges-srl-ontonotes_loss: 0.0192
09/16 11:29:24 AM: Update 37427: task edges-srl-ontonotes, batch 427 (37427): mcc: 0.7275, acc: 0.6433, precision: 0.7884, recall: 0.6783, f1: 0.7292, edges-srl-ontonotes_loss: 0.0198
09/16 11:29:34 AM: Update 37592: task edges-srl-ontonotes, batch 592 (37592): mcc: 0.7257, acc: 0.6412, precision: 0.7867, recall: 0.6765, f1: 0.7275, edges-srl-ontonotes_loss: 0.0199
09/16 11:29:44 AM: Update 37833: task edges-srl-ontonotes, batch 833 (37833): mcc: 0.7233, acc: 0.6374, precision: 0.7847, recall: 0.6738, f1: 0.7250, edges-srl-ontonotes_loss: 0.0199
09/16 11:29:51 AM: ***** Step 38000 / Validation 38 *****
09/16 11:29:51 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:29:51 AM: Validating...
09/16 11:29:54 AM: Evaluate: task edges-srl-ontonotes, batch 67 (157): mcc: 0.7623, acc: 0.6964, precision: 0.8281, recall: 0.7078, f1: 0.7632, edges-srl-ontonotes_loss: 0.0176
09/16 11:29:57 AM: Updating LR scheduler:
09/16 11:29:57 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:29:57 AM: 	# validation passes without improvement: 0
09/16 11:29:57 AM: edges-srl-ontonotes_loss: training: 0.020186 validation: 0.017293
09/16 11:29:57 AM: macro_avg: validation: 0.764051
09/16 11:29:57 AM: micro_avg: validation: 0.000000
09/16 11:29:57 AM: edges-srl-ontonotes_mcc: training: 0.718973 validation: 0.762975
09/16 11:29:57 AM: edges-srl-ontonotes_acc: training: 0.632299 validation: 0.698945
09/16 11:29:57 AM: edges-srl-ontonotes_precision: training: 0.781329 validation: 0.826968
09/16 11:29:57 AM: edges-srl-ontonotes_recall: training: 0.668806 validation: 0.710030
09/16 11:29:57 AM: edges-srl-ontonotes_f1: training: 0.720702 validation: 0.764051
09/16 11:29:57 AM: Global learning rate: 3.125e-06
09/16 11:29:57 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:30:04 AM: Update 38134: task edges-srl-ontonotes, batch 134 (38134): mcc: 0.6926, acc: 0.6009, precision: 0.7630, recall: 0.6364, f1: 0.6939, edges-srl-ontonotes_loss: 0.0219
09/16 11:30:14 AM: Update 38369: task edges-srl-ontonotes, batch 369 (38369): mcc: 0.6964, acc: 0.6072, precision: 0.7642, recall: 0.6422, f1: 0.6979, edges-srl-ontonotes_loss: 0.0216
09/16 11:30:24 AM: Update 38518: task edges-srl-ontonotes, batch 518 (38518): mcc: 0.7030, acc: 0.6131, precision: 0.7705, recall: 0.6489, f1: 0.7045, edges-srl-ontonotes_loss: 0.0212
09/16 11:30:34 AM: Update 38735: task edges-srl-ontonotes, batch 735 (38735): mcc: 0.7077, acc: 0.6191, precision: 0.7736, recall: 0.6549, f1: 0.7093, edges-srl-ontonotes_loss: 0.0208
09/16 11:30:44 AM: Update 38994: task edges-srl-ontonotes, batch 994 (38994): mcc: 0.7113, acc: 0.6229, precision: 0.7759, recall: 0.6595, f1: 0.7129, edges-srl-ontonotes_loss: 0.0206
09/16 11:30:45 AM: ***** Step 39000 / Validation 39 *****
09/16 11:30:45 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:30:45 AM: Validating...
09/16 11:30:50 AM: Updating LR scheduler:
09/16 11:30:50 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:30:50 AM: 	# validation passes without improvement: 1
09/16 11:30:50 AM: edges-srl-ontonotes_loss: training: 0.020594 validation: 0.017378
09/16 11:30:50 AM: macro_avg: validation: 0.762365
09/16 11:30:50 AM: micro_avg: validation: 0.000000
09/16 11:30:50 AM: edges-srl-ontonotes_mcc: training: 0.711624 validation: 0.761105
09/16 11:30:50 AM: edges-srl-ontonotes_acc: training: 0.623221 validation: 0.697560
09/16 11:30:50 AM: edges-srl-ontonotes_precision: training: 0.776105 validation: 0.822926
09/16 11:30:50 AM: edges-srl-ontonotes_recall: training: 0.659848 validation: 0.710107
09/16 11:30:50 AM: edges-srl-ontonotes_f1: training: 0.713270 validation: 0.762365
09/16 11:30:50 AM: Global learning rate: 3.125e-06
09/16 11:30:50 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:30:54 AM: Update 39079: task edges-srl-ontonotes, batch 79 (39079): mcc: 0.7218, acc: 0.6363, precision: 0.7792, recall: 0.6759, f1: 0.7239, edges-srl-ontonotes_loss: 0.0201
09/16 11:31:04 AM: Update 39327: task edges-srl-ontonotes, batch 327 (39327): mcc: 0.7305, acc: 0.6448, precision: 0.7903, recall: 0.6821, f1: 0.7323, edges-srl-ontonotes_loss: 0.0195
09/16 11:31:14 AM: Update 39584: task edges-srl-ontonotes, batch 584 (39584): mcc: 0.7241, acc: 0.6379, precision: 0.7852, recall: 0.6748, f1: 0.7259, edges-srl-ontonotes_loss: 0.0199
09/16 11:31:24 AM: Update 39766: task edges-srl-ontonotes, batch 766 (39766): mcc: 0.7218, acc: 0.6348, precision: 0.7839, recall: 0.6719, f1: 0.7236, edges-srl-ontonotes_loss: 0.0200
09/16 11:31:35 AM: Update 39987: task edges-srl-ontonotes, batch 987 (39987): mcc: 0.7208, acc: 0.6338, precision: 0.7833, recall: 0.6705, f1: 0.7225, edges-srl-ontonotes_loss: 0.0201
09/16 11:31:36 AM: ***** Step 40000 / Validation 40 *****
09/16 11:31:36 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
09/16 11:31:36 AM: Validating...
09/16 11:31:41 AM: Updating LR scheduler:
09/16 11:31:41 AM: 	Best result seen so far for macro_avg: 0.767
09/16 11:31:41 AM: 	# validation passes without improvement: 2
09/16 11:31:41 AM: Ran out of early stopping patience. Stopping training.
09/16 11:31:41 AM: edges-srl-ontonotes_loss: training: 0.020134 validation: 0.017465
09/16 11:31:41 AM: macro_avg: validation: 0.760272
09/16 11:31:41 AM: micro_avg: validation: 0.000000
09/16 11:31:41 AM: edges-srl-ontonotes_mcc: training: 0.720510 validation: 0.759134
09/16 11:31:41 AM: edges-srl-ontonotes_acc: training: 0.633611 validation: 0.695866
09/16 11:31:41 AM: edges-srl-ontonotes_precision: training: 0.783020 validation: 0.822918
09/16 11:31:41 AM: edges-srl-ontonotes_recall: training: 0.670162 validation: 0.706489
09/16 11:31:41 AM: edges-srl-ontonotes_f1: training: 0.722208 validation: 0.760272
09/16 11:31:41 AM: Global learning rate: 3.125e-06
09/16 11:31:41 AM: Saving checkpoints to: ./experiments/srl-ontonotes-mnli-only/run
09/16 11:31:41 AM: Stopped training after 40 validation checks
09/16 11:31:41 AM: Trained edges-srl-ontonotes for 40000 batches or 5.529 epochs
09/16 11:31:41 AM: ***** VALIDATION RESULTS *****
09/16 11:31:41 AM: edges-srl-ontonotes_f1 (for best val pass 30): edges-srl-ontonotes_loss: 0.01726, macro_avg: 0.76697, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.76561, edges-srl-ontonotes_acc: 0.70303, edges-srl-ontonotes_precision: 0.82529, edges-srl-ontonotes_recall: 0.71634, edges-srl-ontonotes_f1: 0.76697
09/16 11:31:41 AM: micro_avg (for best val pass 1): edges-srl-ontonotes_loss: 0.02756, macro_avg: 0.63827, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.64617, edges-srl-ontonotes_acc: 0.51944, edges-srl-ontonotes_precision: 0.79032, edges-srl-ontonotes_recall: 0.53529, edges-srl-ontonotes_f1: 0.63827
09/16 11:31:41 AM: macro_avg (for best val pass 30): edges-srl-ontonotes_loss: 0.01726, macro_avg: 0.76697, micro_avg: 0.00000, edges-srl-ontonotes_mcc: 0.76561, edges-srl-ontonotes_acc: 0.70303, edges-srl-ontonotes_precision: 0.82529, edges-srl-ontonotes_recall: 0.71634, edges-srl-ontonotes_f1: 0.76697
09/16 11:31:41 AM: Evaluating...
09/16 11:31:41 AM: Loaded model state from ./experiments/srl-ontonotes-mnli-only/run/edges-srl-ontonotes/model_state_target_train_val_30.best.th
09/16 11:31:41 AM: Evaluating on: edges-srl-ontonotes, split: val
09/16 11:32:11 AM: 	Task edges-srl-ontonotes: batch 901
09/16 11:32:15 AM: Task 'edges-srl-ontonotes': sorting predictions by 'idx'
09/16 11:32:15 AM: Finished evaluating on: edges-srl-ontonotes
09/16 11:32:15 AM: Task 'edges-srl-ontonotes': joining predictions with input split 'val'
09/16 11:32:22 AM: Task 'edges-srl-ontonotes': Wrote predictions to ./experiments/srl-ontonotes-mnli-only/run
09/16 11:32:22 AM: Wrote all preds for split 'val' to ./experiments/srl-ontonotes-mnli-only/run
09/16 11:32:22 AM: Evaluating on: edges-srl-ontonotes, split: test
09/16 11:32:46 AM: Task 'edges-srl-ontonotes': sorting predictions by 'idx'
09/16 11:32:46 AM: Finished evaluating on: edges-srl-ontonotes
09/16 11:32:46 AM: Task 'edges-srl-ontonotes': joining predictions with input split 'test'
09/16 11:32:50 AM: Task 'edges-srl-ontonotes': Wrote predictions to ./experiments/srl-ontonotes-mnli-only/run
09/16 11:32:50 AM: Wrote all preds for split 'test' to ./experiments/srl-ontonotes-mnli-only/run
09/16 11:32:50 AM: Writing results for split 'val' to ./experiments/srl-ontonotes-mnli-only/results.tsv
09/16 11:32:50 AM: micro_avg: 0.000, macro_avg: 0.749, edges-srl-ontonotes_mcc: 0.748, edges-srl-ontonotes_acc: 0.684, edges-srl-ontonotes_precision: 0.812, edges-srl-ontonotes_recall: 0.696, edges-srl-ontonotes_f1: 0.749
09/16 11:32:50 AM: Done!
09/16 11:32:50 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
