10/01 03:08:54 AM: Git branch: master
10/01 03:08:54 AM: Git SHA: 62183b2d03f2fae12b41eef8779808b6d354875e
10/01 03:08:54 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/dep-ud-ewt-sstseed2-top/",
  "exp_name": "experiments/dep-ud-ewt-sstseed2-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/dep-ud-ewt-sstseed2-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/sstseed2",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/dep-ud-ewt-sstseed2-top__run",
  "run_dir": "./experiments/dep-ud-ewt-sstseed2-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-dep-ud-ewt",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 03:08:54 AM: Saved config to ./experiments/dep-ud-ewt-sstseed2-top/run/params.conf
10/01 03:08:54 AM: Using random seed 1234
10/01 03:09:18 AM: Using GPU 0
10/01 03:09:18 AM: Loading tasks...
10/01 03:09:18 AM: Writing pre-preprocessed tasks to ./experiments/dep-ud-ewt-sstseed2-top/
10/01 03:09:18 AM: 	Creating task edges-dep-ud-ewt from scratch.
10/01 03:09:19 AM: Read=12522, Skip=0, Total=12522 from ./probing_data/edges/dep_ewt/en_ewt-ud-train.json.retokenized.bert-base-uncased
10/01 03:09:19 AM: Read=2000, Skip=0, Total=2000 from ./probing_data/edges/dep_ewt/en_ewt-ud-dev.json.retokenized.bert-base-uncased
10/01 03:09:19 AM: Read=2075, Skip=0, Total=2075 from ./probing_data/edges/dep_ewt/en_ewt-ud-test.json.retokenized.bert-base-uncased
10/01 03:09:20 AM: 	Task 'edges-dep-ud-ewt': |train|=12522 |val|=2000 |test|=2075
10/01 03:09:20 AM: 	Finished loading tasks: edges-dep-ud-ewt.
10/01 03:09:20 AM: 	Building vocab from scratch.
10/01 03:09:20 AM: 	Counting units for task edges-dep-ud-ewt.
10/01 03:09:20 AM: 	Task 'edges-dep-ud-ewt': adding vocab namespace 'edges-dep-ud-ewt_labels'
10/01 03:09:21 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:21 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 03:09:21 AM: 	Saved vocab to ./experiments/dep-ud-ewt-sstseed2-top/vocab
10/01 03:09:21 AM: Loading token dictionary from ./experiments/dep-ud-ewt-sstseed2-top/vocab.
10/01 03:09:21 AM: 	Loaded vocab from ./experiments/dep-ud-ewt-sstseed2-top/vocab
10/01 03:09:21 AM: 	Vocab namespace edges-dep-ud-ewt_labels: size 49
10/01 03:09:21 AM: 	Vocab namespace tokens: size 14333
10/01 03:09:21 AM: 	Vocab namespace bert_uncased: size 30524
10/01 03:09:21 AM: 	Vocab namespace chars: size 81
10/01 03:09:21 AM: 	Finished building vocab.
10/01 03:09:21 AM: 	Task edges-dep-ud-ewt (train): Indexing from scratch.
10/01 03:09:26 AM: 	Task edges-dep-ud-ewt (train): Saved 12522 instances to ./experiments/dep-ud-ewt-sstseed2-top/preproc/edges-dep-ud-ewt__train_data
10/01 03:09:26 AM: 	Task edges-dep-ud-ewt (val): Indexing from scratch.
10/01 03:09:27 AM: 	Task edges-dep-ud-ewt (val): Saved 2000 instances to ./experiments/dep-ud-ewt-sstseed2-top/preproc/edges-dep-ud-ewt__val_data
10/01 03:09:27 AM: 	Task edges-dep-ud-ewt (test): Indexing from scratch.
10/01 03:09:27 AM: 	Task edges-dep-ud-ewt (test): Saved 2075 instances to ./experiments/dep-ud-ewt-sstseed2-top/preproc/edges-dep-ud-ewt__test_data
10/01 03:09:27 AM: 	Finished indexing tasks
10/01 03:09:27 AM: 	Creating trimmed target-only version of edges-dep-ud-ewt train.
10/01 03:09:27 AM: 	  Training on 
10/01 03:09:27 AM: 	  Evaluating on edges-dep-ud-ewt
10/01 03:09:27 AM: 	Finished loading tasks in 9.472s
10/01 03:09:27 AM: 	 Tasks: ['edges-dep-ud-ewt']
10/01 03:09:27 AM: Building model...
10/01 03:09:27 AM: Using BERT model (bert-base-uncased).
10/01 03:09:27 AM: LOADING A FUNETUNED MODEL from: 
10/01 03:09:27 AM: models/sstseed2
10/01 03:09:27 AM: loading configuration file models/sstseed2/config.json
10/01 03:09:27 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst-2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 03:09:27 AM: loading weights file models/sstseed2/pytorch_model.bin
10/01 03:09:31 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpcdo7n2ef
10/01 03:09:33 AM: copying /tmp/tmpcdo7n2ef to cache at ./experiments/dep-ud-ewt-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:33 AM: creating metadata file for ./experiments/dep-ud-ewt-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:33 AM: removing temp file /tmp/tmpcdo7n2ef
10/01 03:09:33 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/dep-ud-ewt-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:33 AM: Initializing parameters
10/01 03:09:33 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 03:09:33 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 03:09:33 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 03:09:33 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 03:09:33 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 03:09:33 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 03:09:33 AM: 	Task 'edges-dep-ud-ewt' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-dep-ud-ewt"
}
10/01 03:09:37 AM: Model specification:
10/01 03:09:37 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-dep-ud-ewt_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=49, bias=True)
      )
    )
  )
)
10/01 03:09:37 AM: Model parameters:
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:09:37 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.classifier.classifier.4.weight: Trainable parameter, count 12544 with torch.Size([49, 256])
10/01 03:09:37 AM: 	edges-dep-ud-ewt_mdl.classifier.classifier.4.bias: Trainable parameter, count 49 with torch.Size([49])
10/01 03:09:37 AM: Total number of parameters: 110151473 (1.10151e+08)
10/01 03:09:37 AM: Number of trainable parameters: 669233 (669233)
10/01 03:09:37 AM: Finished building model in 9.731s
10/01 03:09:37 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-dep-ud-ewt 

10/01 03:09:41 AM: patience = 9
10/01 03:09:41 AM: val_interval = 1000
10/01 03:09:41 AM: max_vals = 250
10/01 03:09:41 AM: cuda_device = 0
10/01 03:09:41 AM: grad_norm = 5.0
10/01 03:09:41 AM: grad_clipping = None
10/01 03:09:41 AM: lr_decay = 0.99
10/01 03:09:41 AM: min_lr = 1e-06
10/01 03:09:41 AM: keep_all_checkpoints = 0
10/01 03:09:41 AM: val_data_limit = 5000
10/01 03:09:41 AM: max_epochs = -1
10/01 03:09:41 AM: dec_val_scale = 250
10/01 03:09:41 AM: training_data_fraction = 1
10/01 03:09:41 AM: type = adam
10/01 03:09:41 AM: parameter_groups = None
10/01 03:09:41 AM: Number of trainable parameters: 669233
10/01 03:09:41 AM: infer_type_and_cast = True
10/01 03:09:41 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:41 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:41 AM: lr = 0.0001
10/01 03:09:41 AM: amsgrad = True
10/01 03:09:41 AM: type = reduce_on_plateau
10/01 03:09:41 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:41 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:41 AM: mode = max
10/01 03:09:41 AM: factor = 0.5
10/01 03:09:41 AM: patience = 3
10/01 03:09:41 AM: threshold = 0.0001
10/01 03:09:41 AM: threshold_mode = abs
10/01 03:09:41 AM: verbose = True
10/01 03:09:41 AM: type = adam
10/01 03:09:41 AM: parameter_groups = None
10/01 03:09:41 AM: Number of trainable parameters: 669233
10/01 03:09:41 AM: infer_type_and_cast = True
10/01 03:09:41 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:41 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:41 AM: lr = 0.0001
10/01 03:09:41 AM: amsgrad = True
10/01 03:09:41 AM: type = reduce_on_plateau
10/01 03:09:41 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:09:41 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:09:41 AM: mode = max
10/01 03:09:41 AM: factor = 0.5
10/01 03:09:41 AM: patience = 3
10/01 03:09:41 AM: threshold = 0.0001
10/01 03:09:41 AM: threshold_mode = abs
10/01 03:09:41 AM: verbose = True
10/01 03:09:41 AM: Starting training without restoring from a checkpoint.
10/01 03:09:41 AM: Training examples per task, before any subsampling: {'edges-dep-ud-ewt': 12522}
10/01 03:09:41 AM: Beginning training with stopping criteria based on metric: edges-dep-ud-ewt_f1
10/01 03:09:51 AM: Update 37: task edges-dep-ud-ewt, batch 37 (37): mcc: 0.0064, acc: 0.0073, precision: 0.0229, recall: 0.1319, f1: 0.0390, edges-dep-ud-ewt_loss: 0.3346
10/01 03:10:01 AM: Update 106: task edges-dep-ud-ewt, batch 106 (106): mcc: 0.0073, acc: 0.0079, precision: 0.0250, recall: 0.0577, f1: 0.0349, edges-dep-ud-ewt_loss: 0.2098
10/01 03:10:11 AM: Update 166: task edges-dep-ud-ewt, batch 166 (166): mcc: 0.0148, acc: 0.0159, precision: 0.0325, recall: 0.0468, f1: 0.0383, edges-dep-ud-ewt_loss: 0.1705
10/01 03:10:22 AM: Update 226: task edges-dep-ud-ewt, batch 226 (226): mcc: 0.0238, acc: 0.0231, precision: 0.0430, recall: 0.0455, f1: 0.0442, edges-dep-ud-ewt_loss: 0.1491
10/01 03:10:32 AM: Update 283: task edges-dep-ud-ewt, batch 283 (283): mcc: 0.0327, acc: 0.0289, precision: 0.0554, recall: 0.0465, f1: 0.0506, edges-dep-ud-ewt_loss: 0.1360
10/01 03:10:42 AM: Update 330: task edges-dep-ud-ewt, batch 330 (330): mcc: 0.0395, acc: 0.0327, precision: 0.0662, recall: 0.0475, f1: 0.0553, edges-dep-ud-ewt_loss: 0.1282
10/01 03:10:52 AM: Update 390: task edges-dep-ud-ewt, batch 390 (390): mcc: 0.0482, acc: 0.0372, precision: 0.0808, recall: 0.0497, f1: 0.0616, edges-dep-ud-ewt_loss: 0.1205
10/01 03:11:02 AM: Update 426: task edges-dep-ud-ewt, batch 426 (426): mcc: 0.0570, acc: 0.0421, precision: 0.0954, recall: 0.0533, f1: 0.0684, edges-dep-ud-ewt_loss: 0.1165
10/01 03:11:12 AM: Update 487: task edges-dep-ud-ewt, batch 487 (487): mcc: 0.0731, acc: 0.0517, precision: 0.1213, recall: 0.0617, f1: 0.0818, edges-dep-ud-ewt_loss: 0.1107
10/01 03:11:22 AM: Update 544: task edges-dep-ud-ewt, batch 544 (544): mcc: 0.0932, acc: 0.0636, precision: 0.1540, recall: 0.0727, f1: 0.0988, edges-dep-ud-ewt_loss: 0.1059
10/01 03:11:32 AM: Update 611: task edges-dep-ud-ewt, batch 611 (611): mcc: 0.1155, acc: 0.0770, precision: 0.1903, recall: 0.0854, f1: 0.1179, edges-dep-ud-ewt_loss: 0.1008
10/01 03:11:43 AM: Update 669: task edges-dep-ud-ewt, batch 669 (669): mcc: 0.1398, acc: 0.0916, precision: 0.2299, recall: 0.0994, f1: 0.1388, edges-dep-ud-ewt_loss: 0.0969
10/01 03:11:53 AM: Update 717: task edges-dep-ud-ewt, batch 717 (717): mcc: 0.1586, acc: 0.1026, precision: 0.2610, recall: 0.1102, f1: 0.1549, edges-dep-ud-ewt_loss: 0.0941
10/01 03:12:03 AM: Update 781: task edges-dep-ud-ewt, batch 781 (781): mcc: 0.1795, acc: 0.1147, precision: 0.2960, recall: 0.1220, f1: 0.1728, edges-dep-ud-ewt_loss: 0.0909
10/01 03:12:13 AM: Update 820: task edges-dep-ud-ewt, batch 820 (820): mcc: 0.1949, acc: 0.1240, precision: 0.3208, recall: 0.1312, f1: 0.1862, edges-dep-ud-ewt_loss: 0.0890
10/01 03:12:23 AM: Update 883: task edges-dep-ud-ewt, batch 883 (883): mcc: 0.2162, acc: 0.1369, precision: 0.3548, recall: 0.1441, f1: 0.2050, edges-dep-ud-ewt_loss: 0.0862
10/01 03:12:33 AM: Update 941: task edges-dep-ud-ewt, batch 941 (941): mcc: 0.2376, acc: 0.1501, precision: 0.3883, recall: 0.1574, f1: 0.2240, edges-dep-ud-ewt_loss: 0.0838
10/01 03:12:42 AM: ***** Step 1000 / Validation 1 *****
10/01 03:12:42 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:12:42 AM: Validating...
10/01 03:12:43 AM: Evaluate: task edges-dep-ud-ewt, batch 2 (63): mcc: 0.5916, acc: 0.3688, precision: 0.9560, recall: 0.3715, f1: 0.5350, edges-dep-ud-ewt_loss: 0.0413
10/01 03:12:54 AM: Evaluate: task edges-dep-ud-ewt, batch 56 (63): mcc: 0.5946, acc: 0.3755, precision: 0.9497, recall: 0.3778, f1: 0.5405, edges-dep-ud-ewt_loss: 0.0426
10/01 03:12:55 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:12:55 AM: Best result seen so far for micro.
10/01 03:12:55 AM: Best result seen so far for macro.
10/01 03:12:55 AM: Updating LR scheduler:
10/01 03:12:55 AM: 	Best result seen so far for macro_avg: 0.535
10/01 03:12:55 AM: 	# validation passes without improvement: 0
10/01 03:12:55 AM: edges-dep-ud-ewt_loss: training: 0.081465 validation: 0.042728
10/01 03:12:55 AM: macro_avg: validation: 0.535271
10/01 03:12:55 AM: micro_avg: validation: 0.000000
10/01 03:12:55 AM: edges-dep-ud-ewt_mcc: training: 0.256322 validation: 0.590588
10/01 03:12:55 AM: edges-dep-ud-ewt_acc: training: 0.161915 validation: 0.370450
10/01 03:12:55 AM: edges-dep-ud-ewt_precision: training: 0.416696 validation: 0.950041
10/01 03:12:55 AM: edges-dep-ud-ewt_recall: training: 0.169355 validation: 0.372601
10/01 03:12:55 AM: edges-dep-ud-ewt_f1: training: 0.240831 validation: 0.535271
10/01 03:12:55 AM: Global learning rate: 0.0001
10/01 03:12:55 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:13:04 AM: Update 1051: task edges-dep-ud-ewt, batch 51 (1051): mcc: 0.5793, acc: 0.3764, precision: 0.8841, recall: 0.3862, f1: 0.5376, edges-dep-ud-ewt_loss: 0.0448
10/01 03:13:14 AM: Update 1100: task edges-dep-ud-ewt, batch 100 (1100): mcc: 0.5848, acc: 0.3826, precision: 0.8825, recall: 0.3942, f1: 0.5450, edges-dep-ud-ewt_loss: 0.0444
10/01 03:13:24 AM: Update 1163: task edges-dep-ud-ewt, batch 163 (1163): mcc: 0.5826, acc: 0.3815, precision: 0.8783, recall: 0.3932, f1: 0.5432, edges-dep-ud-ewt_loss: 0.0448
10/01 03:13:34 AM: Update 1202: task edges-dep-ud-ewt, batch 202 (1202): mcc: 0.5856, acc: 0.3859, precision: 0.8772, recall: 0.3978, f1: 0.5473, edges-dep-ud-ewt_loss: 0.0445
10/01 03:13:44 AM: Update 1262: task edges-dep-ud-ewt, batch 262 (1262): mcc: 0.5911, acc: 0.3934, precision: 0.8750, recall: 0.4061, f1: 0.5548, edges-dep-ud-ewt_loss: 0.0439
10/01 03:13:55 AM: Update 1326: task edges-dep-ud-ewt, batch 326 (1326): mcc: 0.5976, acc: 0.4021, precision: 0.8741, recall: 0.4155, f1: 0.5633, edges-dep-ud-ewt_loss: 0.0429
10/01 03:14:05 AM: Update 1381: task edges-dep-ud-ewt, batch 381 (1381): mcc: 0.6026, acc: 0.4087, precision: 0.8729, recall: 0.4230, f1: 0.5699, edges-dep-ud-ewt_loss: 0.0425
10/01 03:14:15 AM: Update 1433: task edges-dep-ud-ewt, batch 433 (1433): mcc: 0.6071, acc: 0.4145, precision: 0.8726, recall: 0.4294, f1: 0.5756, edges-dep-ud-ewt_loss: 0.0421
10/01 03:14:27 AM: Update 1490: task edges-dep-ud-ewt, batch 490 (1490): mcc: 0.6109, acc: 0.4200, precision: 0.8715, recall: 0.4353, f1: 0.5806, edges-dep-ud-ewt_loss: 0.0415
10/01 03:14:37 AM: Update 1550: task edges-dep-ud-ewt, batch 550 (1550): mcc: 0.6127, acc: 0.4228, precision: 0.8699, recall: 0.4387, f1: 0.5833, edges-dep-ud-ewt_loss: 0.0414
10/01 03:14:47 AM: Update 1586: task edges-dep-ud-ewt, batch 586 (1586): mcc: 0.6140, acc: 0.4249, precision: 0.8683, recall: 0.4414, f1: 0.5852, edges-dep-ud-ewt_loss: 0.0413
10/01 03:14:57 AM: Update 1637: task edges-dep-ud-ewt, batch 637 (1637): mcc: 0.6174, acc: 0.4296, precision: 0.8678, recall: 0.4465, f1: 0.5896, edges-dep-ud-ewt_loss: 0.0411
10/01 03:15:07 AM: Update 1688: task edges-dep-ud-ewt, batch 688 (1688): mcc: 0.6206, acc: 0.4341, precision: 0.8673, recall: 0.4514, f1: 0.5938, edges-dep-ud-ewt_loss: 0.0408
10/01 03:15:18 AM: Update 1751: task edges-dep-ud-ewt, batch 751 (1751): mcc: 0.6237, acc: 0.4383, precision: 0.8670, recall: 0.4561, f1: 0.5977, edges-dep-ud-ewt_loss: 0.0403
10/01 03:15:28 AM: Update 1814: task edges-dep-ud-ewt, batch 814 (1814): mcc: 0.6268, acc: 0.4426, precision: 0.8665, recall: 0.4608, f1: 0.6016, edges-dep-ud-ewt_loss: 0.0399
10/01 03:15:38 AM: Update 1877: task edges-dep-ud-ewt, batch 877 (1877): mcc: 0.6298, acc: 0.4467, precision: 0.8660, recall: 0.4655, f1: 0.6055, edges-dep-ud-ewt_loss: 0.0395
10/01 03:15:48 AM: Update 1924: task edges-dep-ud-ewt, batch 924 (1924): mcc: 0.6313, acc: 0.4489, precision: 0.8653, recall: 0.4680, f1: 0.6074, edges-dep-ud-ewt_loss: 0.0394
10/01 03:15:58 AM: Update 1961: task edges-dep-ud-ewt, batch 961 (1961): mcc: 0.6323, acc: 0.4505, precision: 0.8645, recall: 0.4700, f1: 0.6089, edges-dep-ud-ewt_loss: 0.0393
10/01 03:16:04 AM: ***** Step 2000 / Validation 2 *****
10/01 03:16:04 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:16:04 AM: Validating...
10/01 03:16:08 AM: Evaluate: task edges-dep-ud-ewt, batch 18 (63): mcc: 0.7138, acc: 0.5486, precision: 0.9227, recall: 0.5586, f1: 0.6959, edges-dep-ud-ewt_loss: 0.0304
10/01 03:16:16 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:16:16 AM: Best result seen so far for macro.
10/01 03:16:16 AM: Updating LR scheduler:
10/01 03:16:16 AM: 	Best result seen so far for macro_avg: 0.686
10/01 03:16:16 AM: 	# validation passes without improvement: 0
10/01 03:16:16 AM: edges-dep-ud-ewt_loss: training: 0.039080 validation: 0.031180
10/01 03:16:16 AM: macro_avg: validation: 0.685766
10/01 03:16:16 AM: micro_avg: validation: 0.000000
10/01 03:16:16 AM: edges-dep-ud-ewt_mcc: training: 0.633979 validation: 0.705500
10/01 03:16:16 AM: edges-dep-ud-ewt_acc: training: 0.452736 validation: 0.536400
10/01 03:16:16 AM: edges-dep-ud-ewt_precision: training: 0.864206 validation: 0.923762
10/01 03:16:16 AM: edges-dep-ud-ewt_recall: training: 0.472565 validation: 0.545281
10/01 03:16:16 AM: edges-dep-ud-ewt_f1: training: 0.611015 validation: 0.685766
10/01 03:16:16 AM: Global learning rate: 0.0001
10/01 03:16:16 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:16:19 AM: Update 2012: task edges-dep-ud-ewt, batch 12 (2012): mcc: 0.6601, acc: 0.4942, precision: 0.8508, recall: 0.5201, f1: 0.6455, edges-dep-ud-ewt_loss: 0.0337
10/01 03:16:29 AM: Update 2067: task edges-dep-ud-ewt, batch 67 (2067): mcc: 0.6761, acc: 0.5148, precision: 0.8536, recall: 0.5435, f1: 0.6641, edges-dep-ud-ewt_loss: 0.0347
10/01 03:16:39 AM: Update 2126: task edges-dep-ud-ewt, batch 126 (2126): mcc: 0.6768, acc: 0.5157, precision: 0.8529, recall: 0.5450, f1: 0.6650, edges-dep-ud-ewt_loss: 0.0346
10/01 03:16:49 AM: Update 2174: task edges-dep-ud-ewt, batch 174 (2174): mcc: 0.6792, acc: 0.5191, precision: 0.8533, recall: 0.5485, f1: 0.6678, edges-dep-ud-ewt_loss: 0.0340
10/01 03:16:59 AM: Update 2235: task edges-dep-ud-ewt, batch 235 (2235): mcc: 0.6806, acc: 0.5210, precision: 0.8533, recall: 0.5508, f1: 0.6695, edges-dep-ud-ewt_loss: 0.0338
10/01 03:17:09 AM: Update 2290: task edges-dep-ud-ewt, batch 290 (2290): mcc: 0.6803, acc: 0.5207, precision: 0.8523, recall: 0.5510, f1: 0.6693, edges-dep-ud-ewt_loss: 0.0339
10/01 03:17:19 AM: Update 2351: task edges-dep-ud-ewt, batch 351 (2351): mcc: 0.6789, acc: 0.5188, precision: 0.8512, recall: 0.5495, f1: 0.6679, edges-dep-ud-ewt_loss: 0.0343
10/01 03:17:29 AM: Update 2384: task edges-dep-ud-ewt, batch 384 (2384): mcc: 0.6802, acc: 0.5205, precision: 0.8515, recall: 0.5513, f1: 0.6693, edges-dep-ud-ewt_loss: 0.0342
10/01 03:17:40 AM: Update 2439: task edges-dep-ud-ewt, batch 439 (2439): mcc: 0.6815, acc: 0.5223, precision: 0.8516, recall: 0.5534, f1: 0.6708, edges-dep-ud-ewt_loss: 0.0340
10/01 03:17:50 AM: Update 2496: task edges-dep-ud-ewt, batch 496 (2496): mcc: 0.6830, acc: 0.5244, precision: 0.8517, recall: 0.5558, f1: 0.6726, edges-dep-ud-ewt_loss: 0.0338
10/01 03:18:00 AM: Update 2555: task edges-dep-ud-ewt, batch 555 (2555): mcc: 0.6842, acc: 0.5261, precision: 0.8517, recall: 0.5576, f1: 0.6740, edges-dep-ud-ewt_loss: 0.0335
10/01 03:18:10 AM: Update 2615: task edges-dep-ud-ewt, batch 615 (2615): mcc: 0.6848, acc: 0.5271, precision: 0.8514, recall: 0.5588, f1: 0.6748, edges-dep-ud-ewt_loss: 0.0333
10/01 03:18:21 AM: Update 2666: task edges-dep-ud-ewt, batch 666 (2666): mcc: 0.6858, acc: 0.5286, precision: 0.8512, recall: 0.5606, f1: 0.6760, edges-dep-ud-ewt_loss: 0.0332
10/01 03:18:31 AM: Update 2721: task edges-dep-ud-ewt, batch 721 (2721): mcc: 0.6856, acc: 0.5284, precision: 0.8505, recall: 0.5606, f1: 0.6758, edges-dep-ud-ewt_loss: 0.0333
10/01 03:18:41 AM: Update 2762: task edges-dep-ud-ewt, batch 762 (2762): mcc: 0.6855, acc: 0.5285, precision: 0.8497, recall: 0.5610, f1: 0.6758, edges-dep-ud-ewt_loss: 0.0333
10/01 03:18:51 AM: Update 2818: task edges-dep-ud-ewt, batch 818 (2818): mcc: 0.6864, acc: 0.5297, precision: 0.8497, recall: 0.5626, f1: 0.6769, edges-dep-ud-ewt_loss: 0.0332
10/01 03:19:01 AM: Update 2871: task edges-dep-ud-ewt, batch 871 (2871): mcc: 0.6875, acc: 0.5313, precision: 0.8494, recall: 0.5645, f1: 0.6782, edges-dep-ud-ewt_loss: 0.0331
10/01 03:19:11 AM: Update 2932: task edges-dep-ud-ewt, batch 932 (2932): mcc: 0.6883, acc: 0.5327, precision: 0.8490, recall: 0.5661, f1: 0.6793, edges-dep-ud-ewt_loss: 0.0329
10/01 03:19:21 AM: Update 2985: task edges-dep-ud-ewt, batch 985 (2985): mcc: 0.6894, acc: 0.5341, precision: 0.8489, recall: 0.5678, f1: 0.6805, edges-dep-ud-ewt_loss: 0.0329
10/01 03:19:23 AM: ***** Step 3000 / Validation 3 *****
10/01 03:19:23 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:19:23 AM: Validating...
10/01 03:19:31 AM: Evaluate: task edges-dep-ud-ewt, batch 38 (63): mcc: 0.7570, acc: 0.6145, precision: 0.9169, recall: 0.6315, f1: 0.7479, edges-dep-ud-ewt_loss: 0.0263
10/01 03:19:35 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:19:35 AM: Best result seen so far for macro.
10/01 03:19:35 AM: Updating LR scheduler:
10/01 03:19:35 AM: 	Best result seen so far for macro_avg: 0.731
10/01 03:19:35 AM: 	# validation passes without improvement: 0
10/01 03:19:35 AM: edges-dep-ud-ewt_loss: training: 0.032849 validation: 0.027744
10/01 03:19:35 AM: macro_avg: validation: 0.731209
10/01 03:19:35 AM: micro_avg: validation: 0.000000
10/01 03:19:35 AM: edges-dep-ud-ewt_mcc: training: 0.689501 validation: 0.742929
10/01 03:19:35 AM: edges-dep-ud-ewt_acc: training: 0.534378 validation: 0.593031
10/01 03:19:35 AM: edges-dep-ud-ewt_precision: training: 0.848866 validation: 0.918670
10/01 03:19:35 AM: edges-dep-ud-ewt_recall: training: 0.568121 validation: 0.607288
10/01 03:19:35 AM: edges-dep-ud-ewt_f1: training: 0.680682 validation: 0.731209
10/01 03:19:35 AM: Global learning rate: 0.0001
10/01 03:19:35 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:19:41 AM: Update 3038: task edges-dep-ud-ewt, batch 38 (3038): mcc: 0.7092, acc: 0.5633, precision: 0.8430, recall: 0.6048, f1: 0.7043, edges-dep-ud-ewt_loss: 0.0283
10/01 03:19:51 AM: Update 3084: task edges-dep-ud-ewt, batch 84 (3084): mcc: 0.6995, acc: 0.5493, precision: 0.8423, recall: 0.5892, f1: 0.6934, edges-dep-ud-ewt_loss: 0.0308
10/01 03:20:04 AM: Update 3137: task edges-dep-ud-ewt, batch 137 (3137): mcc: 0.6952, acc: 0.5442, precision: 0.8401, recall: 0.5835, f1: 0.6887, edges-dep-ud-ewt_loss: 0.0320
10/01 03:20:14 AM: Update 3193: task edges-dep-ud-ewt, batch 193 (3193): mcc: 0.6980, acc: 0.5481, precision: 0.8411, recall: 0.5875, f1: 0.6918, edges-dep-ud-ewt_loss: 0.0318
10/01 03:20:24 AM: Update 3246: task edges-dep-ud-ewt, batch 246 (3246): mcc: 0.7000, acc: 0.5501, precision: 0.8427, recall: 0.5897, f1: 0.6939, edges-dep-ud-ewt_loss: 0.0316
10/01 03:20:34 AM: Update 3307: task edges-dep-ud-ewt, batch 307 (3307): mcc: 0.7024, acc: 0.5530, precision: 0.8432, recall: 0.5932, f1: 0.6965, edges-dep-ud-ewt_loss: 0.0312
10/01 03:20:44 AM: Update 3369: task edges-dep-ud-ewt, batch 369 (3369): mcc: 0.7036, acc: 0.5545, precision: 0.8439, recall: 0.5948, f1: 0.6978, edges-dep-ud-ewt_loss: 0.0309
10/01 03:20:54 AM: Update 3430: task edges-dep-ud-ewt, batch 430 (3430): mcc: 0.7042, acc: 0.5557, precision: 0.8435, recall: 0.5961, f1: 0.6986, edges-dep-ud-ewt_loss: 0.0307
10/01 03:21:04 AM: Update 3484: task edges-dep-ud-ewt, batch 484 (3484): mcc: 0.7038, acc: 0.5553, precision: 0.8428, recall: 0.5959, f1: 0.6981, edges-dep-ud-ewt_loss: 0.0309
10/01 03:21:16 AM: Update 3529: task edges-dep-ud-ewt, batch 529 (3529): mcc: 0.7026, acc: 0.5539, precision: 0.8419, recall: 0.5947, f1: 0.6970, edges-dep-ud-ewt_loss: 0.0311
10/01 03:21:26 AM: Update 3590: task edges-dep-ud-ewt, batch 590 (3590): mcc: 0.7035, acc: 0.5551, precision: 0.8420, recall: 0.5960, f1: 0.6980, edges-dep-ud-ewt_loss: 0.0309
10/01 03:21:36 AM: Update 3646: task edges-dep-ud-ewt, batch 646 (3646): mcc: 0.7045, acc: 0.5563, precision: 0.8423, recall: 0.5974, f1: 0.6990, edges-dep-ud-ewt_loss: 0.0308
10/01 03:21:46 AM: Update 3704: task edges-dep-ud-ewt, batch 704 (3704): mcc: 0.7058, acc: 0.5582, precision: 0.8429, recall: 0.5992, f1: 0.7005, edges-dep-ud-ewt_loss: 0.0307
10/01 03:21:56 AM: Update 3770: task edges-dep-ud-ewt, batch 770 (3770): mcc: 0.7068, acc: 0.5596, precision: 0.8432, recall: 0.6007, f1: 0.7016, edges-dep-ud-ewt_loss: 0.0305
10/01 03:22:07 AM: Update 3829: task edges-dep-ud-ewt, batch 829 (3829): mcc: 0.7076, acc: 0.5607, precision: 0.8431, recall: 0.6021, f1: 0.7025, edges-dep-ud-ewt_loss: 0.0305
10/01 03:22:17 AM: Update 3882: task edges-dep-ud-ewt, batch 882 (3882): mcc: 0.7071, acc: 0.5602, precision: 0.8425, recall: 0.6017, f1: 0.7020, edges-dep-ud-ewt_loss: 0.0306
10/01 03:22:27 AM: Update 3921: task edges-dep-ud-ewt, batch 921 (3921): mcc: 0.7065, acc: 0.5595, precision: 0.8418, recall: 0.6012, f1: 0.7015, edges-dep-ud-ewt_loss: 0.0307
10/01 03:22:37 AM: Update 3981: task edges-dep-ud-ewt, batch 981 (3981): mcc: 0.7073, acc: 0.5606, precision: 0.8420, recall: 0.6023, f1: 0.7023, edges-dep-ud-ewt_loss: 0.0306
10/01 03:22:40 AM: ***** Step 4000 / Validation 4 *****
10/01 03:22:40 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:22:40 AM: Validating...
10/01 03:22:47 AM: Evaluate: task edges-dep-ud-ewt, batch 31 (63): mcc: 0.7847, acc: 0.6615, precision: 0.9102, recall: 0.6827, f1: 0.7802, edges-dep-ud-ewt_loss: 0.0233
10/01 03:22:52 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:22:52 AM: Best result seen so far for macro.
10/01 03:22:52 AM: Updating LR scheduler:
10/01 03:22:52 AM: 	Best result seen so far for macro_avg: 0.773
10/01 03:22:52 AM: 	# validation passes without improvement: 0
10/01 03:22:52 AM: edges-dep-ud-ewt_loss: training: 0.030550 validation: 0.024076
10/01 03:22:52 AM: macro_avg: validation: 0.773171
10/01 03:22:52 AM: micro_avg: validation: 0.000000
10/01 03:22:52 AM: edges-dep-ud-ewt_mcc: training: 0.707374 validation: 0.778904
10/01 03:22:52 AM: edges-dep-ud-ewt_acc: training: 0.560726 validation: 0.651294
10/01 03:22:52 AM: edges-dep-ud-ewt_precision: training: 0.841849 validation: 0.914695
10/01 03:22:52 AM: edges-dep-ud-ewt_recall: training: 0.602606 validation: 0.669574
10/01 03:22:52 AM: edges-dep-ud-ewt_f1: training: 0.702415 validation: 0.773171
10/01 03:22:52 AM: Global learning rate: 0.0001
10/01 03:22:52 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:22:57 AM: Update 4028: task edges-dep-ud-ewt, batch 28 (4028): mcc: 0.7157, acc: 0.5720, precision: 0.8428, recall: 0.6160, f1: 0.7118, edges-dep-ud-ewt_loss: 0.0294
10/01 03:23:07 AM: Update 4088: task edges-dep-ud-ewt, batch 88 (4088): mcc: 0.7198, acc: 0.5776, precision: 0.8455, recall: 0.6209, f1: 0.7160, edges-dep-ud-ewt_loss: 0.0288
10/01 03:23:17 AM: Update 4148: task edges-dep-ud-ewt, batch 148 (4148): mcc: 0.7205, acc: 0.5792, precision: 0.8447, recall: 0.6227, f1: 0.7169, edges-dep-ud-ewt_loss: 0.0290
10/01 03:23:27 AM: Update 4209: task edges-dep-ud-ewt, batch 209 (4209): mcc: 0.7216, acc: 0.5802, precision: 0.8455, recall: 0.6239, f1: 0.7180, edges-dep-ud-ewt_loss: 0.0289
10/01 03:23:38 AM: Update 4252: task edges-dep-ud-ewt, batch 252 (4252): mcc: 0.7194, acc: 0.5773, precision: 0.8440, recall: 0.6214, f1: 0.7158, edges-dep-ud-ewt_loss: 0.0292
10/01 03:23:50 AM: Update 4313: task edges-dep-ud-ewt, batch 313 (4313): mcc: 0.7159, acc: 0.5727, precision: 0.8415, recall: 0.6172, f1: 0.7121, edges-dep-ud-ewt_loss: 0.0298
10/01 03:24:00 AM: Update 4371: task edges-dep-ud-ewt, batch 371 (4371): mcc: 0.7167, acc: 0.5738, precision: 0.8417, recall: 0.6185, f1: 0.7130, edges-dep-ud-ewt_loss: 0.0296
10/01 03:24:10 AM: Update 4434: task edges-dep-ud-ewt, batch 434 (4434): mcc: 0.7177, acc: 0.5752, precision: 0.8419, recall: 0.6201, f1: 0.7141, edges-dep-ud-ewt_loss: 0.0294
10/01 03:24:21 AM: Update 4496: task edges-dep-ud-ewt, batch 496 (4496): mcc: 0.7185, acc: 0.5762, precision: 0.8421, recall: 0.6211, f1: 0.7149, edges-dep-ud-ewt_loss: 0.0293
10/01 03:24:31 AM: Update 4559: task edges-dep-ud-ewt, batch 559 (4559): mcc: 0.7194, acc: 0.5776, precision: 0.8421, recall: 0.6227, f1: 0.7160, edges-dep-ud-ewt_loss: 0.0291
10/01 03:24:41 AM: Update 4610: task edges-dep-ud-ewt, batch 610 (4610): mcc: 0.7201, acc: 0.5786, precision: 0.8424, recall: 0.6237, f1: 0.7167, edges-dep-ud-ewt_loss: 0.0291
10/01 03:24:51 AM: Update 4658: task edges-dep-ud-ewt, batch 658 (4658): mcc: 0.7195, acc: 0.5777, precision: 0.8422, recall: 0.6229, f1: 0.7161, edges-dep-ud-ewt_loss: 0.0292
10/01 03:25:02 AM: Update 4705: task edges-dep-ud-ewt, batch 705 (4705): mcc: 0.7187, acc: 0.5766, precision: 0.8414, recall: 0.6220, f1: 0.7153, edges-dep-ud-ewt_loss: 0.0294
10/01 03:25:12 AM: Update 4758: task edges-dep-ud-ewt, batch 758 (4758): mcc: 0.7190, acc: 0.5771, precision: 0.8414, recall: 0.6226, f1: 0.7156, edges-dep-ud-ewt_loss: 0.0293
10/01 03:25:22 AM: Update 4816: task edges-dep-ud-ewt, batch 816 (4816): mcc: 0.7193, acc: 0.5775, precision: 0.8409, recall: 0.6234, f1: 0.7160, edges-dep-ud-ewt_loss: 0.0292
10/01 03:25:32 AM: Update 4884: task edges-dep-ud-ewt, batch 884 (4884): mcc: 0.7199, acc: 0.5784, precision: 0.8410, recall: 0.6244, f1: 0.7167, edges-dep-ud-ewt_loss: 0.0290
10/01 03:25:42 AM: Update 4943: task edges-dep-ud-ewt, batch 943 (4943): mcc: 0.7204, acc: 0.5792, precision: 0.8411, recall: 0.6252, f1: 0.7173, edges-dep-ud-ewt_loss: 0.0290
10/01 03:25:52 AM: ***** Step 5000 / Validation 5 *****
10/01 03:25:52 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:25:52 AM: Validating...
10/01 03:25:52 AM: Evaluate: task edges-dep-ud-ewt, batch 1 (63): mcc: 0.8042, acc: 0.6864, precision: 0.9168, recall: 0.7115, f1: 0.8012, edges-dep-ud-ewt_loss: 0.0220
10/01 03:26:02 AM: Evaluate: task edges-dep-ud-ewt, batch 53 (63): mcc: 0.7892, acc: 0.6669, precision: 0.9161, recall: 0.6860, f1: 0.7845, edges-dep-ud-ewt_loss: 0.0229
10/01 03:26:04 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:26:04 AM: Best result seen so far for macro.
10/01 03:26:04 AM: Updating LR scheduler:
10/01 03:26:04 AM: 	Best result seen so far for macro_avg: 0.783
10/01 03:26:04 AM: 	# validation passes without improvement: 0
10/01 03:26:04 AM: edges-dep-ud-ewt_loss: training: 0.028932 validation: 0.022982
10/01 03:26:04 AM: macro_avg: validation: 0.783057
10/01 03:26:04 AM: micro_avg: validation: 0.000000
10/01 03:26:04 AM: edges-dep-ud-ewt_mcc: training: 0.720955 validation: 0.787969
10/01 03:26:04 AM: edges-dep-ud-ewt_acc: training: 0.579960 validation: 0.664476
10/01 03:26:04 AM: edges-dep-ud-ewt_precision: training: 0.841322 validation: 0.917041
10/01 03:26:04 AM: edges-dep-ud-ewt_recall: training: 0.626012 validation: 0.683234
10/01 03:26:04 AM: edges-dep-ud-ewt_f1: training: 0.717870 validation: 0.783057
10/01 03:26:04 AM: Global learning rate: 0.0001
10/01 03:26:04 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:26:12 AM: Update 5038: task edges-dep-ud-ewt, batch 38 (5038): mcc: 0.7192, acc: 0.5777, precision: 0.8372, recall: 0.6261, f1: 0.7164, edges-dep-ud-ewt_loss: 0.0308
10/01 03:26:25 AM: Update 5097: task edges-dep-ud-ewt, batch 97 (5097): mcc: 0.7136, acc: 0.5707, precision: 0.8355, recall: 0.6179, f1: 0.7104, edges-dep-ud-ewt_loss: 0.0312
10/01 03:26:35 AM: Update 5165: task edges-dep-ud-ewt, batch 165 (5165): mcc: 0.7199, acc: 0.5794, precision: 0.8386, recall: 0.6262, f1: 0.7170, edges-dep-ud-ewt_loss: 0.0296
10/01 03:26:46 AM: Update 5220: task edges-dep-ud-ewt, batch 220 (5220): mcc: 0.7226, acc: 0.5822, precision: 0.8404, recall: 0.6295, f1: 0.7198, edges-dep-ud-ewt_loss: 0.0292
10/01 03:26:56 AM: Update 5283: task edges-dep-ud-ewt, batch 283 (5283): mcc: 0.7241, acc: 0.5841, precision: 0.8408, recall: 0.6319, f1: 0.7215, edges-dep-ud-ewt_loss: 0.0291
10/01 03:27:06 AM: Update 5344: task edges-dep-ud-ewt, batch 344 (5344): mcc: 0.7252, acc: 0.5855, precision: 0.8412, recall: 0.6333, f1: 0.7226, edges-dep-ud-ewt_loss: 0.0289
10/01 03:27:16 AM: Update 5398: task edges-dep-ud-ewt, batch 398 (5398): mcc: 0.7260, acc: 0.5866, precision: 0.8415, recall: 0.6345, f1: 0.7235, edges-dep-ud-ewt_loss: 0.0285
10/01 03:27:27 AM: Update 5441: task edges-dep-ud-ewt, batch 441 (5441): mcc: 0.7248, acc: 0.5851, precision: 0.8406, recall: 0.6331, f1: 0.7223, edges-dep-ud-ewt_loss: 0.0288
10/01 03:27:37 AM: Update 5489: task edges-dep-ud-ewt, batch 489 (5489): mcc: 0.7236, acc: 0.5835, precision: 0.8396, recall: 0.6318, f1: 0.7210, edges-dep-ud-ewt_loss: 0.0290
10/01 03:27:47 AM: Update 5541: task edges-dep-ud-ewt, batch 541 (5541): mcc: 0.7241, acc: 0.5845, precision: 0.8393, recall: 0.6329, f1: 0.7217, edges-dep-ud-ewt_loss: 0.0288
10/01 03:27:58 AM: Update 5599: task edges-dep-ud-ewt, batch 599 (5599): mcc: 0.7247, acc: 0.5854, precision: 0.8396, recall: 0.6338, f1: 0.7223, edges-dep-ud-ewt_loss: 0.0287
10/01 03:28:08 AM: Update 5657: task edges-dep-ud-ewt, batch 657 (5657): mcc: 0.7256, acc: 0.5865, precision: 0.8401, recall: 0.6349, f1: 0.7232, edges-dep-ud-ewt_loss: 0.0286
10/01 03:28:18 AM: Update 5718: task edges-dep-ud-ewt, batch 718 (5718): mcc: 0.7261, acc: 0.5871, precision: 0.8404, recall: 0.6355, f1: 0.7237, edges-dep-ud-ewt_loss: 0.0285
10/01 03:28:28 AM: Update 5783: task edges-dep-ud-ewt, batch 783 (5783): mcc: 0.7270, acc: 0.5883, precision: 0.8409, recall: 0.6368, f1: 0.7248, edges-dep-ud-ewt_loss: 0.0284
10/01 03:28:38 AM: Update 5841: task edges-dep-ud-ewt, batch 841 (5841): mcc: 0.7265, acc: 0.5877, precision: 0.8404, recall: 0.6362, f1: 0.7242, edges-dep-ud-ewt_loss: 0.0285
10/01 03:28:48 AM: Update 5881: task edges-dep-ud-ewt, batch 881 (5881): mcc: 0.7259, acc: 0.5870, precision: 0.8399, recall: 0.6356, f1: 0.7236, edges-dep-ud-ewt_loss: 0.0285
10/01 03:28:58 AM: Update 5937: task edges-dep-ud-ewt, batch 937 (5937): mcc: 0.7263, acc: 0.5877, precision: 0.8398, recall: 0.6363, f1: 0.7241, edges-dep-ud-ewt_loss: 0.0284
10/01 03:29:08 AM: Update 5999: task edges-dep-ud-ewt, batch 999 (5999): mcc: 0.7269, acc: 0.5885, precision: 0.8400, recall: 0.6372, f1: 0.7247, edges-dep-ud-ewt_loss: 0.0284
10/01 03:29:08 AM: ***** Step 6000 / Validation 6 *****
10/01 03:29:08 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:29:08 AM: Validating...
10/01 03:29:18 AM: Evaluate: task edges-dep-ud-ewt, batch 44 (63): mcc: 0.7923, acc: 0.6686, precision: 0.9252, recall: 0.6845, f1: 0.7868, edges-dep-ud-ewt_loss: 0.0219
10/01 03:29:21 AM: Updating LR scheduler:
10/01 03:29:21 AM: 	Best result seen so far for macro_avg: 0.783
10/01 03:29:21 AM: 	# validation passes without improvement: 1
10/01 03:29:21 AM: edges-dep-ud-ewt_loss: training: 0.028373 validation: 0.022592
10/01 03:29:21 AM: macro_avg: validation: 0.779562
10/01 03:29:21 AM: micro_avg: validation: 0.000000
10/01 03:29:21 AM: edges-dep-ud-ewt_mcc: training: 0.726898 validation: 0.786212
10/01 03:29:21 AM: edges-dep-ud-ewt_acc: training: 0.588471 validation: 0.657746
10/01 03:29:21 AM: edges-dep-ud-ewt_precision: training: 0.840060 validation: 0.928112
10/01 03:29:21 AM: edges-dep-ud-ewt_recall: training: 0.637190 validation: 0.672003
10/01 03:29:21 AM: edges-dep-ud-ewt_f1: training: 0.724695 validation: 0.779562
10/01 03:29:21 AM: Global learning rate: 0.0001
10/01 03:29:21 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:29:28 AM: Update 6051: task edges-dep-ud-ewt, batch 51 (6051): mcc: 0.7380, acc: 0.6054, precision: 0.8432, recall: 0.6540, f1: 0.7366, edges-dep-ud-ewt_loss: 0.0262
10/01 03:29:38 AM: Update 6108: task edges-dep-ud-ewt, batch 108 (6108): mcc: 0.7372, acc: 0.6029, precision: 0.8430, recall: 0.6528, f1: 0.7358, edges-dep-ud-ewt_loss: 0.0269
10/01 03:29:48 AM: Update 6169: task edges-dep-ud-ewt, batch 169 (6169): mcc: 0.7373, acc: 0.6030, precision: 0.8436, recall: 0.6525, f1: 0.7358, edges-dep-ud-ewt_loss: 0.0268
10/01 03:29:59 AM: Update 6213: task edges-dep-ud-ewt, batch 213 (6213): mcc: 0.7348, acc: 0.6001, precision: 0.8412, recall: 0.6501, f1: 0.7334, edges-dep-ud-ewt_loss: 0.0273
10/01 03:30:12 AM: Update 6273: task edges-dep-ud-ewt, batch 273 (6273): mcc: 0.7304, acc: 0.5943, precision: 0.8394, recall: 0.6438, f1: 0.7287, edges-dep-ud-ewt_loss: 0.0281
10/01 03:30:22 AM: Update 6329: task edges-dep-ud-ewt, batch 329 (6329): mcc: 0.7314, acc: 0.5953, precision: 0.8402, recall: 0.6449, f1: 0.7297, edges-dep-ud-ewt_loss: 0.0280
10/01 03:30:32 AM: Update 6395: task edges-dep-ud-ewt, batch 395 (6395): mcc: 0.7338, acc: 0.5984, precision: 0.8419, recall: 0.6477, f1: 0.7322, edges-dep-ud-ewt_loss: 0.0276
10/01 03:30:42 AM: Update 6451: task edges-dep-ud-ewt, batch 451 (6451): mcc: 0.7340, acc: 0.5988, precision: 0.8416, recall: 0.6483, f1: 0.7324, edges-dep-ud-ewt_loss: 0.0276
10/01 03:30:52 AM: Update 6506: task edges-dep-ud-ewt, batch 506 (6506): mcc: 0.7345, acc: 0.5995, precision: 0.8422, recall: 0.6488, f1: 0.7329, edges-dep-ud-ewt_loss: 0.0275
10/01 03:31:02 AM: Update 6569: task edges-dep-ud-ewt, batch 569 (6569): mcc: 0.7349, acc: 0.5999, precision: 0.8422, recall: 0.6494, f1: 0.7334, edges-dep-ud-ewt_loss: 0.0274
10/01 03:31:12 AM: Update 6624: task edges-dep-ud-ewt, batch 624 (6624): mcc: 0.7341, acc: 0.5989, precision: 0.8416, recall: 0.6484, f1: 0.7325, edges-dep-ud-ewt_loss: 0.0275
10/01 03:31:23 AM: Update 6666: task edges-dep-ud-ewt, batch 666 (6666): mcc: 0.7331, acc: 0.5976, precision: 0.8411, recall: 0.6471, f1: 0.7314, edges-dep-ud-ewt_loss: 0.0277
10/01 03:31:33 AM: Update 6724: task edges-dep-ud-ewt, batch 724 (6724): mcc: 0.7336, acc: 0.5984, precision: 0.8413, recall: 0.6479, f1: 0.7320, edges-dep-ud-ewt_loss: 0.0277
10/01 03:31:43 AM: Update 6794: task edges-dep-ud-ewt, batch 794 (6794): mcc: 0.7345, acc: 0.5994, precision: 0.8414, recall: 0.6493, f1: 0.7330, edges-dep-ud-ewt_loss: 0.0275
10/01 03:31:53 AM: Update 6855: task edges-dep-ud-ewt, batch 855 (6855): mcc: 0.7349, acc: 0.5998, precision: 0.8418, recall: 0.6496, f1: 0.7333, edges-dep-ud-ewt_loss: 0.0274
10/01 03:32:03 AM: Update 6916: task edges-dep-ud-ewt, batch 916 (6916): mcc: 0.7352, acc: 0.6003, precision: 0.8415, recall: 0.6504, f1: 0.7337, edges-dep-ud-ewt_loss: 0.0274
10/01 03:32:13 AM: Update 6969: task edges-dep-ud-ewt, batch 969 (6969): mcc: 0.7356, acc: 0.6007, precision: 0.8419, recall: 0.6508, f1: 0.7342, edges-dep-ud-ewt_loss: 0.0273
10/01 03:32:20 AM: ***** Step 7000 / Validation 7 *****
10/01 03:32:20 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:32:20 AM: Validating...
10/01 03:32:23 AM: Evaluate: task edges-dep-ud-ewt, batch 12 (63): mcc: 0.8005, acc: 0.6840, precision: 0.9158, recall: 0.7058, f1: 0.7972, edges-dep-ud-ewt_loss: 0.0210
10/01 03:32:32 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:32:32 AM: Best result seen so far for macro.
10/01 03:32:32 AM: Updating LR scheduler:
10/01 03:32:32 AM: 	Best result seen so far for macro_avg: 0.794
10/01 03:32:32 AM: 	# validation passes without improvement: 0
10/01 03:32:32 AM: edges-dep-ud-ewt_loss: training: 0.027347 validation: 0.021583
10/01 03:32:32 AM: macro_avg: validation: 0.794149
10/01 03:32:32 AM: micro_avg: validation: 0.000000
10/01 03:32:32 AM: edges-dep-ud-ewt_mcc: training: 0.735113 validation: 0.798488
10/01 03:32:32 AM: edges-dep-ud-ewt_acc: training: 0.600086 validation: 0.679490
10/01 03:32:32 AM: edges-dep-ud-ewt_precision: training: 0.841570 validation: 0.922254
10/01 03:32:32 AM: edges-dep-ud-ewt_recall: training: 0.650254 validation: 0.697292
10/01 03:32:32 AM: edges-dep-ud-ewt_f1: training: 0.733645 validation: 0.794149
10/01 03:32:32 AM: Global learning rate: 0.0001
10/01 03:32:32 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:32:33 AM: Update 7006: task edges-dep-ud-ewt, batch 6 (7006): mcc: 0.7151, acc: 0.5778, precision: 0.8361, recall: 0.6199, f1: 0.7119, edges-dep-ud-ewt_loss: 0.0311
10/01 03:32:46 AM: Update 7057: task edges-dep-ud-ewt, batch 57 (7057): mcc: 0.7199, acc: 0.5824, precision: 0.8326, recall: 0.6308, f1: 0.7178, edges-dep-ud-ewt_loss: 0.0300
10/01 03:32:56 AM: Update 7122: task edges-dep-ud-ewt, batch 122 (7122): mcc: 0.7302, acc: 0.5952, precision: 0.8390, recall: 0.6436, f1: 0.7285, edges-dep-ud-ewt_loss: 0.0281
10/01 03:33:06 AM: Update 7183: task edges-dep-ud-ewt, batch 183 (7183): mcc: 0.7353, acc: 0.6007, precision: 0.8422, recall: 0.6501, f1: 0.7338, edges-dep-ud-ewt_loss: 0.0271
10/01 03:33:16 AM: Update 7237: task edges-dep-ud-ewt, batch 237 (7237): mcc: 0.7377, acc: 0.6037, precision: 0.8431, recall: 0.6536, f1: 0.7364, edges-dep-ud-ewt_loss: 0.0271
10/01 03:33:26 AM: Update 7298: task edges-dep-ud-ewt, batch 298 (7298): mcc: 0.7392, acc: 0.6056, precision: 0.8436, recall: 0.6557, f1: 0.7379, edges-dep-ud-ewt_loss: 0.0268
10/01 03:33:36 AM: Update 7357: task edges-dep-ud-ewt, batch 357 (7357): mcc: 0.7407, acc: 0.6077, precision: 0.8443, recall: 0.6579, f1: 0.7395, edges-dep-ud-ewt_loss: 0.0266
10/01 03:33:46 AM: Update 7411: task edges-dep-ud-ewt, batch 411 (7411): mcc: 0.7388, acc: 0.6056, precision: 0.8430, recall: 0.6556, f1: 0.7376, edges-dep-ud-ewt_loss: 0.0269
10/01 03:33:57 AM: Update 7449: task edges-dep-ud-ewt, batch 449 (7449): mcc: 0.7374, acc: 0.6040, precision: 0.8420, recall: 0.6540, f1: 0.7362, edges-dep-ud-ewt_loss: 0.0272
10/01 03:34:07 AM: Update 7509: task edges-dep-ud-ewt, batch 509 (7509): mcc: 0.7382, acc: 0.6048, precision: 0.8425, recall: 0.6548, f1: 0.7369, edges-dep-ud-ewt_loss: 0.0270
10/01 03:34:17 AM: Update 7576: task edges-dep-ud-ewt, batch 576 (7576): mcc: 0.7395, acc: 0.6062, precision: 0.8433, recall: 0.6565, f1: 0.7383, edges-dep-ud-ewt_loss: 0.0267
10/01 03:34:27 AM: Update 7634: task edges-dep-ud-ewt, batch 634 (7634): mcc: 0.7398, acc: 0.6068, precision: 0.8433, recall: 0.6570, f1: 0.7386, edges-dep-ud-ewt_loss: 0.0267
10/01 03:34:37 AM: Update 7693: task edges-dep-ud-ewt, batch 693 (7693): mcc: 0.7403, acc: 0.6076, precision: 0.8436, recall: 0.6578, f1: 0.7392, edges-dep-ud-ewt_loss: 0.0267
10/01 03:34:47 AM: Update 7748: task edges-dep-ud-ewt, batch 748 (7748): mcc: 0.7407, acc: 0.6082, precision: 0.8437, recall: 0.6584, f1: 0.7396, edges-dep-ud-ewt_loss: 0.0267
10/01 03:34:57 AM: Update 7802: task edges-dep-ud-ewt, batch 802 (7802): mcc: 0.7400, acc: 0.6074, precision: 0.8431, recall: 0.6576, f1: 0.7389, edges-dep-ud-ewt_loss: 0.0268
10/01 03:35:08 AM: Update 7841: task edges-dep-ud-ewt, batch 841 (7841): mcc: 0.7392, acc: 0.6064, precision: 0.8427, recall: 0.6566, f1: 0.7381, edges-dep-ud-ewt_loss: 0.0269
10/01 03:35:18 AM: Update 7901: task edges-dep-ud-ewt, batch 901 (7901): mcc: 0.7397, acc: 0.6069, precision: 0.8429, recall: 0.6573, f1: 0.7386, edges-dep-ud-ewt_loss: 0.0268
10/01 03:35:28 AM: Update 7959: task edges-dep-ud-ewt, batch 959 (7959): mcc: 0.7402, acc: 0.6077, precision: 0.8431, recall: 0.6580, f1: 0.7391, edges-dep-ud-ewt_loss: 0.0267
10/01 03:35:33 AM: ***** Step 8000 / Validation 8 *****
10/01 03:35:33 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:35:33 AM: Validating...
10/01 03:35:38 AM: Evaluate: task edges-dep-ud-ewt, batch 22 (63): mcc: 0.8119, acc: 0.7029, precision: 0.9195, recall: 0.7228, f1: 0.8094, edges-dep-ud-ewt_loss: 0.0199
10/01 03:35:45 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:35:45 AM: Best result seen so far for macro.
10/01 03:35:45 AM: Updating LR scheduler:
10/01 03:35:45 AM: 	Best result seen so far for macro_avg: 0.802
10/01 03:35:45 AM: 	# validation passes without improvement: 0
10/01 03:35:45 AM: edges-dep-ud-ewt_loss: training: 0.026681 validation: 0.020897
10/01 03:35:45 AM: macro_avg: validation: 0.801794
10/01 03:35:45 AM: micro_avg: validation: 0.000000
10/01 03:35:45 AM: edges-dep-ud-ewt_mcc: training: 0.740443 validation: 0.805529
10/01 03:35:45 AM: edges-dep-ud-ewt_acc: training: 0.607950 validation: 0.690681
10/01 03:35:45 AM: edges-dep-ud-ewt_precision: training: 0.843050 validation: 0.923752
10/01 03:35:45 AM: edges-dep-ud-ewt_recall: training: 0.658389 validation: 0.708284
10/01 03:35:45 AM: edges-dep-ud-ewt_f1: training: 0.739364 validation: 0.801794
10/01 03:35:45 AM: Global learning rate: 0.0001
10/01 03:35:45 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:35:48 AM: Update 8018: task edges-dep-ud-ewt, batch 18 (8018): mcc: 0.7490, acc: 0.6183, precision: 0.8447, recall: 0.6721, f1: 0.7486, edges-dep-ud-ewt_loss: 0.0252
10/01 03:35:58 AM: Update 8070: task edges-dep-ud-ewt, batch 70 (8070): mcc: 0.7489, acc: 0.6196, precision: 0.8462, recall: 0.6707, f1: 0.7483, edges-dep-ud-ewt_loss: 0.0260
10/01 03:36:08 AM: Update 8129: task edges-dep-ud-ewt, batch 129 (8129): mcc: 0.7491, acc: 0.6201, precision: 0.8470, recall: 0.6704, f1: 0.7484, edges-dep-ud-ewt_loss: 0.0258
10/01 03:36:18 AM: Update 8181: task edges-dep-ud-ewt, batch 181 (8181): mcc: 0.7454, acc: 0.6156, precision: 0.8459, recall: 0.6648, f1: 0.7445, edges-dep-ud-ewt_loss: 0.0262
10/01 03:36:30 AM: Update 8233: task edges-dep-ud-ewt, batch 233 (8233): mcc: 0.7411, acc: 0.6102, precision: 0.8432, recall: 0.6594, f1: 0.7401, edges-dep-ud-ewt_loss: 0.0269
10/01 03:36:40 AM: Update 8289: task edges-dep-ud-ewt, batch 289 (8289): mcc: 0.7420, acc: 0.6113, precision: 0.8441, recall: 0.6603, f1: 0.7410, edges-dep-ud-ewt_loss: 0.0267
10/01 03:36:51 AM: Update 8349: task edges-dep-ud-ewt, batch 349 (8349): mcc: 0.7437, acc: 0.6134, precision: 0.8449, recall: 0.6626, f1: 0.7427, edges-dep-ud-ewt_loss: 0.0265
10/01 03:37:01 AM: Update 8405: task edges-dep-ud-ewt, batch 405 (8405): mcc: 0.7447, acc: 0.6151, precision: 0.8452, recall: 0.6642, f1: 0.7438, edges-dep-ud-ewt_loss: 0.0264
10/01 03:37:11 AM: Update 8467: task edges-dep-ud-ewt, batch 467 (8467): mcc: 0.7452, acc: 0.6156, precision: 0.8450, recall: 0.6652, f1: 0.7444, edges-dep-ud-ewt_loss: 0.0262
10/01 03:37:21 AM: Update 8531: task edges-dep-ud-ewt, batch 531 (8531): mcc: 0.7461, acc: 0.6168, precision: 0.8456, recall: 0.6663, f1: 0.7454, edges-dep-ud-ewt_loss: 0.0261
10/01 03:37:31 AM: Update 8581: task edges-dep-ud-ewt, batch 581 (8581): mcc: 0.7449, acc: 0.6153, precision: 0.8451, recall: 0.6646, f1: 0.7441, edges-dep-ud-ewt_loss: 0.0262
10/01 03:37:42 AM: Update 8625: task edges-dep-ud-ewt, batch 625 (8625): mcc: 0.7437, acc: 0.6137, precision: 0.8443, recall: 0.6632, f1: 0.7429, edges-dep-ud-ewt_loss: 0.0264
10/01 03:37:52 AM: Update 8678: task edges-dep-ud-ewt, batch 678 (8678): mcc: 0.7438, acc: 0.6139, precision: 0.8443, recall: 0.6633, f1: 0.7429, edges-dep-ud-ewt_loss: 0.0264
10/01 03:38:02 AM: Update 8738: task edges-dep-ud-ewt, batch 738 (8738): mcc: 0.7447, acc: 0.6152, precision: 0.8448, recall: 0.6644, f1: 0.7438, edges-dep-ud-ewt_loss: 0.0263
10/01 03:38:12 AM: Update 8792: task edges-dep-ud-ewt, batch 792 (8792): mcc: 0.7453, acc: 0.6160, precision: 0.8451, recall: 0.6653, f1: 0.7445, edges-dep-ud-ewt_loss: 0.0263
10/01 03:38:22 AM: Update 8859: task edges-dep-ud-ewt, batch 859 (8859): mcc: 0.7460, acc: 0.6167, precision: 0.8457, recall: 0.6660, f1: 0.7452, edges-dep-ud-ewt_loss: 0.0262
10/01 03:38:33 AM: Update 8921: task edges-dep-ud-ewt, batch 921 (8921): mcc: 0.7462, acc: 0.6170, precision: 0.8456, recall: 0.6665, f1: 0.7455, edges-dep-ud-ewt_loss: 0.0260
10/01 03:38:43 AM: Update 8978: task edges-dep-ud-ewt, batch 978 (8978): mcc: 0.7459, acc: 0.6167, precision: 0.8453, recall: 0.6662, f1: 0.7451, edges-dep-ud-ewt_loss: 0.0261
10/01 03:38:47 AM: ***** Step 9000 / Validation 9 *****
10/01 03:38:47 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:38:47 AM: Validating...
10/01 03:38:53 AM: Evaluate: task edges-dep-ud-ewt, batch 30 (63): mcc: 0.8171, acc: 0.7130, precision: 0.9177, recall: 0.7333, f1: 0.8152, edges-dep-ud-ewt_loss: 0.0196
10/01 03:38:58 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:38:58 AM: Best result seen so far for macro.
10/01 03:38:58 AM: Updating LR scheduler:
10/01 03:38:58 AM: 	Best result seen so far for macro_avg: 0.811
10/01 03:38:58 AM: 	# validation passes without improvement: 0
10/01 03:38:58 AM: edges-dep-ud-ewt_loss: training: 0.026173 validation: 0.020235
10/01 03:38:58 AM: macro_avg: validation: 0.810613
10/01 03:38:58 AM: micro_avg: validation: 0.000000
10/01 03:38:58 AM: edges-dep-ud-ewt_mcc: training: 0.745537 validation: 0.813314
10/01 03:38:58 AM: edges-dep-ud-ewt_acc: training: 0.616260 validation: 0.705615
10/01 03:38:58 AM: edges-dep-ud-ewt_precision: training: 0.845209 validation: 0.921839
10/01 03:38:58 AM: edges-dep-ud-ewt_recall: training: 0.665599 validation: 0.723337
10/01 03:38:58 AM: edges-dep-ud-ewt_f1: training: 0.744728 validation: 0.810613
10/01 03:38:58 AM: Global learning rate: 0.0001
10/01 03:38:58 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:39:05 AM: Update 9017: task edges-dep-ud-ewt, batch 17 (9017): mcc: 0.7264, acc: 0.5901, precision: 0.8344, recall: 0.6407, f1: 0.7248, edges-dep-ud-ewt_loss: 0.0294
10/01 03:39:15 AM: Update 9074: task edges-dep-ud-ewt, batch 74 (9074): mcc: 0.7478, acc: 0.6191, precision: 0.8470, recall: 0.6682, f1: 0.7470, edges-dep-ud-ewt_loss: 0.0261
10/01 03:39:26 AM: Update 9132: task edges-dep-ud-ewt, batch 132 (9132): mcc: 0.7510, acc: 0.6235, precision: 0.8483, recall: 0.6727, f1: 0.7504, edges-dep-ud-ewt_loss: 0.0254
10/01 03:39:36 AM: Update 9197: task edges-dep-ud-ewt, batch 197 (9197): mcc: 0.7523, acc: 0.6252, precision: 0.8491, recall: 0.6744, f1: 0.7517, edges-dep-ud-ewt_loss: 0.0250
10/01 03:39:46 AM: Update 9259: task edges-dep-ud-ewt, batch 259 (9259): mcc: 0.7538, acc: 0.6271, precision: 0.8495, recall: 0.6766, f1: 0.7533, edges-dep-ud-ewt_loss: 0.0249
10/01 03:39:56 AM: Update 9311: task edges-dep-ud-ewt, batch 311 (9311): mcc: 0.7539, acc: 0.6273, precision: 0.8496, recall: 0.6768, f1: 0.7534, edges-dep-ud-ewt_loss: 0.0249
10/01 03:40:06 AM: Update 9363: task edges-dep-ud-ewt, batch 363 (9363): mcc: 0.7524, acc: 0.6252, precision: 0.8489, recall: 0.6746, f1: 0.7518, edges-dep-ud-ewt_loss: 0.0253
10/01 03:40:17 AM: Update 9409: task edges-dep-ud-ewt, batch 409 (9409): mcc: 0.7501, acc: 0.6222, precision: 0.8477, recall: 0.6716, f1: 0.7494, edges-dep-ud-ewt_loss: 0.0257
10/01 03:40:27 AM: Update 9464: task edges-dep-ud-ewt, batch 464 (9464): mcc: 0.7502, acc: 0.6225, precision: 0.8473, recall: 0.6721, f1: 0.7496, edges-dep-ud-ewt_loss: 0.0257
10/01 03:40:37 AM: Update 9517: task edges-dep-ud-ewt, batch 517 (9517): mcc: 0.7508, acc: 0.6232, precision: 0.8478, recall: 0.6727, f1: 0.7502, edges-dep-ud-ewt_loss: 0.0257
10/01 03:40:48 AM: Update 9582: task edges-dep-ud-ewt, batch 582 (9582): mcc: 0.7519, acc: 0.6247, precision: 0.8486, recall: 0.6741, f1: 0.7513, edges-dep-ud-ewt_loss: 0.0254
10/01 03:40:58 AM: Update 9639: task edges-dep-ud-ewt, batch 639 (9639): mcc: 0.7522, acc: 0.6251, precision: 0.8485, recall: 0.6747, f1: 0.7517, edges-dep-ud-ewt_loss: 0.0253
10/01 03:41:08 AM: Update 9703: task edges-dep-ud-ewt, batch 703 (9703): mcc: 0.7528, acc: 0.6259, precision: 0.8488, recall: 0.6755, f1: 0.7523, edges-dep-ud-ewt_loss: 0.0251
10/01 03:41:18 AM: Update 9756: task edges-dep-ud-ewt, batch 756 (9756): mcc: 0.7519, acc: 0.6249, precision: 0.8484, recall: 0.6743, f1: 0.7514, edges-dep-ud-ewt_loss: 0.0253
10/01 03:41:29 AM: Update 9801: task edges-dep-ud-ewt, batch 801 (9801): mcc: 0.7512, acc: 0.6240, precision: 0.8479, recall: 0.6735, f1: 0.7507, edges-dep-ud-ewt_loss: 0.0254
10/01 03:41:40 AM: Update 9859: task edges-dep-ud-ewt, batch 859 (9859): mcc: 0.7517, acc: 0.6247, precision: 0.8482, recall: 0.6741, f1: 0.7512, edges-dep-ud-ewt_loss: 0.0253
10/01 03:41:50 AM: Update 9922: task edges-dep-ud-ewt, batch 922 (9922): mcc: 0.7524, acc: 0.6256, precision: 0.8486, recall: 0.6749, f1: 0.7519, edges-dep-ud-ewt_loss: 0.0253
10/01 03:42:00 AM: Update 9986: task edges-dep-ud-ewt, batch 986 (9986): mcc: 0.7528, acc: 0.6261, precision: 0.8489, recall: 0.6753, f1: 0.7522, edges-dep-ud-ewt_loss: 0.0252
10/01 03:42:02 AM: ***** Step 10000 / Validation 10 *****
10/01 03:42:02 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:42:02 AM: Validating...
10/01 03:42:10 AM: Evaluate: task edges-dep-ud-ewt, batch 36 (63): mcc: 0.8297, acc: 0.7335, precision: 0.9194, recall: 0.7544, f1: 0.8287, edges-dep-ud-ewt_loss: 0.0187
10/01 03:42:14 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:42:14 AM: Best result seen so far for macro.
10/01 03:42:14 AM: Updating LR scheduler:
10/01 03:42:14 AM: 	Best result seen so far for macro_avg: 0.825
10/01 03:42:14 AM: 	# validation passes without improvement: 0
10/01 03:42:14 AM: edges-dep-ud-ewt_loss: training: 0.025181 validation: 0.019184
10/01 03:42:14 AM: macro_avg: validation: 0.825201
10/01 03:42:14 AM: micro_avg: validation: 0.000000
10/01 03:42:14 AM: edges-dep-ud-ewt_mcc: training: 0.752848 validation: 0.826813
10/01 03:42:14 AM: edges-dep-ud-ewt_acc: training: 0.626239 validation: 0.727360
10/01 03:42:14 AM: edges-dep-ud-ewt_precision: training: 0.848905 validation: 0.923342
10/01 03:42:14 AM: edges-dep-ud-ewt_recall: training: 0.675501 validation: 0.745918
10/01 03:42:14 AM: edges-dep-ud-ewt_f1: training: 0.752341 validation: 0.825201
10/01 03:42:14 AM: Global learning rate: 0.0001
10/01 03:42:14 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:42:20 AM: Update 10033: task edges-dep-ud-ewt, batch 33 (10033): mcc: 0.7581, acc: 0.6343, precision: 0.8496, recall: 0.6842, f1: 0.7580, edges-dep-ud-ewt_loss: 0.0248
10/01 03:42:30 AM: Update 10087: task edges-dep-ud-ewt, batch 87 (10087): mcc: 0.7572, acc: 0.6331, precision: 0.8503, recall: 0.6821, f1: 0.7569, edges-dep-ud-ewt_loss: 0.0248
10/01 03:42:40 AM: Update 10137: task edges-dep-ud-ewt, batch 137 (10137): mcc: 0.7551, acc: 0.6304, precision: 0.8490, recall: 0.6794, f1: 0.7548, edges-dep-ud-ewt_loss: 0.0252
10/01 03:42:52 AM: Update 10193: task edges-dep-ud-ewt, batch 193 (10193): mcc: 0.7509, acc: 0.6248, precision: 0.8474, recall: 0.6732, f1: 0.7504, edges-dep-ud-ewt_loss: 0.0260
10/01 03:43:03 AM: Update 10245: task edges-dep-ud-ewt, batch 245 (10245): mcc: 0.7529, acc: 0.6276, precision: 0.8484, recall: 0.6759, f1: 0.7524, edges-dep-ud-ewt_loss: 0.0259
10/01 03:43:13 AM: Update 10300: task edges-dep-ud-ewt, batch 300 (10300): mcc: 0.7544, acc: 0.6298, precision: 0.8492, recall: 0.6780, f1: 0.7540, edges-dep-ud-ewt_loss: 0.0254
10/01 03:43:23 AM: Update 10365: task edges-dep-ud-ewt, batch 365 (10365): mcc: 0.7561, acc: 0.6319, precision: 0.8504, recall: 0.6800, f1: 0.7557, edges-dep-ud-ewt_loss: 0.0250
10/01 03:43:33 AM: Update 10430: task edges-dep-ud-ewt, batch 430 (10430): mcc: 0.7575, acc: 0.6337, precision: 0.8510, recall: 0.6821, f1: 0.7572, edges-dep-ud-ewt_loss: 0.0248
10/01 03:43:43 AM: Update 10491: task edges-dep-ud-ewt, batch 491 (10491): mcc: 0.7580, acc: 0.6345, precision: 0.8510, recall: 0.6829, f1: 0.7577, edges-dep-ud-ewt_loss: 0.0247
10/01 03:43:53 AM: Update 10543: task edges-dep-ud-ewt, batch 543 (10543): mcc: 0.7569, acc: 0.6332, precision: 0.8504, recall: 0.6814, f1: 0.7566, edges-dep-ud-ewt_loss: 0.0249
10/01 03:44:03 AM: Update 10585: task edges-dep-ud-ewt, batch 585 (10585): mcc: 0.7557, acc: 0.6317, precision: 0.8497, recall: 0.6799, f1: 0.7554, edges-dep-ud-ewt_loss: 0.0251
10/01 03:44:13 AM: Update 10636: task edges-dep-ud-ewt, batch 636 (10636): mcc: 0.7563, acc: 0.6325, precision: 0.8504, recall: 0.6805, f1: 0.7560, edges-dep-ud-ewt_loss: 0.0250
10/01 03:44:23 AM: Update 10698: task edges-dep-ud-ewt, batch 698 (10698): mcc: 0.7572, acc: 0.6336, precision: 0.8510, recall: 0.6816, f1: 0.7569, edges-dep-ud-ewt_loss: 0.0248
10/01 03:44:34 AM: Update 10764: task edges-dep-ud-ewt, batch 764 (10764): mcc: 0.7581, acc: 0.6348, precision: 0.8515, recall: 0.6826, f1: 0.7577, edges-dep-ud-ewt_loss: 0.0247
10/01 03:44:44 AM: Update 10826: task edges-dep-ud-ewt, batch 826 (10826): mcc: 0.7586, acc: 0.6356, precision: 0.8518, recall: 0.6834, f1: 0.7584, edges-dep-ud-ewt_loss: 0.0246
10/01 03:44:54 AM: Update 10885: task edges-dep-ud-ewt, batch 885 (10885): mcc: 0.7591, acc: 0.6362, precision: 0.8521, recall: 0.6839, f1: 0.7588, edges-dep-ud-ewt_loss: 0.0246
10/01 03:45:04 AM: Update 10939: task edges-dep-ud-ewt, batch 939 (10939): mcc: 0.7586, acc: 0.6356, precision: 0.8520, recall: 0.6832, f1: 0.7583, edges-dep-ud-ewt_loss: 0.0247
10/01 03:45:14 AM: Update 10977: task edges-dep-ud-ewt, batch 977 (10977): mcc: 0.7578, acc: 0.6346, precision: 0.8513, recall: 0.6823, f1: 0.7575, edges-dep-ud-ewt_loss: 0.0249
10/01 03:45:19 AM: ***** Step 11000 / Validation 11 *****
10/01 03:45:19 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:45:19 AM: Validating...
10/01 03:45:24 AM: Evaluate: task edges-dep-ud-ewt, batch 23 (63): mcc: 0.8335, acc: 0.7368, precision: 0.9285, recall: 0.7536, f1: 0.8319, edges-dep-ud-ewt_loss: 0.0179
10/01 03:45:31 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:45:31 AM: Best result seen so far for macro.
10/01 03:45:31 AM: Updating LR scheduler:
10/01 03:45:31 AM: 	Best result seen so far for macro_avg: 0.826
10/01 03:45:31 AM: 	# validation passes without improvement: 0
10/01 03:45:31 AM: edges-dep-ud-ewt_loss: training: 0.024884 validation: 0.018866
10/01 03:45:31 AM: macro_avg: validation: 0.825970
10/01 03:45:31 AM: micro_avg: validation: 0.000000
10/01 03:45:31 AM: edges-dep-ud-ewt_mcc: training: 0.757777 validation: 0.828402
10/01 03:45:31 AM: edges-dep-ud-ewt_acc: training: 0.634573 validation: 0.726762
10/01 03:45:31 AM: edges-dep-ud-ewt_precision: training: 0.851374 validation: 0.932605
10/01 03:45:31 AM: edges-dep-ud-ewt_recall: training: 0.682214 validation: 0.741219
10/01 03:45:31 AM: edges-dep-ud-ewt_f1: training: 0.757465 validation: 0.825970
10/01 03:45:31 AM: Global learning rate: 0.0001
10/01 03:45:31 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:45:35 AM: Update 11017: task edges-dep-ud-ewt, batch 17 (11017): mcc: 0.7613, acc: 0.6403, precision: 0.8533, recall: 0.6869, f1: 0.7611, edges-dep-ud-ewt_loss: 0.0234
10/01 03:45:45 AM: Update 11086: task edges-dep-ud-ewt, batch 86 (11086): mcc: 0.7668, acc: 0.6482, precision: 0.8569, recall: 0.6937, f1: 0.7667, edges-dep-ud-ewt_loss: 0.0220
10/01 03:45:55 AM: Update 11149: task edges-dep-ud-ewt, batch 149 (11149): mcc: 0.7685, acc: 0.6505, precision: 0.8572, recall: 0.6964, f1: 0.7685, edges-dep-ud-ewt_loss: 0.0226
10/01 03:46:05 AM: Update 11211: task edges-dep-ud-ewt, batch 211 (11211): mcc: 0.7685, acc: 0.6508, precision: 0.8566, recall: 0.6971, f1: 0.7686, edges-dep-ud-ewt_loss: 0.0227
10/01 03:46:15 AM: Update 11265: task edges-dep-ud-ewt, batch 265 (11265): mcc: 0.7677, acc: 0.6490, precision: 0.8565, recall: 0.6956, f1: 0.7677, edges-dep-ud-ewt_loss: 0.0230
10/01 03:46:25 AM: Update 11321: task edges-dep-ud-ewt, batch 321 (11321): mcc: 0.7659, acc: 0.6465, precision: 0.8559, recall: 0.6929, f1: 0.7658, edges-dep-ud-ewt_loss: 0.0235
10/01 03:46:37 AM: Update 11369: task edges-dep-ud-ewt, batch 369 (11369): mcc: 0.7623, acc: 0.6418, precision: 0.8534, recall: 0.6886, f1: 0.7622, edges-dep-ud-ewt_loss: 0.0240
10/01 03:46:47 AM: Update 11430: task edges-dep-ud-ewt, batch 430 (11430): mcc: 0.7629, acc: 0.6428, precision: 0.8537, recall: 0.6895, f1: 0.7628, edges-dep-ud-ewt_loss: 0.0237
10/01 03:46:57 AM: Update 11491: task edges-dep-ud-ewt, batch 491 (11491): mcc: 0.7638, acc: 0.6440, precision: 0.8542, recall: 0.6906, f1: 0.7637, edges-dep-ud-ewt_loss: 0.0237
10/01 03:47:07 AM: Update 11548: task edges-dep-ud-ewt, batch 548 (11548): mcc: 0.7648, acc: 0.6453, precision: 0.8550, recall: 0.6917, f1: 0.7648, edges-dep-ud-ewt_loss: 0.0237
10/01 03:47:17 AM: Update 11605: task edges-dep-ud-ewt, batch 605 (11605): mcc: 0.7648, acc: 0.6454, precision: 0.8549, recall: 0.6918, f1: 0.7647, edges-dep-ud-ewt_loss: 0.0238
10/01 03:47:28 AM: Update 11670: task edges-dep-ud-ewt, batch 670 (11670): mcc: 0.7657, acc: 0.6464, precision: 0.8557, recall: 0.6927, f1: 0.7656, edges-dep-ud-ewt_loss: 0.0236
10/01 03:47:38 AM: Update 11721: task edges-dep-ud-ewt, batch 721 (11721): mcc: 0.7649, acc: 0.6456, precision: 0.8552, recall: 0.6918, f1: 0.7649, edges-dep-ud-ewt_loss: 0.0238
10/01 03:47:48 AM: Update 11761: task edges-dep-ud-ewt, batch 761 (11761): mcc: 0.7639, acc: 0.6442, precision: 0.8547, recall: 0.6903, f1: 0.7638, edges-dep-ud-ewt_loss: 0.0240
10/01 03:47:58 AM: Update 11820: task edges-dep-ud-ewt, batch 820 (11820): mcc: 0.7643, acc: 0.6448, precision: 0.8549, recall: 0.6909, f1: 0.7642, edges-dep-ud-ewt_loss: 0.0240
10/01 03:48:08 AM: Update 11877: task edges-dep-ud-ewt, batch 877 (11877): mcc: 0.7648, acc: 0.6454, precision: 0.8553, recall: 0.6915, f1: 0.7647, edges-dep-ud-ewt_loss: 0.0239
10/01 03:48:18 AM: Update 11939: task edges-dep-ud-ewt, batch 939 (11939): mcc: 0.7651, acc: 0.6460, precision: 0.8553, recall: 0.6921, f1: 0.7651, edges-dep-ud-ewt_loss: 0.0238
10/01 03:48:28 AM: Update 11998: task edges-dep-ud-ewt, batch 998 (11998): mcc: 0.7655, acc: 0.6465, precision: 0.8555, recall: 0.6925, f1: 0.7654, edges-dep-ud-ewt_loss: 0.0238
10/01 03:48:28 AM: ***** Step 12000 / Validation 12 *****
10/01 03:48:28 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:48:28 AM: Validating...
10/01 03:48:38 AM: Evaluate: task edges-dep-ud-ewt, batch 48 (63): mcc: 0.8421, acc: 0.7530, precision: 0.9262, recall: 0.7709, f1: 0.8414, edges-dep-ud-ewt_loss: 0.0178
10/01 03:48:40 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:48:40 AM: Best result seen so far for macro.
10/01 03:48:40 AM: Updating LR scheduler:
10/01 03:48:40 AM: 	Best result seen so far for macro_avg: 0.838
10/01 03:48:40 AM: 	# validation passes without improvement: 0
10/01 03:48:40 AM: edges-dep-ud-ewt_loss: training: 0.023767 validation: 0.018095
10/01 03:48:40 AM: macro_avg: validation: 0.838272
10/01 03:48:40 AM: micro_avg: validation: 0.000000
10/01 03:48:40 AM: edges-dep-ud-ewt_mcc: training: 0.765486 validation: 0.839237
10/01 03:48:40 AM: edges-dep-ud-ewt_acc: training: 0.646533 validation: 0.748507
10/01 03:48:40 AM: edges-dep-ud-ewt_precision: training: 0.855551 validation: 0.927075
10/01 03:48:40 AM: edges-dep-ud-ewt_recall: training: 0.692485 validation: 0.764994
10/01 03:48:40 AM: edges-dep-ud-ewt_f1: training: 0.765430 validation: 0.838272
10/01 03:48:40 AM: Global learning rate: 0.0001
10/01 03:48:40 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:48:48 AM: Update 12050: task edges-dep-ud-ewt, batch 50 (12050): mcc: 0.7758, acc: 0.6612, precision: 0.8622, recall: 0.7054, f1: 0.7760, edges-dep-ud-ewt_loss: 0.0222
10/01 03:48:58 AM: Update 12092: task edges-dep-ud-ewt, batch 92 (12092): mcc: 0.7686, acc: 0.6517, precision: 0.8584, recall: 0.6957, f1: 0.7685, edges-dep-ud-ewt_loss: 0.0238
10/01 03:49:11 AM: Update 12153: task edges-dep-ud-ewt, batch 153 (12153): mcc: 0.7611, acc: 0.6422, precision: 0.8533, recall: 0.6865, f1: 0.7609, edges-dep-ud-ewt_loss: 0.0249
10/01 03:49:21 AM: Update 12208: task edges-dep-ud-ewt, batch 208 (12208): mcc: 0.7628, acc: 0.6445, precision: 0.8548, recall: 0.6883, f1: 0.7625, edges-dep-ud-ewt_loss: 0.0247
10/01 03:49:32 AM: Update 12270: task edges-dep-ud-ewt, batch 270 (12270): mcc: 0.7665, acc: 0.6492, precision: 0.8572, recall: 0.6929, f1: 0.7664, edges-dep-ud-ewt_loss: 0.0242
10/01 03:49:42 AM: Update 12329: task edges-dep-ud-ewt, batch 329 (12329): mcc: 0.7682, acc: 0.6511, precision: 0.8583, recall: 0.6951, f1: 0.7681, edges-dep-ud-ewt_loss: 0.0238
10/01 03:49:52 AM: Update 12392: task edges-dep-ud-ewt, batch 392 (12392): mcc: 0.7686, acc: 0.6518, precision: 0.8580, recall: 0.6961, f1: 0.7686, edges-dep-ud-ewt_loss: 0.0236
10/01 03:50:02 AM: Update 12454: task edges-dep-ud-ewt, batch 454 (12454): mcc: 0.7696, acc: 0.6530, precision: 0.8588, recall: 0.6972, f1: 0.7696, edges-dep-ud-ewt_loss: 0.0234
10/01 03:50:12 AM: Update 12511: task edges-dep-ud-ewt, batch 511 (12511): mcc: 0.7682, acc: 0.6512, precision: 0.8577, recall: 0.6955, f1: 0.7682, edges-dep-ud-ewt_loss: 0.0237
10/01 03:50:23 AM: Update 12546: task edges-dep-ud-ewt, batch 546 (12546): mcc: 0.7670, acc: 0.6496, precision: 0.8571, recall: 0.6938, f1: 0.7669, edges-dep-ud-ewt_loss: 0.0239
10/01 03:50:33 AM: Update 12604: task edges-dep-ud-ewt, batch 604 (12604): mcc: 0.7674, acc: 0.6502, precision: 0.8576, recall: 0.6942, f1: 0.7673, edges-dep-ud-ewt_loss: 0.0239
10/01 03:50:43 AM: Update 12666: task edges-dep-ud-ewt, batch 666 (12666): mcc: 0.7683, acc: 0.6513, precision: 0.8582, recall: 0.6953, f1: 0.7682, edges-dep-ud-ewt_loss: 0.0237
10/01 03:50:53 AM: Update 12716: task edges-dep-ud-ewt, batch 716 (12716): mcc: 0.7690, acc: 0.6522, precision: 0.8586, recall: 0.6962, f1: 0.7689, edges-dep-ud-ewt_loss: 0.0236
10/01 03:51:03 AM: Update 12773: task edges-dep-ud-ewt, batch 773 (12773): mcc: 0.7696, acc: 0.6531, precision: 0.8589, recall: 0.6971, f1: 0.7696, edges-dep-ud-ewt_loss: 0.0236
10/01 03:51:13 AM: Update 12842: task edges-dep-ud-ewt, batch 842 (12842): mcc: 0.7702, acc: 0.6539, precision: 0.8592, recall: 0.6978, f1: 0.7702, edges-dep-ud-ewt_loss: 0.0234
10/01 03:51:23 AM: Update 12896: task edges-dep-ud-ewt, batch 896 (12896): mcc: 0.7698, acc: 0.6535, precision: 0.8591, recall: 0.6972, f1: 0.7697, edges-dep-ud-ewt_loss: 0.0235
10/01 03:51:34 AM: Update 12937: task edges-dep-ud-ewt, batch 937 (12937): mcc: 0.7688, acc: 0.6524, precision: 0.8585, recall: 0.6960, f1: 0.7688, edges-dep-ud-ewt_loss: 0.0237
10/01 03:51:44 AM: Update 12995: task edges-dep-ud-ewt, batch 995 (12995): mcc: 0.7694, acc: 0.6530, precision: 0.8588, recall: 0.6967, f1: 0.7693, edges-dep-ud-ewt_loss: 0.0235
10/01 03:51:45 AM: ***** Step 13000 / Validation 13 *****
10/01 03:51:45 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:51:45 AM: Validating...
10/01 03:51:54 AM: Evaluate: task edges-dep-ud-ewt, batch 43 (63): mcc: 0.8452, acc: 0.7598, precision: 0.9228, recall: 0.7793, f1: 0.8450, edges-dep-ud-ewt_loss: 0.0174
10/01 03:51:57 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:51:57 AM: Best result seen so far for macro.
10/01 03:51:57 AM: Updating LR scheduler:
10/01 03:51:57 AM: 	Best result seen so far for macro_avg: 0.844
10/01 03:51:57 AM: 	# validation passes without improvement: 0
10/01 03:51:57 AM: edges-dep-ud-ewt_loss: training: 0.023532 validation: 0.017651
10/01 03:51:57 AM: macro_avg: validation: 0.844482
10/01 03:51:57 AM: micro_avg: validation: 0.000000
10/01 03:51:57 AM: edges-dep-ud-ewt_mcc: training: 0.769374 validation: 0.844784
10/01 03:51:57 AM: edges-dep-ud-ewt_acc: training: 0.653008 validation: 0.758582
10/01 03:51:57 AM: edges-dep-ud-ewt_precision: training: 0.858868 validation: 0.924038
10/01 03:51:57 AM: edges-dep-ud-ewt_recall: training: 0.696678 validation: 0.777539
10/01 03:51:57 AM: edges-dep-ud-ewt_f1: training: 0.769318 validation: 0.844482
10/01 03:51:57 AM: Global learning rate: 0.0001
10/01 03:51:57 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:52:04 AM: Update 13044: task edges-dep-ud-ewt, batch 44 (13044): mcc: 0.7812, acc: 0.6703, precision: 0.8645, recall: 0.7132, f1: 0.7816, edges-dep-ud-ewt_loss: 0.0228
10/01 03:52:14 AM: Update 13106: task edges-dep-ud-ewt, batch 106 (13106): mcc: 0.7804, acc: 0.6693, precision: 0.8645, recall: 0.7117, f1: 0.7807, edges-dep-ud-ewt_loss: 0.0225
10/01 03:52:24 AM: Update 13170: task edges-dep-ud-ewt, batch 170 (13170): mcc: 0.7807, acc: 0.6687, precision: 0.8654, recall: 0.7115, f1: 0.7810, edges-dep-ud-ewt_loss: 0.0226
10/01 03:52:34 AM: Update 13228: task edges-dep-ud-ewt, batch 228 (13228): mcc: 0.7805, acc: 0.6681, precision: 0.8664, recall: 0.7104, f1: 0.7806, edges-dep-ud-ewt_loss: 0.0224
10/01 03:52:44 AM: Update 13270: task edges-dep-ud-ewt, batch 270 (13270): mcc: 0.7776, acc: 0.6646, precision: 0.8641, recall: 0.7070, f1: 0.7777, edges-dep-ud-ewt_loss: 0.0227
10/01 03:52:57 AM: Update 13329: task edges-dep-ud-ewt, batch 329 (13329): mcc: 0.7743, acc: 0.6606, precision: 0.8622, recall: 0.7027, f1: 0.7743, edges-dep-ud-ewt_loss: 0.0233
10/01 03:53:07 AM: Update 13382: task edges-dep-ud-ewt, batch 382 (13382): mcc: 0.7748, acc: 0.6610, precision: 0.8628, recall: 0.7032, f1: 0.7749, edges-dep-ud-ewt_loss: 0.0232
10/01 03:53:17 AM: Update 13433: task edges-dep-ud-ewt, batch 433 (13433): mcc: 0.7746, acc: 0.6608, precision: 0.8627, recall: 0.7029, f1: 0.7746, edges-dep-ud-ewt_loss: 0.0232
10/01 03:53:27 AM: Update 13499: task edges-dep-ud-ewt, batch 499 (13499): mcc: 0.7755, acc: 0.6621, precision: 0.8633, recall: 0.7039, f1: 0.7755, edges-dep-ud-ewt_loss: 0.0229
10/01 03:53:37 AM: Update 13563: task edges-dep-ud-ewt, batch 563 (13563): mcc: 0.7763, acc: 0.6632, precision: 0.8637, recall: 0.7051, f1: 0.7764, edges-dep-ud-ewt_loss: 0.0228
10/01 03:53:47 AM: Update 13627: task edges-dep-ud-ewt, batch 627 (13627): mcc: 0.7771, acc: 0.6642, precision: 0.8643, recall: 0.7060, f1: 0.7772, edges-dep-ud-ewt_loss: 0.0227
10/01 03:53:57 AM: Update 13677: task edges-dep-ud-ewt, batch 677 (13677): mcc: 0.7763, acc: 0.6633, precision: 0.8638, recall: 0.7049, f1: 0.7763, edges-dep-ud-ewt_loss: 0.0229
10/01 03:54:08 AM: Update 13721: task edges-dep-ud-ewt, batch 721 (13721): mcc: 0.7752, acc: 0.6619, precision: 0.8631, recall: 0.7035, f1: 0.7752, edges-dep-ud-ewt_loss: 0.0231
10/01 03:54:18 AM: Update 13782: task edges-dep-ud-ewt, batch 782 (13782): mcc: 0.7753, acc: 0.6621, precision: 0.8630, recall: 0.7037, f1: 0.7753, edges-dep-ud-ewt_loss: 0.0229
10/01 03:54:28 AM: Update 13843: task edges-dep-ud-ewt, batch 843 (13843): mcc: 0.7757, acc: 0.6628, precision: 0.8634, recall: 0.7043, f1: 0.7758, edges-dep-ud-ewt_loss: 0.0229
10/01 03:54:38 AM: Update 13903: task edges-dep-ud-ewt, batch 903 (13903): mcc: 0.7762, acc: 0.6635, precision: 0.8635, recall: 0.7050, f1: 0.7763, edges-dep-ud-ewt_loss: 0.0229
10/01 03:54:48 AM: Update 13963: task edges-dep-ud-ewt, batch 963 (13963): mcc: 0.7765, acc: 0.6639, precision: 0.8636, recall: 0.7055, f1: 0.7766, edges-dep-ud-ewt_loss: 0.0228
10/01 03:54:54 AM: ***** Step 14000 / Validation 14 *****
10/01 03:54:54 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:54:54 AM: Validating...
10/01 03:54:58 AM: Evaluate: task edges-dep-ud-ewt, batch 18 (63): mcc: 0.8553, acc: 0.7804, precision: 0.9137, recall: 0.8057, f1: 0.8563, edges-dep-ud-ewt_loss: 0.0165
10/01 03:55:07 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:55:07 AM: Best result seen so far for macro.
10/01 03:55:07 AM: Updating LR scheduler:
10/01 03:55:07 AM: 	Best result seen so far for macro_avg: 0.856
10/01 03:55:07 AM: 	# validation passes without improvement: 0
10/01 03:55:07 AM: edges-dep-ud-ewt_loss: training: 0.022735 validation: 0.016896
10/01 03:55:07 AM: macro_avg: validation: 0.856243
10/01 03:55:07 AM: micro_avg: validation: 0.000000
10/01 03:55:07 AM: edges-dep-ud-ewt_mcc: training: 0.776869 validation: 0.855532
10/01 03:55:07 AM: edges-dep-ud-ewt_acc: training: 0.664382 validation: 0.778813
10/01 03:55:07 AM: edges-dep-ud-ewt_precision: training: 0.863852 validation: 0.918995
10/01 03:55:07 AM: edges-dep-ud-ewt_recall: training: 0.705934 validation: 0.801513
10/01 03:55:07 AM: edges-dep-ud-ewt_f1: training: 0.776950 validation: 0.856243
10/01 03:55:07 AM: Global learning rate: 0.0001
10/01 03:55:07 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:55:09 AM: Update 14008: task edges-dep-ud-ewt, batch 8 (14008): mcc: 0.7689, acc: 0.6550, precision: 0.8612, recall: 0.6940, f1: 0.7686, edges-dep-ud-ewt_loss: 0.0221
10/01 03:55:19 AM: Update 14061: task edges-dep-ud-ewt, batch 61 (14061): mcc: 0.7715, acc: 0.6590, precision: 0.8594, recall: 0.7000, f1: 0.7716, edges-dep-ud-ewt_loss: 0.0240
10/01 03:55:31 AM: Update 14113: task edges-dep-ud-ewt, batch 113 (14113): mcc: 0.7646, acc: 0.6492, precision: 0.8560, recall: 0.6905, f1: 0.7644, edges-dep-ud-ewt_loss: 0.0249
10/01 03:55:41 AM: Update 14176: task edges-dep-ud-ewt, batch 176 (14176): mcc: 0.7723, acc: 0.6588, precision: 0.8608, recall: 0.7004, f1: 0.7723, edges-dep-ud-ewt_loss: 0.0233
10/01 03:55:51 AM: Update 14230: task edges-dep-ud-ewt, batch 230 (14230): mcc: 0.7759, acc: 0.6634, precision: 0.8626, recall: 0.7053, f1: 0.7760, edges-dep-ud-ewt_loss: 0.0229
10/01 03:56:01 AM: Update 14296: task edges-dep-ud-ewt, batch 296 (14296): mcc: 0.7776, acc: 0.6658, precision: 0.8633, recall: 0.7077, f1: 0.7777, edges-dep-ud-ewt_loss: 0.0227
10/01 03:56:11 AM: Update 14352: task edges-dep-ud-ewt, batch 352 (14352): mcc: 0.7782, acc: 0.6662, precision: 0.8640, recall: 0.7082, f1: 0.7784, edges-dep-ud-ewt_loss: 0.0227
10/01 03:56:21 AM: Update 14409: task edges-dep-ud-ewt, batch 409 (14409): mcc: 0.7785, acc: 0.6669, precision: 0.8639, recall: 0.7088, f1: 0.7787, edges-dep-ud-ewt_loss: 0.0226
10/01 03:56:31 AM: Update 14459: task edges-dep-ud-ewt, batch 459 (14459): mcc: 0.7774, acc: 0.6653, precision: 0.8636, recall: 0.7071, f1: 0.7776, edges-dep-ud-ewt_loss: 0.0227
10/01 03:56:42 AM: Update 14505: task edges-dep-ud-ewt, batch 505 (14505): mcc: 0.7759, acc: 0.6634, precision: 0.8626, recall: 0.7052, f1: 0.7760, edges-dep-ud-ewt_loss: 0.0230
10/01 03:56:52 AM: Update 14553: task edges-dep-ud-ewt, batch 553 (14553): mcc: 0.7761, acc: 0.6637, precision: 0.8630, recall: 0.7053, f1: 0.7762, edges-dep-ud-ewt_loss: 0.0229
10/01 03:57:02 AM: Update 14612: task edges-dep-ud-ewt, batch 612 (14612): mcc: 0.7768, acc: 0.6647, precision: 0.8632, recall: 0.7063, f1: 0.7769, edges-dep-ud-ewt_loss: 0.0229
10/01 03:57:12 AM: Update 14674: task edges-dep-ud-ewt, batch 674 (14674): mcc: 0.7775, acc: 0.6657, precision: 0.8635, recall: 0.7073, f1: 0.7777, edges-dep-ud-ewt_loss: 0.0228
10/01 03:57:23 AM: Update 14733: task edges-dep-ud-ewt, batch 733 (14733): mcc: 0.7781, acc: 0.6665, precision: 0.8638, recall: 0.7081, f1: 0.7783, edges-dep-ud-ewt_loss: 0.0226
10/01 03:57:33 AM: Update 14802: task edges-dep-ud-ewt, batch 802 (14802): mcc: 0.7787, acc: 0.6673, precision: 0.8643, recall: 0.7088, f1: 0.7789, edges-dep-ud-ewt_loss: 0.0225
10/01 03:57:43 AM: Update 14853: task edges-dep-ud-ewt, batch 853 (14853): mcc: 0.7782, acc: 0.6667, precision: 0.8640, recall: 0.7082, f1: 0.7784, edges-dep-ud-ewt_loss: 0.0226
10/01 03:57:53 AM: Update 14897: task edges-dep-ud-ewt, batch 897 (14897): mcc: 0.7775, acc: 0.6658, precision: 0.8636, recall: 0.7073, f1: 0.7777, edges-dep-ud-ewt_loss: 0.0227
10/01 03:58:04 AM: Update 14958: task edges-dep-ud-ewt, batch 958 (14958): mcc: 0.7782, acc: 0.6668, precision: 0.8642, recall: 0.7081, f1: 0.7784, edges-dep-ud-ewt_loss: 0.0226
10/01 03:58:11 AM: ***** Step 15000 / Validation 15 *****
10/01 03:58:11 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 03:58:11 AM: Validating...
10/01 03:58:14 AM: Evaluate: task edges-dep-ud-ewt, batch 12 (63): mcc: 0.8542, acc: 0.7787, precision: 0.9140, recall: 0.8034, f1: 0.8551, edges-dep-ud-ewt_loss: 0.0164
10/01 03:58:23 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 03:58:23 AM: Best result seen so far for macro.
10/01 03:58:23 AM: Updating LR scheduler:
10/01 03:58:23 AM: 	Best result seen so far for macro_avg: 0.859
10/01 03:58:23 AM: 	# validation passes without improvement: 0
10/01 03:58:23 AM: edges-dep-ud-ewt_loss: training: 0.022528 validation: 0.016578
10/01 03:58:23 AM: macro_avg: validation: 0.858960
10/01 03:58:23 AM: micro_avg: validation: 0.000000
10/01 03:58:23 AM: edges-dep-ud-ewt_mcc: training: 0.778557 validation: 0.858090
10/01 03:58:23 AM: edges-dep-ud-ewt_acc: training: 0.667276 validation: 0.784667
10/01 03:58:23 AM: edges-dep-ud-ewt_precision: training: 0.864289 validation: 0.918473
10/01 03:58:23 AM: edges-dep-ud-ewt_recall: training: 0.708590 validation: 0.806691
10/01 03:58:23 AM: edges-dep-ud-ewt_f1: training: 0.778733 validation: 0.858960
10/01 03:58:23 AM: Global learning rate: 0.0001
10/01 03:58:23 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 03:58:24 AM: Update 15006: task edges-dep-ud-ewt, batch 6 (15006): mcc: 0.7763, acc: 0.6673, precision: 0.8494, recall: 0.7171, f1: 0.7777, edges-dep-ud-ewt_loss: 0.0237
10/01 03:58:34 AM: Update 15067: task edges-dep-ud-ewt, batch 67 (15067): mcc: 0.7872, acc: 0.6802, precision: 0.8681, recall: 0.7209, f1: 0.7877, edges-dep-ud-ewt_loss: 0.0210
10/01 03:58:44 AM: Update 15124: task edges-dep-ud-ewt, batch 124 (15124): mcc: 0.7887, acc: 0.6818, precision: 0.8697, recall: 0.7223, f1: 0.7892, edges-dep-ud-ewt_loss: 0.0217
10/01 03:58:54 AM: Update 15186: task edges-dep-ud-ewt, batch 186 (15186): mcc: 0.7886, acc: 0.6815, precision: 0.8693, recall: 0.7224, f1: 0.7891, edges-dep-ud-ewt_loss: 0.0213
10/01 03:59:04 AM: Update 15244: task edges-dep-ud-ewt, batch 244 (15244): mcc: 0.7857, acc: 0.6780, precision: 0.8671, recall: 0.7192, f1: 0.7862, edges-dep-ud-ewt_loss: 0.0218
10/01 03:59:16 AM: Update 15289: task edges-dep-ud-ewt, batch 289 (15289): mcc: 0.7817, acc: 0.6726, precision: 0.8647, recall: 0.7138, f1: 0.7820, edges-dep-ud-ewt_loss: 0.0224
10/01 03:59:26 AM: Update 15346: task edges-dep-ud-ewt, batch 346 (15346): mcc: 0.7824, acc: 0.6734, precision: 0.8653, recall: 0.7147, f1: 0.7828, edges-dep-ud-ewt_loss: 0.0222
10/01 03:59:36 AM: Update 15414: task edges-dep-ud-ewt, batch 414 (15414): mcc: 0.7843, acc: 0.6758, precision: 0.8668, recall: 0.7167, f1: 0.7847, edges-dep-ud-ewt_loss: 0.0220
10/01 03:59:46 AM: Update 15466: task edges-dep-ud-ewt, batch 466 (15466): mcc: 0.7842, acc: 0.6758, precision: 0.8665, recall: 0.7168, f1: 0.7846, edges-dep-ud-ewt_loss: 0.0220
10/01 03:59:56 AM: Update 15530: task edges-dep-ud-ewt, batch 530 (15530): mcc: 0.7848, acc: 0.6766, precision: 0.8670, recall: 0.7176, f1: 0.7853, edges-dep-ud-ewt_loss: 0.0219
10/01 04:00:06 AM: Update 15588: task edges-dep-ud-ewt, batch 588 (15588): mcc: 0.7851, acc: 0.6770, precision: 0.8671, recall: 0.7180, f1: 0.7856, edges-dep-ud-ewt_loss: 0.0218
10/01 04:00:17 AM: Update 15636: task edges-dep-ud-ewt, batch 636 (15636): mcc: 0.7841, acc: 0.6756, precision: 0.8665, recall: 0.7166, f1: 0.7845, edges-dep-ud-ewt_loss: 0.0219
10/01 04:00:27 AM: Update 15681: task edges-dep-ud-ewt, batch 681 (15681): mcc: 0.7830, acc: 0.6744, precision: 0.8659, recall: 0.7152, f1: 0.7833, edges-dep-ud-ewt_loss: 0.0221
10/01 04:00:37 AM: Update 15736: task edges-dep-ud-ewt, batch 736 (15736): mcc: 0.7834, acc: 0.6750, precision: 0.8662, recall: 0.7157, f1: 0.7838, edges-dep-ud-ewt_loss: 0.0221
10/01 04:00:47 AM: Update 15799: task edges-dep-ud-ewt, batch 799 (15799): mcc: 0.7842, acc: 0.6760, precision: 0.8668, recall: 0.7167, f1: 0.7846, edges-dep-ud-ewt_loss: 0.0220
10/01 04:00:58 AM: Update 15864: task edges-dep-ud-ewt, batch 864 (15864): mcc: 0.7846, acc: 0.6765, precision: 0.8671, recall: 0.7172, f1: 0.7850, edges-dep-ud-ewt_loss: 0.0219
10/01 04:01:08 AM: Update 15923: task edges-dep-ud-ewt, batch 923 (15923): mcc: 0.7850, acc: 0.6770, precision: 0.8672, recall: 0.7176, f1: 0.7854, edges-dep-ud-ewt_loss: 0.0218
10/01 04:01:18 AM: Update 15979: task edges-dep-ud-ewt, batch 979 (15979): mcc: 0.7852, acc: 0.6773, precision: 0.8674, recall: 0.7179, f1: 0.7856, edges-dep-ud-ewt_loss: 0.0218
10/01 04:01:22 AM: ***** Step 16000 / Validation 16 *****
10/01 04:01:22 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 04:01:22 AM: Validating...
10/01 04:01:28 AM: Evaluate: task edges-dep-ud-ewt, batch 24 (63): mcc: 0.8617, acc: 0.7891, precision: 0.9240, recall: 0.8085, f1: 0.8624, edges-dep-ud-ewt_loss: 0.0155
10/01 04:01:35 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 04:01:35 AM: Best result seen so far for macro.
10/01 04:01:35 AM: Updating LR scheduler:
10/01 04:01:35 AM: 	Best result seen so far for macro_avg: 0.860
10/01 04:01:35 AM: 	# validation passes without improvement: 0
10/01 04:01:35 AM: edges-dep-ud-ewt_loss: training: 0.021787 validation: 0.016379
10/01 04:01:35 AM: macro_avg: validation: 0.859606
10/01 04:01:35 AM: micro_avg: validation: 0.000000
10/01 04:01:35 AM: edges-dep-ud-ewt_mcc: training: 0.785128 validation: 0.859180
10/01 04:01:35 AM: edges-dep-ud-ewt_acc: training: 0.677191 validation: 0.784269
10/01 04:01:35 AM: edges-dep-ud-ewt_precision: training: 0.867414 validation: 0.925820
10/01 04:01:35 AM: edges-dep-ud-ewt_recall: training: 0.717765 validation: 0.802230
10/01 04:01:35 AM: edges-dep-ud-ewt_f1: training: 0.785526 validation: 0.859606
10/01 04:01:35 AM: Global learning rate: 0.0001
10/01 04:01:35 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 04:01:38 AM: Update 16017: task edges-dep-ud-ewt, batch 17 (16017): mcc: 0.7745, acc: 0.6641, precision: 0.8650, recall: 0.7008, f1: 0.7743, edges-dep-ud-ewt_loss: 0.0247
10/01 04:01:50 AM: Update 16073: task edges-dep-ud-ewt, batch 73 (16073): mcc: 0.7695, acc: 0.6577, precision: 0.8593, recall: 0.6965, f1: 0.7694, edges-dep-ud-ewt_loss: 0.0248
10/01 04:02:00 AM: Update 16132: task edges-dep-ud-ewt, batch 132 (16132): mcc: 0.7775, acc: 0.6681, precision: 0.8633, recall: 0.7075, f1: 0.7777, edges-dep-ud-ewt_loss: 0.0232
10/01 04:02:10 AM: Update 16189: task edges-dep-ud-ewt, batch 189 (16189): mcc: 0.7822, acc: 0.6743, precision: 0.8658, recall: 0.7139, f1: 0.7825, edges-dep-ud-ewt_loss: 0.0226
10/01 04:02:20 AM: Update 16256: task edges-dep-ud-ewt, batch 256 (16256): mcc: 0.7845, acc: 0.6770, precision: 0.8673, recall: 0.7166, f1: 0.7848, edges-dep-ud-ewt_loss: 0.0220
10/01 04:02:30 AM: Update 16312: task edges-dep-ud-ewt, batch 312 (16312): mcc: 0.7857, acc: 0.6789, precision: 0.8678, recall: 0.7185, f1: 0.7861, edges-dep-ud-ewt_loss: 0.0219
10/01 04:02:40 AM: Update 16375: task edges-dep-ud-ewt, batch 375 (16375): mcc: 0.7871, acc: 0.6803, precision: 0.8687, recall: 0.7202, f1: 0.7875, edges-dep-ud-ewt_loss: 0.0216
10/01 04:02:51 AM: Update 16420: task edges-dep-ud-ewt, batch 420 (16420): mcc: 0.7853, acc: 0.6778, precision: 0.8676, recall: 0.7180, f1: 0.7857, edges-dep-ud-ewt_loss: 0.0218
10/01 04:03:01 AM: Update 16465: task edges-dep-ud-ewt, batch 465 (16465): mcc: 0.7836, acc: 0.6756, precision: 0.8666, recall: 0.7158, f1: 0.7840, edges-dep-ud-ewt_loss: 0.0221
10/01 04:03:12 AM: Update 16526: task edges-dep-ud-ewt, batch 526 (16526): mcc: 0.7846, acc: 0.6769, precision: 0.8672, recall: 0.7170, f1: 0.7850, edges-dep-ud-ewt_loss: 0.0220
10/01 04:03:22 AM: Update 16587: task edges-dep-ud-ewt, batch 587 (16587): mcc: 0.7857, acc: 0.6782, precision: 0.8680, recall: 0.7183, f1: 0.7861, edges-dep-ud-ewt_loss: 0.0218
10/01 04:03:32 AM: Update 16647: task edges-dep-ud-ewt, batch 647 (16647): mcc: 0.7863, acc: 0.6789, precision: 0.8683, recall: 0.7192, f1: 0.7867, edges-dep-ud-ewt_loss: 0.0217
10/01 04:03:42 AM: Update 16703: task edges-dep-ud-ewt, batch 703 (16703): mcc: 0.7865, acc: 0.6793, precision: 0.8682, recall: 0.7195, f1: 0.7869, edges-dep-ud-ewt_loss: 0.0217
10/01 04:03:52 AM: Update 16766: task edges-dep-ud-ewt, batch 766 (16766): mcc: 0.7870, acc: 0.6800, precision: 0.8686, recall: 0.7201, f1: 0.7874, edges-dep-ud-ewt_loss: 0.0216
10/01 04:04:02 AM: Update 16810: task edges-dep-ud-ewt, batch 810 (16810): mcc: 0.7863, acc: 0.6793, precision: 0.8681, recall: 0.7193, f1: 0.7867, edges-dep-ud-ewt_loss: 0.0217
10/01 04:04:13 AM: Update 16857: task edges-dep-ud-ewt, batch 857 (16857): mcc: 0.7854, acc: 0.6782, precision: 0.8676, recall: 0.7181, f1: 0.7858, edges-dep-ud-ewt_loss: 0.0218
10/01 04:04:23 AM: Update 16915: task edges-dep-ud-ewt, batch 915 (16915): mcc: 0.7855, acc: 0.6783, precision: 0.8676, recall: 0.7182, f1: 0.7859, edges-dep-ud-ewt_loss: 0.0218
10/01 04:04:33 AM: Update 16978: task edges-dep-ud-ewt, batch 978 (16978): mcc: 0.7863, acc: 0.6793, precision: 0.8681, recall: 0.7192, f1: 0.7867, edges-dep-ud-ewt_loss: 0.0216
10/01 04:04:37 AM: ***** Step 17000 / Validation 17 *****
10/01 04:04:37 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 04:04:37 AM: Validating...
10/01 04:04:43 AM: Evaluate: task edges-dep-ud-ewt, batch 29 (63): mcc: 0.8648, acc: 0.7959, precision: 0.9192, recall: 0.8185, f1: 0.8659, edges-dep-ud-ewt_loss: 0.0154
10/01 04:04:49 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 04:04:49 AM: Best result seen so far for macro.
10/01 04:04:49 AM: Updating LR scheduler:
10/01 04:04:49 AM: 	Best result seen so far for macro_avg: 0.864
10/01 04:04:49 AM: 	# validation passes without improvement: 0
10/01 04:04:49 AM: edges-dep-ud-ewt_loss: training: 0.021581 validation: 0.016032
10/01 04:04:49 AM: macro_avg: validation: 0.863697
10/01 04:04:49 AM: micro_avg: validation: 0.000000
10/01 04:04:49 AM: edges-dep-ud-ewt_mcc: training: 0.786429 validation: 0.862806
10/01 04:04:49 AM: edges-dep-ud-ewt_acc: training: 0.679536 validation: 0.792473
10/01 04:04:49 AM: edges-dep-ud-ewt_precision: training: 0.868201 validation: 0.921517
10/01 04:04:49 AM: edges-dep-ud-ewt_recall: training: 0.719446 validation: 0.812704
10/01 04:04:49 AM: edges-dep-ud-ewt_f1: training: 0.786854 validation: 0.863697
10/01 04:04:49 AM: Global learning rate: 0.0001
10/01 04:04:49 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 04:04:53 AM: Update 17025: task edges-dep-ud-ewt, batch 25 (17025): mcc: 0.7976, acc: 0.6913, precision: 0.8785, recall: 0.7308, f1: 0.7979, edges-dep-ud-ewt_loss: 0.0220
10/01 04:05:04 AM: Update 17080: task edges-dep-ud-ewt, batch 80 (17080): mcc: 0.7925, acc: 0.6869, precision: 0.8728, recall: 0.7265, f1: 0.7929, edges-dep-ud-ewt_loss: 0.0213
10/01 04:05:14 AM: Update 17143: task edges-dep-ud-ewt, batch 143 (17143): mcc: 0.7955, acc: 0.6911, precision: 0.8751, recall: 0.7299, f1: 0.7960, edges-dep-ud-ewt_loss: 0.0210
10/01 04:05:24 AM: Update 17191: task edges-dep-ud-ewt, batch 191 (17191): mcc: 0.7925, acc: 0.6877, precision: 0.8731, recall: 0.7263, f1: 0.7930, edges-dep-ud-ewt_loss: 0.0213
10/01 04:05:36 AM: Update 17249: task edges-dep-ud-ewt, batch 249 (17249): mcc: 0.7891, acc: 0.6836, precision: 0.8711, recall: 0.7218, f1: 0.7895, edges-dep-ud-ewt_loss: 0.0220
10/01 04:05:46 AM: Update 17305: task edges-dep-ud-ewt, batch 305 (17305): mcc: 0.7897, acc: 0.6845, precision: 0.8709, recall: 0.7231, f1: 0.7902, edges-dep-ud-ewt_loss: 0.0217
10/01 04:05:56 AM: Update 17359: task edges-dep-ud-ewt, batch 359 (17359): mcc: 0.7901, acc: 0.6850, precision: 0.8710, recall: 0.7236, f1: 0.7905, edges-dep-ud-ewt_loss: 0.0217
10/01 04:06:06 AM: Update 17427: task edges-dep-ud-ewt, batch 427 (17427): mcc: 0.7914, acc: 0.6867, precision: 0.8717, recall: 0.7254, f1: 0.7918, edges-dep-ud-ewt_loss: 0.0212
10/01 04:06:17 AM: Update 17486: task edges-dep-ud-ewt, batch 486 (17486): mcc: 0.7917, acc: 0.6872, precision: 0.8716, recall: 0.7261, f1: 0.7922, edges-dep-ud-ewt_loss: 0.0212
10/01 04:06:27 AM: Update 17547: task edges-dep-ud-ewt, batch 547 (17547): mcc: 0.7921, acc: 0.6878, precision: 0.8719, recall: 0.7266, f1: 0.7926, edges-dep-ud-ewt_loss: 0.0211
10/01 04:06:37 AM: Update 17604: task edges-dep-ud-ewt, batch 604 (17604): mcc: 0.7909, acc: 0.6862, precision: 0.8712, recall: 0.7250, f1: 0.7914, edges-dep-ud-ewt_loss: 0.0213
10/01 04:06:47 AM: Update 17641: task edges-dep-ud-ewt, batch 641 (17641): mcc: 0.7899, acc: 0.6849, precision: 0.8706, recall: 0.7238, f1: 0.7904, edges-dep-ud-ewt_loss: 0.0215
10/01 04:06:57 AM: Update 17696: task edges-dep-ud-ewt, batch 696 (17696): mcc: 0.7903, acc: 0.6853, precision: 0.8710, recall: 0.7241, f1: 0.7908, edges-dep-ud-ewt_loss: 0.0214
10/01 04:07:07 AM: Update 17754: task edges-dep-ud-ewt, batch 754 (17754): mcc: 0.7908, acc: 0.6859, precision: 0.8713, recall: 0.7247, f1: 0.7913, edges-dep-ud-ewt_loss: 0.0214
10/01 04:07:17 AM: Update 17820: task edges-dep-ud-ewt, batch 820 (17820): mcc: 0.7914, acc: 0.6867, precision: 0.8717, recall: 0.7255, f1: 0.7919, edges-dep-ud-ewt_loss: 0.0213
10/01 04:07:27 AM: Update 17881: task edges-dep-ud-ewt, batch 881 (17881): mcc: 0.7919, acc: 0.6875, precision: 0.8719, recall: 0.7262, f1: 0.7924, edges-dep-ud-ewt_loss: 0.0212
10/01 04:07:37 AM: Update 17942: task edges-dep-ud-ewt, batch 942 (17942): mcc: 0.7921, acc: 0.6878, precision: 0.8719, recall: 0.7266, f1: 0.7926, edges-dep-ud-ewt_loss: 0.0211
10/01 04:07:47 AM: Update 17992: task edges-dep-ud-ewt, batch 992 (17992): mcc: 0.7914, acc: 0.6869, precision: 0.8714, recall: 0.7256, f1: 0.7919, edges-dep-ud-ewt_loss: 0.0212
10/01 04:07:49 AM: ***** Step 18000 / Validation 18 *****
10/01 04:07:49 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 04:07:49 AM: Validating...
10/01 04:07:57 AM: Evaluate: task edges-dep-ud-ewt, batch 43 (63): mcc: 0.8655, acc: 0.7955, precision: 0.9198, recall: 0.8193, f1: 0.8667, edges-dep-ud-ewt_loss: 0.0155
10/01 04:08:01 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 04:08:01 AM: Best result seen so far for macro.
10/01 04:08:01 AM: Updating LR scheduler:
10/01 04:08:01 AM: 	Best result seen so far for macro_avg: 0.867
10/01 04:08:01 AM: 	# validation passes without improvement: 0
10/01 04:08:01 AM: edges-dep-ud-ewt_loss: training: 0.021232 validation: 0.015627
10/01 04:08:01 AM: macro_avg: validation: 0.866800
10/01 04:08:01 AM: micro_avg: validation: 0.000000
10/01 04:08:01 AM: edges-dep-ud-ewt_mcc: training: 0.791265 validation: 0.865794
10/01 04:08:01 AM: edges-dep-ud-ewt_acc: training: 0.686753 validation: 0.796057
10/01 04:08:01 AM: edges-dep-ud-ewt_precision: training: 0.871383 validation: 0.921741
10/01 04:08:01 AM: edges-dep-ud-ewt_recall: training: 0.725475 validation: 0.818041
10/01 04:08:01 AM: edges-dep-ud-ewt_f1: training: 0.791763 validation: 0.866800
10/01 04:08:01 AM: Global learning rate: 0.0001
10/01 04:08:01 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 04:08:10 AM: Update 18033: task edges-dep-ud-ewt, batch 33 (18033): mcc: 0.7759, acc: 0.6678, precision: 0.8627, recall: 0.7052, f1: 0.7760, edges-dep-ud-ewt_loss: 0.0242
10/01 04:08:20 AM: Update 18089: task edges-dep-ud-ewt, batch 89 (18089): mcc: 0.7875, acc: 0.6824, precision: 0.8695, recall: 0.7202, f1: 0.7878, edges-dep-ud-ewt_loss: 0.0222
10/01 04:08:30 AM: Update 18148: task edges-dep-ud-ewt, batch 148 (18148): mcc: 0.7915, acc: 0.6883, precision: 0.8715, recall: 0.7259, f1: 0.7920, edges-dep-ud-ewt_loss: 0.0217
10/01 04:08:40 AM: Update 18213: task edges-dep-ud-ewt, batch 213 (18213): mcc: 0.7945, acc: 0.6917, precision: 0.8740, recall: 0.7291, f1: 0.7950, edges-dep-ud-ewt_loss: 0.0210
10/01 04:08:50 AM: Update 18271: task edges-dep-ud-ewt, batch 271 (18271): mcc: 0.7952, acc: 0.6924, precision: 0.8736, recall: 0.7307, f1: 0.7958, edges-dep-ud-ewt_loss: 0.0208
10/01 04:09:00 AM: Update 18332: task edges-dep-ud-ewt, batch 332 (18332): mcc: 0.7960, acc: 0.6937, precision: 0.8737, recall: 0.7320, f1: 0.7966, edges-dep-ud-ewt_loss: 0.0205
10/01 04:09:10 AM: Update 18377: task edges-dep-ud-ewt, batch 377 (18377): mcc: 0.7937, acc: 0.6907, precision: 0.8725, recall: 0.7290, f1: 0.7943, edges-dep-ud-ewt_loss: 0.0208
10/01 04:09:21 AM: Update 18425: task edges-dep-ud-ewt, batch 425 (18425): mcc: 0.7920, acc: 0.6882, precision: 0.8713, recall: 0.7268, f1: 0.7925, edges-dep-ud-ewt_loss: 0.0212
10/01 04:09:31 AM: Update 18480: task edges-dep-ud-ewt, batch 480 (18480): mcc: 0.7916, acc: 0.6878, precision: 0.8707, recall: 0.7266, f1: 0.7922, edges-dep-ud-ewt_loss: 0.0212
10/01 04:09:41 AM: Update 18546: task edges-dep-ud-ewt, batch 546 (18546): mcc: 0.7929, acc: 0.6896, precision: 0.8716, recall: 0.7282, f1: 0.7935, edges-dep-ud-ewt_loss: 0.0209
10/01 04:09:51 AM: Update 18600: task edges-dep-ud-ewt, batch 600 (18600): mcc: 0.7935, acc: 0.6903, precision: 0.8721, recall: 0.7289, f1: 0.7941, edges-dep-ud-ewt_loss: 0.0209
10/01 04:10:01 AM: Update 18657: task edges-dep-ud-ewt, batch 657 (18657): mcc: 0.7936, acc: 0.6904, precision: 0.8722, recall: 0.7290, f1: 0.7942, edges-dep-ud-ewt_loss: 0.0208
10/01 04:10:12 AM: Update 18724: task edges-dep-ud-ewt, batch 724 (18724): mcc: 0.7944, acc: 0.6914, precision: 0.8727, recall: 0.7300, f1: 0.7950, edges-dep-ud-ewt_loss: 0.0207
10/01 04:10:22 AM: Update 18773: task edges-dep-ud-ewt, batch 773 (18773): mcc: 0.7937, acc: 0.6906, precision: 0.8723, recall: 0.7291, f1: 0.7943, edges-dep-ud-ewt_loss: 0.0209
10/01 04:10:33 AM: Update 18817: task edges-dep-ud-ewt, batch 817 (18817): mcc: 0.7927, acc: 0.6894, precision: 0.8717, recall: 0.7278, f1: 0.7933, edges-dep-ud-ewt_loss: 0.0210
10/01 04:10:43 AM: Update 18876: task edges-dep-ud-ewt, batch 876 (18876): mcc: 0.7932, acc: 0.6900, precision: 0.8720, recall: 0.7284, f1: 0.7938, edges-dep-ud-ewt_loss: 0.0210
10/01 04:10:53 AM: Update 18939: task edges-dep-ud-ewt, batch 939 (18939): mcc: 0.7935, acc: 0.6904, precision: 0.8721, recall: 0.7289, f1: 0.7941, edges-dep-ud-ewt_loss: 0.0209
10/01 04:11:03 AM: Update 18990: task edges-dep-ud-ewt, batch 990 (18990): mcc: 0.7937, acc: 0.6907, precision: 0.8722, recall: 0.7292, f1: 0.7943, edges-dep-ud-ewt_loss: 0.0209
10/01 04:11:05 AM: ***** Step 19000 / Validation 19 *****
10/01 04:11:05 AM: edges-dep-ud-ewt: trained on 1000 batches, 2.551 epochs
10/01 04:11:05 AM: Validating...
10/01 04:11:14 AM: Evaluate: task edges-dep-ud-ewt, batch 42 (63): mcc: 0.8687, acc: 0.8026, precision: 0.9198, recall: 0.8253, f1: 0.8700, edges-dep-ud-ewt_loss: 0.0151
10/01 04:11:17 AM: Best result seen so far for edges-dep-ud-ewt.
10/01 04:11:17 AM: Best result seen so far for macro.
10/01 04:11:17 AM: Updating LR scheduler:
10/01 04:11:17 AM: 	Best result seen so far for macro_avg: 0.869
10/01 04:11:17 AM: 	# validation passes without improvement: 0
10/01 04:11:17 AM: edges-dep-ud-ewt_loss: training: 0.020869 validation: 0.015413
10/01 04:11:17 AM: macro_avg: validation: 0.869476
10/01 04:11:17 AM: micro_avg: validation: 0.000000
10/01 04:11:17 AM: edges-dep-ud-ewt_mcc: training: 0.793799 validation: 0.868216
10/01 04:11:17 AM: edges-dep-ud-ewt_acc: training: 0.690834 validation: 0.802708
10/01 04:11:17 AM: edges-dep-ud-ewt_precision: training: 0.872288 validation: 0.918958
10/01 04:11:17 AM: edges-dep-ud-ewt_recall: training: 0.729281 validation: 0.825050
10/01 04:11:17 AM: edges-dep-ud-ewt_f1: training: 0.794400 validation: 0.869476
10/01 04:11:17 AM: Global learning rate: 0.0001
10/01 04:11:17 AM: Saving checkpoints to: ./experiments/dep-ud-ewt-sstseed2-top/run
10/01 04:11:24 AM: Update 19039: task edges-dep-ud-ewt, batch 39 (19039): mcc: 0.8013, acc: 0.7025, precision: 0.8748, recall: 0.7407, f1: 0.8022, edges-dep-ud-ewt_loss: 0.0205
10/01 04:11:34 AM: Update 19097: task edges-dep-ud-ewt, batch 97 (19097): mcc: 0.7999, acc: 0.7000, precision: 0.8746, recall: 0.7384, f1: 0.8007, edges-dep-ud-ewt_loss: 0.0200
10/01 04:11:44 AM: Update 19154: task edges-dep-ud-ewt, batch 154 (19154): mcc: 0.7976, acc: 0.6963, precision: 0.8739, recall: 0.7347, f1: 0.7983, edges-dep-ud-ewt_loss: 0.0205
10/01 04:11:57 AM: Update 19209: task edges-dep-ud-ewt, batch 209 (19209): mcc: 0.7920, acc: 0.6886, precision: 0.8714, recall: 0.7267, f1: 0.7925, edges-dep-ud-ewt_loss: 0.0214
10/01 04:12:07 AM: Update 19258: task edges-dep-ud-ewt, batch 258 (19258): mcc: 0.7927, acc: 0.6894, precision: 0.8720, recall: 0.7275, f1: 0.7933, edges-dep-ud-ewt_loss: 0.0213
10/01 04:12:18 AM: Update 19319: task edges-dep-ud-ewt, batch 319 (19319): mcc: 0.7947, acc: 0.6921, precision: 0.8733, recall: 0.7300, f1: 0.7953, edges-dep-ud-ewt_loss: 0.0211
10/01 04:12:28 AM: Update 19383: task edges-dep-ud-ewt, batch 383 (19383): mcc: 0.7955, acc: 0.6934, precision: 0.8736, recall: 0.7313, f1: 0.7961, edges-dep-ud-ewt_loss: 0.0207
10/01 04:12:38 AM: Update 19444: task edges-dep-ud-ewt, batch 444 (19444): mcc: 0.7958, acc: 0.6935, precision: 0.8737, recall: 0.7316, f1: 0.7964, edges-dep-ud-ewt_loss: 0.0206
10/01 04:12:48 AM: Update 19503: task edges-dep-ud-ewt, batch 503 (19503): mcc: 0.7967, acc: 0.6947, precision: 0.8747, recall: 0.7325, f1: 0.7973, edges-dep-ud-ewt_loss: 0.0205
10/01 04:12:58 AM: Update 19556: task edges-dep-ud-ewt, batch 556 (19556): mcc: 0.7954, acc: 0.6930, precision: 0.8738, recall: 0.7309, f1: 0.7960, edges-dep-ud-ewt_loss: 0.0207
10/01 04:13:09 AM: Update 19601: task edges-dep-ud-ewt, batch 601 (19601): mcc: 0.7941, acc: 0.6912, precision: 0.8729, recall: 0.7292, f1: 0.7946, edges-dep-ud-ewt_loss: 0.0209
10/01 04:13:19 AM: Update 19659: task edges-dep-ud-ewt, batch 659 (19659): mcc: 0.7943, acc: 0.6916, precision: 0.8729, recall: 0.7296, f1: 0.7949, edges-dep-ud-ewt_loss: 0.0208
10/01 04:13:29 AM: Update 19724: task edges-dep-ud-ewt, batch 724 (19724): mcc: 0.7950, acc: 0.6926, precision: 0.8733, recall: 0.7307, f1: 0.7956, edges-dep-ud-ewt_loss: 0.0207
