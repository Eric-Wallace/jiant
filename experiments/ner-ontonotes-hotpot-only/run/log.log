09/09 03:49:08 PM: Git branch: master
09/09 03:49:08 PM: Git SHA: e26b1963b48bdbc7f001ca620a782c5a262276eb
09/09 03:49:10 PM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes- models/hotpot-only/",
  "exp_name": "experiments/ner-ontonotes- models/hotpot-only",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes- models/hotpot-only/run/log.log",
  "lr_patience": 5,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 20,
  "pretrain_tasks": "",
  "pretrained_dir": "models/hotpot",
  "pytorch_transformers_output_mode": "only",
  "remote_log_name": "experiments/ner-ontonotes- models/hotpot-only__run",
  "run_dir": "./experiments/ner-ontonotes- models/hotpot-only/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
09/09 03:49:10 PM: Saved config to ./experiments/ner-ontonotes- models/hotpot-only/run/params.conf
09/09 03:49:10 PM: Using random seed 1234
09/09 03:49:10 PM: Using GPU 0
09/09 03:49:10 PM: Loading tasks...
09/09 03:49:10 PM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes- models/hotpot-only/
09/09 03:49:10 PM: 	Creating task edges-ner-ontonotes from scratch.
09/09 03:49:12 PM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
09/09 03:49:12 PM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
09/09 03:49:13 PM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
09/09 03:49:13 PM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
09/09 03:49:13 PM: 	Finished loading tasks: edges-ner-ontonotes.
09/09 03:49:13 PM: 	Building vocab from scratch.
09/09 03:49:13 PM: 	Counting units for task edges-ner-ontonotes.
09/09 03:49:16 PM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
09/09 03:49:16 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /cliphomes/ewallac2/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:16 PM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
09/09 03:49:17 PM: 	Saved vocab to ./experiments/ner-ontonotes- models/hotpot-only/vocab
09/09 03:49:17 PM: Loading token dictionary from ./experiments/ner-ontonotes- models/hotpot-only/vocab.
09/09 03:49:17 PM: 	Loaded vocab from ./experiments/ner-ontonotes- models/hotpot-only/vocab
09/09 03:49:17 PM: 	Vocab namespace tokens: size 22840
09/09 03:49:17 PM: 	Vocab namespace chars: size 77
09/09 03:49:17 PM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
09/09 03:49:17 PM: 	Vocab namespace bert_uncased: size 30524
09/09 03:49:17 PM: 	Finished building vocab.
09/09 03:49:17 PM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
09/09 03:49:30 PM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes- models/hotpot-only/preproc/edges-ner-ontonotes__train_data
09/09 03:49:30 PM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
09/09 03:49:32 PM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes- models/hotpot-only/preproc/edges-ner-ontonotes__val_data
09/09 03:49:32 PM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
09/09 03:49:34 PM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes- models/hotpot-only/preproc/edges-ner-ontonotes__test_data
09/09 03:49:34 PM: 	Finished indexing tasks
09/09 03:49:34 PM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
09/09 03:49:34 PM: 	  Training on 
09/09 03:49:34 PM: 	  Evaluating on edges-ner-ontonotes
09/09 03:49:34 PM: 	Finished loading tasks in 23.665s
09/09 03:49:34 PM: 	 Tasks: ['edges-ner-ontonotes']
09/09 03:49:34 PM: Building model...
09/09 03:49:34 PM: Using BERT model (bert-base-uncased).
09/09 03:49:34 PM: LOADING A FUNETUNED MODEL from: 
09/09 03:49:34 PM: models/hotpot
09/09 03:49:34 PM: loading configuration file models/hotpot/config.json
09/09 03:49:34 PM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

09/09 03:49:34 PM: loading weights file models/hotpot/pytorch_model.bin
09/09 03:49:39 PM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmppuarez8c
09/09 03:49:39 PM: copying /tmp/tmppuarez8c to cache at ./experiments/ner-ontonotes- models/hotpot-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:39 PM: creating metadata file for ./experiments/ner-ontonotes- models/hotpot-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:39 PM: removing temp file /tmp/tmppuarez8c
09/09 03:49:39 PM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes- models/hotpot-only/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
09/09 03:49:39 PM: Initializing parameters
09/09 03:49:39 PM: Done initializing parameters; the following parameters are using their default initialization from their code
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.position_embeddings.weight
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
09/09 03:49:39 PM:    _text_field_embedder.model.embeddings.word_embeddings.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
09/09 03:49:39 PM:    _text_field_embedder.model.pooler.dense.bias
09/09 03:49:39 PM:    _text_field_embedder.model.pooler.dense.weight
09/09 03:49:39 PM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
09/09 03:49:45 PM: Model specification:
09/09 03:49:45 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): BertLayerNorm()
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): BertLayerNorm()
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
09/09 03:49:45 PM: Model parameters:
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
09/09 03:49:45 PM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
09/09 03:49:45 PM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
09/09 03:49:45 PM: Total number of parameters: 109688338 (1.09688e+08)
09/09 03:49:45 PM: Number of trainable parameters: 206098 (206098)
09/09 03:49:45 PM: Finished building model in 11.627s
09/09 03:49:45 PM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

09/09 03:49:52 PM: patience = 20
09/09 03:49:52 PM: val_interval = 1000
09/09 03:49:52 PM: max_vals = 250
09/09 03:49:52 PM: cuda_device = 0
09/09 03:49:52 PM: grad_norm = 5.0
09/09 03:49:52 PM: grad_clipping = None
09/09 03:49:52 PM: lr_decay = 0.99
09/09 03:49:52 PM: min_lr = 1e-06
09/09 03:49:52 PM: keep_all_checkpoints = 0
09/09 03:49:52 PM: val_data_limit = 5000
09/09 03:49:52 PM: max_epochs = -1
09/09 03:49:52 PM: dec_val_scale = 250
09/09 03:49:52 PM: training_data_fraction = 1
09/09 03:49:52 PM: type = adam
09/09 03:49:52 PM: parameter_groups = None
09/09 03:49:52 PM: Number of trainable parameters: 206098
09/09 03:49:52 PM: infer_type_and_cast = True
09/09 03:49:52 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:52 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:52 PM: lr = 0.0001
09/09 03:49:52 PM: amsgrad = True
09/09 03:49:52 PM: type = reduce_on_plateau
09/09 03:49:52 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:52 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:52 PM: mode = max
09/09 03:49:52 PM: factor = 0.5
09/09 03:49:52 PM: patience = 5
09/09 03:49:52 PM: threshold = 0.0001
09/09 03:49:52 PM: threshold_mode = abs
09/09 03:49:52 PM: verbose = True
09/09 03:49:52 PM: type = adam
09/09 03:49:52 PM: parameter_groups = None
09/09 03:49:52 PM: Number of trainable parameters: 206098
09/09 03:49:52 PM: infer_type_and_cast = True
09/09 03:49:52 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:52 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:52 PM: lr = 0.0001
09/09 03:49:52 PM: amsgrad = True
09/09 03:49:52 PM: type = reduce_on_plateau
09/09 03:49:52 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
09/09 03:49:52 PM: CURRENTLY DEFINED PARAMETERS: 
09/09 03:49:52 PM: mode = max
09/09 03:49:52 PM: factor = 0.5
09/09 03:49:52 PM: patience = 5
09/09 03:49:52 PM: threshold = 0.0001
09/09 03:49:52 PM: threshold_mode = abs
09/09 03:49:52 PM: verbose = True
09/09 03:49:52 PM: Starting training without restoring from a checkpoint.
09/09 03:49:52 PM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
09/09 03:49:52 PM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
09/09 03:50:02 PM: Update 247: task edges-ner-ontonotes, batch 247 (247): mcc: 0.3670, acc: 0.3322, precision: 0.4228, recall: 0.3789, f1: 0.3996, edges-ner-ontonotes_loss: 0.1978
09/09 03:50:12 PM: Update 395: task edges-ner-ontonotes, batch 395 (395): mcc: 0.5046, acc: 0.4508, precision: 0.5839, recall: 0.4800, f1: 0.5269, edges-ner-ontonotes_loss: 0.1543
09/09 03:50:22 PM: Update 631: task edges-ner-ontonotes, batch 631 (631): mcc: 0.6217, acc: 0.5572, precision: 0.7101, recall: 0.5778, f1: 0.6371, edges-ner-ontonotes_loss: 0.1209
09/09 03:50:32 PM: Update 917: task edges-ner-ontonotes, batch 917 (917): mcc: 0.6858, acc: 0.6166, precision: 0.7715, recall: 0.6379, f1: 0.6984, edges-ner-ontonotes_loss: 0.1008
09/09 03:50:37 PM: ***** Step 1000 / Validation 1 *****
09/09 03:50:37 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:50:37 PM: Validating...
09/09 03:50:42 PM: Evaluate: task edges-ner-ontonotes, batch 120 (157): mcc: 0.8063, acc: 0.7363, precision: 0.8837, recall: 0.7536, f1: 0.8135, edges-ner-ontonotes_loss: 0.0615
09/09 03:50:43 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:50:43 PM: Best result seen so far for micro.
09/09 03:50:43 PM: Best result seen so far for macro.
09/09 03:50:43 PM: Updating LR scheduler:
09/09 03:50:43 PM: 	Best result seen so far for macro_avg: 0.823
09/09 03:50:43 PM: 	# validation passes without improvement: 0
09/09 03:50:43 PM: edges-ner-ontonotes_loss: training: 0.096971 validation: 0.058104
09/09 03:50:43 PM: macro_avg: validation: 0.822596
09/09 03:50:43 PM: micro_avg: validation: 0.000000
09/09 03:50:43 PM: edges-ner-ontonotes_mcc: training: 0.697062 validation: 0.815418
09/09 03:50:43 PM: edges-ner-ontonotes_acc: training: 0.627433 validation: 0.748180
09/09 03:50:43 PM: edges-ner-ontonotes_precision: training: 0.781453 validation: 0.887815
09/09 03:50:43 PM: edges-ner-ontonotes_recall: training: 0.649160 validation: 0.766303
09/09 03:50:43 PM: edges-ner-ontonotes_f1: training: 0.709190 validation: 0.822596
09/09 03:50:43 PM: Global learning rate: 0.0001
09/09 03:50:43 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:50:52 PM: Update 1243: task edges-ner-ontonotes, batch 243 (1243): mcc: 0.8298, acc: 0.7638, precision: 0.8907, recall: 0.7895, f1: 0.8371, edges-ner-ontonotes_loss: 0.0487
09/09 03:51:02 PM: Update 1474: task edges-ner-ontonotes, batch 474 (1474): mcc: 0.8176, acc: 0.7515, precision: 0.8856, recall: 0.7722, f1: 0.8250, edges-ner-ontonotes_loss: 0.0547
09/09 03:51:12 PM: Update 1759: task edges-ner-ontonotes, batch 759 (1759): mcc: 0.8261, acc: 0.7635, precision: 0.8927, recall: 0.7811, f1: 0.8332, edges-ner-ontonotes_loss: 0.0529
09/09 03:51:22 PM: Update 2000: task edges-ner-ontonotes, batch 1000 (2000): mcc: 0.8323, acc: 0.7729, precision: 0.8962, recall: 0.7892, f1: 0.8393, edges-ner-ontonotes_loss: 0.0509
09/09 03:51:22 PM: ***** Step 2000 / Validation 2 *****
09/09 03:51:22 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:51:22 PM: Validating...
09/09 03:51:27 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:51:27 PM: Best result seen so far for macro.
09/09 03:51:27 PM: Updating LR scheduler:
09/09 03:51:27 PM: 	Best result seen so far for macro_avg: 0.844
09/09 03:51:27 PM: 	# validation passes without improvement: 0
09/09 03:51:27 PM: edges-ner-ontonotes_loss: training: 0.050910 validation: 0.049353
09/09 03:51:27 PM: macro_avg: validation: 0.843685
09/09 03:51:27 PM: micro_avg: validation: 0.000000
09/09 03:51:27 PM: edges-ner-ontonotes_mcc: training: 0.832332 validation: 0.836637
09/09 03:51:27 PM: edges-ner-ontonotes_acc: training: 0.772941 validation: 0.786548
09/09 03:51:27 PM: edges-ner-ontonotes_precision: training: 0.896193 validation: 0.895876
09/09 03:51:27 PM: edges-ner-ontonotes_recall: training: 0.789169 validation: 0.797240
09/09 03:51:27 PM: edges-ner-ontonotes_f1: training: 0.839283 validation: 0.843685
09/09 03:51:27 PM: Global learning rate: 0.0001
09/09 03:51:27 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:51:32 PM: Update 2146: task edges-ner-ontonotes, batch 146 (2146): mcc: 0.8581, acc: 0.8080, precision: 0.9119, recall: 0.8216, f1: 0.8644, edges-ner-ontonotes_loss: 0.0424
09/09 03:51:42 PM: Update 2370: task edges-ner-ontonotes, batch 370 (2370): mcc: 0.8502, acc: 0.7969, precision: 0.9004, recall: 0.8178, f1: 0.8571, edges-ner-ontonotes_loss: 0.0432
09/09 03:51:52 PM: Update 2587: task edges-ner-ontonotes, batch 587 (2587): mcc: 0.8517, acc: 0.7985, precision: 0.8985, recall: 0.8222, f1: 0.8587, edges-ner-ontonotes_loss: 0.0425
09/09 03:52:02 PM: Update 2810: task edges-ner-ontonotes, batch 810 (2810): mcc: 0.8535, acc: 0.8004, precision: 0.8994, recall: 0.8247, f1: 0.8604, edges-ner-ontonotes_loss: 0.0416
09/09 03:52:08 PM: ***** Step 3000 / Validation 3 *****
09/09 03:52:08 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:52:08 PM: Validating...
09/09 03:52:12 PM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.8370, acc: 0.7863, precision: 0.8930, recall: 0.8005, f1: 0.8443, edges-ner-ontonotes_loss: 0.0497
09/09 03:52:13 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:52:13 PM: Best result seen so far for macro.
09/09 03:52:13 PM: Updating LR scheduler:
09/09 03:52:13 PM: 	Best result seen so far for macro_avg: 0.851
09/09 03:52:13 PM: 	# validation passes without improvement: 0
09/09 03:52:13 PM: edges-ner-ontonotes_loss: training: 0.043816 validation: 0.047267
09/09 03:52:13 PM: macro_avg: validation: 0.850506
09/09 03:52:13 PM: micro_avg: validation: 0.000000
09/09 03:52:13 PM: edges-ner-ontonotes_mcc: training: 0.850149 validation: 0.843619
09/09 03:52:13 PM: edges-ner-ontonotes_acc: training: 0.797032 validation: 0.793449
09/09 03:52:13 PM: edges-ner-ontonotes_precision: training: 0.897783 validation: 0.899602
09/09 03:52:13 PM: edges-ner-ontonotes_recall: training: 0.820088 validation: 0.806491
09/09 03:52:13 PM: edges-ner-ontonotes_f1: training: 0.857178 validation: 0.850506
09/09 03:52:13 PM: Global learning rate: 0.0001
09/09 03:52:13 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:52:22 PM: Update 3190: task edges-ner-ontonotes, batch 190 (3190): mcc: 0.8497, acc: 0.7998, precision: 0.9017, recall: 0.8155, f1: 0.8565, edges-ner-ontonotes_loss: 0.0476
09/09 03:52:32 PM: Update 3441: task edges-ner-ontonotes, batch 441 (3441): mcc: 0.8647, acc: 0.8207, precision: 0.9109, recall: 0.8344, f1: 0.8710, edges-ner-ontonotes_loss: 0.0424
09/09 03:52:44 PM: Update 3739: task edges-ner-ontonotes, batch 739 (3739): mcc: 0.8649, acc: 0.8202, precision: 0.9104, recall: 0.8354, f1: 0.8713, edges-ner-ontonotes_loss: 0.0415
09/09 03:52:53 PM: ***** Step 4000 / Validation 4 *****
09/09 03:52:53 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:52:53 PM: Validating...
09/09 03:52:54 PM: Evaluate: task edges-ner-ontonotes, batch 35 (157): mcc: 0.8158, acc: 0.7818, precision: 0.8606, recall: 0.7920, f1: 0.8249, edges-ner-ontonotes_loss: 0.0564
09/09 03:52:58 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:52:58 PM: Best result seen so far for macro.
09/09 03:52:58 PM: Updating LR scheduler:
09/09 03:52:58 PM: 	Best result seen so far for macro_avg: 0.861
09/09 03:52:58 PM: 	# validation passes without improvement: 0
09/09 03:52:58 PM: edges-ner-ontonotes_loss: training: 0.040796 validation: 0.045206
09/09 03:52:58 PM: macro_avg: validation: 0.860514
09/09 03:52:58 PM: micro_avg: validation: 0.000000
09/09 03:52:58 PM: edges-ner-ontonotes_mcc: training: 0.863830 validation: 0.853292
09/09 03:52:58 PM: edges-ner-ontonotes_acc: training: 0.817956 validation: 0.810358
09/09 03:52:58 PM: edges-ner-ontonotes_precision: training: 0.906958 validation: 0.893800
09/09 03:52:58 PM: edges-ner-ontonotes_recall: training: 0.836575 validation: 0.829618
09/09 03:52:58 PM: edges-ner-ontonotes_f1: training: 0.870346 validation: 0.860514
09/09 03:52:58 PM: Global learning rate: 0.0001
09/09 03:52:58 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:53:04 PM: Update 4129: task edges-ner-ontonotes, batch 129 (4129): mcc: 0.8622, acc: 0.8129, precision: 0.8991, recall: 0.8411, f1: 0.8691, edges-ner-ontonotes_loss: 0.0374
09/09 03:53:15 PM: Update 4365: task edges-ner-ontonotes, batch 365 (4365): mcc: 0.8653, acc: 0.8154, precision: 0.9027, recall: 0.8433, f1: 0.8720, edges-ner-ontonotes_loss: 0.0374
09/09 03:53:26 PM: Update 4669: task edges-ner-ontonotes, batch 669 (4669): mcc: 0.8565, acc: 0.8062, precision: 0.8990, recall: 0.8306, f1: 0.8635, edges-ner-ontonotes_loss: 0.0430
09/09 03:53:39 PM: Update 4982: task edges-ner-ontonotes, batch 982 (4982): mcc: 0.8632, acc: 0.8167, precision: 0.9042, recall: 0.8381, f1: 0.8699, edges-ner-ontonotes_loss: 0.0410
09/09 03:53:40 PM: ***** Step 5000 / Validation 5 *****
09/09 03:53:40 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:53:40 PM: Validating...
09/09 03:53:45 PM: Updating LR scheduler:
09/09 03:53:45 PM: 	Best result seen so far for macro_avg: 0.861
09/09 03:53:45 PM: 	# validation passes without improvement: 1
09/09 03:53:45 PM: edges-ner-ontonotes_loss: training: 0.041086 validation: 0.044324
09/09 03:53:45 PM: macro_avg: validation: 0.856529
09/09 03:53:45 PM: micro_avg: validation: 0.000000
09/09 03:53:45 PM: edges-ner-ontonotes_mcc: training: 0.862910 validation: 0.849703
09/09 03:53:45 PM: edges-ner-ontonotes_acc: training: 0.816330 validation: 0.807022
09/09 03:53:45 PM: edges-ner-ontonotes_precision: training: 0.903888 validation: 0.901332
09/09 03:53:45 PM: edges-ner-ontonotes_recall: training: 0.837780 validation: 0.815969
09/09 03:53:45 PM: edges-ner-ontonotes_f1: training: 0.869580 validation: 0.856529
09/09 03:53:45 PM: Global learning rate: 0.0001
09/09 03:53:45 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:53:49 PM: Update 5127: task edges-ner-ontonotes, batch 127 (5127): mcc: 0.8608, acc: 0.8129, precision: 0.9045, recall: 0.8334, f1: 0.8675, edges-ner-ontonotes_loss: 0.0411
09/09 03:53:59 PM: Update 5382: task edges-ner-ontonotes, batch 382 (5382): mcc: 0.8683, acc: 0.8235, precision: 0.9070, recall: 0.8447, f1: 0.8747, edges-ner-ontonotes_loss: 0.0384
09/09 03:54:10 PM: Update 5608: task edges-ner-ontonotes, batch 608 (5608): mcc: 0.8693, acc: 0.8240, precision: 0.9060, recall: 0.8476, f1: 0.8758, edges-ner-ontonotes_loss: 0.0376
09/09 03:54:20 PM: Update 5896: task edges-ner-ontonotes, batch 896 (5896): mcc: 0.8688, acc: 0.8227, precision: 0.9047, recall: 0.8478, f1: 0.8753, edges-ner-ontonotes_loss: 0.0372
09/09 03:54:25 PM: ***** Step 6000 / Validation 6 *****
09/09 03:54:25 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:54:25 PM: Validating...
09/09 03:54:30 PM: Evaluate: task edges-ner-ontonotes, batch 145 (157): mcc: 0.8548, acc: 0.8131, precision: 0.9054, recall: 0.8215, f1: 0.8614, edges-ner-ontonotes_loss: 0.0452
09/09 03:54:30 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:54:30 PM: Best result seen so far for macro.
09/09 03:54:30 PM: Updating LR scheduler:
09/09 03:54:30 PM: 	Best result seen so far for macro_avg: 0.862
09/09 03:54:30 PM: 	# validation passes without improvement: 0
09/09 03:54:30 PM: edges-ner-ontonotes_loss: training: 0.038213 validation: 0.044649
09/09 03:54:30 PM: macro_avg: validation: 0.861984
09/09 03:54:30 PM: micro_avg: validation: 0.000000
09/09 03:54:30 PM: edges-ner-ontonotes_mcc: training: 0.866520 validation: 0.855435
09/09 03:54:30 PM: edges-ner-ontonotes_acc: training: 0.819961 validation: 0.813694
09/09 03:54:30 PM: edges-ner-ontonotes_precision: training: 0.903433 validation: 0.906475
09/09 03:54:30 PM: edges-ner-ontonotes_recall: training: 0.844875 validation: 0.821656
09/09 03:54:30 PM: edges-ner-ontonotes_f1: training: 0.873173 validation: 0.861984
09/09 03:54:30 PM: Global learning rate: 0.0001
09/09 03:54:30 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:54:41 PM: Update 6225: task edges-ner-ontonotes, batch 225 (6225): mcc: 0.8509, acc: 0.8026, precision: 0.8965, recall: 0.8227, f1: 0.8580, edges-ner-ontonotes_loss: 0.0475
09/09 03:54:53 PM: Update 6538: task edges-ner-ontonotes, batch 538 (6538): mcc: 0.8700, acc: 0.8286, precision: 0.9093, recall: 0.8457, f1: 0.8764, edges-ner-ontonotes_loss: 0.0404
09/09 03:55:03 PM: Update 6826: task edges-ner-ontonotes, batch 826 (6826): mcc: 0.8717, acc: 0.8301, precision: 0.9101, recall: 0.8482, f1: 0.8780, edges-ner-ontonotes_loss: 0.0392
09/09 03:55:11 PM: ***** Step 7000 / Validation 7 *****
09/09 03:55:11 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:55:11 PM: Validating...
09/09 03:55:13 PM: Evaluate: task edges-ner-ontonotes, batch 37 (157): mcc: 0.8284, acc: 0.7952, precision: 0.8667, recall: 0.8094, f1: 0.8371, edges-ner-ontonotes_loss: 0.0533
09/09 03:55:17 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:55:17 PM: Best result seen so far for macro.
09/09 03:55:17 PM: Updating LR scheduler:
09/09 03:55:17 PM: 	Best result seen so far for macro_avg: 0.865
09/09 03:55:17 PM: 	# validation passes without improvement: 0
09/09 03:55:17 PM: edges-ner-ontonotes_loss: training: 0.038784 validation: 0.044144
09/09 03:55:17 PM: macro_avg: validation: 0.864545
09/09 03:55:17 PM: micro_avg: validation: 0.000000
09/09 03:55:17 PM: edges-ner-ontonotes_mcc: training: 0.870488 validation: 0.857349
09/09 03:55:17 PM: edges-ner-ontonotes_acc: training: 0.827714 validation: 0.815893
09/09 03:55:17 PM: edges-ner-ontonotes_precision: training: 0.907896 validation: 0.893392
09/09 03:55:17 PM: edges-ner-ontonotes_recall: training: 0.847952 validation: 0.837504
09/09 03:55:17 PM: edges-ner-ontonotes_f1: training: 0.876901 validation: 0.864545
09/09 03:55:17 PM: Global learning rate: 0.0001
09/09 03:55:17 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:55:25 PM: Update 7164: task edges-ner-ontonotes, batch 164 (7164): mcc: 0.8721, acc: 0.8257, precision: 0.9039, recall: 0.8547, f1: 0.8786, edges-ner-ontonotes_loss: 0.0356
09/09 03:55:35 PM: Update 7451: task edges-ner-ontonotes, batch 451 (7451): mcc: 0.8712, acc: 0.8246, precision: 0.9042, recall: 0.8528, f1: 0.8778, edges-ner-ontonotes_loss: 0.0357
09/09 03:55:45 PM: Update 7703: task edges-ner-ontonotes, batch 703 (7703): mcc: 0.8642, acc: 0.8168, precision: 0.9009, recall: 0.8430, f1: 0.8710, edges-ner-ontonotes_loss: 0.0397
09/09 03:55:55 PM: Update 7951: task edges-ner-ontonotes, batch 951 (7951): mcc: 0.8670, acc: 0.8212, precision: 0.9036, recall: 0.8456, f1: 0.8736, edges-ner-ontonotes_loss: 0.0393
09/09 03:55:57 PM: ***** Step 8000 / Validation 8 *****
09/09 03:55:57 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:55:57 PM: Validating...
09/09 03:56:01 PM: Updating LR scheduler:
09/09 03:56:01 PM: 	Best result seen so far for macro_avg: 0.865
09/09 03:56:01 PM: 	# validation passes without improvement: 1
09/09 03:56:01 PM: edges-ner-ontonotes_loss: training: 0.038980 validation: 0.044878
09/09 03:56:01 PM: macro_avg: validation: 0.856518
09/09 03:56:01 PM: micro_avg: validation: 0.000000
09/09 03:56:01 PM: edges-ner-ontonotes_mcc: training: 0.868281 validation: 0.849600
09/09 03:56:01 PM: edges-ner-ontonotes_acc: training: 0.823025 validation: 0.808766
09/09 03:56:01 PM: edges-ner-ontonotes_precision: training: 0.904707 validation: 0.899741
09/09 03:56:01 PM: edges-ner-ontonotes_recall: training: 0.846915 validation: 0.817258
09/09 03:56:01 PM: edges-ner-ontonotes_f1: training: 0.874858 validation: 0.856518
09/09 03:56:01 PM: Global learning rate: 0.0001
09/09 03:56:01 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:56:07 PM: Update 8094: task edges-ner-ontonotes, batch 94 (8094): mcc: 0.8941, acc: 0.8585, precision: 0.9250, recall: 0.8753, f1: 0.8994, edges-ner-ontonotes_loss: 0.0331
09/09 03:56:17 PM: Update 8392: task edges-ner-ontonotes, batch 392 (8392): mcc: 0.8794, acc: 0.8406, precision: 0.9127, recall: 0.8599, f1: 0.8855, edges-ner-ontonotes_loss: 0.0359
09/09 03:56:27 PM: Update 8614: task edges-ner-ontonotes, batch 614 (8614): mcc: 0.8757, acc: 0.8341, precision: 0.9079, recall: 0.8576, f1: 0.8821, edges-ner-ontonotes_loss: 0.0361
09/09 03:56:37 PM: Update 8864: task edges-ner-ontonotes, batch 864 (8864): mcc: 0.8747, acc: 0.8315, precision: 0.9068, recall: 0.8567, f1: 0.8811, edges-ner-ontonotes_loss: 0.0358
09/09 03:56:41 PM: ***** Step 9000 / Validation 9 *****
09/09 03:56:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:56:41 PM: Validating...
09/09 03:56:47 PM: Evaluate: task edges-ner-ontonotes, batch 144 (157): mcc: 0.8588, acc: 0.8156, precision: 0.8992, recall: 0.8346, f1: 0.8657, edges-ner-ontonotes_loss: 0.0445
09/09 03:56:47 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:56:47 PM: Best result seen so far for macro.
09/09 03:56:47 PM: Updating LR scheduler:
09/09 03:56:47 PM: 	Best result seen so far for macro_avg: 0.867
09/09 03:56:47 PM: 	# validation passes without improvement: 0
09/09 03:56:47 PM: edges-ner-ontonotes_loss: training: 0.035705 validation: 0.043792
09/09 03:56:47 PM: macro_avg: validation: 0.867254
09/09 03:56:47 PM: micro_avg: validation: 0.000000
09/09 03:56:47 PM: edges-ner-ontonotes_mcc: training: 0.875037 validation: 0.860413
09/09 03:56:47 PM: edges-ner-ontonotes_acc: training: 0.831721 validation: 0.817789
09/09 03:56:47 PM: edges-ner-ontonotes_precision: training: 0.907153 validation: 0.900686
09/09 03:56:47 PM: edges-ner-ontonotes_recall: training: 0.857087 validation: 0.836215
09/09 03:56:47 PM: edges-ner-ontonotes_f1: training: 0.881410 validation: 0.867254
09/09 03:56:47 PM: Global learning rate: 0.0001
09/09 03:56:47 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:56:57 PM: Update 9223: task edges-ner-ontonotes, batch 223 (9223): mcc: 0.8565, acc: 0.8102, precision: 0.8962, recall: 0.8334, f1: 0.8636, edges-ner-ontonotes_loss: 0.0447
09/09 03:57:07 PM: Update 9501: task edges-ner-ontonotes, batch 501 (9501): mcc: 0.8648, acc: 0.8207, precision: 0.9029, recall: 0.8421, f1: 0.8715, edges-ner-ontonotes_loss: 0.0417
09/09 03:57:17 PM: Update 9747: task edges-ner-ontonotes, batch 747 (9747): mcc: 0.8704, acc: 0.8283, precision: 0.9073, recall: 0.8484, f1: 0.8769, edges-ner-ontonotes_loss: 0.0397
09/09 03:57:27 PM: Update 9985: task edges-ner-ontonotes, batch 985 (9985): mcc: 0.8730, acc: 0.8320, precision: 0.9088, recall: 0.8518, f1: 0.8794, edges-ner-ontonotes_loss: 0.0387
09/09 03:57:27 PM: ***** Step 10000 / Validation 10 *****
09/09 03:57:27 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:57:27 PM: Validating...
09/09 03:57:32 PM: Updating LR scheduler:
09/09 03:57:32 PM: 	Best result seen so far for macro_avg: 0.867
09/09 03:57:32 PM: 	# validation passes without improvement: 1
09/09 03:57:32 PM: edges-ner-ontonotes_loss: training: 0.038614 validation: 0.043422
09/09 03:57:32 PM: macro_avg: validation: 0.863322
09/09 03:57:32 PM: micro_avg: validation: 0.000000
09/09 03:57:32 PM: edges-ner-ontonotes_mcc: training: 0.873072 validation: 0.855957
09/09 03:57:32 PM: edges-ner-ontonotes_acc: training: 0.831947 validation: 0.820367
09/09 03:57:32 PM: edges-ner-ontonotes_precision: training: 0.908748 validation: 0.889756
09/09 03:57:32 PM: edges-ner-ontonotes_recall: training: 0.851917 validation: 0.838414
09/09 03:57:32 PM: edges-ner-ontonotes_f1: training: 0.879415 validation: 0.863322
09/09 03:57:32 PM: Global learning rate: 0.0001
09/09 03:57:32 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:57:37 PM: Update 10130: task edges-ner-ontonotes, batch 130 (10130): mcc: 0.8732, acc: 0.8269, precision: 0.9041, recall: 0.8566, f1: 0.8797, edges-ner-ontonotes_loss: 0.0357
09/09 03:57:47 PM: Update 10355: task edges-ner-ontonotes, batch 355 (10355): mcc: 0.8715, acc: 0.8252, precision: 0.9015, recall: 0.8560, f1: 0.8782, edges-ner-ontonotes_loss: 0.0352
09/09 03:57:57 PM: Update 10589: task edges-ner-ontonotes, batch 589 (10589): mcc: 0.8736, acc: 0.8280, precision: 0.9036, recall: 0.8579, f1: 0.8801, edges-ner-ontonotes_loss: 0.0348
09/09 03:58:07 PM: Update 10888: task edges-ner-ontonotes, batch 888 (10888): mcc: 0.8679, acc: 0.8220, precision: 0.9013, recall: 0.8494, f1: 0.8746, edges-ner-ontonotes_loss: 0.0385
09/09 03:58:12 PM: ***** Step 11000 / Validation 11 *****
09/09 03:58:12 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:58:12 PM: Validating...
09/09 03:58:17 PM: Evaluate: task edges-ner-ontonotes, batch 151 (157): mcc: 0.8528, acc: 0.8114, precision: 0.9021, recall: 0.8209, f1: 0.8596, edges-ner-ontonotes_loss: 0.0443
09/09 03:58:17 PM: Updating LR scheduler:
09/09 03:58:17 PM: 	Best result seen so far for macro_avg: 0.867
09/09 03:58:17 PM: 	# validation passes without improvement: 2
09/09 03:58:17 PM: edges-ner-ontonotes_loss: training: 0.038296 validation: 0.043916
09/09 03:58:17 PM: macro_avg: validation: 0.859536
09/09 03:58:17 PM: micro_avg: validation: 0.000000
09/09 03:58:17 PM: edges-ner-ontonotes_mcc: training: 0.869231 validation: 0.852743
09/09 03:58:17 PM: edges-ner-ontonotes_acc: training: 0.824057 validation: 0.811495
09/09 03:58:17 PM: edges-ner-ontonotes_precision: training: 0.902520 validation: 0.902083
09/09 03:58:17 PM: edges-ner-ontonotes_recall: training: 0.850770 validation: 0.820822
09/09 03:58:17 PM: edges-ner-ontonotes_f1: training: 0.875881 validation: 0.859536
09/09 03:58:17 PM: Global learning rate: 0.0001
09/09 03:58:17 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:58:27 PM: Update 11253: task edges-ner-ontonotes, batch 253 (11253): mcc: 0.8917, acc: 0.8563, precision: 0.9219, recall: 0.8739, f1: 0.8972, edges-ner-ontonotes_loss: 0.0324
09/09 03:58:39 PM: Update 11519: task edges-ner-ontonotes, batch 519 (11519): mcc: 0.8854, acc: 0.8483, precision: 0.9165, recall: 0.8673, f1: 0.8912, edges-ner-ontonotes_loss: 0.0341
09/09 03:58:49 PM: Update 11808: task edges-ner-ontonotes, batch 808 (11808): mcc: 0.8814, acc: 0.8415, precision: 0.9113, recall: 0.8648, f1: 0.8875, edges-ner-ontonotes_loss: 0.0344
09/09 03:58:57 PM: ***** Step 12000 / Validation 12 *****
09/09 03:58:57 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:58:57 PM: Validating...
09/09 03:58:59 PM: Evaluate: task edges-ner-ontonotes, batch 68 (157): mcc: 0.8568, acc: 0.8196, precision: 0.8981, recall: 0.8320, f1: 0.8638, edges-ner-ontonotes_loss: 0.0504
09/09 03:59:02 PM: Best result seen so far for edges-ner-ontonotes.
09/09 03:59:02 PM: Best result seen so far for macro.
09/09 03:59:02 PM: Updating LR scheduler:
09/09 03:59:02 PM: 	Best result seen so far for macro_avg: 0.870
09/09 03:59:02 PM: 	# validation passes without improvement: 0
09/09 03:59:02 PM: edges-ner-ontonotes_loss: training: 0.034444 validation: 0.043589
09/09 03:59:02 PM: macro_avg: validation: 0.869521
09/09 03:59:02 PM: micro_avg: validation: 0.000000
09/09 03:59:02 PM: edges-ner-ontonotes_mcc: training: 0.879758 validation: 0.862645
09/09 03:59:02 PM: edges-ner-ontonotes_acc: training: 0.839058 validation: 0.824082
09/09 03:59:02 PM: edges-ner-ontonotes_precision: training: 0.909563 validation: 0.899360
09/09 03:59:02 PM: edges-ner-ontonotes_recall: training: 0.863533 validation: 0.841598
09/09 03:59:02 PM: edges-ner-ontonotes_f1: training: 0.885951 validation: 0.869521
09/09 03:59:02 PM: Global learning rate: 0.0001
09/09 03:59:02 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:59:09 PM: Update 12145: task edges-ner-ontonotes, batch 145 (12145): mcc: 0.8783, acc: 0.8330, precision: 0.9093, recall: 0.8611, f1: 0.8845, edges-ner-ontonotes_loss: 0.0332
09/09 03:59:21 PM: Update 12449: task edges-ner-ontonotes, batch 449 (12449): mcc: 0.8629, acc: 0.8161, precision: 0.9001, recall: 0.8413, f1: 0.8697, edges-ner-ontonotes_loss: 0.0418
09/09 03:59:33 PM: Update 12762: task edges-ner-ontonotes, batch 762 (12762): mcc: 0.8739, acc: 0.8316, precision: 0.9077, recall: 0.8544, f1: 0.8802, edges-ner-ontonotes_loss: 0.0382
09/09 03:59:41 PM: ***** Step 13000 / Validation 13 *****
09/09 03:59:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 03:59:41 PM: Validating...
09/09 03:59:43 PM: Evaluate: task edges-ner-ontonotes, batch 63 (157): mcc: 0.8788, acc: 0.8502, precision: 0.9075, recall: 0.8638, f1: 0.8851, edges-ner-ontonotes_loss: 0.0400
09/09 03:59:46 PM: Updating LR scheduler:
09/09 03:59:46 PM: 	Best result seen so far for macro_avg: 0.870
09/09 03:59:46 PM: 	# validation passes without improvement: 1
09/09 03:59:46 PM: edges-ner-ontonotes_loss: training: 0.037661 validation: 0.042807
09/09 03:59:46 PM: macro_avg: validation: 0.866628
09/09 03:59:46 PM: micro_avg: validation: 0.000000
09/09 03:59:46 PM: edges-ner-ontonotes_mcc: training: 0.875110 validation: 0.859467
09/09 03:59:46 PM: edges-ner-ontonotes_acc: training: 0.833329 validation: 0.824992
09/09 03:59:46 PM: edges-ner-ontonotes_precision: training: 0.908731 validation: 0.893533
09/09 03:59:46 PM: edges-ner-ontonotes_recall: training: 0.855706 validation: 0.841295
09/09 03:59:46 PM: edges-ner-ontonotes_f1: training: 0.881422 validation: 0.866628
09/09 03:59:46 PM: Global learning rate: 0.0001
09/09 03:59:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 03:59:53 PM: Update 13143: task edges-ner-ontonotes, batch 143 (13143): mcc: 0.8778, acc: 0.8358, precision: 0.9071, recall: 0.8623, f1: 0.8842, edges-ner-ontonotes_loss: 0.0344
09/09 04:00:04 PM: Update 13388: task edges-ner-ontonotes, batch 388 (13388): mcc: 0.8762, acc: 0.8327, precision: 0.9039, recall: 0.8624, f1: 0.8827, edges-ner-ontonotes_loss: 0.0346
09/09 04:00:14 PM: Update 13679: task edges-ner-ontonotes, batch 679 (13679): mcc: 0.8761, acc: 0.8319, precision: 0.9047, recall: 0.8614, f1: 0.8825, edges-ner-ontonotes_loss: 0.0344
09/09 04:00:24 PM: Update 13935: task edges-ner-ontonotes, batch 935 (13935): mcc: 0.8707, acc: 0.8257, precision: 0.9018, recall: 0.8543, f1: 0.8774, edges-ner-ontonotes_loss: 0.0373
09/09 04:00:26 PM: ***** Step 14000 / Validation 14 *****
09/09 04:00:26 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:00:26 PM: Validating...
09/09 04:00:31 PM: Updating LR scheduler:
09/09 04:00:31 PM: 	Best result seen so far for macro_avg: 0.870
09/09 04:00:31 PM: 	# validation passes without improvement: 2
09/09 04:00:31 PM: edges-ner-ontonotes_loss: training: 0.037623 validation: 0.044778
09/09 04:00:31 PM: macro_avg: validation: 0.859735
09/09 04:00:31 PM: micro_avg: validation: 0.000000
09/09 04:00:31 PM: edges-ner-ontonotes_mcc: training: 0.870251 validation: 0.852845
09/09 04:00:31 PM: edges-ner-ontonotes_acc: training: 0.825102 validation: 0.813239
09/09 04:00:31 PM: edges-ner-ontonotes_precision: training: 0.901895 validation: 0.900332
09/09 04:00:31 PM: edges-ner-ontonotes_recall: training: 0.853265 validation: 0.822642
09/09 04:00:31 PM: edges-ner-ontonotes_f1: training: 0.876906 validation: 0.859735
09/09 04:00:31 PM: Global learning rate: 0.0001
09/09 04:00:31 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:00:34 PM: Update 14038: task edges-ner-ontonotes, batch 38 (14038): mcc: 0.8891, acc: 0.8508, precision: 0.9178, recall: 0.8729, f1: 0.8948, edges-ner-ontonotes_loss: 0.0345
09/09 04:00:44 PM: Update 14318: task edges-ner-ontonotes, batch 318 (14318): mcc: 0.8947, acc: 0.8616, precision: 0.9210, recall: 0.8803, f1: 0.9002, edges-ner-ontonotes_loss: 0.0328
09/09 04:00:54 PM: Update 14617: task edges-ner-ontonotes, batch 617 (14617): mcc: 0.8874, acc: 0.8509, precision: 0.9169, recall: 0.8708, f1: 0.8932, edges-ner-ontonotes_loss: 0.0339
09/09 04:01:04 PM: Update 14820: task edges-ner-ontonotes, batch 820 (14820): mcc: 0.8835, acc: 0.8446, precision: 0.9126, recall: 0.8675, f1: 0.8895, edges-ner-ontonotes_loss: 0.0341
09/09 04:01:12 PM: ***** Step 15000 / Validation 15 *****
09/09 04:01:12 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:01:12 PM: Validating...
09/09 04:01:14 PM: Evaluate: task edges-ner-ontonotes, batch 80 (157): mcc: 0.8588, acc: 0.8215, precision: 0.8971, recall: 0.8367, f1: 0.8658, edges-ner-ontonotes_loss: 0.0480
09/09 04:01:17 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:01:17 PM: Best result seen so far for macro.
09/09 04:01:17 PM: Updating LR scheduler:
09/09 04:01:17 PM: 	Best result seen so far for macro_avg: 0.870
09/09 04:01:17 PM: 	# validation passes without improvement: 3
09/09 04:01:17 PM: edges-ner-ontonotes_loss: training: 0.033983 validation: 0.043406
09/09 04:01:17 PM: macro_avg: validation: 0.869572
09/09 04:01:17 PM: micro_avg: validation: 0.000000
09/09 04:01:17 PM: edges-ner-ontonotes_mcc: training: 0.882420 validation: 0.862611
09/09 04:01:17 PM: edges-ner-ontonotes_acc: training: 0.842649 validation: 0.823931
09/09 04:01:17 PM: edges-ner-ontonotes_precision: training: 0.911393 validation: 0.897314
09/09 04:01:17 PM: edges-ner-ontonotes_recall: training: 0.866717 validation: 0.843494
09/09 04:01:17 PM: edges-ner-ontonotes_f1: training: 0.888494 validation: 0.869572
09/09 04:01:17 PM: Global learning rate: 0.0001
09/09 04:01:17 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:01:24 PM: Update 15212: task edges-ner-ontonotes, batch 212 (15212): mcc: 0.8773, acc: 0.8338, precision: 0.9049, recall: 0.8634, f1: 0.8837, edges-ner-ontonotes_loss: 0.0336
09/09 04:01:34 PM: Update 15451: task edges-ner-ontonotes, batch 451 (15451): mcc: 0.8686, acc: 0.8240, precision: 0.9012, recall: 0.8508, f1: 0.8753, edges-ner-ontonotes_loss: 0.0385
09/09 04:01:44 PM: Update 15698: task edges-ner-ontonotes, batch 698 (15698): mcc: 0.8706, acc: 0.8280, precision: 0.9030, recall: 0.8530, f1: 0.8772, edges-ner-ontonotes_loss: 0.0384
09/09 04:01:54 PM: Update 15950: task edges-ner-ontonotes, batch 950 (15950): mcc: 0.8751, acc: 0.8340, precision: 0.9069, recall: 0.8575, f1: 0.8815, edges-ner-ontonotes_loss: 0.0369
09/09 04:01:56 PM: ***** Step 16000 / Validation 16 *****
09/09 04:01:56 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:01:56 PM: Validating...
09/09 04:02:01 PM: Updating LR scheduler:
09/09 04:02:01 PM: 	Best result seen so far for macro_avg: 0.870
09/09 04:02:01 PM: 	# validation passes without improvement: 4
09/09 04:02:01 PM: edges-ner-ontonotes_loss: training: 0.036746 validation: 0.043308
09/09 04:02:01 PM: macro_avg: validation: 0.866839
09/09 04:02:01 PM: micro_avg: validation: 0.000000
09/09 04:02:01 PM: edges-ner-ontonotes_mcc: training: 0.875733 validation: 0.859878
09/09 04:02:01 PM: edges-ner-ontonotes_acc: training: 0.834633 validation: 0.825144
09/09 04:02:01 PM: edges-ner-ontonotes_precision: training: 0.907566 validation: 0.898130
09/09 04:02:01 PM: edges-ner-ontonotes_recall: training: 0.857982 validation: 0.837655
09/09 04:02:01 PM: edges-ner-ontonotes_f1: training: 0.882078 validation: 0.866839
09/09 04:02:01 PM: Global learning rate: 0.0001
09/09 04:02:01 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:02:04 PM: Update 16099: task edges-ner-ontonotes, batch 99 (16099): mcc: 0.8823, acc: 0.8440, precision: 0.9142, recall: 0.8638, f1: 0.8883, edges-ner-ontonotes_loss: 0.0354
09/09 04:02:14 PM: Update 16333: task edges-ner-ontonotes, batch 333 (16333): mcc: 0.8760, acc: 0.8346, precision: 0.9055, recall: 0.8604, f1: 0.8824, edges-ner-ontonotes_loss: 0.0353
09/09 04:02:24 PM: Update 16551: task edges-ner-ontonotes, batch 551 (16551): mcc: 0.8766, acc: 0.8340, precision: 0.9053, recall: 0.8618, f1: 0.8830, edges-ner-ontonotes_loss: 0.0347
09/09 04:02:34 PM: Update 16813: task edges-ner-ontonotes, batch 813 (16813): mcc: 0.8772, acc: 0.8339, precision: 0.9055, recall: 0.8628, f1: 0.8836, edges-ner-ontonotes_loss: 0.0343
09/09 04:02:41 PM: ***** Step 17000 / Validation 17 *****
09/09 04:02:41 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:02:41 PM: Validating...
09/09 04:02:44 PM: Evaluate: task edges-ner-ontonotes, batch 120 (157): mcc: 0.8496, acc: 0.8125, precision: 0.8941, recall: 0.8226, f1: 0.8568, edges-ner-ontonotes_loss: 0.0473
09/09 04:02:46 PM: Updating LR scheduler:
09/09 04:02:46 PM: 	Best result seen so far for macro_avg: 0.870
09/09 04:02:46 PM: 	# validation passes without improvement: 5
09/09 04:02:46 PM: edges-ner-ontonotes_loss: training: 0.036345 validation: 0.044645
09/09 04:02:46 PM: macro_avg: validation: 0.862997
09/09 04:02:46 PM: micro_avg: validation: 0.000000
09/09 04:02:46 PM: edges-ner-ontonotes_mcc: training: 0.873282 validation: 0.856109
09/09 04:02:46 PM: edges-ner-ontonotes_acc: training: 0.829544 validation: 0.819002
09/09 04:02:46 PM: edges-ner-ontonotes_precision: training: 0.903455 validation: 0.900247
09/09 04:02:46 PM: edges-ner-ontonotes_recall: training: 0.857393 validation: 0.828708
09/09 04:02:46 PM: edges-ner-ontonotes_f1: training: 0.879822 validation: 0.862997
09/09 04:02:46 PM: Global learning rate: 0.0001
09/09 04:02:46 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:02:54 PM: Update 17229: task edges-ner-ontonotes, batch 229 (17229): mcc: 0.8761, acc: 0.8362, precision: 0.9091, recall: 0.8571, f1: 0.8823, edges-ner-ontonotes_loss: 0.0381
09/09 04:03:04 PM: Update 17490: task edges-ner-ontonotes, batch 490 (17490): mcc: 0.8844, acc: 0.8465, precision: 0.9148, recall: 0.8671, f1: 0.8903, edges-ner-ontonotes_loss: 0.0353
09/09 04:03:16 PM: Update 17743: task edges-ner-ontonotes, batch 743 (17743): mcc: 0.8834, acc: 0.8454, precision: 0.9139, recall: 0.8661, f1: 0.8894, edges-ner-ontonotes_loss: 0.0350
09/09 04:03:25 PM: ***** Step 18000 / Validation 18 *****
09/09 04:03:25 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:03:25 PM: Validating...
09/09 04:03:26 PM: Evaluate: task edges-ner-ontonotes, batch 40 (157): mcc: 0.8401, acc: 0.8050, precision: 0.8734, recall: 0.8247, f1: 0.8483, edges-ner-ontonotes_loss: 0.0514
09/09 04:03:30 PM: Updating LR scheduler:
09/09 04:03:30 PM: 	Best result seen so far for macro_avg: 0.870
09/09 04:03:30 PM: 	# validation passes without improvement: 0
09/09 04:03:30 PM: edges-ner-ontonotes_loss: training: 0.034731 validation: 0.043728
09/09 04:03:30 PM: macro_avg: validation: 0.868379
09/09 04:03:30 PM: micro_avg: validation: 0.000000
09/09 04:03:30 PM: edges-ner-ontonotes_mcc: training: 0.881496 validation: 0.861244
09/09 04:03:30 PM: edges-ner-ontonotes_acc: training: 0.841748 validation: 0.820291
09/09 04:03:30 PM: edges-ner-ontonotes_precision: training: 0.910828 validation: 0.893416
09/09 04:03:30 PM: edges-ner-ontonotes_recall: training: 0.865543 validation: 0.844707
09/09 04:03:30 PM: edges-ner-ontonotes_f1: training: 0.887608 validation: 0.868379
09/09 04:03:30 PM: Global learning rate: 5e-05
09/09 04:03:30 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:03:36 PM: Update 18137: task edges-ner-ontonotes, batch 137 (18137): mcc: 0.8714, acc: 0.8257, precision: 0.8992, recall: 0.8580, f1: 0.8781, edges-ner-ontonotes_loss: 0.0346
09/09 04:03:46 PM: Update 18369: task edges-ner-ontonotes, batch 369 (18369): mcc: 0.8752, acc: 0.8296, precision: 0.9032, recall: 0.8612, f1: 0.8817, edges-ner-ontonotes_loss: 0.0340
09/09 04:03:58 PM: Update 18673: task edges-ner-ontonotes, batch 673 (18673): mcc: 0.8680, acc: 0.8228, precision: 0.9002, recall: 0.8506, f1: 0.8747, edges-ner-ontonotes_loss: 0.0388
09/09 04:04:10 PM: Update 18986: task edges-ner-ontonotes, batch 986 (18986): mcc: 0.8755, acc: 0.8333, precision: 0.9061, recall: 0.8589, f1: 0.8819, edges-ner-ontonotes_loss: 0.0366
09/09 04:04:10 PM: ***** Step 19000 / Validation 19 *****
09/09 04:04:10 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:04:10 PM: Validating...
09/09 04:04:15 PM: Updating LR scheduler:
09/09 04:04:15 PM: 	Best result seen so far for macro_avg: 0.870
09/09 04:04:15 PM: 	# validation passes without improvement: 1
09/09 04:04:15 PM: edges-ner-ontonotes_loss: training: 0.036575 validation: 0.043033
09/09 04:04:15 PM: macro_avg: validation: 0.862907
09/09 04:04:15 PM: micro_avg: validation: 0.000000
09/09 04:04:15 PM: edges-ner-ontonotes_mcc: training: 0.875474 validation: 0.856213
09/09 04:04:15 PM: edges-ner-ontonotes_acc: training: 0.833344 validation: 0.815817
09/09 04:04:15 PM: edges-ner-ontonotes_precision: training: 0.906103 validation: 0.903928
09/09 04:04:15 PM: edges-ner-ontonotes_recall: training: 0.858911 validation: 0.825447
09/09 04:04:15 PM: edges-ner-ontonotes_f1: training: 0.881876 validation: 0.862907
09/09 04:04:15 PM: Global learning rate: 5e-05
09/09 04:04:15 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:04:20 PM: Update 19135: task edges-ner-ontonotes, batch 135 (19135): mcc: 0.8759, acc: 0.8352, precision: 0.9085, recall: 0.8574, f1: 0.8822, edges-ner-ontonotes_loss: 0.0349
09/09 04:04:30 PM: Update 19357: task edges-ner-ontonotes, batch 357 (19357): mcc: 0.8778, acc: 0.8366, precision: 0.9090, recall: 0.8605, f1: 0.8841, edges-ner-ontonotes_loss: 0.0345
09/09 04:04:41 PM: Update 19612: task edges-ner-ontonotes, batch 612 (19612): mcc: 0.8790, acc: 0.8376, precision: 0.9079, recall: 0.8637, f1: 0.8853, edges-ner-ontonotes_loss: 0.0338
09/09 04:04:51 PM: Update 19904: task edges-ner-ontonotes, batch 904 (19904): mcc: 0.8777, acc: 0.8342, precision: 0.9068, recall: 0.8624, f1: 0.8841, edges-ner-ontonotes_loss: 0.0338
09/09 04:04:55 PM: ***** Step 20000 / Validation 20 *****
09/09 04:04:55 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:04:55 PM: Validating...
09/09 04:05:00 PM: Updating LR scheduler:
09/09 04:05:00 PM: 	Best result seen so far for macro_avg: 0.870
09/09 04:05:00 PM: 	# validation passes without improvement: 2
09/09 04:05:00 PM: edges-ner-ontonotes_loss: training: 0.034890 validation: 0.042385
09/09 04:05:00 PM: macro_avg: validation: 0.868630
09/09 04:05:00 PM: micro_avg: validation: 0.000000
09/09 04:05:00 PM: edges-ner-ontonotes_mcc: training: 0.875616 validation: 0.862130
09/09 04:05:00 PM: edges-ner-ontonotes_acc: training: 0.831953 validation: 0.823021
09/09 04:05:00 PM: edges-ner-ontonotes_precision: training: 0.905084 validation: 0.907409
09/09 04:05:00 PM: edges-ner-ontonotes_recall: training: 0.860160 validation: 0.833030
09/09 04:05:00 PM: edges-ner-ontonotes_f1: training: 0.882050 validation: 0.868630
09/09 04:05:00 PM: Global learning rate: 5e-05
09/09 04:05:00 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:05:01 PM: Update 20019: task edges-ner-ontonotes, batch 19 (20019): mcc: 0.8395, acc: 0.7853, precision: 0.8847, recall: 0.8129, f1: 0.8473, edges-ner-ontonotes_loss: 0.0468
09/09 04:05:11 PM: Update 20270: task edges-ner-ontonotes, batch 270 (20270): mcc: 0.8625, acc: 0.8183, precision: 0.9003, recall: 0.8404, f1: 0.8693, edges-ner-ontonotes_loss: 0.0421
09/09 04:05:22 PM: Update 20542: task edges-ner-ontonotes, batch 542 (20542): mcc: 0.8791, acc: 0.8402, precision: 0.9112, recall: 0.8608, f1: 0.8853, edges-ner-ontonotes_loss: 0.0367
09/09 04:05:32 PM: Update 20843: task edges-ner-ontonotes, batch 843 (20843): mcc: 0.8811, acc: 0.8428, precision: 0.9123, recall: 0.8634, f1: 0.8872, edges-ner-ontonotes_loss: 0.0357
09/09 04:05:39 PM: ***** Step 21000 / Validation 21 *****
09/09 04:05:39 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:05:39 PM: Validating...
09/09 04:05:42 PM: Evaluate: task edges-ner-ontonotes, batch 108 (157): mcc: 0.8590, acc: 0.8221, precision: 0.8985, recall: 0.8357, f1: 0.8659, edges-ner-ontonotes_loss: 0.0465
09/09 04:05:44 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:05:44 PM: Best result seen so far for macro.
09/09 04:05:44 PM: Updating LR scheduler:
09/09 04:05:44 PM: 	Best result seen so far for macro_avg: 0.873
09/09 04:05:44 PM: 	# validation passes without improvement: 0
09/09 04:05:44 PM: edges-ner-ontonotes_loss: training: 0.035445 validation: 0.042533
09/09 04:05:44 PM: macro_avg: validation: 0.872847
09/09 04:05:44 PM: micro_avg: validation: 0.000000
09/09 04:05:44 PM: edges-ner-ontonotes_mcc: training: 0.880389 validation: 0.866053
09/09 04:05:44 PM: edges-ner-ontonotes_acc: training: 0.841080 validation: 0.828556
09/09 04:05:44 PM: edges-ner-ontonotes_precision: training: 0.910615 validation: 0.900169
09/09 04:05:44 PM: edges-ner-ontonotes_recall: training: 0.863691 validation: 0.847134
09/09 04:05:44 PM: edges-ner-ontonotes_f1: training: 0.886533 validation: 0.872847
09/09 04:05:44 PM: Global learning rate: 5e-05
09/09 04:05:44 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:05:52 PM: Update 21177: task edges-ner-ontonotes, batch 177 (21177): mcc: 0.8772, acc: 0.8324, precision: 0.9061, recall: 0.8621, f1: 0.8835, edges-ner-ontonotes_loss: 0.0335
09/09 04:06:02 PM: Update 21467: task edges-ner-ontonotes, batch 467 (21467): mcc: 0.8776, acc: 0.8325, precision: 0.9060, recall: 0.8629, f1: 0.8839, edges-ner-ontonotes_loss: 0.0337
09/09 04:06:12 PM: Update 21710: task edges-ner-ontonotes, batch 710 (21710): mcc: 0.8716, acc: 0.8264, precision: 0.9032, recall: 0.8546, f1: 0.8782, edges-ner-ontonotes_loss: 0.0371
09/09 04:06:22 PM: Update 21961: task edges-ner-ontonotes, batch 961 (21961): mcc: 0.8736, acc: 0.8300, precision: 0.9048, recall: 0.8567, f1: 0.8801, edges-ner-ontonotes_loss: 0.0368
09/09 04:06:23 PM: ***** Step 22000 / Validation 22 *****
09/09 04:06:23 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:06:23 PM: Validating...
09/09 04:06:28 PM: Updating LR scheduler:
09/09 04:06:28 PM: 	Best result seen so far for macro_avg: 0.873
09/09 04:06:28 PM: 	# validation passes without improvement: 1
09/09 04:06:28 PM: edges-ner-ontonotes_loss: training: 0.036499 validation: 0.043100
09/09 04:06:28 PM: macro_avg: validation: 0.862432
09/09 04:06:28 PM: micro_avg: validation: 0.000000
09/09 04:06:28 PM: edges-ner-ontonotes_mcc: training: 0.874778 validation: 0.855603
09/09 04:06:28 PM: edges-ner-ontonotes_acc: training: 0.831607 validation: 0.818850
09/09 04:06:28 PM: edges-ner-ontonotes_precision: training: 0.905758 validation: 0.901439
09/09 04:06:28 PM: edges-ner-ontonotes_recall: training: 0.857950 validation: 0.826661
09/09 04:06:28 PM: edges-ner-ontonotes_f1: training: 0.881206 validation: 0.862432
09/09 04:06:28 PM: Global learning rate: 5e-05
09/09 04:06:28 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:06:33 PM: Update 22098: task edges-ner-ontonotes, batch 98 (22098): mcc: 0.9017, acc: 0.8702, precision: 0.9261, recall: 0.8883, f1: 0.9068, edges-ner-ontonotes_loss: 0.0302
09/09 04:06:43 PM: Update 22395: task edges-ner-ontonotes, batch 395 (22395): mcc: 0.8872, acc: 0.8512, precision: 0.9172, recall: 0.8699, f1: 0.8930, edges-ner-ontonotes_loss: 0.0333
09/09 04:06:54 PM: Update 22622: task edges-ner-ontonotes, batch 622 (22622): mcc: 0.8831, acc: 0.8438, precision: 0.9121, recall: 0.8673, f1: 0.8891, edges-ner-ontonotes_loss: 0.0335
09/09 04:07:04 PM: Update 22849: task edges-ner-ontonotes, batch 849 (22849): mcc: 0.8817, acc: 0.8408, precision: 0.9104, recall: 0.8664, f1: 0.8879, edges-ner-ontonotes_loss: 0.0333
09/09 04:07:09 PM: ***** Step 23000 / Validation 23 *****
09/09 04:07:09 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:07:09 PM: Validating...
09/09 04:07:14 PM: Evaluate: task edges-ner-ontonotes, batch 153 (157): mcc: 0.8657, acc: 0.8271, precision: 0.8990, recall: 0.8476, f1: 0.8725, edges-ner-ontonotes_loss: 0.0433
09/09 04:07:14 PM: Best result seen so far for edges-ner-ontonotes.
09/09 04:07:14 PM: Best result seen so far for macro.
09/09 04:07:14 PM: Updating LR scheduler:
09/09 04:07:14 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:07:14 PM: 	# validation passes without improvement: 0
09/09 04:07:14 PM: edges-ner-ontonotes_loss: training: 0.033331 validation: 0.042757
09/09 04:07:14 PM: macro_avg: validation: 0.873888
09/09 04:07:14 PM: micro_avg: validation: 0.000000
09/09 04:07:14 PM: edges-ner-ontonotes_mcc: training: 0.881340 validation: 0.867111
09/09 04:07:14 PM: edges-ner-ontonotes_acc: training: 0.839671 validation: 0.828708
09/09 04:07:14 PM: edges-ner-ontonotes_precision: training: 0.909888 validation: 0.900161
09/09 04:07:14 PM: edges-ner-ontonotes_recall: training: 0.866164 validation: 0.849105
09/09 04:07:14 PM: edges-ner-ontonotes_f1: training: 0.887488 validation: 0.873888
09/09 04:07:14 PM: Global learning rate: 5e-05
09/09 04:07:14 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:07:24 PM: Update 23260: task edges-ner-ontonotes, batch 260 (23260): mcc: 0.8593, acc: 0.8133, precision: 0.8962, recall: 0.8385, f1: 0.8664, edges-ner-ontonotes_loss: 0.0434
09/09 04:07:34 PM: Update 23526: task edges-ner-ontonotes, batch 526 (23526): mcc: 0.8725, acc: 0.8308, precision: 0.9055, recall: 0.8540, f1: 0.8790, edges-ner-ontonotes_loss: 0.0389
09/09 04:07:44 PM: Update 23776: task edges-ner-ontonotes, batch 776 (23776): mcc: 0.8768, acc: 0.8364, precision: 0.9088, recall: 0.8588, f1: 0.8831, edges-ner-ontonotes_loss: 0.0370
09/09 04:07:54 PM: Update 23987: task edges-ner-ontonotes, batch 987 (23987): mcc: 0.8778, acc: 0.8375, precision: 0.9094, recall: 0.8601, f1: 0.8841, edges-ner-ontonotes_loss: 0.0363
09/09 04:07:54 PM: ***** Step 24000 / Validation 24 *****
09/09 04:07:54 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:07:54 PM: Validating...
09/09 04:07:59 PM: Updating LR scheduler:
09/09 04:07:59 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:07:59 PM: 	# validation passes without improvement: 1
09/09 04:07:59 PM: edges-ner-ontonotes_loss: training: 0.036309 validation: 0.042097
09/09 04:07:59 PM: macro_avg: validation: 0.871587
09/09 04:07:59 PM: micro_avg: validation: 0.000000
09/09 04:07:59 PM: edges-ner-ontonotes_mcc: training: 0.877715 validation: 0.864560
09/09 04:07:59 PM: edges-ner-ontonotes_acc: training: 0.837312 validation: 0.825902
09/09 04:07:59 PM: edges-ner-ontonotes_precision: training: 0.909081 validation: 0.894619
09/09 04:07:59 PM: edges-ner-ontonotes_recall: training: 0.860202 validation: 0.849712
09/09 04:07:59 PM: edges-ner-ontonotes_f1: training: 0.883967 validation: 0.871587
09/09 04:07:59 PM: Global learning rate: 5e-05
09/09 04:07:59 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:08:04 PM: Update 24129: task edges-ner-ontonotes, batch 129 (24129): mcc: 0.8773, acc: 0.8311, precision: 0.9020, recall: 0.8664, f1: 0.8838, edges-ner-ontonotes_loss: 0.0338
09/09 04:08:14 PM: Update 24381: task edges-ner-ontonotes, batch 381 (24381): mcc: 0.8782, acc: 0.8321, precision: 0.9044, recall: 0.8657, f1: 0.8846, edges-ner-ontonotes_loss: 0.0332
09/09 04:08:24 PM: Update 24618: task edges-ner-ontonotes, batch 618 (24618): mcc: 0.8775, acc: 0.8320, precision: 0.9047, recall: 0.8640, f1: 0.8839, edges-ner-ontonotes_loss: 0.0337
09/09 04:08:35 PM: Update 24897: task edges-ner-ontonotes, batch 897 (24897): mcc: 0.8726, acc: 0.8270, precision: 0.9028, recall: 0.8567, f1: 0.8792, edges-ner-ontonotes_loss: 0.0367
09/09 04:08:38 PM: ***** Step 25000 / Validation 25 *****
09/09 04:08:38 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:08:38 PM: Validating...
09/09 04:08:43 PM: Updating LR scheduler:
09/09 04:08:43 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:08:43 PM: 	# validation passes without improvement: 2
09/09 04:08:43 PM: edges-ner-ontonotes_loss: training: 0.036248 validation: 0.043098
09/09 04:08:43 PM: macro_avg: validation: 0.863829
09/09 04:08:43 PM: micro_avg: validation: 0.000000
09/09 04:08:43 PM: edges-ner-ontonotes_mcc: training: 0.874504 validation: 0.857231
09/09 04:08:43 PM: edges-ner-ontonotes_acc: training: 0.829973 validation: 0.817182
09/09 04:08:43 PM: edges-ner-ontonotes_precision: training: 0.904297 validation: 0.905681
09/09 04:08:43 PM: edges-ner-ontonotes_recall: training: 0.858851 validation: 0.825675
09/09 04:08:43 PM: edges-ner-ontonotes_f1: training: 0.880989 validation: 0.863829
09/09 04:08:43 PM: Global learning rate: 5e-05
09/09 04:08:43 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:08:45 PM: Update 25063: task edges-ner-ontonotes, batch 63 (25063): mcc: 0.8962, acc: 0.8633, precision: 0.9254, recall: 0.8789, f1: 0.9015, edges-ner-ontonotes_loss: 0.0319
09/09 04:08:55 PM: Update 25308: task edges-ner-ontonotes, batch 308 (25308): mcc: 0.8906, acc: 0.8564, precision: 0.9192, recall: 0.8744, f1: 0.8962, edges-ner-ontonotes_loss: 0.0323
09/09 04:09:05 PM: Update 25544: task edges-ner-ontonotes, batch 544 (25544): mcc: 0.8885, acc: 0.8524, precision: 0.9172, recall: 0.8724, f1: 0.8942, edges-ner-ontonotes_loss: 0.0328
09/09 04:09:15 PM: Update 25833: task edges-ner-ontonotes, batch 833 (25833): mcc: 0.8855, acc: 0.8465, precision: 0.9134, recall: 0.8706, f1: 0.8914, edges-ner-ontonotes_loss: 0.0329
09/09 04:09:23 PM: ***** Step 26000 / Validation 26 *****
09/09 04:09:23 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:09:23 PM: Validating...
09/09 04:09:25 PM: Evaluate: task edges-ner-ontonotes, batch 70 (157): mcc: 0.8616, acc: 0.8254, precision: 0.9006, recall: 0.8384, f1: 0.8684, edges-ner-ontonotes_loss: 0.0481
09/09 04:09:28 PM: Updating LR scheduler:
09/09 04:09:28 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:09:28 PM: 	# validation passes without improvement: 3
09/09 04:09:28 PM: edges-ner-ontonotes_loss: training: 0.033056 validation: 0.042819
09/09 04:09:28 PM: macro_avg: validation: 0.871431
09/09 04:09:28 PM: micro_avg: validation: 0.000000
09/09 04:09:28 PM: edges-ner-ontonotes_mcc: training: 0.883688 validation: 0.864719
09/09 04:09:28 PM: edges-ner-ontonotes_acc: training: 0.843517 validation: 0.825220
09/09 04:09:28 PM: edges-ner-ontonotes_precision: training: 0.911746 validation: 0.902584
09/09 04:09:28 PM: edges-ner-ontonotes_recall: training: 0.868735 validation: 0.842357
09/09 04:09:28 PM: edges-ner-ontonotes_f1: training: 0.889721 validation: 0.871431
09/09 04:09:28 PM: Global learning rate: 5e-05
09/09 04:09:28 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:09:35 PM: Update 26167: task edges-ner-ontonotes, batch 167 (26167): mcc: 0.8766, acc: 0.8308, precision: 0.9061, recall: 0.8610, f1: 0.8830, edges-ner-ontonotes_loss: 0.0345
09/09 04:09:46 PM: Update 26453: task edges-ner-ontonotes, batch 453 (26453): mcc: 0.8680, acc: 0.8225, precision: 0.9021, recall: 0.8489, f1: 0.8747, edges-ner-ontonotes_loss: 0.0401
09/09 04:09:58 PM: Update 26766: task edges-ner-ontonotes, batch 766 (26766): mcc: 0.8787, acc: 0.8378, precision: 0.9092, recall: 0.8619, f1: 0.8849, edges-ner-ontonotes_loss: 0.0365
09/09 04:10:06 PM: ***** Step 27000 / Validation 27 *****
09/09 04:10:06 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:10:06 PM: Validating...
09/09 04:10:08 PM: Evaluate: task edges-ner-ontonotes, batch 56 (157): mcc: 0.8757, acc: 0.8506, precision: 0.9045, recall: 0.8608, f1: 0.8821, edges-ner-ontonotes_loss: 0.0406
09/09 04:10:11 PM: Updating LR scheduler:
09/09 04:10:11 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:10:11 PM: 	# validation passes without improvement: 4
09/09 04:10:11 PM: edges-ner-ontonotes_loss: training: 0.035932 validation: 0.042509
09/09 04:10:11 PM: macro_avg: validation: 0.868854
09/09 04:10:11 PM: micro_avg: validation: 0.000000
09/09 04:10:11 PM: edges-ner-ontonotes_mcc: training: 0.879811 validation: 0.862083
09/09 04:10:11 PM: edges-ner-ontonotes_acc: training: 0.839828 validation: 0.826585
09/09 04:10:11 PM: edges-ner-ontonotes_precision: training: 0.910343 validation: 0.901852
09/09 04:10:11 PM: edges-ner-ontonotes_recall: training: 0.862879 validation: 0.838186
09/09 04:10:11 PM: edges-ner-ontonotes_f1: training: 0.885976 validation: 0.868854
09/09 04:10:11 PM: Global learning rate: 5e-05
09/09 04:10:11 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:10:18 PM: Update 27130: task edges-ner-ontonotes, batch 130 (27130): mcc: 0.8808, acc: 0.8410, precision: 0.9082, recall: 0.8667, f1: 0.8870, edges-ner-ontonotes_loss: 0.0343
09/09 04:10:28 PM: Update 27392: task edges-ner-ontonotes, batch 392 (27392): mcc: 0.8797, acc: 0.8366, precision: 0.9063, recall: 0.8666, f1: 0.8860, edges-ner-ontonotes_loss: 0.0336
09/09 04:10:39 PM: Update 27682: task edges-ner-ontonotes, batch 682 (27682): mcc: 0.8795, acc: 0.8359, precision: 0.9059, recall: 0.8666, f1: 0.8858, edges-ner-ontonotes_loss: 0.0334
09/09 04:10:49 PM: Update 27922: task edges-ner-ontonotes, batch 922 (27922): mcc: 0.8755, acc: 0.8318, precision: 0.9043, recall: 0.8607, f1: 0.8820, edges-ner-ontonotes_loss: 0.0357
09/09 04:10:51 PM: ***** Step 28000 / Validation 28 *****
09/09 04:10:51 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:10:51 PM: Validating...
09/09 04:10:57 PM: Updating LR scheduler:
09/09 04:10:57 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:10:57 PM: 	# validation passes without improvement: 5
09/09 04:10:57 PM: edges-ner-ontonotes_loss: training: 0.036433 validation: 0.043540
09/09 04:10:57 PM: macro_avg: validation: 0.864837
09/09 04:10:57 PM: micro_avg: validation: 0.000000
09/09 04:10:57 PM: edges-ner-ontonotes_mcc: training: 0.874257 validation: 0.858211
09/09 04:10:57 PM: edges-ner-ontonotes_acc: training: 0.830675 validation: 0.819836
09/09 04:10:57 PM: edges-ner-ontonotes_precision: training: 0.903246 validation: 0.905165
09/09 04:10:57 PM: edges-ner-ontonotes_recall: training: 0.859410 validation: 0.827950
09/09 04:10:57 PM: edges-ner-ontonotes_f1: training: 0.880783 validation: 0.864837
09/09 04:10:57 PM: Global learning rate: 5e-05
09/09 04:10:57 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:10:59 PM: Update 28009: task edges-ner-ontonotes, batch 9 (28009): mcc: 0.8470, acc: 0.7935, precision: 0.8991, recall: 0.8132, f1: 0.8540, edges-ner-ontonotes_loss: 0.0450
09/09 04:11:11 PM: Update 28322: task edges-ner-ontonotes, batch 322 (28322): mcc: 0.8969, acc: 0.8636, precision: 0.9219, recall: 0.8834, f1: 0.9023, edges-ner-ontonotes_loss: 0.0318
09/09 04:11:21 PM: Update 28621: task edges-ner-ontonotes, batch 621 (28621): mcc: 0.8895, acc: 0.8534, precision: 0.9169, recall: 0.8745, f1: 0.8952, edges-ner-ontonotes_loss: 0.0330
09/09 04:11:31 PM: Update 28870: task edges-ner-ontonotes, batch 870 (28870): mcc: 0.8856, acc: 0.8471, precision: 0.9127, recall: 0.8713, f1: 0.8915, edges-ner-ontonotes_loss: 0.0331
09/09 04:11:37 PM: ***** Step 29000 / Validation 29 *****
09/09 04:11:37 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:11:37 PM: Validating...
09/09 04:11:41 PM: Evaluate: task edges-ner-ontonotes, batch 87 (157): mcc: 0.8666, acc: 0.8322, precision: 0.9037, recall: 0.8448, f1: 0.8733, edges-ner-ontonotes_loss: 0.0457
09/09 04:11:43 PM: Updating LR scheduler:
09/09 04:11:43 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:11:43 PM: 	# validation passes without improvement: 0
09/09 04:11:43 PM: edges-ner-ontonotes_loss: training: 0.033201 validation: 0.042894
09/09 04:11:43 PM: macro_avg: validation: 0.869977
09/09 04:11:43 PM: micro_avg: validation: 0.000000
09/09 04:11:43 PM: edges-ner-ontonotes_mcc: training: 0.884237 validation: 0.863083
09/09 04:11:43 PM: edges-ner-ontonotes_acc: training: 0.844907 validation: 0.825599
09/09 04:11:43 PM: edges-ner-ontonotes_precision: training: 0.911288 validation: 0.898779
09/09 04:11:43 PM: edges-ner-ontonotes_recall: training: 0.870203 validation: 0.842963
09/09 04:11:43 PM: edges-ner-ontonotes_f1: training: 0.890272 validation: 0.869977
09/09 04:11:43 PM: Global learning rate: 2.5e-05
09/09 04:11:43 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:11:51 PM: Update 29224: task edges-ner-ontonotes, batch 224 (29224): mcc: 0.8788, acc: 0.8345, precision: 0.9063, recall: 0.8649, f1: 0.8851, edges-ner-ontonotes_loss: 0.0335
09/09 04:12:01 PM: Update 29457: task edges-ner-ontonotes, batch 457 (29457): mcc: 0.8705, acc: 0.8253, precision: 0.9020, recall: 0.8536, f1: 0.8772, edges-ner-ontonotes_loss: 0.0381
09/09 04:12:11 PM: Update 29742: task edges-ner-ontonotes, batch 742 (29742): mcc: 0.8743, acc: 0.8315, precision: 0.9051, recall: 0.8576, f1: 0.8807, edges-ner-ontonotes_loss: 0.0373
09/09 04:12:21 PM: Update 29985: task edges-ner-ontonotes, batch 985 (29985): mcc: 0.8777, acc: 0.8367, precision: 0.9085, recall: 0.8608, f1: 0.8840, edges-ner-ontonotes_loss: 0.0362
09/09 04:12:21 PM: ***** Step 30000 / Validation 30 *****
09/09 04:12:21 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:12:21 PM: Validating...
09/09 04:12:26 PM: Updating LR scheduler:
09/09 04:12:26 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:12:26 PM: 	# validation passes without improvement: 1
09/09 04:12:26 PM: edges-ner-ontonotes_loss: training: 0.036105 validation: 0.042047
09/09 04:12:26 PM: macro_avg: validation: 0.869948
09/09 04:12:26 PM: micro_avg: validation: 0.000000
09/09 04:12:26 PM: edges-ner-ontonotes_mcc: training: 0.878007 validation: 0.863214
09/09 04:12:26 PM: edges-ner-ontonotes_acc: training: 0.837093 validation: 0.827343
09/09 04:12:26 PM: edges-ner-ontonotes_precision: training: 0.908784 validation: 0.902453
09/09 04:12:26 PM: edges-ner-ontonotes_recall: training: 0.861032 validation: 0.839703
09/09 04:12:26 PM: edges-ner-ontonotes_f1: training: 0.884264 validation: 0.869948
09/09 04:12:26 PM: Global learning rate: 2.5e-05
09/09 04:12:26 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:12:31 PM: Update 30139: task edges-ner-ontonotes, batch 139 (30139): mcc: 0.8845, acc: 0.8496, precision: 0.9116, recall: 0.8703, f1: 0.8905, edges-ner-ontonotes_loss: 0.0341
09/09 04:12:41 PM: Update 30362: task edges-ner-ontonotes, batch 362 (30362): mcc: 0.8810, acc: 0.8406, precision: 0.9087, recall: 0.8666, f1: 0.8872, edges-ner-ontonotes_loss: 0.0339
09/09 04:12:51 PM: Update 30583: task edges-ner-ontonotes, batch 583 (30583): mcc: 0.8810, acc: 0.8394, precision: 0.9076, recall: 0.8678, f1: 0.8873, edges-ner-ontonotes_loss: 0.0336
09/09 04:13:01 PM: Update 30817: task edges-ner-ontonotes, batch 817 (30817): mcc: 0.8804, acc: 0.8380, precision: 0.9076, recall: 0.8667, f1: 0.8867, edges-ner-ontonotes_loss: 0.0335
09/09 04:13:07 PM: ***** Step 31000 / Validation 31 *****
09/09 04:13:07 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:13:07 PM: Validating...
09/09 04:13:11 PM: Evaluate: task edges-ner-ontonotes, batch 124 (157): mcc: 0.8546, acc: 0.8166, precision: 0.9006, recall: 0.8256, f1: 0.8615, edges-ner-ontonotes_loss: 0.0448
09/09 04:13:12 PM: Updating LR scheduler:
09/09 04:13:12 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:13:12 PM: 	# validation passes without improvement: 2
09/09 04:13:12 PM: edges-ner-ontonotes_loss: training: 0.035240 validation: 0.042510
09/09 04:13:12 PM: macro_avg: validation: 0.867684
09/09 04:13:12 PM: micro_avg: validation: 0.000000
09/09 04:13:12 PM: edges-ner-ontonotes_mcc: training: 0.876802 validation: 0.861144
09/09 04:13:12 PM: edges-ner-ontonotes_acc: training: 0.833763 validation: 0.823324
09/09 04:13:12 PM: edges-ner-ontonotes_precision: training: 0.905859 validation: 0.906694
09/09 04:13:12 PM: edges-ner-ontonotes_recall: training: 0.861618 validation: 0.831893
09/09 04:13:12 PM: edges-ner-ontonotes_f1: training: 0.883185 validation: 0.867684
09/09 04:13:12 PM: Global learning rate: 2.5e-05
09/09 04:13:12 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:13:21 PM: Update 31211: task edges-ner-ontonotes, batch 211 (31211): mcc: 0.8704, acc: 0.8278, precision: 0.9044, recall: 0.8512, f1: 0.8770, edges-ner-ontonotes_loss: 0.0396
09/09 04:13:31 PM: Update 31468: task edges-ner-ontonotes, batch 468 (31468): mcc: 0.8841, acc: 0.8461, precision: 0.9138, recall: 0.8675, f1: 0.8900, edges-ner-ontonotes_loss: 0.0350
09/09 04:13:42 PM: Update 31747: task edges-ner-ontonotes, batch 747 (31747): mcc: 0.8843, acc: 0.8464, precision: 0.9139, recall: 0.8678, f1: 0.8903, edges-ner-ontonotes_loss: 0.0345
09/09 04:13:50 PM: ***** Step 32000 / Validation 32 *****
09/09 04:13:50 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:13:50 PM: Validating...
09/09 04:13:52 PM: Evaluate: task edges-ner-ontonotes, batch 49 (157): mcc: 0.8519, acc: 0.8224, precision: 0.8855, recall: 0.8349, f1: 0.8595, edges-ner-ontonotes_loss: 0.0472
09/09 04:13:56 PM: Updating LR scheduler:
09/09 04:13:56 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:13:56 PM: 	# validation passes without improvement: 3
09/09 04:13:56 PM: edges-ner-ontonotes_loss: training: 0.034153 validation: 0.042598
09/09 04:13:56 PM: macro_avg: validation: 0.871360
09/09 04:13:56 PM: micro_avg: validation: 0.000000
09/09 04:13:56 PM: edges-ner-ontonotes_mcc: training: 0.882655 validation: 0.864505
09/09 04:13:56 PM: edges-ner-ontonotes_acc: training: 0.842805 validation: 0.826812
09/09 04:13:56 PM: edges-ner-ontonotes_precision: training: 0.911596 validation: 0.899234
09/09 04:13:56 PM: edges-ner-ontonotes_recall: training: 0.866957 validation: 0.845162
09/09 04:13:56 PM: edges-ner-ontonotes_f1: training: 0.888716 validation: 0.871360
09/09 04:13:56 PM: Global learning rate: 2.5e-05
09/09 04:13:56 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:14:02 PM: Update 32094: task edges-ner-ontonotes, batch 94 (32094): mcc: 0.8817, acc: 0.8368, precision: 0.9082, recall: 0.8685, f1: 0.8879, edges-ner-ontonotes_loss: 0.0328
09/09 04:14:13 PM: Update 32373: task edges-ner-ontonotes, batch 373 (32373): mcc: 0.8796, acc: 0.8336, precision: 0.9063, recall: 0.8664, f1: 0.8859, edges-ner-ontonotes_loss: 0.0330
09/09 04:14:24 PM: Update 32677: task edges-ner-ontonotes, batch 677 (32677): mcc: 0.8718, acc: 0.8262, precision: 0.9031, recall: 0.8550, f1: 0.8784, edges-ner-ontonotes_loss: 0.0378
09/09 04:14:37 PM: Update 32990: task edges-ner-ontonotes, batch 990 (32990): mcc: 0.8787, acc: 0.8364, precision: 0.9084, recall: 0.8627, f1: 0.8850, edges-ner-ontonotes_loss: 0.0357
09/09 04:14:37 PM: ***** Step 33000 / Validation 33 *****
09/09 04:14:37 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:14:37 PM: Validating...
09/09 04:14:42 PM: Updating LR scheduler:
09/09 04:14:42 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:14:42 PM: 	# validation passes without improvement: 4
09/09 04:14:42 PM: edges-ner-ontonotes_loss: training: 0.035666 validation: 0.042597
09/09 04:14:42 PM: macro_avg: validation: 0.865910
09/09 04:14:42 PM: micro_avg: validation: 0.000000
09/09 04:14:42 PM: edges-ner-ontonotes_mcc: training: 0.878539 validation: 0.859266
09/09 04:14:42 PM: edges-ner-ontonotes_acc: training: 0.836252 validation: 0.821201
09/09 04:14:42 PM: edges-ner-ontonotes_precision: training: 0.908281 validation: 0.904801
09/09 04:14:42 PM: edges-ner-ontonotes_recall: training: 0.862506 validation: 0.830224
09/09 04:14:42 PM: edges-ner-ontonotes_f1: training: 0.884802 validation: 0.865910
09/09 04:14:42 PM: Global learning rate: 2.5e-05
09/09 04:14:42 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:14:47 PM: Update 33139: task edges-ner-ontonotes, batch 139 (33139): mcc: 0.8734, acc: 0.8319, precision: 0.9078, recall: 0.8534, f1: 0.8798, edges-ner-ontonotes_loss: 0.0356
09/09 04:14:57 PM: Update 33366: task edges-ner-ontonotes, batch 366 (33366): mcc: 0.8800, acc: 0.8405, precision: 0.9100, recall: 0.8637, f1: 0.8862, edges-ner-ontonotes_loss: 0.0340
09/09 04:15:07 PM: Update 33617: task edges-ner-ontonotes, batch 617 (33617): mcc: 0.8805, acc: 0.8388, precision: 0.9084, recall: 0.8660, f1: 0.8867, edges-ner-ontonotes_loss: 0.0335
09/09 04:15:17 PM: Update 33905: task edges-ner-ontonotes, batch 905 (33905): mcc: 0.8798, acc: 0.8371, precision: 0.9076, recall: 0.8655, f1: 0.8861, edges-ner-ontonotes_loss: 0.0335
09/09 04:15:22 PM: ***** Step 34000 / Validation 34 *****
09/09 04:15:22 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:15:22 PM: Validating...
09/09 04:15:27 PM: Evaluate: task edges-ner-ontonotes, batch 155 (157): mcc: 0.8645, acc: 0.8282, precision: 0.9061, recall: 0.8385, f1: 0.8710, edges-ner-ontonotes_loss: 0.0423
09/09 04:15:27 PM: Updating LR scheduler:
09/09 04:15:27 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:15:27 PM: 	# validation passes without improvement: 5
09/09 04:15:27 PM: edges-ner-ontonotes_loss: training: 0.034353 validation: 0.042040
09/09 04:15:27 PM: macro_avg: validation: 0.871332
09/09 04:15:27 PM: micro_avg: validation: 0.000000
09/09 04:15:27 PM: edges-ner-ontonotes_mcc: training: 0.877391 validation: 0.864800
09/09 04:15:27 PM: edges-ner-ontonotes_acc: training: 0.834479 validation: 0.828480
09/09 04:15:27 PM: edges-ner-ontonotes_precision: training: 0.905830 validation: 0.906498
09/09 04:15:27 PM: edges-ner-ontonotes_recall: training: 0.862742 validation: 0.838793
09/09 04:15:27 PM: edges-ner-ontonotes_f1: training: 0.883761 validation: 0.871332
09/09 04:15:27 PM: Global learning rate: 2.5e-05
09/09 04:15:27 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:15:37 PM: Update 34253: task edges-ner-ontonotes, batch 253 (34253): mcc: 0.8630, acc: 0.8195, precision: 0.8989, recall: 0.8427, f1: 0.8699, edges-ner-ontonotes_loss: 0.0422
09/09 04:15:49 PM: Update 34546: task edges-ner-ontonotes, batch 546 (34546): mcc: 0.8811, acc: 0.8435, precision: 0.9110, recall: 0.8647, f1: 0.8872, edges-ner-ontonotes_loss: 0.0363
09/09 04:15:59 PM: Update 34842: task edges-ner-ontonotes, batch 842 (34842): mcc: 0.8817, acc: 0.8439, precision: 0.9117, recall: 0.8652, f1: 0.8878, edges-ner-ontonotes_loss: 0.0354
09/09 04:16:05 PM: ***** Step 35000 / Validation 35 *****
09/09 04:16:05 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:16:05 PM: Validating...
09/09 04:16:09 PM: Evaluate: task edges-ner-ontonotes, batch 111 (157): mcc: 0.8618, acc: 0.8250, precision: 0.9000, recall: 0.8394, f1: 0.8687, edges-ner-ontonotes_loss: 0.0455
09/09 04:16:11 PM: Updating LR scheduler:
09/09 04:16:11 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:16:11 PM: 	# validation passes without improvement: 0
09/09 04:16:11 PM: edges-ner-ontonotes_loss: training: 0.035154 validation: 0.042223
09/09 04:16:11 PM: macro_avg: validation: 0.871779
09/09 04:16:11 PM: micro_avg: validation: 0.000000
09/09 04:16:11 PM: edges-ner-ontonotes_mcc: training: 0.881176 validation: 0.864904
09/09 04:16:11 PM: edges-ner-ontonotes_acc: training: 0.842566 validation: 0.827495
09/09 04:16:11 PM: edges-ner-ontonotes_precision: training: 0.910391 validation: 0.898583
09/09 04:16:11 PM: edges-ner-ontonotes_recall: training: 0.865372 validation: 0.846527
09/09 04:16:11 PM: edges-ner-ontonotes_f1: training: 0.887311 validation: 0.871779
09/09 04:16:11 PM: Global learning rate: 1.25e-05
09/09 04:16:11 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:16:19 PM: Update 35172: task edges-ner-ontonotes, batch 172 (35172): mcc: 0.8781, acc: 0.8345, precision: 0.9041, recall: 0.8658, f1: 0.8846, edges-ner-ontonotes_loss: 0.0330
09/09 04:16:29 PM: Update 35459: task edges-ner-ontonotes, batch 459 (35459): mcc: 0.8777, acc: 0.8327, precision: 0.9051, recall: 0.8640, f1: 0.8840, edges-ner-ontonotes_loss: 0.0333
09/09 04:16:39 PM: Update 35722: task edges-ner-ontonotes, batch 722 (35722): mcc: 0.8719, acc: 0.8270, precision: 0.9016, recall: 0.8566, f1: 0.8785, edges-ner-ontonotes_loss: 0.0367
09/09 04:16:49 PM: Update 35995: task edges-ner-ontonotes, batch 995 (35995): mcc: 0.8752, acc: 0.8322, precision: 0.9047, recall: 0.8598, f1: 0.8817, edges-ner-ontonotes_loss: 0.0360
09/09 04:16:50 PM: ***** Step 36000 / Validation 36 *****
09/09 04:16:50 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:16:50 PM: Validating...
09/09 04:16:55 PM: Updating LR scheduler:
09/09 04:16:55 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:16:55 PM: 	# validation passes without improvement: 1
09/09 04:16:55 PM: edges-ner-ontonotes_loss: training: 0.035951 validation: 0.042441
09/09 04:16:55 PM: macro_avg: validation: 0.867396
09/09 04:16:55 PM: micro_avg: validation: 0.000000
09/09 04:16:55 PM: edges-ner-ontonotes_mcc: training: 0.875367 validation: 0.860753
09/09 04:16:55 PM: edges-ner-ontonotes_acc: training: 0.832366 validation: 0.823400
09/09 04:16:55 PM: edges-ner-ontonotes_precision: training: 0.904870 validation: 0.904719
09/09 04:16:55 PM: edges-ner-ontonotes_recall: training: 0.859904 validation: 0.833030
09/09 04:16:55 PM: edges-ner-ontonotes_f1: training: 0.881814 validation: 0.867396
09/09 04:16:55 PM: Global learning rate: 1.25e-05
09/09 04:16:55 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:17:01 PM: Update 36102: task edges-ner-ontonotes, batch 102 (36102): mcc: 0.8977, acc: 0.8636, precision: 0.9227, recall: 0.8843, f1: 0.9031, edges-ner-ontonotes_loss: 0.0309
09/09 04:17:11 PM: Update 36402: task edges-ner-ontonotes, batch 402 (36402): mcc: 0.8882, acc: 0.8513, precision: 0.9176, recall: 0.8714, f1: 0.8939, edges-ner-ontonotes_loss: 0.0330
09/09 04:17:21 PM: Update 36652: task edges-ner-ontonotes, batch 652 (36652): mcc: 0.8845, acc: 0.8454, precision: 0.9127, recall: 0.8693, f1: 0.8905, edges-ner-ontonotes_loss: 0.0331
09/09 04:17:31 PM: Update 36859: task edges-ner-ontonotes, batch 859 (36859): mcc: 0.8827, acc: 0.8421, precision: 0.9105, recall: 0.8680, f1: 0.8888, edges-ner-ontonotes_loss: 0.0332
09/09 04:17:36 PM: ***** Step 37000 / Validation 37 *****
09/09 04:17:36 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:17:36 PM: Validating...
09/09 04:17:40 PM: Updating LR scheduler:
09/09 04:17:40 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:17:40 PM: 	# validation passes without improvement: 2
09/09 04:17:40 PM: edges-ner-ontonotes_loss: training: 0.033291 validation: 0.042417
09/09 04:17:40 PM: macro_avg: validation: 0.872248
09/09 04:17:40 PM: micro_avg: validation: 0.000000
09/09 04:17:40 PM: edges-ner-ontonotes_mcc: training: 0.881783 validation: 0.865459
09/09 04:17:40 PM: edges-ner-ontonotes_acc: training: 0.840350 validation: 0.826661
09/09 04:17:40 PM: edges-ner-ontonotes_precision: training: 0.909535 validation: 0.900525
09/09 04:17:40 PM: edges-ner-ontonotes_recall: training: 0.867331 validation: 0.845693
09/09 04:17:40 PM: edges-ner-ontonotes_f1: training: 0.887932 validation: 0.872248
09/09 04:17:40 PM: Global learning rate: 1.25e-05
09/09 04:17:40 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:17:41 PM: Update 37009: task edges-ner-ontonotes, batch 9 (37009): mcc: 0.8676, acc: 0.8166, precision: 0.8911, recall: 0.8589, f1: 0.8747, edges-ner-ontonotes_loss: 0.0351
09/09 04:17:51 PM: Update 37275: task edges-ner-ontonotes, batch 275 (37275): mcc: 0.8605, acc: 0.8143, precision: 0.8955, recall: 0.8413, f1: 0.8676, edges-ner-ontonotes_loss: 0.0417
09/09 04:18:01 PM: Update 37520: task edges-ner-ontonotes, batch 520 (37520): mcc: 0.8728, acc: 0.8314, precision: 0.9053, recall: 0.8548, f1: 0.8793, edges-ner-ontonotes_loss: 0.0384
09/09 04:18:11 PM: Update 37767: task edges-ner-ontonotes, batch 767 (37767): mcc: 0.8772, acc: 0.8378, precision: 0.9084, recall: 0.8599, f1: 0.8835, edges-ner-ontonotes_loss: 0.0366
09/09 04:18:20 PM: ***** Step 38000 / Validation 38 *****
09/09 04:18:20 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:18:20 PM: Validating...
09/09 04:18:21 PM: Evaluate: task edges-ner-ontonotes, batch 24 (157): mcc: 0.8237, acc: 0.7886, precision: 0.8616, recall: 0.8056, f1: 0.8326, edges-ner-ontonotes_loss: 0.0554
09/09 04:18:25 PM: Updating LR scheduler:
09/09 04:18:25 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:18:25 PM: 	# validation passes without improvement: 3
09/09 04:18:25 PM: edges-ner-ontonotes_loss: training: 0.035937 validation: 0.041838
09/09 04:18:25 PM: macro_avg: validation: 0.872238
09/09 04:18:25 PM: micro_avg: validation: 0.000000
09/09 04:18:25 PM: edges-ner-ontonotes_mcc: training: 0.878905 validation: 0.865604
09/09 04:18:25 PM: edges-ner-ontonotes_acc: training: 0.839886 validation: 0.830679
09/09 04:18:25 PM: edges-ner-ontonotes_precision: training: 0.910035 validation: 0.904142
09/09 04:18:25 PM: edges-ner-ontonotes_recall: training: 0.861493 validation: 0.842508
09/09 04:18:25 PM: edges-ner-ontonotes_f1: training: 0.885099 validation: 0.872238
09/09 04:18:25 PM: Global learning rate: 1.25e-05
09/09 04:18:25 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:18:31 PM: Update 38163: task edges-ner-ontonotes, batch 163 (38163): mcc: 0.8760, acc: 0.8321, precision: 0.9042, recall: 0.8617, f1: 0.8825, edges-ner-ontonotes_loss: 0.0340
09/09 04:18:41 PM: Update 38361: task edges-ner-ontonotes, batch 361 (38361): mcc: 0.8788, acc: 0.8349, precision: 0.9060, recall: 0.8652, f1: 0.8851, edges-ner-ontonotes_loss: 0.0332
09/09 04:18:51 PM: Update 38610: task edges-ner-ontonotes, batch 610 (38610): mcc: 0.8778, acc: 0.8335, precision: 0.9049, recall: 0.8643, f1: 0.8842, edges-ner-ontonotes_loss: 0.0336
09/09 04:19:02 PM: Update 38901: task edges-ner-ontonotes, batch 901 (38901): mcc: 0.8724, acc: 0.8276, precision: 0.9022, recall: 0.8568, f1: 0.8790, edges-ner-ontonotes_loss: 0.0368
09/09 04:19:05 PM: ***** Step 39000 / Validation 39 *****
09/09 04:19:05 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:19:05 PM: Validating...
09/09 04:19:11 PM: Updating LR scheduler:
09/09 04:19:11 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:19:11 PM: 	# validation passes without improvement: 4
09/09 04:19:11 PM: edges-ner-ontonotes_loss: training: 0.036400 validation: 0.042462
09/09 04:19:11 PM: macro_avg: validation: 0.867105
09/09 04:19:11 PM: micro_avg: validation: 0.000000
09/09 04:19:11 PM: edges-ner-ontonotes_mcc: training: 0.874057 validation: 0.860573
09/09 04:19:11 PM: edges-ner-ontonotes_acc: training: 0.830308 validation: 0.822187
09/09 04:19:11 PM: edges-ner-ontonotes_precision: training: 0.903516 validation: 0.906871
09/09 04:19:11 PM: edges-ner-ontonotes_recall: training: 0.858777 validation: 0.830679
09/09 04:19:11 PM: edges-ner-ontonotes_f1: training: 0.880578 validation: 0.867105
09/09 04:19:11 PM: Global learning rate: 1.25e-05
09/09 04:19:11 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:19:12 PM: Update 39045: task edges-ner-ontonotes, batch 45 (39045): mcc: 0.9001, acc: 0.8647, precision: 0.9294, recall: 0.8821, f1: 0.9052, edges-ner-ontonotes_loss: 0.0300
09/09 04:19:22 PM: Update 39293: task edges-ner-ontonotes, batch 293 (39293): mcc: 0.8957, acc: 0.8615, precision: 0.9239, recall: 0.8793, f1: 0.9010, edges-ner-ontonotes_loss: 0.0310
09/09 04:19:32 PM: Update 39548: task edges-ner-ontonotes, batch 548 (39548): mcc: 0.8894, acc: 0.8530, precision: 0.9186, recall: 0.8728, f1: 0.8951, edges-ner-ontonotes_loss: 0.0322
09/09 04:19:42 PM: Update 39837: task edges-ner-ontonotes, batch 837 (39837): mcc: 0.8864, acc: 0.8479, precision: 0.9144, recall: 0.8712, f1: 0.8923, edges-ner-ontonotes_loss: 0.0326
09/09 04:19:50 PM: ***** Step 40000 / Validation 40 *****
09/09 04:19:50 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:19:50 PM: Validating...
09/09 04:19:52 PM: Evaluate: task edges-ner-ontonotes, batch 52 (157): mcc: 0.8597, acc: 0.8339, precision: 0.8889, recall: 0.8462, f1: 0.8670, edges-ner-ontonotes_loss: 0.0454
09/09 04:19:56 PM: Updating LR scheduler:
09/09 04:19:56 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:19:56 PM: 	# validation passes without improvement: 5
09/09 04:19:56 PM: edges-ner-ontonotes_loss: training: 0.032744 validation: 0.042441
09/09 04:19:56 PM: macro_avg: validation: 0.872542
09/09 04:19:56 PM: micro_avg: validation: 0.000000
09/09 04:19:56 PM: edges-ner-ontonotes_mcc: training: 0.884936 validation: 0.865752
09/09 04:19:56 PM: edges-ner-ontonotes_acc: training: 0.845395 validation: 0.828405
09/09 04:19:56 PM: edges-ner-ontonotes_precision: training: 0.912578 validation: 0.900379
09/09 04:19:56 PM: edges-ner-ontonotes_recall: training: 0.870253 validation: 0.846375
09/09 04:19:56 PM: edges-ner-ontonotes_f1: training: 0.890913 validation: 0.872542
09/09 04:19:56 PM: Global learning rate: 1.25e-05
09/09 04:19:56 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:20:03 PM: Update 40153: task edges-ner-ontonotes, batch 153 (40153): mcc: 0.8781, acc: 0.8337, precision: 0.9056, recall: 0.8643, f1: 0.8845, edges-ner-ontonotes_loss: 0.0335
09/09 04:20:13 PM: Update 40451: task edges-ner-ontonotes, batch 451 (40451): mcc: 0.8652, acc: 0.8194, precision: 0.8981, recall: 0.8475, f1: 0.8721, edges-ner-ontonotes_loss: 0.0402
09/09 04:20:23 PM: Update 40740: task edges-ner-ontonotes, batch 740 (40740): mcc: 0.8760, acc: 0.8348, precision: 0.9065, recall: 0.8595, f1: 0.8824, edges-ner-ontonotes_loss: 0.0367
09/09 04:20:33 PM: Update 40981: task edges-ner-ontonotes, batch 981 (40981): mcc: 0.8784, acc: 0.8378, precision: 0.9094, recall: 0.8611, f1: 0.8846, edges-ner-ontonotes_loss: 0.0359
09/09 04:20:34 PM: ***** Step 41000 / Validation 41 *****
09/09 04:20:34 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:20:34 PM: Validating...
09/09 04:20:39 PM: Updating LR scheduler:
09/09 04:20:39 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:20:39 PM: 	# validation passes without improvement: 0
09/09 04:20:39 PM: edges-ner-ontonotes_loss: training: 0.035844 validation: 0.041938
09/09 04:20:39 PM: macro_avg: validation: 0.871835
09/09 04:20:39 PM: micro_avg: validation: 0.000000
09/09 04:20:39 PM: edges-ner-ontonotes_mcc: training: 0.878479 validation: 0.865182
09/09 04:20:39 PM: edges-ner-ontonotes_acc: training: 0.837983 validation: 0.829769
09/09 04:20:39 PM: edges-ner-ontonotes_precision: training: 0.909540 validation: 0.903801
09/09 04:20:39 PM: edges-ner-ontonotes_recall: training: 0.861179 validation: 0.842053
09/09 04:20:39 PM: edges-ner-ontonotes_f1: training: 0.884699 validation: 0.871835
09/09 04:20:39 PM: Global learning rate: 6.25e-06
09/09 04:20:39 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:20:44 PM: Update 41083: task edges-ner-ontonotes, batch 83 (41083): mcc: 0.8804, acc: 0.8404, precision: 0.9107, recall: 0.8636, f1: 0.8865, edges-ner-ontonotes_loss: 0.0342
09/09 04:20:54 PM: Update 41372: task edges-ner-ontonotes, batch 372 (41372): mcc: 0.8781, acc: 0.8365, precision: 0.9086, recall: 0.8613, f1: 0.8843, edges-ner-ontonotes_loss: 0.0338
09/09 04:21:04 PM: Update 41600: task edges-ner-ontonotes, batch 600 (41600): mcc: 0.8776, acc: 0.8349, precision: 0.9064, recall: 0.8625, f1: 0.8839, edges-ner-ontonotes_loss: 0.0337
09/09 04:21:14 PM: Update 41832: task edges-ner-ontonotes, batch 832 (41832): mcc: 0.8743, acc: 0.8301, precision: 0.9041, recall: 0.8586, f1: 0.8808, edges-ner-ontonotes_loss: 0.0354
09/09 04:21:20 PM: ***** Step 42000 / Validation 42 *****
09/09 04:21:20 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:21:20 PM: Validating...
09/09 04:21:24 PM: Evaluate: task edges-ner-ontonotes, batch 146 (157): mcc: 0.8655, acc: 0.8311, precision: 0.9049, recall: 0.8417, f1: 0.8721, edges-ner-ontonotes_loss: 0.0425
09/09 04:21:24 PM: Updating LR scheduler:
09/09 04:21:24 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:21:24 PM: 	# validation passes without improvement: 1
09/09 04:21:24 PM: edges-ner-ontonotes_loss: training: 0.036605 validation: 0.041855
09/09 04:21:24 PM: macro_avg: validation: 0.873659
09/09 04:21:24 PM: micro_avg: validation: 0.000000
09/09 04:21:24 PM: edges-ner-ontonotes_mcc: training: 0.872386 validation: 0.867159
09/09 04:21:24 PM: edges-ner-ontonotes_acc: training: 0.827772 validation: 0.832272
09/09 04:21:24 PM: edges-ner-ontonotes_precision: training: 0.902956 validation: 0.906762
09/09 04:21:24 PM: edges-ner-ontonotes_recall: training: 0.856209 validation: 0.842887
09/09 04:21:24 PM: edges-ner-ontonotes_f1: training: 0.878962 validation: 0.873659
09/09 04:21:24 PM: Global learning rate: 6.25e-06
09/09 04:21:24 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:21:34 PM: Update 42250: task edges-ner-ontonotes, batch 250 (42250): mcc: 0.8962, acc: 0.8637, precision: 0.9208, recall: 0.8832, f1: 0.9016, edges-ner-ontonotes_loss: 0.0316
09/09 04:21:44 PM: Update 42497: task edges-ner-ontonotes, batch 497 (42497): mcc: 0.8912, acc: 0.8554, precision: 0.9184, recall: 0.8762, f1: 0.8968, edges-ner-ontonotes_loss: 0.0321
09/09 04:21:54 PM: Update 42721: task edges-ner-ontonotes, batch 721 (42721): mcc: 0.8867, acc: 0.8488, precision: 0.9159, recall: 0.8703, f1: 0.8925, edges-ner-ontonotes_loss: 0.0327
09/09 04:22:04 PM: Update 42952: task edges-ner-ontonotes, batch 952 (42952): mcc: 0.8848, acc: 0.8458, precision: 0.9135, recall: 0.8691, f1: 0.8907, edges-ner-ontonotes_loss: 0.0329
09/09 04:22:06 PM: ***** Step 43000 / Validation 43 *****
09/09 04:22:06 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:22:06 PM: Validating...
09/09 04:22:11 PM: Updating LR scheduler:
09/09 04:22:11 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:22:11 PM: 	# validation passes without improvement: 2
09/09 04:22:11 PM: edges-ner-ontonotes_loss: training: 0.032946 validation: 0.041981
09/09 04:22:11 PM: macro_avg: validation: 0.871761
09/09 04:22:11 PM: micro_avg: validation: 0.000000
09/09 04:22:11 PM: edges-ner-ontonotes_mcc: training: 0.884009 validation: 0.864926
09/09 04:22:11 PM: edges-ner-ontonotes_acc: training: 0.844525 validation: 0.828101
09/09 04:22:11 PM: edges-ner-ontonotes_precision: training: 0.912872 validation: 0.899572
09/09 04:22:11 PM: edges-ner-ontonotes_recall: training: 0.868242 validation: 0.845617
09/09 04:22:11 PM: edges-ner-ontonotes_f1: training: 0.889998 validation: 0.871761
09/09 04:22:11 PM: Global learning rate: 6.25e-06
09/09 04:22:11 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:22:14 PM: Update 43098: task edges-ner-ontonotes, batch 98 (43098): mcc: 0.8790, acc: 0.8355, precision: 0.9059, recall: 0.8657, f1: 0.8854, edges-ner-ontonotes_loss: 0.0333
09/09 04:22:24 PM: Update 43352: task edges-ner-ontonotes, batch 352 (43352): mcc: 0.8738, acc: 0.8290, precision: 0.9017, recall: 0.8602, f1: 0.8804, edges-ner-ontonotes_loss: 0.0358
09/09 04:22:34 PM: Update 43609: task edges-ner-ontonotes, batch 609 (43609): mcc: 0.8695, acc: 0.8246, precision: 0.9003, recall: 0.8535, f1: 0.8763, edges-ner-ontonotes_loss: 0.0385
09/09 04:22:46 PM: Update 43882: task edges-ner-ontonotes, batch 882 (43882): mcc: 0.8773, acc: 0.8358, precision: 0.9061, recall: 0.8624, f1: 0.8837, edges-ner-ontonotes_loss: 0.0361
09/09 04:22:50 PM: ***** Step 44000 / Validation 44 *****
09/09 04:22:50 PM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
09/09 04:22:50 PM: Validating...
09/09 04:22:54 PM: Updating LR scheduler:
09/09 04:22:54 PM: 	Best result seen so far for macro_avg: 0.874
09/09 04:22:54 PM: 	# validation passes without improvement: 3
09/09 04:22:54 PM: Ran out of early stopping patience. Stopping training.
09/09 04:22:54 PM: edges-ner-ontonotes_loss: training: 0.036114 validation: 0.042009
09/09 04:22:54 PM: macro_avg: validation: 0.869555
09/09 04:22:54 PM: micro_avg: validation: 0.000000
09/09 04:22:54 PM: edges-ner-ontonotes_mcc: training: 0.877289 validation: 0.862966
09/09 04:22:54 PM: edges-ner-ontonotes_acc: training: 0.835983 validation: 0.824765
09/09 04:22:54 PM: edges-ner-ontonotes_precision: training: 0.906766 validation: 0.905575
09/09 04:22:54 PM: edges-ner-ontonotes_recall: training: 0.861646 validation: 0.836291
09/09 04:22:54 PM: edges-ner-ontonotes_f1: training: 0.883631 validation: 0.869555
09/09 04:22:54 PM: Global learning rate: 6.25e-06
09/09 04:22:54 PM: Saving checkpoints to: ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:22:54 PM: Stopped training after 44 validation checks
09/09 04:22:54 PM: Trained edges-ner-ontonotes for 44000 batches or 28.314 epochs
09/09 04:22:54 PM: ***** VALIDATION RESULTS *****
09/09 04:22:54 PM: edges-ner-ontonotes_f1 (for best val pass 23): edges-ner-ontonotes_loss: 0.04276, macro_avg: 0.87389, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.86711, edges-ner-ontonotes_acc: 0.82871, edges-ner-ontonotes_precision: 0.90016, edges-ner-ontonotes_recall: 0.84911, edges-ner-ontonotes_f1: 0.87389
09/09 04:22:54 PM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.05810, macro_avg: 0.82260, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.81542, edges-ner-ontonotes_acc: 0.74818, edges-ner-ontonotes_precision: 0.88782, edges-ner-ontonotes_recall: 0.76630, edges-ner-ontonotes_f1: 0.82260
09/09 04:22:54 PM: macro_avg (for best val pass 23): edges-ner-ontonotes_loss: 0.04276, macro_avg: 0.87389, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.86711, edges-ner-ontonotes_acc: 0.82871, edges-ner-ontonotes_precision: 0.90016, edges-ner-ontonotes_recall: 0.84911, edges-ner-ontonotes_f1: 0.87389
09/09 04:22:54 PM: Evaluating...
09/09 04:22:55 PM: Loaded model state from ./experiments/ner-ontonotes- models/hotpot-only/run/edges-ner-ontonotes/model_state_target_train_val_23.best.th
09/09 04:22:55 PM: Evaluating on: edges-ner-ontonotes, split: val
09/09 04:23:05 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/09 04:23:05 PM: Finished evaluating on: edges-ner-ontonotes
09/09 04:23:05 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
09/09 04:23:06 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:23:06 PM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:23:06 PM: Evaluating on: edges-ner-ontonotes, split: test
09/09 04:23:11 PM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
09/09 04:23:11 PM: Finished evaluating on: edges-ner-ontonotes
09/09 04:23:11 PM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
09/09 04:23:12 PM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:23:12 PM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes- models/hotpot-only/run
09/09 04:23:12 PM: Writing results for split 'val' to ./experiments/ner-ontonotes- models/hotpot-only/results.tsv
09/09 04:23:12 PM: micro_avg: 0.000, macro_avg: 0.875, edges-ner-ontonotes_mcc: 0.868, edges-ner-ontonotes_acc: 0.830, edges-ner-ontonotes_precision: 0.902, edges-ner-ontonotes_recall: 0.849, edges-ner-ontonotes_f1: 0.875
09/09 04:23:12 PM: Done!
