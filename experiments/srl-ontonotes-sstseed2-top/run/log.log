10/01 03:08:54 AM: Git branch: master
10/01 03:08:54 AM: Git SHA: 62183b2d03f2fae12b41eef8779808b6d354875e
10/01 03:08:54 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/srl-ontonotes-sstseed2-top/",
  "exp_name": "experiments/srl-ontonotes-sstseed2-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/srl-ontonotes-sstseed2-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/sstseed2",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/srl-ontonotes-sstseed2-top__run",
  "run_dir": "./experiments/srl-ontonotes-sstseed2-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-srl-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 03:08:54 AM: Saved config to ./experiments/srl-ontonotes-sstseed2-top/run/params.conf
10/01 03:08:54 AM: Using random seed 1234
10/01 03:09:18 AM: Using GPU 0
10/01 03:09:18 AM: Loading tasks...
10/01 03:09:18 AM: Writing pre-preprocessed tasks to ./experiments/srl-ontonotes-sstseed2-top/
10/01 03:09:18 AM: 	Creating task edges-srl-ontonotes from scratch.
10/01 03:09:23 AM: Read=231480, Skip=21590, Total=253070 from ./probing_data/edges/ontonotes/srl/train.json.retokenized.bert-base-uncased
10/01 03:09:23 AM: Read=32486, Skip=2811, Total=35297 from ./probing_data/edges/ontonotes/srl/development.json.retokenized.bert-base-uncased
10/01 03:09:24 AM: Read=23800, Skip=2915, Total=26715 from ./probing_data/edges/ontonotes/srl/test.json.retokenized.bert-base-uncased
10/01 03:09:27 AM: 	Task 'edges-srl-ontonotes': |train|=231480 |val|=32486 |test|=23800
10/01 03:09:27 AM: 	Finished loading tasks: edges-srl-ontonotes.
10/01 03:09:27 AM: 	Building vocab from scratch.
10/01 03:09:27 AM: 	Counting units for task edges-srl-ontonotes.
10/01 03:09:34 AM: 	Task 'edges-srl-ontonotes': adding vocab namespace 'edges-srl-ontonotes_labels'
10/01 03:09:35 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:09:35 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 03:09:35 AM: 	Saved vocab to ./experiments/srl-ontonotes-sstseed2-top/vocab
10/01 03:09:35 AM: Loading token dictionary from ./experiments/srl-ontonotes-sstseed2-top/vocab.
10/01 03:09:35 AM: 	Loaded vocab from ./experiments/srl-ontonotes-sstseed2-top/vocab
10/01 03:09:35 AM: 	Vocab namespace edges-srl-ontonotes_labels: size 66
10/01 03:09:35 AM: 	Vocab namespace tokens: size 23662
10/01 03:09:35 AM: 	Vocab namespace bert_uncased: size 30524
10/01 03:09:35 AM: 	Vocab namespace chars: size 76
10/01 03:09:35 AM: 	Finished building vocab.
10/01 03:09:35 AM: 	Task edges-srl-ontonotes (train): Indexing from scratch.
10/01 03:10:19 AM: 	Task edges-srl-ontonotes (train): Saved 231480 instances to ./experiments/srl-ontonotes-sstseed2-top/preproc/edges-srl-ontonotes__train_data
10/01 03:10:19 AM: 	Task edges-srl-ontonotes (val): Indexing from scratch.
10/01 03:10:25 AM: 	Task edges-srl-ontonotes (val): Saved 32486 instances to ./experiments/srl-ontonotes-sstseed2-top/preproc/edges-srl-ontonotes__val_data
10/01 03:10:25 AM: 	Task edges-srl-ontonotes (test): Indexing from scratch.
10/01 03:10:30 AM: 	Task edges-srl-ontonotes (test): Saved 23800 instances to ./experiments/srl-ontonotes-sstseed2-top/preproc/edges-srl-ontonotes__test_data
10/01 03:10:30 AM: 	Finished indexing tasks
10/01 03:10:30 AM: 	Creating trimmed target-only version of edges-srl-ontonotes train.
10/01 03:10:30 AM: 	  Training on 
10/01 03:10:30 AM: 	  Evaluating on edges-srl-ontonotes
10/01 03:10:30 AM: 	Finished loading tasks in 71.798s
10/01 03:10:30 AM: 	 Tasks: ['edges-srl-ontonotes']
10/01 03:10:30 AM: Building model...
10/01 03:10:30 AM: Using BERT model (bert-base-uncased).
10/01 03:10:30 AM: LOADING A FUNETUNED MODEL from: 
10/01 03:10:30 AM: models/sstseed2
10/01 03:10:30 AM: loading configuration file models/sstseed2/config.json
10/01 03:10:30 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "sst-2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 03:10:30 AM: loading weights file models/sstseed2/pytorch_model.bin
10/01 03:10:34 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpgwim1y_t
10/01 03:10:36 AM: copying /tmp/tmpgwim1y_t to cache at ./experiments/srl-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:10:36 AM: creating metadata file for ./experiments/srl-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:10:36 AM: removing temp file /tmp/tmpgwim1y_t
10/01 03:10:36 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/srl-ontonotes-sstseed2-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 03:10:36 AM: Initializing parameters
10/01 03:10:36 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 03:10:36 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 03:10:36 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 03:10:36 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 03:10:36 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 03:10:36 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 03:10:36 AM: 	Task 'edges-srl-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-srl-ontonotes"
}
10/01 03:10:40 AM: Model specification:
10/01 03:10:40 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-srl-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (proj2): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (span_extractor2): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.3)
        (4): Linear(in_features=256, out_features=66, bias=True)
      )
    )
  )
)
10/01 03:10:40 AM: Model parameters:
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 03:10:40 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.proj2.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.proj2.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.weight: Trainable parameter, count 262144 with torch.Size([256, 1024])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.0.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.weight: Trainable parameter, count 256 with torch.Size([256])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.2.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.weight: Trainable parameter, count 16896 with torch.Size([66, 256])
10/01 03:10:40 AM: 	edges-srl-ontonotes_mdl.classifier.classifier.4.bias: Trainable parameter, count 66 with torch.Size([66])
10/01 03:10:40 AM: Total number of parameters: 110155842 (1.10156e+08)
10/01 03:10:40 AM: Number of trainable parameters: 673602 (673602)
10/01 03:10:40 AM: Finished building model in 9.996s
10/01 03:10:40 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-srl-ontonotes 

10/01 03:10:56 AM: patience = 9
10/01 03:10:56 AM: val_interval = 1000
10/01 03:10:56 AM: max_vals = 250
10/01 03:10:56 AM: cuda_device = 0
10/01 03:10:56 AM: grad_norm = 5.0
10/01 03:10:56 AM: grad_clipping = None
10/01 03:10:56 AM: lr_decay = 0.99
10/01 03:10:56 AM: min_lr = 1e-06
10/01 03:10:56 AM: keep_all_checkpoints = 0
10/01 03:10:56 AM: val_data_limit = 5000
10/01 03:10:56 AM: max_epochs = -1
10/01 03:10:56 AM: dec_val_scale = 250
10/01 03:10:56 AM: training_data_fraction = 1
10/01 03:10:56 AM: type = adam
10/01 03:10:56 AM: parameter_groups = None
10/01 03:10:56 AM: Number of trainable parameters: 673602
10/01 03:10:56 AM: infer_type_and_cast = True
10/01 03:10:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:10:56 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:10:56 AM: lr = 0.0001
10/01 03:10:56 AM: amsgrad = True
10/01 03:10:56 AM: type = reduce_on_plateau
10/01 03:10:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:10:56 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:10:56 AM: mode = max
10/01 03:10:56 AM: factor = 0.5
10/01 03:10:56 AM: patience = 3
10/01 03:10:56 AM: threshold = 0.0001
10/01 03:10:56 AM: threshold_mode = abs
10/01 03:10:56 AM: verbose = True
10/01 03:10:56 AM: type = adam
10/01 03:10:56 AM: parameter_groups = None
10/01 03:10:56 AM: Number of trainable parameters: 673602
10/01 03:10:56 AM: infer_type_and_cast = True
10/01 03:10:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:10:56 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:10:56 AM: lr = 0.0001
10/01 03:10:56 AM: amsgrad = True
10/01 03:10:56 AM: type = reduce_on_plateau
10/01 03:10:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 03:10:56 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 03:10:56 AM: mode = max
10/01 03:10:56 AM: factor = 0.5
10/01 03:10:56 AM: patience = 3
10/01 03:10:56 AM: threshold = 0.0001
10/01 03:10:56 AM: threshold_mode = abs
10/01 03:10:56 AM: verbose = True
10/01 03:10:56 AM: Starting training without restoring from a checkpoint.
10/01 03:10:56 AM: Training examples per task, before any subsampling: {'edges-srl-ontonotes': 231480}
10/01 03:10:56 AM: Beginning training with stopping criteria based on metric: edges-srl-ontonotes_f1
10/01 03:11:06 AM: Update 108: task edges-srl-ontonotes, batch 108 (108): mcc: 0.0594, acc: 0.0395, precision: 0.0588, recall: 0.1044, f1: 0.0752, edges-srl-ontonotes_loss: 0.2147
10/01 03:11:16 AM: Update 224: task edges-srl-ontonotes, batch 224 (224): mcc: 0.0464, acc: 0.0246, precision: 0.0647, recall: 0.0553, f1: 0.0596, edges-srl-ontonotes_loss: 0.1432
10/01 03:11:26 AM: Update 326: task edges-srl-ontonotes, batch 326 (326): mcc: 0.0461, acc: 0.0232, precision: 0.0740, recall: 0.0442, f1: 0.0554, edges-srl-ontonotes_loss: 0.1165
10/01 03:11:36 AM: Update 436: task edges-srl-ontonotes, batch 436 (436): mcc: 0.0716, acc: 0.0406, precision: 0.1172, recall: 0.0565, f1: 0.0762, edges-srl-ontonotes_loss: 0.0996
10/01 03:11:46 AM: Update 551: task edges-srl-ontonotes, batch 551 (551): mcc: 0.1348, acc: 0.0830, precision: 0.2135, recall: 0.0964, f1: 0.1328, edges-srl-ontonotes_loss: 0.0878
10/01 03:11:56 AM: Update 635: task edges-srl-ontonotes, batch 635 (635): mcc: 0.1841, acc: 0.1158, precision: 0.2862, recall: 0.1291, f1: 0.1780, edges-srl-ontonotes_loss: 0.0813
10/01 03:12:06 AM: Update 747: task edges-srl-ontonotes, batch 747 (747): mcc: 0.2347, acc: 0.1495, precision: 0.3590, recall: 0.1636, f1: 0.2248, edges-srl-ontonotes_loss: 0.0747
10/01 03:12:16 AM: Update 855: task edges-srl-ontonotes, batch 855 (855): mcc: 0.2796, acc: 0.1813, precision: 0.4195, recall: 0.1961, f1: 0.2673, edges-srl-ontonotes_loss: 0.0697
10/01 03:12:26 AM: Update 959: task edges-srl-ontonotes, batch 959 (959): mcc: 0.3165, acc: 0.2080, precision: 0.4662, recall: 0.2245, f1: 0.3030, edges-srl-ontonotes_loss: 0.0657
10/01 03:12:30 AM: ***** Step 1000 / Validation 1 *****
10/01 03:12:30 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:12:30 AM: Validating...
10/01 03:12:36 AM: Evaluate: task edges-srl-ontonotes, batch 66 (157): mcc: 0.6257, acc: 0.4645, precision: 0.8284, recall: 0.4787, f1: 0.6068, edges-srl-ontonotes_loss: 0.0305
10/01 03:12:45 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:12:45 AM: Best result seen so far for micro.
10/01 03:12:45 AM: Best result seen so far for macro.
10/01 03:12:45 AM: Updating LR scheduler:
10/01 03:12:45 AM: 	Best result seen so far for macro_avg: 0.629
10/01 03:12:45 AM: 	# validation passes without improvement: 0
10/01 03:12:45 AM: edges-srl-ontonotes_loss: training: 0.064278 validation: 0.029151
10/01 03:12:45 AM: macro_avg: validation: 0.628580
10/01 03:12:45 AM: micro_avg: validation: 0.000000
10/01 03:12:45 AM: edges-srl-ontonotes_mcc: training: 0.328619 validation: 0.644620
10/01 03:12:45 AM: edges-srl-ontonotes_acc: training: 0.216954 validation: 0.488338
10/01 03:12:45 AM: edges-srl-ontonotes_precision: training: 0.481088 validation: 0.834012
10/01 03:12:45 AM: edges-srl-ontonotes_recall: training: 0.233945 validation: 0.504349
10/01 03:12:45 AM: edges-srl-ontonotes_f1: training: 0.314805 validation: 0.628580
10/01 03:12:45 AM: Global learning rate: 0.0001
10/01 03:12:45 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:12:46 AM: Update 1010: task edges-srl-ontonotes, batch 10 (1010): mcc: 0.6149, acc: 0.4626, precision: 0.7597, recall: 0.5052, f1: 0.6068, edges-srl-ontonotes_loss: 0.0314
10/01 03:12:56 AM: Update 1122: task edges-srl-ontonotes, batch 122 (1122): mcc: 0.6086, acc: 0.4539, precision: 0.7738, recall: 0.4858, f1: 0.5969, edges-srl-ontonotes_loss: 0.0317
10/01 03:13:06 AM: Update 1226: task edges-srl-ontonotes, batch 226 (1226): mcc: 0.6080, acc: 0.4570, precision: 0.7682, recall: 0.4884, f1: 0.5972, edges-srl-ontonotes_loss: 0.0312
10/01 03:13:16 AM: Update 1318: task edges-srl-ontonotes, batch 318 (1318): mcc: 0.6145, acc: 0.4650, precision: 0.7703, recall: 0.4975, f1: 0.6046, edges-srl-ontonotes_loss: 0.0306
10/01 03:13:26 AM: Update 1424: task edges-srl-ontonotes, batch 424 (1424): mcc: 0.6230, acc: 0.4753, precision: 0.7746, recall: 0.5082, f1: 0.6137, edges-srl-ontonotes_loss: 0.0300
10/01 03:13:36 AM: Update 1526: task edges-srl-ontonotes, batch 526 (1526): mcc: 0.6302, acc: 0.4847, precision: 0.7767, recall: 0.5186, f1: 0.6219, edges-srl-ontonotes_loss: 0.0294
10/01 03:13:47 AM: Update 1618: task edges-srl-ontonotes, batch 618 (1618): mcc: 0.6320, acc: 0.4861, precision: 0.7767, recall: 0.5215, f1: 0.6240, edges-srl-ontonotes_loss: 0.0292
10/01 03:13:57 AM: Update 1721: task edges-srl-ontonotes, batch 721 (1721): mcc: 0.6315, acc: 0.4850, precision: 0.7742, recall: 0.5223, f1: 0.6238, edges-srl-ontonotes_loss: 0.0291
10/01 03:14:07 AM: Update 1819: task edges-srl-ontonotes, batch 819 (1819): mcc: 0.6301, acc: 0.4833, precision: 0.7724, recall: 0.5214, f1: 0.6225, edges-srl-ontonotes_loss: 0.0291
10/01 03:14:17 AM: Update 1888: task edges-srl-ontonotes, batch 888 (1888): mcc: 0.6304, acc: 0.4837, precision: 0.7716, recall: 0.5224, f1: 0.6230, edges-srl-ontonotes_loss: 0.0290
10/01 03:14:27 AM: Update 1989: task edges-srl-ontonotes, batch 989 (1989): mcc: 0.6323, acc: 0.4860, precision: 0.7725, recall: 0.5249, f1: 0.6251, edges-srl-ontonotes_loss: 0.0288
10/01 03:14:28 AM: ***** Step 2000 / Validation 2 *****
10/01 03:14:28 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:14:28 AM: Validating...
10/01 03:14:37 AM: Evaluate: task edges-srl-ontonotes, batch 91 (157): mcc: 0.7050, acc: 0.5708, precision: 0.8411, recall: 0.5971, f1: 0.6984, edges-srl-ontonotes_loss: 0.0236
10/01 03:14:43 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:14:43 AM: Best result seen so far for macro.
10/01 03:14:43 AM: Updating LR scheduler:
10/01 03:14:43 AM: 	Best result seen so far for macro_avg: 0.706
10/01 03:14:43 AM: 	# validation passes without improvement: 0
10/01 03:14:43 AM: edges-srl-ontonotes_loss: training: 0.028799 validation: 0.022925
10/01 03:14:43 AM: macro_avg: validation: 0.706294
10/01 03:14:43 AM: micro_avg: validation: 0.000000
10/01 03:14:43 AM: edges-srl-ontonotes_mcc: training: 0.632290 validation: 0.712626
10/01 03:14:43 AM: edges-srl-ontonotes_acc: training: 0.485916 validation: 0.582480
10/01 03:14:43 AM: edges-srl-ontonotes_precision: training: 0.772045 validation: 0.846452
10/01 03:14:43 AM: edges-srl-ontonotes_recall: training: 0.525141 validation: 0.605958
10/01 03:14:43 AM: edges-srl-ontonotes_f1: training: 0.625096 validation: 0.706294
10/01 03:14:43 AM: Global learning rate: 0.0001
10/01 03:14:43 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:14:47 AM: Update 2035: task edges-srl-ontonotes, batch 35 (2035): mcc: 0.6356, acc: 0.4986, precision: 0.7622, recall: 0.5376, f1: 0.6305, edges-srl-ontonotes_loss: 0.0276
10/01 03:14:57 AM: Update 2139: task edges-srl-ontonotes, batch 139 (2139): mcc: 0.6447, acc: 0.5041, precision: 0.7675, recall: 0.5490, f1: 0.6401, edges-srl-ontonotes_loss: 0.0272
10/01 03:15:07 AM: Update 2226: task edges-srl-ontonotes, batch 226 (2226): mcc: 0.6513, acc: 0.5110, precision: 0.7718, recall: 0.5570, f1: 0.6470, edges-srl-ontonotes_loss: 0.0269
10/01 03:15:17 AM: Update 2316: task edges-srl-ontonotes, batch 316 (2316): mcc: 0.6587, acc: 0.5209, precision: 0.7755, recall: 0.5669, f1: 0.6550, edges-srl-ontonotes_loss: 0.0263
10/01 03:15:27 AM: Update 2413: task edges-srl-ontonotes, batch 413 (2413): mcc: 0.6666, acc: 0.5319, precision: 0.7792, recall: 0.5777, f1: 0.6634, edges-srl-ontonotes_loss: 0.0258
10/01 03:15:37 AM: Update 2507: task edges-srl-ontonotes, batch 507 (2507): mcc: 0.6713, acc: 0.5381, precision: 0.7821, recall: 0.5834, f1: 0.6683, edges-srl-ontonotes_loss: 0.0255
10/01 03:15:47 AM: Update 2610: task edges-srl-ontonotes, batch 610 (2610): mcc: 0.6752, acc: 0.5438, precision: 0.7840, recall: 0.5886, f1: 0.6724, edges-srl-ontonotes_loss: 0.0252
10/01 03:15:57 AM: Update 2715: task edges-srl-ontonotes, batch 715 (2715): mcc: 0.6774, acc: 0.5470, precision: 0.7845, recall: 0.5921, f1: 0.6749, edges-srl-ontonotes_loss: 0.0250
10/01 03:16:10 AM: Update 2818: task edges-srl-ontonotes, batch 818 (2818): mcc: 0.6802, acc: 0.5508, precision: 0.7855, recall: 0.5962, f1: 0.6779, edges-srl-ontonotes_loss: 0.0248
10/01 03:16:20 AM: Update 2921: task edges-srl-ontonotes, batch 921 (2921): mcc: 0.6824, acc: 0.5536, precision: 0.7858, recall: 0.5998, f1: 0.6803, edges-srl-ontonotes_loss: 0.0246
10/01 03:16:28 AM: ***** Step 3000 / Validation 3 *****
10/01 03:16:28 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:16:28 AM: Validating...
10/01 03:16:30 AM: Evaluate: task edges-srl-ontonotes, batch 24 (157): mcc: 0.7144, acc: 0.5951, precision: 0.8276, recall: 0.6229, f1: 0.7108, edges-srl-ontonotes_loss: 0.0220
10/01 03:16:40 AM: Evaluate: task edges-srl-ontonotes, batch 127 (157): mcc: 0.7157, acc: 0.5989, precision: 0.8149, recall: 0.6352, f1: 0.7139, edges-srl-ontonotes_loss: 0.0221
10/01 03:16:43 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:16:43 AM: Best result seen so far for macro.
10/01 03:16:43 AM: Updating LR scheduler:
10/01 03:16:43 AM: 	Best result seen so far for macro_avg: 0.718
10/01 03:16:43 AM: 	# validation passes without improvement: 0
10/01 03:16:43 AM: edges-srl-ontonotes_loss: training: 0.024502 validation: 0.021990
10/01 03:16:43 AM: macro_avg: validation: 0.717650
10/01 03:16:43 AM: micro_avg: validation: 0.000000
10/01 03:16:43 AM: edges-srl-ontonotes_mcc: training: 0.684338 validation: 0.719418
10/01 03:16:43 AM: edges-srl-ontonotes_acc: training: 0.556332 validation: 0.604034
10/01 03:16:43 AM: edges-srl-ontonotes_precision: training: 0.786535 validation: 0.817904
10/01 03:16:43 AM: edges-srl-ontonotes_recall: training: 0.602584 validation: 0.639289
10/01 03:16:43 AM: edges-srl-ontonotes_f1: training: 0.682380 validation: 0.717650
10/01 03:16:43 AM: Global learning rate: 0.0001
10/01 03:16:43 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:16:50 AM: Update 3074: task edges-srl-ontonotes, batch 74 (3074): mcc: 0.7202, acc: 0.6019, precision: 0.8065, recall: 0.6499, f1: 0.7198, edges-srl-ontonotes_loss: 0.0223
10/01 03:17:00 AM: Update 3167: task edges-srl-ontonotes, batch 167 (3167): mcc: 0.7138, acc: 0.5933, precision: 0.8033, recall: 0.6410, f1: 0.7130, edges-srl-ontonotes_loss: 0.0227
10/01 03:17:11 AM: Update 3273: task edges-srl-ontonotes, batch 273 (3273): mcc: 0.7075, acc: 0.5874, precision: 0.7992, recall: 0.6333, f1: 0.7066, edges-srl-ontonotes_loss: 0.0231
10/01 03:17:21 AM: Update 3378: task edges-srl-ontonotes, batch 378 (3378): mcc: 0.7052, acc: 0.5851, precision: 0.7969, recall: 0.6311, f1: 0.7044, edges-srl-ontonotes_loss: 0.0232
10/01 03:17:31 AM: Update 3469: task edges-srl-ontonotes, batch 469 (3469): mcc: 0.7028, acc: 0.5825, precision: 0.7946, recall: 0.6286, f1: 0.7019, edges-srl-ontonotes_loss: 0.0232
10/01 03:17:41 AM: Update 3579: task edges-srl-ontonotes, batch 579 (3579): mcc: 0.7014, acc: 0.5804, precision: 0.7933, recall: 0.6271, f1: 0.7005, edges-srl-ontonotes_loss: 0.0232
10/01 03:17:51 AM: Update 3678: task edges-srl-ontonotes, batch 678 (3678): mcc: 0.7001, acc: 0.5783, precision: 0.7932, recall: 0.6250, f1: 0.6991, edges-srl-ontonotes_loss: 0.0232
10/01 03:18:01 AM: Update 3773: task edges-srl-ontonotes, batch 773 (3773): mcc: 0.7003, acc: 0.5781, precision: 0.7932, recall: 0.6253, f1: 0.6993, edges-srl-ontonotes_loss: 0.0232
10/01 03:18:11 AM: Update 3879: task edges-srl-ontonotes, batch 879 (3879): mcc: 0.7005, acc: 0.5785, precision: 0.7929, recall: 0.6259, f1: 0.6996, edges-srl-ontonotes_loss: 0.0231
10/01 03:18:21 AM: Update 3981: task edges-srl-ontonotes, batch 981 (3981): mcc: 0.7001, acc: 0.5779, precision: 0.7918, recall: 0.6261, f1: 0.6992, edges-srl-ontonotes_loss: 0.0231
10/01 03:18:23 AM: ***** Step 4000 / Validation 4 *****
10/01 03:18:23 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:18:23 AM: Validating...
10/01 03:18:31 AM: Evaluate: task edges-srl-ontonotes, batch 86 (157): mcc: 0.7170, acc: 0.5983, precision: 0.8273, recall: 0.6277, f1: 0.7138, edges-srl-ontonotes_loss: 0.0219
10/01 03:18:40 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:18:40 AM: Best result seen so far for macro.
10/01 03:18:40 AM: Updating LR scheduler:
10/01 03:18:40 AM: 	Best result seen so far for macro_avg: 0.726
10/01 03:18:40 AM: 	# validation passes without improvement: 0
10/01 03:18:40 AM: edges-srl-ontonotes_loss: training: 0.023108 validation: 0.021041
10/01 03:18:40 AM: macro_avg: validation: 0.725582
10/01 03:18:40 AM: micro_avg: validation: 0.000000
10/01 03:18:40 AM: edges-srl-ontonotes_mcc: training: 0.700149 validation: 0.728147
10/01 03:18:40 AM: edges-srl-ontonotes_acc: training: 0.577932 validation: 0.613425
10/01 03:18:40 AM: edges-srl-ontonotes_precision: training: 0.791847 validation: 0.832918
10/01 03:18:40 AM: edges-srl-ontonotes_recall: training: 0.626118 validation: 0.642753
10/01 03:18:40 AM: edges-srl-ontonotes_f1: training: 0.699297 validation: 0.725582
10/01 03:18:40 AM: Global learning rate: 0.0001
10/01 03:18:40 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:18:41 AM: Update 4015: task edges-srl-ontonotes, batch 15 (4015): mcc: 0.6981, acc: 0.5664, precision: 0.8039, recall: 0.6130, f1: 0.6956, edges-srl-ontonotes_loss: 0.0230
10/01 03:18:51 AM: Update 4108: task edges-srl-ontonotes, batch 108 (4108): mcc: 0.7031, acc: 0.5814, precision: 0.7935, recall: 0.6299, f1: 0.7023, edges-srl-ontonotes_loss: 0.0230
10/01 03:19:01 AM: Update 4210: task edges-srl-ontonotes, batch 210 (4210): mcc: 0.7091, acc: 0.5886, precision: 0.7988, recall: 0.6365, f1: 0.7084, edges-srl-ontonotes_loss: 0.0225
10/01 03:19:11 AM: Update 4315: task edges-srl-ontonotes, batch 315 (4315): mcc: 0.7117, acc: 0.5923, precision: 0.7992, recall: 0.6407, f1: 0.7112, edges-srl-ontonotes_loss: 0.0223
10/01 03:19:21 AM: Update 4415: task edges-srl-ontonotes, batch 415 (4415): mcc: 0.7130, acc: 0.5950, precision: 0.7989, recall: 0.6432, f1: 0.7126, edges-srl-ontonotes_loss: 0.0222
10/01 03:19:31 AM: Update 4519: task edges-srl-ontonotes, batch 519 (4519): mcc: 0.7138, acc: 0.5952, precision: 0.7997, recall: 0.6440, f1: 0.7135, edges-srl-ontonotes_loss: 0.0221
10/01 03:19:41 AM: Update 4621: task edges-srl-ontonotes, batch 621 (4621): mcc: 0.7158, acc: 0.5971, precision: 0.8016, recall: 0.6460, f1: 0.7155, edges-srl-ontonotes_loss: 0.0220
10/01 03:19:51 AM: Update 4708: task edges-srl-ontonotes, batch 708 (4708): mcc: 0.7151, acc: 0.5964, precision: 0.8009, recall: 0.6453, f1: 0.7147, edges-srl-ontonotes_loss: 0.0221
10/01 03:20:01 AM: Update 4796: task edges-srl-ontonotes, batch 796 (4796): mcc: 0.7120, acc: 0.5930, precision: 0.7987, recall: 0.6416, f1: 0.7116, edges-srl-ontonotes_loss: 0.0223
10/01 03:20:12 AM: Update 4890: task edges-srl-ontonotes, batch 890 (4890): mcc: 0.7115, acc: 0.5924, precision: 0.7985, recall: 0.6409, f1: 0.7111, edges-srl-ontonotes_loss: 0.0223
10/01 03:20:22 AM: Update 4980: task edges-srl-ontonotes, batch 980 (4980): mcc: 0.7116, acc: 0.5923, precision: 0.7992, recall: 0.6404, f1: 0.7110, edges-srl-ontonotes_loss: 0.0223
10/01 03:20:24 AM: ***** Step 5000 / Validation 5 *****
10/01 03:20:24 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:20:24 AM: Validating...
10/01 03:20:32 AM: Evaluate: task edges-srl-ontonotes, batch 81 (157): mcc: 0.7093, acc: 0.5894, precision: 0.8193, recall: 0.6205, f1: 0.7062, edges-srl-ontonotes_loss: 0.0221
10/01 03:20:39 AM: Updating LR scheduler:
10/01 03:20:39 AM: 	Best result seen so far for macro_avg: 0.726
10/01 03:20:39 AM: 	# validation passes without improvement: 1
10/01 03:20:39 AM: edges-srl-ontonotes_loss: training: 0.022325 validation: 0.021217
10/01 03:20:39 AM: macro_avg: validation: 0.716971
10/01 03:20:39 AM: micro_avg: validation: 0.000000
10/01 03:20:39 AM: edges-srl-ontonotes_mcc: training: 0.711252 validation: 0.719526
10/01 03:20:39 AM: edges-srl-ontonotes_acc: training: 0.592098 validation: 0.603341
10/01 03:20:39 AM: edges-srl-ontonotes_precision: training: 0.798830 validation: 0.824707
10/01 03:20:39 AM: edges-srl-ontonotes_recall: training: 0.640161 validation: 0.634131
10/01 03:20:39 AM: edges-srl-ontonotes_f1: training: 0.710748 validation: 0.716971
10/01 03:20:39 AM: Global learning rate: 0.0001
10/01 03:20:39 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:20:43 AM: Update 5009: task edges-srl-ontonotes, batch 9 (5009): mcc: 0.7050, acc: 0.5831, precision: 0.8141, recall: 0.6172, f1: 0.7021, edges-srl-ontonotes_loss: 0.0239
10/01 03:20:53 AM: Update 5126: task edges-srl-ontonotes, batch 126 (5126): mcc: 0.7011, acc: 0.5763, precision: 0.7925, recall: 0.6272, f1: 0.7002, edges-srl-ontonotes_loss: 0.0228
10/01 03:21:03 AM: Update 5245: task edges-srl-ontonotes, batch 245 (5245): mcc: 0.7082, acc: 0.5859, precision: 0.7971, recall: 0.6361, f1: 0.7075, edges-srl-ontonotes_loss: 0.0222
10/01 03:21:13 AM: Update 5352: task edges-srl-ontonotes, batch 352 (5352): mcc: 0.7129, acc: 0.5924, precision: 0.7994, recall: 0.6426, f1: 0.7125, edges-srl-ontonotes_loss: 0.0219
10/01 03:21:23 AM: Update 5487: task edges-srl-ontonotes, batch 487 (5487): mcc: 0.7198, acc: 0.6006, precision: 0.8025, recall: 0.6524, f1: 0.7197, edges-srl-ontonotes_loss: 0.0214
10/01 03:21:33 AM: Update 5625: task edges-srl-ontonotes, batch 625 (5625): mcc: 0.7268, acc: 0.6089, precision: 0.8060, recall: 0.6621, f1: 0.7270, edges-srl-ontonotes_loss: 0.0209
10/01 03:21:43 AM: Update 5743: task edges-srl-ontonotes, batch 743 (5743): mcc: 0.7281, acc: 0.6103, precision: 0.8069, recall: 0.6636, f1: 0.7283, edges-srl-ontonotes_loss: 0.0208
10/01 03:21:53 AM: Update 5879: task edges-srl-ontonotes, batch 879 (5879): mcc: 0.7307, acc: 0.6131, precision: 0.8082, recall: 0.6673, f1: 0.7310, edges-srl-ontonotes_loss: 0.0206
10/01 03:22:03 AM: Update 5973: task edges-srl-ontonotes, batch 973 (5973): mcc: 0.7313, acc: 0.6135, precision: 0.8085, recall: 0.6682, f1: 0.7317, edges-srl-ontonotes_loss: 0.0206
10/01 03:22:05 AM: ***** Step 6000 / Validation 6 *****
10/01 03:22:05 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:22:05 AM: Validating...
10/01 03:22:13 AM: Evaluate: task edges-srl-ontonotes, batch 85 (157): mcc: 0.7447, acc: 0.6319, precision: 0.8548, recall: 0.6544, f1: 0.7413, edges-srl-ontonotes_loss: 0.0196
10/01 03:22:20 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:22:20 AM: Best result seen so far for macro.
10/01 03:22:20 AM: Updating LR scheduler:
10/01 03:22:20 AM: 	Best result seen so far for macro_avg: 0.752
10/01 03:22:20 AM: 	# validation passes without improvement: 0
10/01 03:22:20 AM: edges-srl-ontonotes_loss: training: 0.020555 validation: 0.018973
10/01 03:22:20 AM: macro_avg: validation: 0.752323
10/01 03:22:20 AM: micro_avg: validation: 0.000000
10/01 03:22:20 AM: edges-srl-ontonotes_mcc: training: 0.731354 validation: 0.754862
10/01 03:22:20 AM: edges-srl-ontonotes_acc: training: 0.613341 validation: 0.645678
10/01 03:22:20 AM: edges-srl-ontonotes_precision: training: 0.808357 validation: 0.857706
10/01 03:22:20 AM: edges-srl-ontonotes_recall: training: 0.668317 validation: 0.670002
10/01 03:22:20 AM: edges-srl-ontonotes_f1: training: 0.731697 validation: 0.752323
10/01 03:22:20 AM: Global learning rate: 0.0001
10/01 03:22:20 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:22:23 AM: Update 6039: task edges-srl-ontonotes, batch 39 (6039): mcc: 0.7313, acc: 0.6108, precision: 0.7977, recall: 0.6772, f1: 0.7325, edges-srl-ontonotes_loss: 0.0201
10/01 03:22:33 AM: Update 6175: task edges-srl-ontonotes, batch 175 (6175): mcc: 0.7352, acc: 0.6188, precision: 0.8055, recall: 0.6777, f1: 0.7361, edges-srl-ontonotes_loss: 0.0201
10/01 03:22:43 AM: Update 6296: task edges-srl-ontonotes, batch 296 (6296): mcc: 0.7417, acc: 0.6270, precision: 0.8109, recall: 0.6849, f1: 0.7426, edges-srl-ontonotes_loss: 0.0197
10/01 03:22:54 AM: Update 6437: task edges-srl-ontonotes, batch 437 (6437): mcc: 0.7502, acc: 0.6374, precision: 0.8192, recall: 0.6934, f1: 0.7511, edges-srl-ontonotes_loss: 0.0192
10/01 03:23:04 AM: Update 6574: task edges-srl-ontonotes, batch 574 (6574): mcc: 0.7577, acc: 0.6479, precision: 0.8256, recall: 0.7016, f1: 0.7585, edges-srl-ontonotes_loss: 0.0187
10/01 03:23:14 AM: Update 6679: task edges-srl-ontonotes, batch 679 (6679): mcc: 0.7530, acc: 0.6418, precision: 0.8216, recall: 0.6963, f1: 0.7538, edges-srl-ontonotes_loss: 0.0191
10/01 03:23:24 AM: Update 6792: task edges-srl-ontonotes, batch 792 (6792): mcc: 0.7500, acc: 0.6387, precision: 0.8189, recall: 0.6932, f1: 0.7508, edges-srl-ontonotes_loss: 0.0194
10/01 03:23:35 AM: Update 6887: task edges-srl-ontonotes, batch 887 (6887): mcc: 0.7477, acc: 0.6360, precision: 0.8171, recall: 0.6905, f1: 0.7485, edges-srl-ontonotes_loss: 0.0195
10/01 03:23:45 AM: Update 6988: task edges-srl-ontonotes, batch 988 (6988): mcc: 0.7427, acc: 0.6299, precision: 0.8136, recall: 0.6845, f1: 0.7435, edges-srl-ontonotes_loss: 0.0199
10/01 03:23:46 AM: ***** Step 7000 / Validation 7 *****
10/01 03:23:46 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:23:46 AM: Validating...
10/01 03:23:55 AM: Evaluate: task edges-srl-ontonotes, batch 93 (157): mcc: 0.7725, acc: 0.6766, precision: 0.8561, recall: 0.7026, f1: 0.7718, edges-srl-ontonotes_loss: 0.0183
10/01 03:24:03 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:24:03 AM: Best result seen so far for macro.
10/01 03:24:03 AM: Updating LR scheduler:
10/01 03:24:03 AM: 	Best result seen so far for macro_avg: 0.777
10/01 03:24:03 AM: 	# validation passes without improvement: 0
10/01 03:24:03 AM: edges-srl-ontonotes_loss: training: 0.019891 validation: 0.017981
10/01 03:24:03 AM: macro_avg: validation: 0.777104
10/01 03:24:03 AM: micro_avg: validation: 0.000000
10/01 03:24:03 AM: edges-srl-ontonotes_mcc: training: 0.742420 validation: 0.777569
10/01 03:24:03 AM: edges-srl-ontonotes_acc: training: 0.629507 validation: 0.683550
10/01 03:24:03 AM: edges-srl-ontonotes_precision: training: 0.813400 validation: 0.858060
10/01 03:24:03 AM: edges-srl-ontonotes_recall: training: 0.684112 validation: 0.710107
10/01 03:24:03 AM: edges-srl-ontonotes_f1: training: 0.743175 validation: 0.777104
10/01 03:24:03 AM: Global learning rate: 0.0001
10/01 03:24:03 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:24:05 AM: Update 7018: task edges-srl-ontonotes, batch 18 (7018): mcc: 0.7107, acc: 0.5909, precision: 0.7935, recall: 0.6435, f1: 0.7107, edges-srl-ontonotes_loss: 0.0219
10/01 03:24:15 AM: Update 7127: task edges-srl-ontonotes, batch 127 (7127): mcc: 0.7114, acc: 0.5941, precision: 0.7899, recall: 0.6477, f1: 0.7118, edges-srl-ontonotes_loss: 0.0217
10/01 03:24:25 AM: Update 7228: task edges-srl-ontonotes, batch 228 (7228): mcc: 0.7113, acc: 0.5938, precision: 0.7910, recall: 0.6467, f1: 0.7116, edges-srl-ontonotes_loss: 0.0220
10/01 03:24:35 AM: Update 7336: task edges-srl-ontonotes, batch 336 (7336): mcc: 0.7181, acc: 0.6027, precision: 0.7974, recall: 0.6536, f1: 0.7184, edges-srl-ontonotes_loss: 0.0216
10/01 03:24:45 AM: Update 7453: task edges-srl-ontonotes, batch 453 (7453): mcc: 0.7231, acc: 0.6083, precision: 0.8022, recall: 0.6586, f1: 0.7233, edges-srl-ontonotes_loss: 0.0213
10/01 03:24:55 AM: Update 7560: task edges-srl-ontonotes, batch 560 (7560): mcc: 0.7279, acc: 0.6150, precision: 0.8055, recall: 0.6644, f1: 0.7282, edges-srl-ontonotes_loss: 0.0210
10/01 03:25:05 AM: Update 7673: task edges-srl-ontonotes, batch 673 (7673): mcc: 0.7331, acc: 0.6214, precision: 0.8090, recall: 0.6709, f1: 0.7335, edges-srl-ontonotes_loss: 0.0207
10/01 03:25:15 AM: Update 7788: task edges-srl-ontonotes, batch 788 (7788): mcc: 0.7357, acc: 0.6250, precision: 0.8112, recall: 0.6739, f1: 0.7362, edges-srl-ontonotes_loss: 0.0205
10/01 03:25:25 AM: Update 7895: task edges-srl-ontonotes, batch 895 (7895): mcc: 0.7379, acc: 0.6279, precision: 0.8124, recall: 0.6767, f1: 0.7384, edges-srl-ontonotes_loss: 0.0204
10/01 03:25:34 AM: ***** Step 8000 / Validation 8 *****
10/01 03:25:34 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:25:34 AM: Validating...
10/01 03:25:35 AM: Evaluate: task edges-srl-ontonotes, batch 8 (157): mcc: 0.7932, acc: 0.6983, precision: 0.8621, recall: 0.7351, f1: 0.7936, edges-srl-ontonotes_loss: 0.0165
10/01 03:25:45 AM: Evaluate: task edges-srl-ontonotes, batch 112 (157): mcc: 0.7949, acc: 0.7116, precision: 0.8564, recall: 0.7431, f1: 0.7958, edges-srl-ontonotes_loss: 0.0166
10/01 03:25:50 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:25:50 AM: Best result seen so far for macro.
10/01 03:25:50 AM: Updating LR scheduler:
10/01 03:25:50 AM: 	Best result seen so far for macro_avg: 0.794
10/01 03:25:50 AM: 	# validation passes without improvement: 0
10/01 03:25:50 AM: edges-srl-ontonotes_loss: training: 0.020346 validation: 0.016822
10/01 03:25:50 AM: macro_avg: validation: 0.793949
10/01 03:25:50 AM: micro_avg: validation: 0.000000
10/01 03:25:50 AM: edges-srl-ontonotes_mcc: training: 0.737858 validation: 0.793063
10/01 03:25:50 AM: edges-srl-ontonotes_acc: training: 0.627836 validation: 0.710954
10/01 03:25:50 AM: edges-srl-ontonotes_precision: training: 0.811798 validation: 0.854570
10/01 03:25:50 AM: edges-srl-ontonotes_recall: training: 0.677186 validation: 0.741359
10/01 03:25:50 AM: edges-srl-ontonotes_f1: training: 0.738407 validation: 0.793949
10/01 03:25:50 AM: Global learning rate: 0.0001
10/01 03:25:50 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:25:55 AM: Update 8064: task edges-srl-ontonotes, batch 64 (8064): mcc: 0.7395, acc: 0.6337, precision: 0.8075, recall: 0.6838, f1: 0.7405, edges-srl-ontonotes_loss: 0.0205
10/01 03:26:05 AM: Update 8178: task edges-srl-ontonotes, batch 178 (8178): mcc: 0.7391, acc: 0.6331, precision: 0.8087, recall: 0.6820, f1: 0.7400, edges-srl-ontonotes_loss: 0.0205
10/01 03:26:15 AM: Update 8257: task edges-srl-ontonotes, batch 257 (8257): mcc: 0.7322, acc: 0.6260, precision: 0.8036, recall: 0.6738, f1: 0.7330, edges-srl-ontonotes_loss: 0.0208
10/01 03:26:25 AM: Update 8376: task edges-srl-ontonotes, batch 376 (8376): mcc: 0.7267, acc: 0.6190, precision: 0.8001, recall: 0.6668, f1: 0.7274, edges-srl-ontonotes_loss: 0.0211
10/01 03:26:36 AM: Update 8484: task edges-srl-ontonotes, batch 484 (8484): mcc: 0.7248, acc: 0.6169, precision: 0.7997, recall: 0.6638, f1: 0.7254, edges-srl-ontonotes_loss: 0.0212
10/01 03:26:46 AM: Update 8577: task edges-srl-ontonotes, batch 577 (8577): mcc: 0.7252, acc: 0.6178, precision: 0.7996, recall: 0.6646, f1: 0.7258, edges-srl-ontonotes_loss: 0.0211
10/01 03:26:56 AM: Update 8682: task edges-srl-ontonotes, batch 682 (8682): mcc: 0.7261, acc: 0.6186, precision: 0.8004, recall: 0.6656, f1: 0.7268, edges-srl-ontonotes_loss: 0.0210
10/01 03:27:06 AM: Update 8795: task edges-srl-ontonotes, batch 795 (8795): mcc: 0.7267, acc: 0.6199, precision: 0.8006, recall: 0.6664, f1: 0.7273, edges-srl-ontonotes_loss: 0.0209
10/01 03:27:16 AM: Update 8886: task edges-srl-ontonotes, batch 886 (8886): mcc: 0.7244, acc: 0.6170, precision: 0.7992, recall: 0.6635, f1: 0.7251, edges-srl-ontonotes_loss: 0.0211
10/01 03:27:26 AM: Update 8988: task edges-srl-ontonotes, batch 988 (8988): mcc: 0.7206, acc: 0.6123, precision: 0.7965, recall: 0.6588, f1: 0.7211, edges-srl-ontonotes_loss: 0.0213
10/01 03:27:27 AM: ***** Step 9000 / Validation 9 *****
10/01 03:27:27 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:27:27 AM: Validating...
10/01 03:27:36 AM: Evaluate: task edges-srl-ontonotes, batch 93 (157): mcc: 0.7840, acc: 0.6963, precision: 0.8582, recall: 0.7217, f1: 0.7841, edges-srl-ontonotes_loss: 0.0170
10/01 03:27:42 AM: Updating LR scheduler:
10/01 03:27:42 AM: 	Best result seen so far for macro_avg: 0.794
10/01 03:27:42 AM: 	# validation passes without improvement: 1
10/01 03:27:42 AM: edges-srl-ontonotes_loss: training: 0.021301 validation: 0.016695
10/01 03:27:42 AM: macro_avg: validation: 0.790812
10/01 03:27:42 AM: micro_avg: validation: 0.000000
10/01 03:27:42 AM: edges-srl-ontonotes_mcc: training: 0.720345 validation: 0.790543
10/01 03:27:42 AM: edges-srl-ontonotes_acc: training: 0.611925 validation: 0.706181
10/01 03:27:42 AM: edges-srl-ontonotes_precision: training: 0.796520 validation: 0.860688
10/01 03:27:42 AM: edges-srl-ontonotes_recall: training: 0.658357 validation: 0.731429
10/01 03:27:42 AM: edges-srl-ontonotes_f1: training: 0.720878 validation: 0.790812
10/01 03:27:42 AM: Global learning rate: 0.0001
10/01 03:27:42 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:27:46 AM: Update 9038: task edges-srl-ontonotes, batch 38 (9038): mcc: 0.7108, acc: 0.5997, precision: 0.7907, recall: 0.6461, f1: 0.7111, edges-srl-ontonotes_loss: 0.0220
10/01 03:27:57 AM: Update 9125: task edges-srl-ontonotes, batch 125 (9125): mcc: 0.7042, acc: 0.5904, precision: 0.7869, recall: 0.6373, f1: 0.7042, edges-srl-ontonotes_loss: 0.0223
10/01 03:28:07 AM: Update 9230: task edges-srl-ontonotes, batch 230 (9230): mcc: 0.7055, acc: 0.5901, precision: 0.7879, recall: 0.6389, f1: 0.7056, edges-srl-ontonotes_loss: 0.0223
10/01 03:28:17 AM: Update 9338: task edges-srl-ontonotes, batch 338 (9338): mcc: 0.7072, acc: 0.5924, precision: 0.7885, recall: 0.6414, f1: 0.7074, edges-srl-ontonotes_loss: 0.0222
10/01 03:28:27 AM: Update 9438: task edges-srl-ontonotes, batch 438 (9438): mcc: 0.7073, acc: 0.5930, precision: 0.7900, recall: 0.6403, f1: 0.7073, edges-srl-ontonotes_loss: 0.0222
10/01 03:28:37 AM: Update 9536: task edges-srl-ontonotes, batch 536 (9536): mcc: 0.7139, acc: 0.6019, precision: 0.7946, recall: 0.6484, f1: 0.7141, edges-srl-ontonotes_loss: 0.0218
10/01 03:28:48 AM: Update 9638: task edges-srl-ontonotes, batch 638 (9638): mcc: 0.7179, acc: 0.6079, precision: 0.7965, recall: 0.6538, f1: 0.7182, edges-srl-ontonotes_loss: 0.0216
10/01 03:28:58 AM: Update 9739: task edges-srl-ontonotes, batch 739 (9739): mcc: 0.7203, acc: 0.6115, precision: 0.7984, recall: 0.6567, f1: 0.7206, edges-srl-ontonotes_loss: 0.0214
10/01 03:29:08 AM: Update 9836: task edges-srl-ontonotes, batch 836 (9836): mcc: 0.7226, acc: 0.6148, precision: 0.7996, recall: 0.6598, f1: 0.7230, edges-srl-ontonotes_loss: 0.0213
10/01 03:29:18 AM: Update 9942: task edges-srl-ontonotes, batch 942 (9942): mcc: 0.7252, acc: 0.6181, precision: 0.8016, recall: 0.6629, f1: 0.7256, edges-srl-ontonotes_loss: 0.0211
10/01 03:29:24 AM: ***** Step 10000 / Validation 10 *****
10/01 03:29:24 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:29:24 AM: Validating...
10/01 03:29:28 AM: Evaluate: task edges-srl-ontonotes, batch 44 (157): mcc: 0.7733, acc: 0.6869, precision: 0.8517, recall: 0.7078, f1: 0.7731, edges-srl-ontonotes_loss: 0.0179
10/01 03:29:38 AM: Evaluate: task edges-srl-ontonotes, batch 146 (157): mcc: 0.7915, acc: 0.7105, precision: 0.8595, recall: 0.7342, f1: 0.7919, edges-srl-ontonotes_loss: 0.0167
10/01 03:29:39 AM: Updating LR scheduler:
10/01 03:29:39 AM: 	Best result seen so far for macro_avg: 0.794
10/01 03:29:39 AM: 	# validation passes without improvement: 2
10/01 03:29:39 AM: edges-srl-ontonotes_loss: training: 0.021063 validation: 0.016878
10/01 03:29:39 AM: macro_avg: validation: 0.790462
10/01 03:29:39 AM: micro_avg: validation: 0.000000
10/01 03:29:39 AM: edges-srl-ontonotes_mcc: training: 0.726062 validation: 0.790060
10/01 03:29:39 AM: edges-srl-ontonotes_acc: training: 0.619477 validation: 0.708721
10/01 03:29:39 AM: edges-srl-ontonotes_precision: training: 0.802266 validation: 0.858587
10/01 03:29:39 AM: edges-srl-ontonotes_recall: training: 0.663866 validation: 0.732353
10/01 03:29:39 AM: edges-srl-ontonotes_f1: training: 0.726534 validation: 0.790462
10/01 03:29:39 AM: Global learning rate: 0.0001
10/01 03:29:39 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:29:48 AM: Update 10064: task edges-srl-ontonotes, batch 64 (10064): mcc: 0.7497, acc: 0.6533, precision: 0.8168, recall: 0.6945, f1: 0.7507, edges-srl-ontonotes_loss: 0.0196
10/01 03:29:58 AM: Update 10166: task edges-srl-ontonotes, batch 166 (10166): mcc: 0.7544, acc: 0.6587, precision: 0.8229, recall: 0.6979, f1: 0.7552, edges-srl-ontonotes_loss: 0.0193
10/01 03:30:08 AM: Update 10272: task edges-srl-ontonotes, batch 272 (10272): mcc: 0.7535, acc: 0.6572, precision: 0.8218, recall: 0.6972, f1: 0.7543, edges-srl-ontonotes_loss: 0.0193
10/01 03:30:19 AM: Update 10377: task edges-srl-ontonotes, batch 377 (10377): mcc: 0.7538, acc: 0.6570, precision: 0.8217, recall: 0.6978, f1: 0.7547, edges-srl-ontonotes_loss: 0.0193
10/01 03:30:29 AM: Update 10481: task edges-srl-ontonotes, batch 481 (10481): mcc: 0.7509, acc: 0.6535, precision: 0.8201, recall: 0.6938, f1: 0.7517, edges-srl-ontonotes_loss: 0.0195
10/01 03:30:39 AM: Update 10587: task edges-srl-ontonotes, batch 587 (10587): mcc: 0.7493, acc: 0.6515, precision: 0.8190, recall: 0.6919, f1: 0.7501, edges-srl-ontonotes_loss: 0.0196
10/01 03:30:50 AM: Update 10690: task edges-srl-ontonotes, batch 690 (10690): mcc: 0.7477, acc: 0.6498, precision: 0.8177, recall: 0.6900, f1: 0.7485, edges-srl-ontonotes_loss: 0.0198
10/01 03:31:00 AM: Update 10797: task edges-srl-ontonotes, batch 797 (10797): mcc: 0.7458, acc: 0.6472, precision: 0.8165, recall: 0.6877, f1: 0.7465, edges-srl-ontonotes_loss: 0.0198
10/01 03:31:10 AM: Update 10905: task edges-srl-ontonotes, batch 905 (10905): mcc: 0.7445, acc: 0.6457, precision: 0.8155, recall: 0.6861, f1: 0.7453, edges-srl-ontonotes_loss: 0.0199
10/01 03:31:19 AM: ***** Step 11000 / Validation 11 *****
10/01 03:31:19 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:31:19 AM: Validating...
10/01 03:31:20 AM: Evaluate: task edges-srl-ontonotes, batch 9 (157): mcc: 0.7630, acc: 0.6683, precision: 0.8527, recall: 0.6884, f1: 0.7618, edges-srl-ontonotes_loss: 0.0181
10/01 03:31:30 AM: Evaluate: task edges-srl-ontonotes, batch 113 (157): mcc: 0.7779, acc: 0.6937, precision: 0.8485, recall: 0.7188, f1: 0.7783, edges-srl-ontonotes_loss: 0.0175
10/01 03:31:34 AM: Updating LR scheduler:
10/01 03:31:34 AM: 	Best result seen so far for macro_avg: 0.794
10/01 03:31:34 AM: 	# validation passes without improvement: 3
10/01 03:31:34 AM: edges-srl-ontonotes_loss: training: 0.019940 validation: 0.017372
10/01 03:31:34 AM: macro_avg: validation: 0.783495
10/01 03:31:34 AM: micro_avg: validation: 0.000000
10/01 03:31:34 AM: edges-srl-ontonotes_mcc: training: 0.743460 validation: 0.783066
10/01 03:31:34 AM: edges-srl-ontonotes_acc: training: 0.644385 validation: 0.700947
10/01 03:31:34 AM: edges-srl-ontonotes_precision: training: 0.814952 validation: 0.852308
10/01 03:31:34 AM: edges-srl-ontonotes_recall: training: 0.684684 validation: 0.724963
10/01 03:31:34 AM: edges-srl-ontonotes_f1: training: 0.744160 validation: 0.783495
10/01 03:31:34 AM: Global learning rate: 0.0001
10/01 03:31:34 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:31:40 AM: Update 11052: task edges-srl-ontonotes, batch 52 (11052): mcc: 0.7426, acc: 0.6418, precision: 0.8154, recall: 0.6827, f1: 0.7432, edges-srl-ontonotes_loss: 0.0199
10/01 03:31:50 AM: Update 11159: task edges-srl-ontonotes, batch 159 (11159): mcc: 0.7333, acc: 0.6351, precision: 0.8062, recall: 0.6738, f1: 0.7340, edges-srl-ontonotes_loss: 0.0204
10/01 03:32:00 AM: Update 11265: task edges-srl-ontonotes, batch 265 (11265): mcc: 0.7355, acc: 0.6362, precision: 0.8094, recall: 0.6749, f1: 0.7360, edges-srl-ontonotes_loss: 0.0202
10/01 03:32:10 AM: Update 11339: task edges-srl-ontonotes, batch 339 (11339): mcc: 0.7373, acc: 0.6389, precision: 0.8107, recall: 0.6770, f1: 0.7379, edges-srl-ontonotes_loss: 0.0202
10/01 03:32:20 AM: Update 11447: task edges-srl-ontonotes, batch 447 (11447): mcc: 0.7386, acc: 0.6403, precision: 0.8114, recall: 0.6789, f1: 0.7392, edges-srl-ontonotes_loss: 0.0201
10/01 03:32:30 AM: Update 11553: task edges-srl-ontonotes, batch 553 (11553): mcc: 0.7411, acc: 0.6442, precision: 0.8129, recall: 0.6821, f1: 0.7418, edges-srl-ontonotes_loss: 0.0200
10/01 03:32:40 AM: Update 11648: task edges-srl-ontonotes, batch 648 (11648): mcc: 0.7423, acc: 0.6461, precision: 0.8139, recall: 0.6835, f1: 0.7430, edges-srl-ontonotes_loss: 0.0199
10/01 03:32:50 AM: Update 11755: task edges-srl-ontonotes, batch 755 (11755): mcc: 0.7433, acc: 0.6473, precision: 0.8149, recall: 0.6844, f1: 0.7440, edges-srl-ontonotes_loss: 0.0199
10/01 03:33:00 AM: Update 11860: task edges-srl-ontonotes, batch 860 (11860): mcc: 0.7440, acc: 0.6484, precision: 0.8149, recall: 0.6857, f1: 0.7447, edges-srl-ontonotes_loss: 0.0198
10/01 03:33:10 AM: Update 11950: task edges-srl-ontonotes, batch 950 (11950): mcc: 0.7443, acc: 0.6487, precision: 0.8154, recall: 0.6859, f1: 0.7450, edges-srl-ontonotes_loss: 0.0198
10/01 03:33:16 AM: ***** Step 12000 / Validation 12 *****
10/01 03:33:16 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:33:16 AM: Validating...
10/01 03:33:21 AM: Evaluate: task edges-srl-ontonotes, batch 53 (157): mcc: 0.7687, acc: 0.6873, precision: 0.8364, recall: 0.7124, f1: 0.7694, edges-srl-ontonotes_loss: 0.0183
10/01 03:33:31 AM: Evaluate: task edges-srl-ontonotes, batch 154 (157): mcc: 0.7836, acc: 0.7053, precision: 0.8462, recall: 0.7313, f1: 0.7846, edges-srl-ontonotes_loss: 0.0174
10/01 03:33:31 AM: Updating LR scheduler:
10/01 03:33:31 AM: 	Best result seen so far for macro_avg: 0.794
10/01 03:33:31 AM: 	# validation passes without improvement: 0
10/01 03:33:31 AM: edges-srl-ontonotes_loss: training: 0.019899 validation: 0.017438
10/01 03:33:31 AM: macro_avg: validation: 0.784519
10/01 03:33:31 AM: micro_avg: validation: 0.000000
10/01 03:33:31 AM: edges-srl-ontonotes_mcc: training: 0.742941 validation: 0.783610
10/01 03:33:31 AM: edges-srl-ontonotes_acc: training: 0.647039 validation: 0.705181
10/01 03:33:31 AM: edges-srl-ontonotes_precision: training: 0.814674 validation: 0.846435
10/01 03:33:31 AM: edges-srl-ontonotes_recall: training: 0.683975 validation: 0.731045
10/01 03:33:31 AM: edges-srl-ontonotes_f1: training: 0.743626 validation: 0.784519
10/01 03:33:31 AM: Global learning rate: 5e-05
10/01 03:33:31 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:33:41 AM: Update 12089: task edges-srl-ontonotes, batch 89 (12089): mcc: 0.7299, acc: 0.6279, precision: 0.8080, recall: 0.6661, f1: 0.7302, edges-srl-ontonotes_loss: 0.0211
10/01 03:33:51 AM: Update 12178: task edges-srl-ontonotes, batch 178 (12178): mcc: 0.7290, acc: 0.6276, precision: 0.8090, recall: 0.6636, f1: 0.7291, edges-srl-ontonotes_loss: 0.0210
10/01 03:34:02 AM: Update 12255: task edges-srl-ontonotes, batch 255 (12255): mcc: 0.7298, acc: 0.6282, precision: 0.8091, recall: 0.6649, f1: 0.7299, edges-srl-ontonotes_loss: 0.0208
10/01 03:34:12 AM: Update 12372: task edges-srl-ontonotes, batch 372 (12372): mcc: 0.7313, acc: 0.6305, precision: 0.8077, recall: 0.6687, f1: 0.7317, edges-srl-ontonotes_loss: 0.0206
10/01 03:34:22 AM: Update 12494: task edges-srl-ontonotes, batch 494 (12494): mcc: 0.7354, acc: 0.6355, precision: 0.8099, recall: 0.6743, f1: 0.7359, edges-srl-ontonotes_loss: 0.0203
10/01 03:34:32 AM: Update 12605: task edges-srl-ontonotes, batch 605 (12605): mcc: 0.7396, acc: 0.6398, precision: 0.8127, recall: 0.6795, f1: 0.7402, edges-srl-ontonotes_loss: 0.0200
10/01 03:34:42 AM: Update 12741: task edges-srl-ontonotes, batch 741 (12741): mcc: 0.7451, acc: 0.6461, precision: 0.8159, recall: 0.6868, f1: 0.7458, edges-srl-ontonotes_loss: 0.0195
10/01 03:34:52 AM: Update 12877: task edges-srl-ontonotes, batch 877 (12877): mcc: 0.7496, acc: 0.6517, precision: 0.8186, recall: 0.6927, f1: 0.7504, edges-srl-ontonotes_loss: 0.0192
10/01 03:35:02 AM: Update 12997: task edges-srl-ontonotes, batch 997 (12997): mcc: 0.7515, acc: 0.6538, precision: 0.8200, recall: 0.6951, f1: 0.7524, edges-srl-ontonotes_loss: 0.0191
10/01 03:35:02 AM: ***** Step 13000 / Validation 13 *****
10/01 03:35:02 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:35:02 AM: Validating...
10/01 03:35:12 AM: Evaluate: task edges-srl-ontonotes, batch 101 (157): mcc: 0.7826, acc: 0.6980, precision: 0.8617, recall: 0.7162, f1: 0.7822, edges-srl-ontonotes_loss: 0.0172
10/01 03:35:18 AM: Updating LR scheduler:
10/01 03:35:18 AM: 	Best result seen so far for macro_avg: 0.794
10/01 03:35:18 AM: 	# validation passes without improvement: 1
10/01 03:35:18 AM: edges-srl-ontonotes_loss: training: 0.019111 validation: 0.016838
10/01 03:35:18 AM: macro_avg: validation: 0.789832
10/01 03:35:18 AM: micro_avg: validation: 0.000000
10/01 03:35:18 AM: edges-srl-ontonotes_mcc: training: 0.751610 validation: 0.789909
10/01 03:35:18 AM: edges-srl-ontonotes_acc: training: 0.653909 validation: 0.708721
10/01 03:35:18 AM: edges-srl-ontonotes_precision: training: 0.820127 validation: 0.864385
10/01 03:35:18 AM: edges-srl-ontonotes_recall: training: 0.695115 validation: 0.727119
10/01 03:35:18 AM: edges-srl-ontonotes_f1: training: 0.752464 validation: 0.789832
10/01 03:35:18 AM: Global learning rate: 5e-05
10/01 03:35:18 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:35:22 AM: Update 13057: task edges-srl-ontonotes, batch 57 (13057): mcc: 0.7578, acc: 0.6619, precision: 0.8259, recall: 0.7014, f1: 0.7586, edges-srl-ontonotes_loss: 0.0186
10/01 03:35:32 AM: Update 13188: task edges-srl-ontonotes, batch 188 (13188): mcc: 0.7642, acc: 0.6695, precision: 0.8299, recall: 0.7098, f1: 0.7652, edges-srl-ontonotes_loss: 0.0182
10/01 03:35:42 AM: Update 13309: task edges-srl-ontonotes, batch 309 (13309): mcc: 0.7637, acc: 0.6668, precision: 0.8292, recall: 0.7095, f1: 0.7647, edges-srl-ontonotes_loss: 0.0182
10/01 03:35:52 AM: Update 13447: task edges-srl-ontonotes, batch 447 (13447): mcc: 0.7654, acc: 0.6688, precision: 0.8292, recall: 0.7126, f1: 0.7665, edges-srl-ontonotes_loss: 0.0181
10/01 03:36:02 AM: Update 13546: task edges-srl-ontonotes, batch 546 (13546): mcc: 0.7666, acc: 0.6705, precision: 0.8297, recall: 0.7143, f1: 0.7677, edges-srl-ontonotes_loss: 0.0180
10/01 03:36:12 AM: Update 13689: task edges-srl-ontonotes, batch 689 (13689): mcc: 0.7724, acc: 0.6777, precision: 0.8348, recall: 0.7206, f1: 0.7735, edges-srl-ontonotes_loss: 0.0177
10/01 03:36:23 AM: Update 13820: task edges-srl-ontonotes, batch 820 (13820): mcc: 0.7759, acc: 0.6822, precision: 0.8372, recall: 0.7250, f1: 0.7771, edges-srl-ontonotes_loss: 0.0175
10/01 03:36:33 AM: Update 13925: task edges-srl-ontonotes, batch 925 (13925): mcc: 0.7724, acc: 0.6782, precision: 0.8345, recall: 0.7208, f1: 0.7735, edges-srl-ontonotes_loss: 0.0178
10/01 03:36:40 AM: ***** Step 14000 / Validation 14 *****
10/01 03:36:40 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:36:40 AM: Validating...
10/01 03:36:43 AM: Evaluate: task edges-srl-ontonotes, batch 32 (157): mcc: 0.7951, acc: 0.7141, precision: 0.8691, recall: 0.7326, f1: 0.7950, edges-srl-ontonotes_loss: 0.0164
10/01 03:36:53 AM: Evaluate: task edges-srl-ontonotes, batch 134 (157): mcc: 0.8105, acc: 0.7388, precision: 0.8720, recall: 0.7584, f1: 0.8112, edges-srl-ontonotes_loss: 0.0157
10/01 03:36:55 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:36:55 AM: Best result seen so far for macro.
10/01 03:36:55 AM: Updating LR scheduler:
10/01 03:36:55 AM: 	Best result seen so far for macro_avg: 0.807
10/01 03:36:55 AM: 	# validation passes without improvement: 0
10/01 03:36:55 AM: edges-srl-ontonotes_loss: training: 0.017840 validation: 0.015980
10/01 03:36:55 AM: macro_avg: validation: 0.807430
10/01 03:36:55 AM: micro_avg: validation: 0.000000
10/01 03:36:55 AM: edges-srl-ontonotes_mcc: training: 0.771127 validation: 0.806872
10/01 03:36:55 AM: edges-srl-ontonotes_acc: training: 0.676914 validation: 0.733585
10/01 03:36:55 AM: edges-srl-ontonotes_precision: training: 0.833584 validation: 0.870571
10/01 03:36:55 AM: edges-srl-ontonotes_recall: training: 0.719266 validation: 0.752829
10/01 03:36:55 AM: edges-srl-ontonotes_f1: training: 0.772217 validation: 0.807430
10/01 03:36:55 AM: Global learning rate: 5e-05
10/01 03:36:55 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:37:03 AM: Update 14090: task edges-srl-ontonotes, batch 90 (14090): mcc: 0.7637, acc: 0.6670, precision: 0.8285, recall: 0.7100, f1: 0.7647, edges-srl-ontonotes_loss: 0.0186
10/01 03:37:13 AM: Update 14177: task edges-srl-ontonotes, batch 177 (14177): mcc: 0.7540, acc: 0.6548, precision: 0.8233, recall: 0.6967, f1: 0.7547, edges-srl-ontonotes_loss: 0.0193
10/01 03:37:23 AM: Update 14285: task edges-srl-ontonotes, batch 285 (14285): mcc: 0.7478, acc: 0.6497, precision: 0.8176, recall: 0.6904, f1: 0.7486, edges-srl-ontonotes_loss: 0.0197
10/01 03:37:33 AM: Update 14392: task edges-srl-ontonotes, batch 392 (14392): mcc: 0.7455, acc: 0.6467, precision: 0.8155, recall: 0.6881, f1: 0.7464, edges-srl-ontonotes_loss: 0.0199
10/01 03:37:45 AM: Update 14493: task edges-srl-ontonotes, batch 493 (14493): mcc: 0.7449, acc: 0.6459, precision: 0.8148, recall: 0.6873, f1: 0.7457, edges-srl-ontonotes_loss: 0.0199
10/01 03:37:55 AM: Update 14614: task edges-srl-ontonotes, batch 614 (14614): mcc: 0.7484, acc: 0.6501, precision: 0.8180, recall: 0.6911, f1: 0.7492, edges-srl-ontonotes_loss: 0.0196
10/01 03:38:05 AM: Update 14738: task edges-srl-ontonotes, batch 738 (14738): mcc: 0.7507, acc: 0.6529, precision: 0.8198, recall: 0.6938, f1: 0.7516, edges-srl-ontonotes_loss: 0.0194
10/01 03:38:15 AM: Update 14840: task edges-srl-ontonotes, batch 840 (14840): mcc: 0.7526, acc: 0.6552, precision: 0.8211, recall: 0.6961, f1: 0.7535, edges-srl-ontonotes_loss: 0.0193
10/01 03:38:25 AM: Update 14960: task edges-srl-ontonotes, batch 960 (14960): mcc: 0.7553, acc: 0.6588, precision: 0.8234, recall: 0.6991, f1: 0.7562, edges-srl-ontonotes_loss: 0.0191
10/01 03:38:28 AM: ***** Step 15000 / Validation 15 *****
10/01 03:38:28 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:38:28 AM: Validating...
10/01 03:38:35 AM: Evaluate: task edges-srl-ontonotes, batch 72 (157): mcc: 0.8007, acc: 0.7272, precision: 0.8696, recall: 0.7424, f1: 0.8010, edges-srl-ontonotes_loss: 0.0157
10/01 03:38:44 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:38:44 AM: Best result seen so far for macro.
10/01 03:38:44 AM: Updating LR scheduler:
10/01 03:38:44 AM: 	Best result seen so far for macro_avg: 0.810
10/01 03:38:44 AM: 	# validation passes without improvement: 0
10/01 03:38:44 AM: edges-srl-ontonotes_loss: training: 0.019059 validation: 0.015366
10/01 03:38:44 AM: macro_avg: validation: 0.810161
10/01 03:38:44 AM: micro_avg: validation: 0.000000
10/01 03:38:44 AM: edges-srl-ontonotes_mcc: training: 0.756571 validation: 0.809691
10/01 03:38:44 AM: edges-srl-ontonotes_acc: training: 0.660345 validation: 0.737665
10/01 03:38:44 AM: edges-srl-ontonotes_precision: training: 0.824213 validation: 0.874142
10/01 03:38:44 AM: edges-srl-ontonotes_recall: training: 0.700673 validation: 0.754907
10/01 03:38:44 AM: edges-srl-ontonotes_f1: training: 0.757438 validation: 0.810161
10/01 03:38:44 AM: Global learning rate: 5e-05
10/01 03:38:44 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:38:45 AM: Update 15017: task edges-srl-ontonotes, batch 17 (15017): mcc: 0.7703, acc: 0.6779, precision: 0.8288, recall: 0.7220, f1: 0.7717, edges-srl-ontonotes_loss: 0.0186
10/01 03:38:55 AM: Update 15124: task edges-srl-ontonotes, batch 124 (15124): mcc: 0.7758, acc: 0.6870, precision: 0.8375, recall: 0.7244, f1: 0.7768, edges-srl-ontonotes_loss: 0.0177
10/01 03:39:05 AM: Update 15239: task edges-srl-ontonotes, batch 239 (15239): mcc: 0.7660, acc: 0.6759, precision: 0.8301, recall: 0.7128, f1: 0.7670, edges-srl-ontonotes_loss: 0.0184
10/01 03:39:15 AM: Update 15354: task edges-srl-ontonotes, batch 354 (15354): mcc: 0.7651, acc: 0.6748, precision: 0.8290, recall: 0.7122, f1: 0.7662, edges-srl-ontonotes_loss: 0.0185
10/01 03:39:25 AM: Update 15435: task edges-srl-ontonotes, batch 435 (15435): mcc: 0.7638, acc: 0.6732, precision: 0.8281, recall: 0.7105, f1: 0.7648, edges-srl-ontonotes_loss: 0.0186
10/01 03:39:35 AM: Update 15542: task edges-srl-ontonotes, batch 542 (15542): mcc: 0.7586, acc: 0.6674, precision: 0.8237, recall: 0.7048, f1: 0.7596, edges-srl-ontonotes_loss: 0.0189
10/01 03:39:46 AM: Update 15655: task edges-srl-ontonotes, batch 655 (15655): mcc: 0.7541, acc: 0.6616, precision: 0.8201, recall: 0.6997, f1: 0.7552, edges-srl-ontonotes_loss: 0.0192
10/01 03:39:56 AM: Update 15755: task edges-srl-ontonotes, batch 755 (15755): mcc: 0.7526, acc: 0.6595, precision: 0.8190, recall: 0.6980, f1: 0.7537, edges-srl-ontonotes_loss: 0.0192
10/01 03:40:06 AM: Update 15865: task edges-srl-ontonotes, batch 865 (15865): mcc: 0.7515, acc: 0.6585, precision: 0.8180, recall: 0.6968, f1: 0.7526, edges-srl-ontonotes_loss: 0.0192
10/01 03:40:16 AM: Update 15972: task edges-srl-ontonotes, batch 972 (15972): mcc: 0.7511, acc: 0.6578, precision: 0.8177, recall: 0.6962, f1: 0.7521, edges-srl-ontonotes_loss: 0.0193
10/01 03:40:18 AM: ***** Step 16000 / Validation 16 *****
10/01 03:40:18 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:40:18 AM: Validating...
10/01 03:40:26 AM: Evaluate: task edges-srl-ontonotes, batch 77 (157): mcc: 0.8043, acc: 0.7325, precision: 0.8668, recall: 0.7513, f1: 0.8049, edges-srl-ontonotes_loss: 0.0158
10/01 03:40:34 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:40:34 AM: Best result seen so far for macro.
10/01 03:40:34 AM: Updating LR scheduler:
10/01 03:40:34 AM: 	Best result seen so far for macro_avg: 0.817
10/01 03:40:34 AM: 	# validation passes without improvement: 0
10/01 03:40:34 AM: edges-srl-ontonotes_loss: training: 0.019244 validation: 0.015192
10/01 03:40:34 AM: macro_avg: validation: 0.816880
10/01 03:40:34 AM: micro_avg: validation: 0.000000
10/01 03:40:34 AM: edges-srl-ontonotes_mcc: training: 0.750920 validation: 0.815935
10/01 03:40:34 AM: edges-srl-ontonotes_acc: training: 0.657653 validation: 0.748826
10/01 03:40:34 AM: edges-srl-ontonotes_precision: training: 0.817506 validation: 0.872214
10/01 03:40:34 AM: edges-srl-ontonotes_recall: training: 0.696106 validation: 0.768147
10/01 03:40:34 AM: edges-srl-ontonotes_f1: training: 0.751937 validation: 0.816880
10/01 03:40:34 AM: Global learning rate: 5e-05
10/01 03:40:34 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:40:36 AM: Update 16024: task edges-srl-ontonotes, batch 24 (16024): mcc: 0.7594, acc: 0.6724, precision: 0.8224, recall: 0.7075, f1: 0.7606, edges-srl-ontonotes_loss: 0.0190
10/01 03:40:46 AM: Update 16117: task edges-srl-ontonotes, batch 117 (16117): mcc: 0.7386, acc: 0.6444, precision: 0.8070, recall: 0.6826, f1: 0.7396, edges-srl-ontonotes_loss: 0.0201
10/01 03:40:56 AM: Update 16220: task edges-srl-ontonotes, batch 220 (16220): mcc: 0.7332, acc: 0.6341, precision: 0.8072, recall: 0.6727, f1: 0.7338, edges-srl-ontonotes_loss: 0.0206
10/01 03:41:06 AM: Update 16321: task edges-srl-ontonotes, batch 321 (16321): mcc: 0.7291, acc: 0.6292, precision: 0.8043, recall: 0.6677, f1: 0.7296, edges-srl-ontonotes_loss: 0.0208
10/01 03:41:16 AM: Update 16394: task edges-srl-ontonotes, batch 394 (16394): mcc: 0.7271, acc: 0.6266, precision: 0.8030, recall: 0.6653, f1: 0.7276, edges-srl-ontonotes_loss: 0.0210
10/01 03:41:26 AM: Update 16502: task edges-srl-ontonotes, batch 502 (16502): mcc: 0.7273, acc: 0.6269, precision: 0.8030, recall: 0.6654, f1: 0.7278, edges-srl-ontonotes_loss: 0.0210
10/01 03:41:36 AM: Update 16608: task edges-srl-ontonotes, batch 608 (16608): mcc: 0.7260, acc: 0.6248, precision: 0.8021, recall: 0.6639, f1: 0.7265, edges-srl-ontonotes_loss: 0.0210
10/01 03:41:46 AM: Update 16696: task edges-srl-ontonotes, batch 696 (16696): mcc: 0.7265, acc: 0.6251, precision: 0.8024, recall: 0.6644, f1: 0.7269, edges-srl-ontonotes_loss: 0.0209
10/01 03:41:56 AM: Update 16793: task edges-srl-ontonotes, batch 793 (16793): mcc: 0.7300, acc: 0.6299, precision: 0.8046, recall: 0.6691, f1: 0.7306, edges-srl-ontonotes_loss: 0.0207
10/01 03:42:06 AM: Update 16896: task edges-srl-ontonotes, batch 896 (16896): mcc: 0.7333, acc: 0.6342, precision: 0.8070, recall: 0.6730, f1: 0.7339, edges-srl-ontonotes_loss: 0.0204
10/01 03:42:16 AM: Update 16993: task edges-srl-ontonotes, batch 993 (16993): mcc: 0.7354, acc: 0.6370, precision: 0.8079, recall: 0.6759, f1: 0.7361, edges-srl-ontonotes_loss: 0.0203
10/01 03:42:18 AM: ***** Step 17000 / Validation 17 *****
10/01 03:42:18 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:42:18 AM: Validating...
10/01 03:42:26 AM: Evaluate: task edges-srl-ontonotes, batch 85 (157): mcc: 0.8027, acc: 0.7327, precision: 0.8667, recall: 0.7486, f1: 0.8033, edges-srl-ontonotes_loss: 0.0159
10/01 03:42:33 AM: Updating LR scheduler:
10/01 03:42:33 AM: 	Best result seen so far for macro_avg: 0.817
10/01 03:42:33 AM: 	# validation passes without improvement: 1
10/01 03:42:33 AM: edges-srl-ontonotes_loss: training: 0.020313 validation: 0.015467
10/01 03:42:33 AM: macro_avg: validation: 0.811992
10/01 03:42:33 AM: micro_avg: validation: 0.000000
10/01 03:42:33 AM: edges-srl-ontonotes_mcc: training: 0.735360 validation: 0.811173
10/01 03:42:33 AM: edges-srl-ontonotes_acc: training: 0.636996 validation: 0.743592
10/01 03:42:33 AM: edges-srl-ontonotes_precision: training: 0.807969 validation: 0.870323
10/01 03:42:33 AM: edges-srl-ontonotes_recall: training: 0.675894 validation: 0.760988
10/01 03:42:33 AM: edges-srl-ontonotes_f1: training: 0.736053 validation: 0.811992
10/01 03:42:33 AM: Global learning rate: 5e-05
10/01 03:42:33 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:42:36 AM: Update 17028: task edges-srl-ontonotes, batch 28 (17028): mcc: 0.7521, acc: 0.6588, precision: 0.8197, recall: 0.6963, f1: 0.7530, edges-srl-ontonotes_loss: 0.0192
10/01 03:42:46 AM: Update 17132: task edges-srl-ontonotes, batch 132 (17132): mcc: 0.7627, acc: 0.6740, precision: 0.8252, recall: 0.7111, f1: 0.7639, edges-srl-ontonotes_loss: 0.0187
10/01 03:42:56 AM: Update 17236: task edges-srl-ontonotes, batch 236 (17236): mcc: 0.7629, acc: 0.6737, precision: 0.8270, recall: 0.7099, f1: 0.7640, edges-srl-ontonotes_loss: 0.0186
10/01 03:43:06 AM: Update 17324: task edges-srl-ontonotes, batch 324 (17324): mcc: 0.7597, acc: 0.6698, precision: 0.8246, recall: 0.7060, f1: 0.7607, edges-srl-ontonotes_loss: 0.0187
10/01 03:43:17 AM: Update 17426: task edges-srl-ontonotes, batch 426 (17426): mcc: 0.7615, acc: 0.6725, precision: 0.8261, recall: 0.7080, f1: 0.7625, edges-srl-ontonotes_loss: 0.0186
10/01 03:43:27 AM: Update 17521: task edges-srl-ontonotes, batch 521 (17521): mcc: 0.7637, acc: 0.6748, precision: 0.8283, recall: 0.7103, f1: 0.7648, edges-srl-ontonotes_loss: 0.0185
10/01 03:43:37 AM: Update 17617: task edges-srl-ontonotes, batch 617 (17617): mcc: 0.7651, acc: 0.6764, precision: 0.8295, recall: 0.7117, f1: 0.7661, edges-srl-ontonotes_loss: 0.0185
10/01 03:43:47 AM: Update 17684: task edges-srl-ontonotes, batch 684 (17684): mcc: 0.7637, acc: 0.6751, precision: 0.8281, recall: 0.7104, f1: 0.7648, edges-srl-ontonotes_loss: 0.0186
10/01 03:43:57 AM: Update 17783: task edges-srl-ontonotes, batch 783 (17783): mcc: 0.7622, acc: 0.6734, precision: 0.8270, recall: 0.7085, f1: 0.7632, edges-srl-ontonotes_loss: 0.0187
10/01 03:44:07 AM: Update 17879: task edges-srl-ontonotes, batch 879 (17879): mcc: 0.7616, acc: 0.6726, precision: 0.8268, recall: 0.7076, f1: 0.7626, edges-srl-ontonotes_loss: 0.0187
10/01 03:44:17 AM: Update 17965: task edges-srl-ontonotes, batch 965 (17965): mcc: 0.7605, acc: 0.6715, precision: 0.8259, recall: 0.7065, f1: 0.7615, edges-srl-ontonotes_loss: 0.0188
10/01 03:44:20 AM: ***** Step 18000 / Validation 18 *****
10/01 03:44:20 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:44:20 AM: Validating...
10/01 03:44:27 AM: Evaluate: task edges-srl-ontonotes, batch 64 (157): mcc: 0.7921, acc: 0.7113, precision: 0.8723, recall: 0.7245, f1: 0.7915, edges-srl-ontonotes_loss: 0.0164
10/01 03:44:37 AM: Updating LR scheduler:
10/01 03:44:37 AM: 	Best result seen so far for macro_avg: 0.817
10/01 03:44:37 AM: 	# validation passes without improvement: 2
10/01 03:44:37 AM: edges-srl-ontonotes_loss: training: 0.018823 validation: 0.015544
10/01 03:44:37 AM: macro_avg: validation: 0.808007
10/01 03:44:37 AM: micro_avg: validation: 0.000000
10/01 03:44:37 AM: edges-srl-ontonotes_mcc: training: 0.760191 validation: 0.807889
10/01 03:44:37 AM: edges-srl-ontonotes_acc: training: 0.670955 validation: 0.734432
10/01 03:44:37 AM: edges-srl-ontonotes_precision: training: 0.825707 validation: 0.877345
10/01 03:44:37 AM: edges-srl-ontonotes_recall: training: 0.706012 validation: 0.748826
10/01 03:44:37 AM: edges-srl-ontonotes_f1: training: 0.761183 validation: 0.808007
10/01 03:44:37 AM: Global learning rate: 5e-05
10/01 03:44:37 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:44:37 AM: Update 18002: task edges-srl-ontonotes, batch 2 (18002): mcc: 0.7292, acc: 0.6250, precision: 0.8058, recall: 0.6667, f1: 0.7296, edges-srl-ontonotes_loss: 0.0200
10/01 03:44:47 AM: Update 18100: task edges-srl-ontonotes, batch 100 (18100): mcc: 0.7533, acc: 0.6611, precision: 0.8232, recall: 0.6956, f1: 0.7541, edges-srl-ontonotes_loss: 0.0192
10/01 03:44:57 AM: Update 18198: task edges-srl-ontonotes, batch 198 (18198): mcc: 0.7522, acc: 0.6596, precision: 0.8213, recall: 0.6952, f1: 0.7530, edges-srl-ontonotes_loss: 0.0192
10/01 03:45:07 AM: Update 18287: task edges-srl-ontonotes, batch 287 (18287): mcc: 0.7549, acc: 0.6622, precision: 0.8240, recall: 0.6979, f1: 0.7557, edges-srl-ontonotes_loss: 0.0190
10/01 03:45:17 AM: Update 18386: task edges-srl-ontonotes, batch 386 (18386): mcc: 0.7518, acc: 0.6592, precision: 0.8215, recall: 0.6943, f1: 0.7526, edges-srl-ontonotes_loss: 0.0192
10/01 03:45:27 AM: Update 18484: task edges-srl-ontonotes, batch 484 (18484): mcc: 0.7499, acc: 0.6569, precision: 0.8207, recall: 0.6914, f1: 0.7506, edges-srl-ontonotes_loss: 0.0193
10/01 03:45:39 AM: Update 18562: task edges-srl-ontonotes, batch 562 (18562): mcc: 0.7501, acc: 0.6576, precision: 0.8209, recall: 0.6916, f1: 0.7507, edges-srl-ontonotes_loss: 0.0192
10/01 03:45:49 AM: Update 18666: task edges-srl-ontonotes, batch 666 (18666): mcc: 0.7525, acc: 0.6607, precision: 0.8225, recall: 0.6947, f1: 0.7532, edges-srl-ontonotes_loss: 0.0191
10/01 03:45:59 AM: Update 18771: task edges-srl-ontonotes, batch 771 (18771): mcc: 0.7539, acc: 0.6626, precision: 0.8235, recall: 0.6964, f1: 0.7546, edges-srl-ontonotes_loss: 0.0191
10/01 03:46:10 AM: Update 18875: task edges-srl-ontonotes, batch 875 (18875): mcc: 0.7551, acc: 0.6641, precision: 0.8242, recall: 0.6980, f1: 0.7559, edges-srl-ontonotes_loss: 0.0190
10/01 03:46:20 AM: Update 18975: task edges-srl-ontonotes, batch 975 (18975): mcc: 0.7562, acc: 0.6656, precision: 0.8252, recall: 0.6991, f1: 0.7569, edges-srl-ontonotes_loss: 0.0190
10/01 03:46:22 AM: ***** Step 19000 / Validation 19 *****
10/01 03:46:22 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:46:22 AM: Validating...
10/01 03:46:30 AM: Evaluate: task edges-srl-ontonotes, batch 80 (157): mcc: 0.7918, acc: 0.7186, precision: 0.8577, recall: 0.7364, f1: 0.7924, edges-srl-ontonotes_loss: 0.0164
10/01 03:46:38 AM: Updating LR scheduler:
10/01 03:46:38 AM: 	Best result seen so far for macro_avg: 0.817
10/01 03:46:38 AM: 	# validation passes without improvement: 3
10/01 03:46:38 AM: edges-srl-ontonotes_loss: training: 0.018957 validation: 0.015769
10/01 03:46:38 AM: macro_avg: validation: 0.805790
10/01 03:46:38 AM: micro_avg: validation: 0.000000
10/01 03:46:38 AM: edges-srl-ontonotes_mcc: training: 0.756437 validation: 0.804948
10/01 03:46:38 AM: edges-srl-ontonotes_acc: training: 0.666036 validation: 0.735509
10/01 03:46:38 AM: edges-srl-ontonotes_precision: training: 0.825170 validation: 0.864936
10/01 03:46:38 AM: edges-srl-ontonotes_recall: training: 0.699606 validation: 0.754214
10/01 03:46:38 AM: edges-srl-ontonotes_f1: training: 0.757218 validation: 0.805790
10/01 03:46:38 AM: Global learning rate: 5e-05
10/01 03:46:38 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:46:40 AM: Update 19027: task edges-srl-ontonotes, batch 27 (19027): mcc: 0.7794, acc: 0.6947, precision: 0.8460, recall: 0.7237, f1: 0.7801, edges-srl-ontonotes_loss: 0.0181
10/01 03:46:50 AM: Update 19130: task edges-srl-ontonotes, batch 130 (19130): mcc: 0.7661, acc: 0.6792, precision: 0.8295, recall: 0.7135, f1: 0.7671, edges-srl-ontonotes_loss: 0.0184
10/01 03:47:00 AM: Update 19218: task edges-srl-ontonotes, batch 218 (19218): mcc: 0.7604, acc: 0.6731, precision: 0.8274, recall: 0.7049, f1: 0.7613, edges-srl-ontonotes_loss: 0.0187
10/01 03:47:10 AM: Update 19306: task edges-srl-ontonotes, batch 306 (19306): mcc: 0.7500, acc: 0.6596, precision: 0.8196, recall: 0.6926, f1: 0.7508, edges-srl-ontonotes_loss: 0.0194
10/01 03:47:20 AM: Update 19393: task edges-srl-ontonotes, batch 393 (19393): mcc: 0.7462, acc: 0.6544, precision: 0.8179, recall: 0.6871, f1: 0.7468, edges-srl-ontonotes_loss: 0.0196
10/01 03:47:30 AM: Update 19485: task edges-srl-ontonotes, batch 485 (19485): mcc: 0.7455, acc: 0.6532, precision: 0.8178, recall: 0.6861, f1: 0.7462, edges-srl-ontonotes_loss: 0.0197
10/01 03:47:40 AM: Update 19586: task edges-srl-ontonotes, batch 586 (19586): mcc: 0.7471, acc: 0.6554, precision: 0.8182, recall: 0.6886, f1: 0.7478, edges-srl-ontonotes_loss: 0.0195
10/01 03:47:50 AM: Update 19702: task edges-srl-ontonotes, batch 702 (19702): mcc: 0.7485, acc: 0.6569, precision: 0.8189, recall: 0.6906, f1: 0.7493, edges-srl-ontonotes_loss: 0.0194
10/01 03:48:02 AM: Update 19814: task edges-srl-ontonotes, batch 814 (19814): mcc: 0.7507, acc: 0.6591, precision: 0.8207, recall: 0.6929, f1: 0.7514, edges-srl-ontonotes_loss: 0.0193
10/01 03:48:12 AM: Update 19948: task edges-srl-ontonotes, batch 948 (19948): mcc: 0.7552, acc: 0.6641, precision: 0.8233, recall: 0.6990, f1: 0.7561, edges-srl-ontonotes_loss: 0.0189
10/01 03:48:16 AM: ***** Step 20000 / Validation 20 *****
10/01 03:48:16 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:48:16 AM: Validating...
10/01 03:48:22 AM: Evaluate: task edges-srl-ontonotes, batch 65 (157): mcc: 0.7896, acc: 0.7085, precision: 0.8645, recall: 0.7264, f1: 0.7895, edges-srl-ontonotes_loss: 0.0165
10/01 03:48:32 AM: Updating LR scheduler:
10/01 03:48:32 AM: 	Best result seen so far for macro_avg: 0.817
10/01 03:48:32 AM: 	# validation passes without improvement: 0
10/01 03:48:32 AM: edges-srl-ontonotes_loss: training: 0.018793 validation: 0.015654
10/01 03:48:32 AM: macro_avg: validation: 0.806856
10/01 03:48:32 AM: micro_avg: validation: 0.000000
10/01 03:48:32 AM: edges-srl-ontonotes_mcc: training: 0.756915 validation: 0.806495
10/01 03:48:32 AM: edges-srl-ontonotes_acc: training: 0.665928 validation: 0.732584
10/01 03:48:32 AM: edges-srl-ontonotes_precision: training: 0.824382 validation: 0.872962
10/01 03:48:32 AM: edges-srl-ontonotes_recall: training: 0.701156 validation: 0.750058
10/01 03:48:32 AM: edges-srl-ontonotes_f1: training: 0.757792 validation: 0.806856
10/01 03:48:32 AM: Global learning rate: 2.5e-05
10/01 03:48:32 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:48:32 AM: Update 20012: task edges-srl-ontonotes, batch 12 (20012): mcc: 0.7886, acc: 0.6930, precision: 0.8416, recall: 0.7445, f1: 0.7901, edges-srl-ontonotes_loss: 0.0167
10/01 03:48:43 AM: Update 20132: task edges-srl-ontonotes, batch 132 (20132): mcc: 0.7914, acc: 0.7028, precision: 0.8477, recall: 0.7443, f1: 0.7927, edges-srl-ontonotes_loss: 0.0163
10/01 03:48:53 AM: Update 20265: task edges-srl-ontonotes, batch 265 (20265): mcc: 0.7838, acc: 0.6953, precision: 0.8427, recall: 0.7347, f1: 0.7850, edges-srl-ontonotes_loss: 0.0168
10/01 03:49:03 AM: Update 20396: task edges-srl-ontonotes, batch 396 (20396): mcc: 0.7830, acc: 0.6944, precision: 0.8426, recall: 0.7332, f1: 0.7841, edges-srl-ontonotes_loss: 0.0169
10/01 03:49:13 AM: Update 20518: task edges-srl-ontonotes, batch 518 (20518): mcc: 0.7816, acc: 0.6929, precision: 0.8414, recall: 0.7318, f1: 0.7828, edges-srl-ontonotes_loss: 0.0170
10/01 03:49:23 AM: Update 20653: task edges-srl-ontonotes, batch 653 (20653): mcc: 0.7822, acc: 0.6936, precision: 0.8415, recall: 0.7327, f1: 0.7833, edges-srl-ontonotes_loss: 0.0170
10/01 03:49:33 AM: Update 20753: task edges-srl-ontonotes, batch 753 (20753): mcc: 0.7825, acc: 0.6944, precision: 0.8421, recall: 0.7327, f1: 0.7836, edges-srl-ontonotes_loss: 0.0170
10/01 03:49:43 AM: Update 20892: task edges-srl-ontonotes, batch 892 (20892): mcc: 0.7836, acc: 0.6957, precision: 0.8425, recall: 0.7345, f1: 0.7848, edges-srl-ontonotes_loss: 0.0169
10/01 03:49:51 AM: ***** Step 21000 / Validation 21 *****
10/01 03:49:51 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:49:51 AM: Validating...
10/01 03:49:53 AM: Evaluate: task edges-srl-ontonotes, batch 23 (157): mcc: 0.8109, acc: 0.7338, precision: 0.8824, recall: 0.7500, f1: 0.8108, edges-srl-ontonotes_loss: 0.0151
10/01 03:50:03 AM: Evaluate: task edges-srl-ontonotes, batch 126 (157): mcc: 0.8151, acc: 0.7489, precision: 0.8739, recall: 0.7651, f1: 0.8159, edges-srl-ontonotes_loss: 0.0150
10/01 03:50:06 AM: Updating LR scheduler:
10/01 03:50:06 AM: 	Best result seen so far for macro_avg: 0.817
10/01 03:50:06 AM: 	# validation passes without improvement: 1
10/01 03:50:06 AM: edges-srl-ontonotes_loss: training: 0.016820 validation: 0.015304
10/01 03:50:06 AM: macro_avg: validation: 0.812294
10/01 03:50:06 AM: micro_avg: validation: 0.000000
10/01 03:50:06 AM: edges-srl-ontonotes_mcc: training: 0.785704 validation: 0.811601
10/01 03:50:06 AM: edges-srl-ontonotes_acc: training: 0.698173 validation: 0.743592
10/01 03:50:06 AM: edges-srl-ontonotes_precision: training: 0.844232 validation: 0.872536
10/01 03:50:06 AM: edges-srl-ontonotes_recall: training: 0.736842 validation: 0.759834
10/01 03:50:06 AM: edges-srl-ontonotes_f1: training: 0.786890 validation: 0.812294
10/01 03:50:06 AM: Global learning rate: 2.5e-05
10/01 03:50:06 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:50:13 AM: Update 21076: task edges-srl-ontonotes, batch 76 (21076): mcc: 0.7932, acc: 0.7077, precision: 0.8479, recall: 0.7475, f1: 0.7946, edges-srl-ontonotes_loss: 0.0162
10/01 03:50:23 AM: Update 21186: task edges-srl-ontonotes, batch 186 (21186): mcc: 0.7744, acc: 0.6851, precision: 0.8355, recall: 0.7236, f1: 0.7755, edges-srl-ontonotes_loss: 0.0175
10/01 03:50:33 AM: Update 21295: task edges-srl-ontonotes, batch 295 (21295): mcc: 0.7719, acc: 0.6832, precision: 0.8319, recall: 0.7221, f1: 0.7731, edges-srl-ontonotes_loss: 0.0177
10/01 03:50:43 AM: Update 21383: task edges-srl-ontonotes, batch 383 (21383): mcc: 0.7688, acc: 0.6802, precision: 0.8298, recall: 0.7183, f1: 0.7700, edges-srl-ontonotes_loss: 0.0180
10/01 03:50:53 AM: Update 21483: task edges-srl-ontonotes, batch 483 (21483): mcc: 0.7624, acc: 0.6720, precision: 0.8255, recall: 0.7103, f1: 0.7636, edges-srl-ontonotes_loss: 0.0184
10/01 03:51:03 AM: Update 21590: task edges-srl-ontonotes, batch 590 (21590): mcc: 0.7593, acc: 0.6680, precision: 0.8235, recall: 0.7063, f1: 0.7604, edges-srl-ontonotes_loss: 0.0186
10/01 03:51:14 AM: Update 21693: task edges-srl-ontonotes, batch 693 (21693): mcc: 0.7566, acc: 0.6651, precision: 0.8216, recall: 0.7029, f1: 0.7576, edges-srl-ontonotes_loss: 0.0188
10/01 03:51:24 AM: Update 21775: task edges-srl-ontonotes, batch 775 (21775): mcc: 0.7570, acc: 0.6657, precision: 0.8226, recall: 0.7028, f1: 0.7580, edges-srl-ontonotes_loss: 0.0188
10/01 03:51:34 AM: Update 21900: task edges-srl-ontonotes, batch 900 (21900): mcc: 0.7588, acc: 0.6678, precision: 0.8240, recall: 0.7050, f1: 0.7599, edges-srl-ontonotes_loss: 0.0187
10/01 03:51:42 AM: ***** Step 22000 / Validation 22 *****
10/01 03:51:42 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:51:42 AM: Validating...
10/01 03:51:44 AM: Evaluate: task edges-srl-ontonotes, batch 17 (157): mcc: 0.8186, acc: 0.7443, precision: 0.8869, recall: 0.7601, f1: 0.8187, edges-srl-ontonotes_loss: 0.0144
10/01 03:51:54 AM: Evaluate: task edges-srl-ontonotes, batch 120 (157): mcc: 0.8191, acc: 0.7535, precision: 0.8782, recall: 0.7687, f1: 0.8198, edges-srl-ontonotes_loss: 0.0147
10/01 03:51:57 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:51:57 AM: Best result seen so far for macro.
10/01 03:51:57 AM: Updating LR scheduler:
10/01 03:51:57 AM: 	Best result seen so far for macro_avg: 0.818
10/01 03:51:57 AM: 	# validation passes without improvement: 0
10/01 03:51:57 AM: edges-srl-ontonotes_loss: training: 0.018660 validation: 0.014854
10/01 03:51:57 AM: macro_avg: validation: 0.817969
10/01 03:51:57 AM: micro_avg: validation: 0.000000
10/01 03:51:57 AM: edges-srl-ontonotes_mcc: training: 0.759425 validation: 0.817242
10/01 03:51:57 AM: edges-srl-ontonotes_acc: training: 0.668400 validation: 0.751058
10/01 03:51:57 AM: edges-srl-ontonotes_precision: training: 0.824799 validation: 0.876606
10/01 03:51:57 AM: edges-srl-ontonotes_recall: training: 0.705393 validation: 0.766685
10/01 03:51:57 AM: edges-srl-ontonotes_f1: training: 0.760437 validation: 0.817969
10/01 03:51:57 AM: Global learning rate: 2.5e-05
10/01 03:51:57 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:52:04 AM: Update 22058: task edges-srl-ontonotes, batch 58 (22058): mcc: 0.7637, acc: 0.6692, precision: 0.8278, recall: 0.7106, f1: 0.7648, edges-srl-ontonotes_loss: 0.0184
10/01 03:52:14 AM: Update 22177: task edges-srl-ontonotes, batch 177 (22177): mcc: 0.7762, acc: 0.6860, precision: 0.8365, recall: 0.7262, f1: 0.7774, edges-srl-ontonotes_loss: 0.0175
10/01 03:52:24 AM: Update 22293: task edges-srl-ontonotes, batch 293 (22293): mcc: 0.7805, acc: 0.6928, precision: 0.8401, recall: 0.7308, f1: 0.7816, edges-srl-ontonotes_loss: 0.0174
10/01 03:52:34 AM: Update 22398: task edges-srl-ontonotes, batch 398 (22398): mcc: 0.7783, acc: 0.6907, precision: 0.8385, recall: 0.7282, f1: 0.7795, edges-srl-ontonotes_loss: 0.0175
10/01 03:52:44 AM: Update 22514: task edges-srl-ontonotes, batch 514 (22514): mcc: 0.7750, acc: 0.6870, precision: 0.8369, recall: 0.7235, f1: 0.7761, edges-srl-ontonotes_loss: 0.0177
10/01 03:52:54 AM: Update 22625: task edges-srl-ontonotes, batch 625 (22625): mcc: 0.7731, acc: 0.6854, precision: 0.8351, recall: 0.7216, f1: 0.7742, edges-srl-ontonotes_loss: 0.0178
10/01 03:53:04 AM: Update 22708: task edges-srl-ontonotes, batch 708 (22708): mcc: 0.7724, acc: 0.6849, precision: 0.8339, recall: 0.7213, f1: 0.7735, edges-srl-ontonotes_loss: 0.0179
10/01 03:53:14 AM: Update 22817: task edges-srl-ontonotes, batch 817 (22817): mcc: 0.7687, acc: 0.6807, precision: 0.8307, recall: 0.7174, f1: 0.7699, edges-srl-ontonotes_loss: 0.0181
10/01 03:53:24 AM: Update 22934: task edges-srl-ontonotes, batch 934 (22934): mcc: 0.7660, acc: 0.6775, precision: 0.8290, recall: 0.7139, f1: 0.7671, edges-srl-ontonotes_loss: 0.0183
10/01 03:53:31 AM: ***** Step 23000 / Validation 23 *****
10/01 03:53:31 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:53:31 AM: Validating...
10/01 03:53:34 AM: Evaluate: task edges-srl-ontonotes, batch 30 (157): mcc: 0.8159, acc: 0.7445, precision: 0.8826, recall: 0.7590, f1: 0.8162, edges-srl-ontonotes_loss: 0.0150
10/01 03:53:44 AM: Evaluate: task edges-srl-ontonotes, batch 132 (157): mcc: 0.8246, acc: 0.7626, precision: 0.8794, recall: 0.7779, f1: 0.8255, edges-srl-ontonotes_loss: 0.0144
10/01 03:53:47 AM: Best result seen so far for edges-srl-ontonotes.
10/01 03:53:47 AM: Best result seen so far for macro.
10/01 03:53:47 AM: Updating LR scheduler:
10/01 03:53:47 AM: 	Best result seen so far for macro_avg: 0.821
10/01 03:53:47 AM: 	# validation passes without improvement: 0
10/01 03:53:47 AM: edges-srl-ontonotes_loss: training: 0.018408 validation: 0.014706
10/01 03:53:47 AM: macro_avg: validation: 0.821021
10/01 03:53:47 AM: micro_avg: validation: 0.000000
10/01 03:53:47 AM: edges-srl-ontonotes_mcc: training: 0.764600 validation: 0.820142
10/01 03:53:47 AM: edges-srl-ontonotes_acc: training: 0.676134 validation: 0.756755
10/01 03:53:47 AM: edges-srl-ontonotes_precision: training: 0.827659 validation: 0.876595
10/01 03:53:47 AM: edges-srl-ontonotes_recall: training: 0.712415 validation: 0.772073
10/01 03:53:47 AM: edges-srl-ontonotes_f1: training: 0.765725 validation: 0.821021
10/01 03:53:47 AM: Global learning rate: 2.5e-05
10/01 03:53:47 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:53:54 AM: Update 23079: task edges-srl-ontonotes, batch 79 (23079): mcc: 0.7560, acc: 0.6675, precision: 0.8225, recall: 0.7012, f1: 0.7570, edges-srl-ontonotes_loss: 0.0185
10/01 03:54:04 AM: Update 23188: task edges-srl-ontonotes, batch 188 (23188): mcc: 0.7531, acc: 0.6662, precision: 0.8186, recall: 0.6990, f1: 0.7541, edges-srl-ontonotes_loss: 0.0187
10/01 03:54:14 AM: Update 23297: task edges-srl-ontonotes, batch 297 (23297): mcc: 0.7542, acc: 0.6662, precision: 0.8213, recall: 0.6989, f1: 0.7552, edges-srl-ontonotes_loss: 0.0188
10/01 03:54:25 AM: Update 23387: task edges-srl-ontonotes, batch 387 (23387): mcc: 0.7469, acc: 0.6571, precision: 0.8151, recall: 0.6908, f1: 0.7478, edges-srl-ontonotes_loss: 0.0193
10/01 03:54:35 AM: Update 23492: task edges-srl-ontonotes, batch 492 (23492): mcc: 0.7418, acc: 0.6503, precision: 0.8113, recall: 0.6848, f1: 0.7427, edges-srl-ontonotes_loss: 0.0197
10/01 03:54:45 AM: Update 23594: task edges-srl-ontonotes, batch 594 (23594): mcc: 0.7402, acc: 0.6478, precision: 0.8106, recall: 0.6824, f1: 0.7410, edges-srl-ontonotes_loss: 0.0198
10/01 03:54:55 AM: Update 23684: task edges-srl-ontonotes, batch 684 (23684): mcc: 0.7394, acc: 0.6463, precision: 0.8102, recall: 0.6813, f1: 0.7402, edges-srl-ontonotes_loss: 0.0199
10/01 03:55:05 AM: Update 23785: task edges-srl-ontonotes, batch 785 (23785): mcc: 0.7382, acc: 0.6445, precision: 0.8097, recall: 0.6796, f1: 0.7390, edges-srl-ontonotes_loss: 0.0199
10/01 03:55:15 AM: Update 23894: task edges-srl-ontonotes, batch 894 (23894): mcc: 0.7375, acc: 0.6435, precision: 0.8093, recall: 0.6786, f1: 0.7382, edges-srl-ontonotes_loss: 0.0200
10/01 03:55:25 AM: Update 23968: task edges-srl-ontonotes, batch 968 (23968): mcc: 0.7377, acc: 0.6437, precision: 0.8095, recall: 0.6788, f1: 0.7384, edges-srl-ontonotes_loss: 0.0200
10/01 03:55:28 AM: ***** Step 24000 / Validation 24 *****
10/01 03:55:28 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:55:28 AM: Validating...
10/01 03:55:35 AM: Evaluate: task edges-srl-ontonotes, batch 74 (157): mcc: 0.8068, acc: 0.7408, precision: 0.8648, recall: 0.7577, f1: 0.8077, edges-srl-ontonotes_loss: 0.0153
10/01 03:55:43 AM: Updating LR scheduler:
10/01 03:55:43 AM: 	Best result seen so far for macro_avg: 0.821
10/01 03:55:43 AM: 	# validation passes without improvement: 1
10/01 03:55:43 AM: edges-srl-ontonotes_loss: training: 0.019908 validation: 0.014762
10/01 03:55:43 AM: macro_avg: validation: 0.820979
10/01 03:55:43 AM: micro_avg: validation: 0.000000
10/01 03:55:43 AM: edges-srl-ontonotes_mcc: training: 0.738841 validation: 0.819849
10/01 03:55:43 AM: edges-srl-ontonotes_acc: training: 0.645120 validation: 0.757294
10/01 03:55:43 AM: edges-srl-ontonotes_precision: training: 0.810268 validation: 0.872272
10/01 03:55:43 AM: edges-srl-ontonotes_recall: training: 0.680266 validation: 0.775383
10/01 03:55:43 AM: edges-srl-ontonotes_f1: training: 0.739598 validation: 0.820979
10/01 03:55:43 AM: Global learning rate: 2.5e-05
10/01 03:55:43 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:55:45 AM: Update 24018: task edges-srl-ontonotes, batch 18 (24018): mcc: 0.7742, acc: 0.6920, precision: 0.8404, recall: 0.7190, f1: 0.7750, edges-srl-ontonotes_loss: 0.0176
10/01 03:55:55 AM: Update 24121: task edges-srl-ontonotes, batch 121 (24121): mcc: 0.7602, acc: 0.6739, precision: 0.8250, recall: 0.7066, f1: 0.7612, edges-srl-ontonotes_loss: 0.0186
10/01 03:56:05 AM: Update 24219: task edges-srl-ontonotes, batch 219 (24219): mcc: 0.7654, acc: 0.6800, precision: 0.8285, recall: 0.7131, f1: 0.7665, edges-srl-ontonotes_loss: 0.0183
10/01 03:56:15 AM: Update 24314: task edges-srl-ontonotes, batch 314 (24314): mcc: 0.7648, acc: 0.6794, precision: 0.8283, recall: 0.7122, f1: 0.7659, edges-srl-ontonotes_loss: 0.0183
10/01 03:56:25 AM: Update 24421: task edges-srl-ontonotes, batch 421 (24421): mcc: 0.7642, acc: 0.6784, precision: 0.8270, recall: 0.7123, f1: 0.7654, edges-srl-ontonotes_loss: 0.0183
10/01 03:56:35 AM: Update 24521: task edges-srl-ontonotes, batch 521 (24521): mcc: 0.7637, acc: 0.6779, precision: 0.8267, recall: 0.7115, f1: 0.7648, edges-srl-ontonotes_loss: 0.0184
10/01 03:56:45 AM: Update 24617: task edges-srl-ontonotes, batch 617 (24617): mcc: 0.7652, acc: 0.6789, precision: 0.8287, recall: 0.7126, f1: 0.7663, edges-srl-ontonotes_loss: 0.0183
10/01 03:56:55 AM: Update 24721: task edges-srl-ontonotes, batch 721 (24721): mcc: 0.7656, acc: 0.6796, precision: 0.8289, recall: 0.7132, f1: 0.7667, edges-srl-ontonotes_loss: 0.0183
10/01 03:57:05 AM: Update 24827: task edges-srl-ontonotes, batch 827 (24827): mcc: 0.7665, acc: 0.6808, precision: 0.8297, recall: 0.7142, f1: 0.7676, edges-srl-ontonotes_loss: 0.0182
10/01 03:57:15 AM: Update 24902: task edges-srl-ontonotes, batch 902 (24902): mcc: 0.7674, acc: 0.6817, precision: 0.8305, recall: 0.7151, f1: 0.7685, edges-srl-ontonotes_loss: 0.0182
10/01 03:57:25 AM: ***** Step 25000 / Validation 25 *****
10/01 03:57:25 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:57:25 AM: Validating...
10/01 03:57:25 AM: Evaluate: task edges-srl-ontonotes, batch 7 (157): mcc: 0.8228, acc: 0.7463, precision: 0.8948, recall: 0.7610, f1: 0.8225, edges-srl-ontonotes_loss: 0.0141
10/01 03:57:35 AM: Evaluate: task edges-srl-ontonotes, batch 110 (157): mcc: 0.8145, acc: 0.7499, precision: 0.8716, recall: 0.7660, f1: 0.8154, edges-srl-ontonotes_loss: 0.0149
10/01 03:57:40 AM: Updating LR scheduler:
10/01 03:57:40 AM: 	Best result seen so far for macro_avg: 0.821
10/01 03:57:40 AM: 	# validation passes without improvement: 2
10/01 03:57:40 AM: edges-srl-ontonotes_loss: training: 0.018292 validation: 0.014893
10/01 03:57:40 AM: macro_avg: validation: 0.817561
10/01 03:57:40 AM: micro_avg: validation: 0.000000
10/01 03:57:40 AM: edges-srl-ontonotes_mcc: training: 0.765956 validation: 0.816605
10/01 03:57:40 AM: edges-srl-ontonotes_acc: training: 0.679869 validation: 0.752675
10/01 03:57:40 AM: edges-srl-ontonotes_precision: training: 0.829550 validation: 0.872576
10/01 03:57:40 AM: edges-srl-ontonotes_recall: training: 0.713265 validation: 0.769071
10/01 03:57:40 AM: edges-srl-ontonotes_f1: training: 0.767025 validation: 0.817561
10/01 03:57:40 AM: Global learning rate: 2.5e-05
10/01 03:57:40 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:57:45 AM: Update 25055: task edges-srl-ontonotes, batch 55 (25055): mcc: 0.7514, acc: 0.6633, precision: 0.8151, recall: 0.6991, f1: 0.7526, edges-srl-ontonotes_loss: 0.0192
10/01 03:57:56 AM: Update 25165: task edges-srl-ontonotes, batch 165 (25165): mcc: 0.7566, acc: 0.6699, precision: 0.8214, recall: 0.7031, f1: 0.7576, edges-srl-ontonotes_loss: 0.0191
10/01 03:58:06 AM: Update 25261: task edges-srl-ontonotes, batch 261 (25261): mcc: 0.7554, acc: 0.6673, precision: 0.8204, recall: 0.7017, f1: 0.7565, edges-srl-ontonotes_loss: 0.0191
10/01 03:58:16 AM: Update 25369: task edges-srl-ontonotes, batch 369 (25369): mcc: 0.7539, acc: 0.6652, precision: 0.8197, recall: 0.6996, f1: 0.7549, edges-srl-ontonotes_loss: 0.0191
10/01 03:58:26 AM: Update 25475: task edges-srl-ontonotes, batch 475 (25475): mcc: 0.7554, acc: 0.6664, precision: 0.8218, recall: 0.7007, f1: 0.7564, edges-srl-ontonotes_loss: 0.0190
10/01 03:58:36 AM: Update 25568: task edges-srl-ontonotes, batch 568 (25568): mcc: 0.7545, acc: 0.6659, precision: 0.8211, recall: 0.6996, f1: 0.7555, edges-srl-ontonotes_loss: 0.0191
10/01 03:58:46 AM: Update 25676: task edges-srl-ontonotes, batch 676 (25676): mcc: 0.7545, acc: 0.6663, precision: 0.8214, recall: 0.6993, f1: 0.7555, edges-srl-ontonotes_loss: 0.0191
10/01 03:58:56 AM: Update 25782: task edges-srl-ontonotes, batch 782 (25782): mcc: 0.7536, acc: 0.6655, precision: 0.8210, recall: 0.6981, f1: 0.7546, edges-srl-ontonotes_loss: 0.0191
10/01 03:59:06 AM: Update 25856: task edges-srl-ontonotes, batch 856 (25856): mcc: 0.7538, acc: 0.6659, precision: 0.8212, recall: 0.6981, f1: 0.7547, edges-srl-ontonotes_loss: 0.0191
10/01 03:59:16 AM: Update 25962: task edges-srl-ontonotes, batch 962 (25962): mcc: 0.7552, acc: 0.6677, precision: 0.8227, recall: 0.6995, f1: 0.7561, edges-srl-ontonotes_loss: 0.0190
10/01 03:59:20 AM: ***** Step 26000 / Validation 26 *****
10/01 03:59:20 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 03:59:20 AM: Validating...
10/01 03:59:26 AM: Evaluate: task edges-srl-ontonotes, batch 67 (157): mcc: 0.8001, acc: 0.7296, precision: 0.8657, recall: 0.7447, f1: 0.8006, edges-srl-ontonotes_loss: 0.0158
10/01 03:59:35 AM: Updating LR scheduler:
10/01 03:59:35 AM: 	Best result seen so far for macro_avg: 0.821
10/01 03:59:35 AM: 	# validation passes without improvement: 3
10/01 03:59:35 AM: edges-srl-ontonotes_loss: training: 0.018930 validation: 0.015108
10/01 03:59:35 AM: macro_avg: validation: 0.815046
10/01 03:59:35 AM: micro_avg: validation: 0.000000
10/01 03:59:35 AM: edges-srl-ontonotes_mcc: training: 0.756207 validation: 0.814273
10/01 03:59:35 AM: edges-srl-ontonotes_acc: training: 0.668914 validation: 0.748056
10/01 03:59:35 AM: edges-srl-ontonotes_precision: training: 0.823549 validation: 0.873515
10/01 03:59:35 AM: edges-srl-ontonotes_recall: training: 0.700579 validation: 0.763913
10/01 03:59:35 AM: edges-srl-ontonotes_f1: training: 0.757103 validation: 0.815046
10/01 03:59:35 AM: Global learning rate: 2.5e-05
10/01 03:59:35 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 03:59:36 AM: Update 26011: task edges-srl-ontonotes, batch 11 (26011): mcc: 0.7813, acc: 0.6971, precision: 0.8432, recall: 0.7296, f1: 0.7823, edges-srl-ontonotes_loss: 0.0179
10/01 03:59:46 AM: Update 26118: task edges-srl-ontonotes, batch 118 (26118): mcc: 0.7657, acc: 0.6792, precision: 0.8297, recall: 0.7128, f1: 0.7668, edges-srl-ontonotes_loss: 0.0184
10/01 03:59:56 AM: Update 26213: task edges-srl-ontonotes, batch 213 (26213): mcc: 0.7683, acc: 0.6828, precision: 0.8319, recall: 0.7155, f1: 0.7693, edges-srl-ontonotes_loss: 0.0183
10/01 04:00:06 AM: Update 26317: task edges-srl-ontonotes, batch 317 (26317): mcc: 0.7672, acc: 0.6826, precision: 0.8303, recall: 0.7148, f1: 0.7683, edges-srl-ontonotes_loss: 0.0183
10/01 04:00:16 AM: Update 26424: task edges-srl-ontonotes, batch 424 (26424): mcc: 0.7679, acc: 0.6836, precision: 0.8319, recall: 0.7148, f1: 0.7689, edges-srl-ontonotes_loss: 0.0183
10/01 04:00:26 AM: Update 26501: task edges-srl-ontonotes, batch 501 (26501): mcc: 0.7630, acc: 0.6771, precision: 0.8294, recall: 0.7081, f1: 0.7639, edges-srl-ontonotes_loss: 0.0186
10/01 04:00:36 AM: Update 26594: task edges-srl-ontonotes, batch 594 (26594): mcc: 0.7601, acc: 0.6730, precision: 0.8279, recall: 0.7040, f1: 0.7609, edges-srl-ontonotes_loss: 0.0188
10/01 04:00:46 AM: Update 26686: task edges-srl-ontonotes, batch 686 (26686): mcc: 0.7574, acc: 0.6692, precision: 0.8264, recall: 0.7002, f1: 0.7581, edges-srl-ontonotes_loss: 0.0190
10/01 04:00:56 AM: Update 26775: task edges-srl-ontonotes, batch 775 (26775): mcc: 0.7568, acc: 0.6682, precision: 0.8261, recall: 0.6994, f1: 0.7575, edges-srl-ontonotes_loss: 0.0190
10/01 04:01:06 AM: Update 26895: task edges-srl-ontonotes, batch 895 (26895): mcc: 0.7576, acc: 0.6693, precision: 0.8263, recall: 0.7008, f1: 0.7584, edges-srl-ontonotes_loss: 0.0189
10/01 04:01:15 AM: ***** Step 27000 / Validation 27 *****
10/01 04:01:15 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 04:01:15 AM: Validating...
10/01 04:01:17 AM: Evaluate: task edges-srl-ontonotes, batch 12 (157): mcc: 0.8132, acc: 0.7373, precision: 0.8813, recall: 0.7552, f1: 0.8134, edges-srl-ontonotes_loss: 0.0146
10/01 04:01:27 AM: Evaluate: task edges-srl-ontonotes, batch 116 (157): mcc: 0.8129, acc: 0.7449, precision: 0.8754, recall: 0.7598, f1: 0.8135, edges-srl-ontonotes_loss: 0.0151
10/01 04:01:31 AM: Updating LR scheduler:
10/01 04:01:31 AM: 	Best result seen so far for macro_avg: 0.821
10/01 04:01:31 AM: 	# validation passes without improvement: 0
10/01 04:01:31 AM: edges-srl-ontonotes_loss: training: 0.018738 validation: 0.015115
10/01 04:01:31 AM: macro_avg: validation: 0.814172
10/01 04:01:31 AM: micro_avg: validation: 0.000000
10/01 04:01:31 AM: edges-srl-ontonotes_mcc: training: 0.759132 validation: 0.813516
10/01 04:01:31 AM: edges-srl-ontonotes_acc: training: 0.670743 validation: 0.745824
10/01 04:01:31 AM: edges-srl-ontonotes_precision: training: 0.827062 validation: 0.874635
10/01 04:01:31 AM: edges-srl-ontonotes_recall: training: 0.702906 validation: 0.761527
10/01 04:01:31 AM: edges-srl-ontonotes_f1: training: 0.759946 validation: 0.814172
10/01 04:01:31 AM: Global learning rate: 1.25e-05
10/01 04:01:31 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 04:01:39 AM: Update 27060: task edges-srl-ontonotes, batch 60 (27060): mcc: 0.7751, acc: 0.6872, precision: 0.8391, recall: 0.7217, f1: 0.7760, edges-srl-ontonotes_loss: 0.0176
10/01 04:01:49 AM: Update 27196: task edges-srl-ontonotes, batch 196 (27196): mcc: 0.7809, acc: 0.6961, precision: 0.8395, recall: 0.7321, f1: 0.7821, edges-srl-ontonotes_loss: 0.0169
10/01 04:01:59 AM: Update 27331: task edges-srl-ontonotes, batch 331 (27331): mcc: 0.7850, acc: 0.7011, precision: 0.8416, recall: 0.7378, f1: 0.7863, edges-srl-ontonotes_loss: 0.0166
10/01 04:02:09 AM: Update 27450: task edges-srl-ontonotes, batch 450 (27450): mcc: 0.7851, acc: 0.7005, precision: 0.8427, recall: 0.7371, f1: 0.7864, edges-srl-ontonotes_loss: 0.0166
10/01 04:02:19 AM: Update 27579: task edges-srl-ontonotes, batch 579 (27579): mcc: 0.7842, acc: 0.6990, precision: 0.8428, recall: 0.7352, f1: 0.7854, edges-srl-ontonotes_loss: 0.0167
10/01 04:02:29 AM: Update 27699: task edges-srl-ontonotes, batch 699 (27699): mcc: 0.7836, acc: 0.6981, precision: 0.8424, recall: 0.7346, f1: 0.7848, edges-srl-ontonotes_loss: 0.0167
10/01 04:02:39 AM: Update 27835: task edges-srl-ontonotes, batch 835 (27835): mcc: 0.7840, acc: 0.6985, precision: 0.8430, recall: 0.7348, f1: 0.7852, edges-srl-ontonotes_loss: 0.0167
10/01 04:02:49 AM: Update 27972: task edges-srl-ontonotes, batch 972 (27972): mcc: 0.7844, acc: 0.6987, precision: 0.8433, recall: 0.7352, f1: 0.7856, edges-srl-ontonotes_loss: 0.0167
10/01 04:02:54 AM: ***** Step 28000 / Validation 28 *****
10/01 04:02:54 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 04:02:54 AM: Validating...
10/01 04:02:59 AM: Evaluate: task edges-srl-ontonotes, batch 54 (157): mcc: 0.7985, acc: 0.7238, precision: 0.8699, recall: 0.7381, f1: 0.7986, edges-srl-ontonotes_loss: 0.0159
10/01 04:03:09 AM: Evaluate: task edges-srl-ontonotes, batch 156 (157): mcc: 0.8169, acc: 0.7489, precision: 0.8805, recall: 0.7626, f1: 0.8174, edges-srl-ontonotes_loss: 0.0148
10/01 04:03:09 AM: Updating LR scheduler:
10/01 04:03:09 AM: 	Best result seen so far for macro_avg: 0.821
10/01 04:03:09 AM: 	# validation passes without improvement: 1
10/01 04:03:09 AM: edges-srl-ontonotes_loss: training: 0.016772 validation: 0.014856
10/01 04:03:09 AM: macro_avg: validation: 0.817259
10/01 04:03:09 AM: micro_avg: validation: 0.000000
10/01 04:03:09 AM: edges-srl-ontonotes_mcc: training: 0.783924 validation: 0.816820
10/01 04:03:09 AM: edges-srl-ontonotes_acc: training: 0.698121 validation: 0.748826
10/01 04:03:09 AM: edges-srl-ontonotes_precision: training: 0.842750 validation: 0.880455
10/01 04:03:09 AM: edges-srl-ontonotes_recall: training: 0.734853 validation: 0.762528
10/01 04:03:09 AM: edges-srl-ontonotes_f1: training: 0.785112 validation: 0.817259
10/01 04:03:09 AM: Global learning rate: 1.25e-05
10/01 04:03:09 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 04:03:19 AM: Update 28136: task edges-srl-ontonotes, batch 136 (28136): mcc: 0.7999, acc: 0.7154, precision: 0.8554, recall: 0.7532, f1: 0.8011, edges-srl-ontonotes_loss: 0.0160
10/01 04:03:29 AM: Update 28280: task edges-srl-ontonotes, batch 280 (28280): mcc: 0.8007, acc: 0.7159, precision: 0.8569, recall: 0.7534, f1: 0.8018, edges-srl-ontonotes_loss: 0.0159
10/01 04:03:39 AM: Update 28377: task edges-srl-ontonotes, batch 377 (28377): mcc: 0.7956, acc: 0.7104, precision: 0.8526, recall: 0.7478, f1: 0.7968, edges-srl-ontonotes_loss: 0.0163
10/01 04:03:49 AM: Update 28482: task edges-srl-ontonotes, batch 482 (28482): mcc: 0.7894, acc: 0.7037, precision: 0.8471, recall: 0.7410, f1: 0.7905, edges-srl-ontonotes_loss: 0.0168
10/01 04:03:59 AM: Update 28595: task edges-srl-ontonotes, batch 595 (28595): mcc: 0.7851, acc: 0.6986, precision: 0.8437, recall: 0.7362, f1: 0.7863, edges-srl-ontonotes_loss: 0.0171
10/01 04:04:09 AM: Update 28685: task edges-srl-ontonotes, batch 685 (28685): mcc: 0.7808, acc: 0.6935, precision: 0.8403, recall: 0.7311, f1: 0.7819, edges-srl-ontonotes_loss: 0.0174
10/01 04:04:19 AM: Update 28784: task edges-srl-ontonotes, batch 784 (28784): mcc: 0.7767, acc: 0.6890, precision: 0.8374, recall: 0.7261, f1: 0.7778, edges-srl-ontonotes_loss: 0.0176
10/01 04:04:29 AM: Update 28889: task edges-srl-ontonotes, batch 889 (28889): mcc: 0.7735, acc: 0.6852, precision: 0.8352, recall: 0.7222, f1: 0.7746, edges-srl-ontonotes_loss: 0.0179
10/01 04:04:42 AM: Update 28985: task edges-srl-ontonotes, batch 985 (28985): mcc: 0.7722, acc: 0.6839, precision: 0.8343, recall: 0.7206, f1: 0.7733, edges-srl-ontonotes_loss: 0.0179
10/01 04:04:43 AM: ***** Step 29000 / Validation 29 *****
10/01 04:04:43 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 04:04:43 AM: Validating...
10/01 04:04:52 AM: Evaluate: task edges-srl-ontonotes, batch 82 (157): mcc: 0.8111, acc: 0.7449, precision: 0.8737, recall: 0.7578, f1: 0.8116, edges-srl-ontonotes_loss: 0.0152
10/01 04:05:00 AM: Updating LR scheduler:
10/01 04:05:00 AM: 	Best result seen so far for macro_avg: 0.821
10/01 04:05:00 AM: 	# validation passes without improvement: 2
10/01 04:05:00 AM: edges-srl-ontonotes_loss: training: 0.017922 validation: 0.014757
10/01 04:05:00 AM: macro_avg: validation: 0.820536
10/01 04:05:00 AM: micro_avg: validation: 0.000000
10/01 04:05:00 AM: edges-srl-ontonotes_mcc: training: 0.772327 validation: 0.819716
10/01 04:05:00 AM: edges-srl-ontonotes_acc: training: 0.684039 validation: 0.755523
10/01 04:05:00 AM: edges-srl-ontonotes_precision: training: 0.834559 validation: 0.877179
10/01 04:05:00 AM: edges-srl-ontonotes_recall: training: 0.720626 validation: 0.770764
10/01 04:05:00 AM: edges-srl-ontonotes_f1: training: 0.773419 validation: 0.820536
10/01 04:05:00 AM: Global learning rate: 1.25e-05
10/01 04:05:00 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 04:05:02 AM: Update 29021: task edges-srl-ontonotes, batch 21 (29021): mcc: 0.7709, acc: 0.6755, precision: 0.8346, recall: 0.7178, f1: 0.7718, edges-srl-ontonotes_loss: 0.0180
10/01 04:05:12 AM: Update 29130: task edges-srl-ontonotes, batch 130 (29130): mcc: 0.7747, acc: 0.6858, precision: 0.8389, recall: 0.7211, f1: 0.7756, edges-srl-ontonotes_loss: 0.0179
10/01 04:05:22 AM: Update 29241: task edges-srl-ontonotes, batch 241 (29241): mcc: 0.7741, acc: 0.6851, precision: 0.8396, recall: 0.7195, f1: 0.7749, edges-srl-ontonotes_loss: 0.0179
10/01 04:05:32 AM: Update 29339: task edges-srl-ontonotes, batch 339 (29339): mcc: 0.7765, acc: 0.6880, precision: 0.8415, recall: 0.7223, f1: 0.7774, edges-srl-ontonotes_loss: 0.0177
10/01 04:05:42 AM: Update 29446: task edges-srl-ontonotes, batch 446 (29446): mcc: 0.7792, acc: 0.6917, precision: 0.8427, recall: 0.7263, f1: 0.7802, edges-srl-ontonotes_loss: 0.0176
10/01 04:05:52 AM: Update 29552: task edges-srl-ontonotes, batch 552 (29552): mcc: 0.7795, acc: 0.6923, precision: 0.8423, recall: 0.7270, f1: 0.7804, edges-srl-ontonotes_loss: 0.0175
10/01 04:06:02 AM: Update 29646: task edges-srl-ontonotes, batch 646 (29646): mcc: 0.7776, acc: 0.6907, precision: 0.8405, recall: 0.7251, f1: 0.7786, edges-srl-ontonotes_loss: 0.0176
10/01 04:06:12 AM: Update 29750: task edges-srl-ontonotes, batch 750 (29750): mcc: 0.7762, acc: 0.6892, precision: 0.8394, recall: 0.7236, f1: 0.7772, edges-srl-ontonotes_loss: 0.0177
10/01 04:06:22 AM: Update 29861: task edges-srl-ontonotes, batch 861 (29861): mcc: 0.7755, acc: 0.6886, precision: 0.8385, recall: 0.7231, f1: 0.7765, edges-srl-ontonotes_loss: 0.0177
10/01 04:06:32 AM: Update 29960: task edges-srl-ontonotes, batch 960 (29960): mcc: 0.7745, acc: 0.6877, precision: 0.8373, recall: 0.7221, f1: 0.7755, edges-srl-ontonotes_loss: 0.0177
10/01 04:06:36 AM: ***** Step 30000 / Validation 30 *****
10/01 04:06:36 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 04:06:36 AM: Validating...
10/01 04:06:42 AM: Evaluate: task edges-srl-ontonotes, batch 60 (157): mcc: 0.8082, acc: 0.7401, precision: 0.8718, recall: 0.7542, f1: 0.8088, edges-srl-ontonotes_loss: 0.0153
10/01 04:06:52 AM: Evaluate: task edges-srl-ontonotes, batch 156 (157): mcc: 0.8223, acc: 0.7593, precision: 0.8784, recall: 0.7745, f1: 0.8232, edges-srl-ontonotes_loss: 0.0145
10/01 04:06:52 AM: Best result seen so far for edges-srl-ontonotes.
10/01 04:06:52 AM: Best result seen so far for macro.
10/01 04:06:52 AM: Updating LR scheduler:
10/01 04:06:52 AM: 	Best result seen so far for macro_avg: 0.823
10/01 04:06:52 AM: 	# validation passes without improvement: 0
10/01 04:06:52 AM: edges-srl-ontonotes_loss: training: 0.017792 validation: 0.014532
10/01 04:06:52 AM: macro_avg: validation: 0.823072
10/01 04:06:52 AM: micro_avg: validation: 0.000000
10/01 04:06:52 AM: edges-srl-ontonotes_mcc: training: 0.773666 validation: 0.822198
10/01 04:06:52 AM: edges-srl-ontonotes_acc: training: 0.686775 validation: 0.759218
10/01 04:06:52 AM: edges-srl-ontonotes_precision: training: 0.836532 validation: 0.878296
10/01 04:06:52 AM: edges-srl-ontonotes_recall: training: 0.721371 validation: 0.774382
10/01 04:06:52 AM: edges-srl-ontonotes_f1: training: 0.774695 validation: 0.823072
10/01 04:06:52 AM: Global learning rate: 1.25e-05
10/01 04:06:52 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 04:07:02 AM: Update 30104: task edges-srl-ontonotes, batch 104 (30104): mcc: 0.7499, acc: 0.6601, precision: 0.8177, recall: 0.6941, f1: 0.7509, edges-srl-ontonotes_loss: 0.0192
10/01 04:07:12 AM: Update 30206: task edges-srl-ontonotes, batch 206 (30206): mcc: 0.7507, acc: 0.6596, precision: 0.8173, recall: 0.6958, f1: 0.7517, edges-srl-ontonotes_loss: 0.0193
10/01 04:07:22 AM: Update 30273: task edges-srl-ontonotes, batch 273 (30273): mcc: 0.7506, acc: 0.6597, precision: 0.8180, recall: 0.6952, f1: 0.7516, edges-srl-ontonotes_loss: 0.0192
10/01 04:07:32 AM: Update 30373: task edges-srl-ontonotes, batch 373 (30373): mcc: 0.7540, acc: 0.6631, precision: 0.8219, recall: 0.6980, f1: 0.7549, edges-srl-ontonotes_loss: 0.0190
10/01 04:07:43 AM: Update 30475: task edges-srl-ontonotes, batch 475 (30475): mcc: 0.7542, acc: 0.6636, precision: 0.8218, recall: 0.6984, f1: 0.7551, edges-srl-ontonotes_loss: 0.0189
10/01 04:07:53 AM: Update 30570: task edges-srl-ontonotes, batch 570 (30570): mcc: 0.7534, acc: 0.6629, precision: 0.8210, recall: 0.6977, f1: 0.7543, edges-srl-ontonotes_loss: 0.0189
10/01 04:08:03 AM: Update 30670: task edges-srl-ontonotes, batch 670 (30670): mcc: 0.7514, acc: 0.6605, precision: 0.8197, recall: 0.6951, f1: 0.7522, edges-srl-ontonotes_loss: 0.0192
10/01 04:08:13 AM: Update 30771: task edges-srl-ontonotes, batch 771 (30771): mcc: 0.7478, acc: 0.6558, precision: 0.8171, recall: 0.6907, f1: 0.7486, edges-srl-ontonotes_loss: 0.0194
10/01 04:08:23 AM: Update 30864: task edges-srl-ontonotes, batch 864 (30864): mcc: 0.7467, acc: 0.6544, precision: 0.8165, recall: 0.6892, f1: 0.7475, edges-srl-ontonotes_loss: 0.0195
10/01 04:08:33 AM: Update 30970: task edges-srl-ontonotes, batch 970 (30970): mcc: 0.7459, acc: 0.6534, precision: 0.8160, recall: 0.6883, f1: 0.7467, edges-srl-ontonotes_loss: 0.0196
10/01 04:08:36 AM: ***** Step 31000 / Validation 31 *****
10/01 04:08:36 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 04:08:36 AM: Validating...
10/01 04:08:43 AM: Evaluate: task edges-srl-ontonotes, batch 73 (157): mcc: 0.8121, acc: 0.7462, precision: 0.8723, recall: 0.7609, f1: 0.8128, edges-srl-ontonotes_loss: 0.0149
10/01 04:08:51 AM: Updating LR scheduler:
10/01 04:08:51 AM: 	Best result seen so far for macro_avg: 0.823
10/01 04:08:51 AM: 	# validation passes without improvement: 1
10/01 04:08:51 AM: edges-srl-ontonotes_loss: training: 0.019586 validation: 0.014503
10/01 04:08:51 AM: macro_avg: validation: 0.822056
10/01 04:08:51 AM: micro_avg: validation: 0.000000
10/01 04:08:51 AM: edges-srl-ontonotes_mcc: training: 0.745359 validation: 0.821204
10/01 04:08:51 AM: edges-srl-ontonotes_acc: training: 0.652787 validation: 0.757832
10/01 04:08:51 AM: edges-srl-ontonotes_precision: training: 0.815687 validation: 0.877863
10/01 04:08:51 AM: edges-srl-ontonotes_recall: training: 0.687513 validation: 0.772920
10/01 04:08:51 AM: edges-srl-ontonotes_f1: training: 0.746135 validation: 0.822056
10/01 04:08:51 AM: Global learning rate: 1.25e-05
10/01 04:08:51 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 04:08:53 AM: Update 31017: task edges-srl-ontonotes, batch 17 (31017): mcc: 0.7433, acc: 0.6481, precision: 0.8138, recall: 0.6855, f1: 0.7441, edges-srl-ontonotes_loss: 0.0197
10/01 04:09:03 AM: Update 31120: task edges-srl-ontonotes, batch 120 (31120): mcc: 0.7358, acc: 0.6410, precision: 0.8065, recall: 0.6780, f1: 0.7367, edges-srl-ontonotes_loss: 0.0201
10/01 04:09:13 AM: Update 31193: task edges-srl-ontonotes, batch 193 (31193): mcc: 0.7381, acc: 0.6449, precision: 0.8085, recall: 0.6804, f1: 0.7389, edges-srl-ontonotes_loss: 0.0200
10/01 04:09:23 AM: Update 31292: task edges-srl-ontonotes, batch 292 (31292): mcc: 0.7493, acc: 0.6592, precision: 0.8178, recall: 0.6929, f1: 0.7502, edges-srl-ontonotes_loss: 0.0193
10/01 04:09:33 AM: Update 31390: task edges-srl-ontonotes, batch 390 (31390): mcc: 0.7530, acc: 0.6642, precision: 0.8201, recall: 0.6977, f1: 0.7540, edges-srl-ontonotes_loss: 0.0191
10/01 04:09:44 AM: Update 31489: task edges-srl-ontonotes, batch 489 (31489): mcc: 0.7560, acc: 0.6688, precision: 0.8224, recall: 0.7012, f1: 0.7570, edges-srl-ontonotes_loss: 0.0189
10/01 04:09:54 AM: Update 31591: task edges-srl-ontonotes, batch 591 (31591): mcc: 0.7573, acc: 0.6699, precision: 0.8236, recall: 0.7026, f1: 0.7583, edges-srl-ontonotes_loss: 0.0188
10/01 04:10:04 AM: Update 31691: task edges-srl-ontonotes, batch 691 (31691): mcc: 0.7590, acc: 0.6718, precision: 0.8245, recall: 0.7049, f1: 0.7600, edges-srl-ontonotes_loss: 0.0187
10/01 04:10:14 AM: Update 31796: task edges-srl-ontonotes, batch 796 (31796): mcc: 0.7603, acc: 0.6734, precision: 0.8257, recall: 0.7061, f1: 0.7613, edges-srl-ontonotes_loss: 0.0185
10/01 04:10:24 AM: Update 31888: task edges-srl-ontonotes, batch 888 (31888): mcc: 0.7621, acc: 0.6758, precision: 0.8271, recall: 0.7083, f1: 0.7631, edges-srl-ontonotes_loss: 0.0184
10/01 04:10:34 AM: Update 31991: task edges-srl-ontonotes, batch 991 (31991): mcc: 0.7635, acc: 0.6776, precision: 0.8282, recall: 0.7100, f1: 0.7645, edges-srl-ontonotes_loss: 0.0184
10/01 04:10:35 AM: ***** Step 32000 / Validation 32 *****
10/01 04:10:35 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 04:10:35 AM: Validating...
10/01 04:10:44 AM: Evaluate: task edges-srl-ontonotes, batch 96 (157): mcc: 0.8146, acc: 0.7497, precision: 0.8737, recall: 0.7645, f1: 0.8154, edges-srl-ontonotes_loss: 0.0147
10/01 04:10:51 AM: Updating LR scheduler:
10/01 04:10:51 AM: 	Best result seen so far for macro_avg: 0.823
10/01 04:10:51 AM: 	# validation passes without improvement: 2
10/01 04:10:51 AM: edges-srl-ontonotes_loss: training: 0.018358 validation: 0.014560
10/01 04:10:51 AM: macro_avg: validation: 0.820935
10/01 04:10:51 AM: micro_avg: validation: 0.000000
10/01 04:10:51 AM: edges-srl-ontonotes_mcc: training: 0.763623 validation: 0.820042
10/01 04:10:51 AM: edges-srl-ontonotes_acc: training: 0.677819 validation: 0.757063
10/01 04:10:51 AM: edges-srl-ontonotes_precision: training: 0.828242 validation: 0.876299
10/01 04:10:51 AM: edges-srl-ontonotes_recall: training: 0.710115 validation: 0.772150
10/01 04:10:51 AM: edges-srl-ontonotes_f1: training: 0.764643 validation: 0.820935
10/01 04:10:51 AM: Global learning rate: 1.25e-05
10/01 04:10:51 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 04:10:55 AM: Update 32040: task edges-srl-ontonotes, batch 40 (32040): mcc: 0.7792, acc: 0.6936, precision: 0.8469, recall: 0.7225, f1: 0.7798, edges-srl-ontonotes_loss: 0.0174
10/01 04:11:05 AM: Update 32115: task edges-srl-ontonotes, batch 115 (32115): mcc: 0.7782, acc: 0.6949, precision: 0.8429, recall: 0.7242, f1: 0.7791, edges-srl-ontonotes_loss: 0.0175
10/01 04:11:15 AM: Update 32216: task edges-srl-ontonotes, batch 216 (32216): mcc: 0.7676, acc: 0.6823, precision: 0.8320, recall: 0.7142, f1: 0.7686, edges-srl-ontonotes_loss: 0.0182
10/01 04:11:25 AM: Update 32321: task edges-srl-ontonotes, batch 321 (32321): mcc: 0.7650, acc: 0.6801, precision: 0.8303, recall: 0.7108, f1: 0.7659, edges-srl-ontonotes_loss: 0.0184
10/01 04:11:35 AM: Update 32424: task edges-srl-ontonotes, batch 424 (32424): mcc: 0.7652, acc: 0.6798, precision: 0.8308, recall: 0.7109, f1: 0.7662, edges-srl-ontonotes_loss: 0.0184
10/01 04:11:45 AM: Update 32523: task edges-srl-ontonotes, batch 523 (32523): mcc: 0.7640, acc: 0.6781, precision: 0.8300, recall: 0.7093, f1: 0.7649, edges-srl-ontonotes_loss: 0.0184
10/01 04:11:55 AM: Update 32628: task edges-srl-ontonotes, batch 628 (32628): mcc: 0.7633, acc: 0.6775, precision: 0.8288, recall: 0.7089, f1: 0.7642, edges-srl-ontonotes_loss: 0.0185
10/01 04:12:05 AM: Update 32729: task edges-srl-ontonotes, batch 729 (32729): mcc: 0.7624, acc: 0.6760, precision: 0.8280, recall: 0.7081, f1: 0.7634, edges-srl-ontonotes_loss: 0.0186
10/01 04:12:15 AM: Update 32822: task edges-srl-ontonotes, batch 822 (32822): mcc: 0.7611, acc: 0.6744, precision: 0.8267, recall: 0.7069, f1: 0.7621, edges-srl-ontonotes_loss: 0.0187
10/01 04:12:25 AM: Update 32928: task edges-srl-ontonotes, batch 928 (32928): mcc: 0.7606, acc: 0.6742, precision: 0.8262, recall: 0.7064, f1: 0.7616, edges-srl-ontonotes_loss: 0.0187
10/01 04:12:32 AM: ***** Step 33000 / Validation 33 *****
10/01 04:12:32 AM: edges-srl-ontonotes: trained on 1000 batches, 0.138 epochs
10/01 04:12:32 AM: Validating...
10/01 04:12:35 AM: Evaluate: task edges-srl-ontonotes, batch 32 (157): mcc: 0.8073, acc: 0.7344, precision: 0.8761, recall: 0.7488, f1: 0.8075, edges-srl-ontonotes_loss: 0.0151
10/01 04:12:45 AM: Evaluate: task edges-srl-ontonotes, batch 133 (157): mcc: 0.8215, acc: 0.7579, precision: 0.8790, recall: 0.7724, f1: 0.8222, edges-srl-ontonotes_loss: 0.0144
10/01 04:12:47 AM: Updating LR scheduler:
10/01 04:12:47 AM: 	Best result seen so far for macro_avg: 0.823
10/01 04:12:47 AM: 	# validation passes without improvement: 3
10/01 04:12:47 AM: edges-srl-ontonotes_loss: training: 0.018683 validation: 0.014683
10/01 04:12:47 AM: macro_avg: validation: 0.818812
10/01 04:12:47 AM: micro_avg: validation: 0.000000
10/01 04:12:47 AM: edges-srl-ontonotes_mcc: training: 0.760434 validation: 0.818063
10/01 04:12:47 AM: edges-srl-ontonotes_acc: training: 0.674260 validation: 0.753368
10/01 04:12:47 AM: edges-srl-ontonotes_precision: training: 0.825899 validation: 0.876934
10/01 04:12:47 AM: edges-srl-ontonotes_recall: training: 0.706293 validation: 0.767916
10/01 04:12:47 AM: edges-srl-ontonotes_f1: training: 0.761427 validation: 0.818812
10/01 04:12:47 AM: Global learning rate: 1.25e-05
10/01 04:12:47 AM: Saving checkpoints to: ./experiments/srl-ontonotes-sstseed2-top/run
10/01 04:12:55 AM: Update 33068: task edges-srl-ontonotes, batch 68 (33068): mcc: 0.7480, acc: 0.6592, precision: 0.8165, recall: 0.6916, f1: 0.7489, edges-srl-ontonotes_loss: 0.0193
10/01 04:13:05 AM: Update 33172: task edges-srl-ontonotes, batch 172 (33172): mcc: 0.7597, acc: 0.6748, precision: 0.8245, recall: 0.7061, f1: 0.7607, edges-srl-ontonotes_loss: 0.0187
10/01 04:13:15 AM: Update 33276: task edges-srl-ontonotes, batch 276 (33276): mcc: 0.7638, acc: 0.6794, precision: 0.8280, recall: 0.7106, f1: 0.7648, edges-srl-ontonotes_loss: 0.0185
10/01 04:13:27 AM: Update 33367: task edges-srl-ontonotes, batch 367 (33367): mcc: 0.7648, acc: 0.6808, precision: 0.8287, recall: 0.7118, f1: 0.7658, edges-srl-ontonotes_loss: 0.0183
10/01 04:13:37 AM: Update 33471: task edges-srl-ontonotes, batch 471 (33471): mcc: 0.7660, acc: 0.6817, precision: 0.8296, recall: 0.7132, f1: 0.7670, edges-srl-ontonotes_loss: 0.0183
