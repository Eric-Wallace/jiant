10/01 04:33:43 AM: Git branch: master
10/01 04:33:43 AM: Git SHA: 8a5d6bbc81dc2562b6a149e8b00815e7e9113c4c
10/01 04:33:43 AM: Parsed args: 
{
  "allow_missing_task_map": 1,
  "allow_untrained_encoder_parameters": 1,
  "do_pretrain": 0,
  "exp_dir": "./experiments/ner-ontonotes-allstrings-top/",
  "exp_name": "experiments/ner-ontonotes-allstrings-top",
  "input_module": "bert-base-uncased",
  "local_log_path": "./experiments/ner-ontonotes-allstrings-top/run/log.log",
  "lr_patience": 3,
  "max_seq_len": 512,
  "output_mode": "top",
  "patience": 9,
  "pretrain_tasks": "",
  "pretrained_dir": "models/allstrings",
  "pytorch_transformers_output_mode": "top",
  "remote_log_name": "experiments/ner-ontonotes-allstrings-top__run",
  "run_dir": "./experiments/ner-ontonotes-allstrings-top/run",
  "run_name": "run",
  "sent_enc": "none",
  "sep_embs_for_skip": 1,
  "target_tasks": "edges-ner-ontonotes",
  "tokenizer": "bert-base-uncased",
  "write_preds": "val,test"
}
10/01 04:33:43 AM: Saved config to ./experiments/ner-ontonotes-allstrings-top/run/params.conf
10/01 04:33:43 AM: Using random seed 1234
10/01 04:33:45 AM: Using GPU 0
10/01 04:33:45 AM: Loading tasks...
10/01 04:33:45 AM: Writing pre-preprocessed tasks to ./experiments/ner-ontonotes-allstrings-top/
10/01 04:33:45 AM: 	Creating task edges-ner-ontonotes from scratch.
10/01 04:33:46 AM: Read=49706, Skip=66106, Total=115812 from ./probing_data/edges/ontonotes/ner/train.json.retokenized.bert-base-uncased
10/01 04:33:47 AM: Read=7610, Skip=8070, Total=15680 from ./probing_data/edges/ontonotes/ner/development.json.retokenized.bert-base-uncased
10/01 04:33:47 AM: Read=5099, Skip=7118, Total=12217 from ./probing_data/edges/ontonotes/ner/test.json.retokenized.bert-base-uncased
10/01 04:33:47 AM: 	Task 'edges-ner-ontonotes': |train|=49706 |val|=7610 |test|=5099
10/01 04:33:47 AM: 	Finished loading tasks: edges-ner-ontonotes.
10/01 04:33:47 AM: 	Building vocab from scratch.
10/01 04:33:47 AM: 	Counting units for task edges-ner-ontonotes.
10/01 04:33:48 AM: 	Task 'edges-ner-ontonotes': adding vocab namespace 'edges-ner-ontonotes_labels'
10/01 04:33:49 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ericwallace/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:33:49 AM: Added pytorch_transformers vocab (bert-base-uncased): 30522 tokens
10/01 04:33:49 AM: 	Saved vocab to ./experiments/ner-ontonotes-allstrings-top/vocab
10/01 04:33:49 AM: Loading token dictionary from ./experiments/ner-ontonotes-allstrings-top/vocab.
10/01 04:33:49 AM: 	Loaded vocab from ./experiments/ner-ontonotes-allstrings-top/vocab
10/01 04:33:49 AM: 	Vocab namespace bert_uncased: size 30524
10/01 04:33:49 AM: 	Vocab namespace tokens: size 22840
10/01 04:33:49 AM: 	Vocab namespace chars: size 77
10/01 04:33:49 AM: 	Vocab namespace edges-ner-ontonotes_labels: size 18
10/01 04:33:49 AM: 	Finished building vocab.
10/01 04:33:49 AM: 	Task edges-ner-ontonotes (train): Indexing from scratch.
10/01 04:33:55 AM: 	Task edges-ner-ontonotes (train): Saved 49706 instances to ./experiments/ner-ontonotes-allstrings-top/preproc/edges-ner-ontonotes__train_data
10/01 04:33:55 AM: 	Task edges-ner-ontonotes (val): Indexing from scratch.
10/01 04:33:56 AM: 	Task edges-ner-ontonotes (val): Saved 7610 instances to ./experiments/ner-ontonotes-allstrings-top/preproc/edges-ner-ontonotes__val_data
10/01 04:33:56 AM: 	Task edges-ner-ontonotes (test): Indexing from scratch.
10/01 04:33:57 AM: 	Task edges-ner-ontonotes (test): Saved 5099 instances to ./experiments/ner-ontonotes-allstrings-top/preproc/edges-ner-ontonotes__test_data
10/01 04:33:57 AM: 	Finished indexing tasks
10/01 04:33:57 AM: 	Creating trimmed target-only version of edges-ner-ontonotes train.
10/01 04:33:57 AM: 	  Training on 
10/01 04:33:57 AM: 	  Evaluating on edges-ner-ontonotes
10/01 04:33:57 AM: 	Finished loading tasks in 11.525s
10/01 04:33:57 AM: 	 Tasks: ['edges-ner-ontonotes']
10/01 04:33:57 AM: Building model...
10/01 04:33:57 AM: Using BERT model (bert-base-uncased).
10/01 04:33:57 AM: LOADING A FUNETUNED MODEL from: 
10/01 04:33:57 AM: models/allstrings
10/01 04:33:57 AM: loading configuration file models/allstrings/config.json
10/01 04:33:57 AM: Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "random-memorize-all-binary",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/01 04:33:57 AM: loading weights file models/allstrings/pytorch_model.bin
10/01 04:34:00 AM: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpj907mir3
10/01 04:34:02 AM: copying /tmp/tmpj907mir3 to cache at ./experiments/ner-ontonotes-allstrings-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:34:02 AM: creating metadata file for ./experiments/ner-ontonotes-allstrings-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:34:02 AM: removing temp file /tmp/tmpj907mir3
10/01 04:34:02 AM: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./experiments/ner-ontonotes-allstrings-top/pytorch_transformers_cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
10/01 04:34:02 AM: Initializing parameters
10/01 04:34:02 AM: Done initializing parameters; the following parameters are using their default initialization from their code
10/01 04:34:02 AM:    _text_field_embedder.model.embeddings.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.embeddings.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.embeddings.position_embeddings.weight
10/01 04:34:02 AM:    _text_field_embedder.model.embeddings.token_type_embeddings.weight
10/01 04:34:02 AM:    _text_field_embedder.model.embeddings.word_embeddings.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.0.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.1.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.10.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.11.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.2.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.3.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.4.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.5.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.6.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.7.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.8.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.key.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.query.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.attention.self.value.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.intermediate.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.encoder.layer.9.output.dense.weight
10/01 04:34:02 AM:    _text_field_embedder.model.pooler.dense.bias
10/01 04:34:02 AM:    _text_field_embedder.model.pooler.dense.weight
10/01 04:34:02 AM: 	Task 'edges-ner-ontonotes' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "pool_type": "first",
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.3,
  "cls_loss_fn": "sigmoid",
  "cls_span_pooling": "attn",
  "edgeprobe_cnn_context": 0,
  "edgeprobe_symmetric": 0,
  "use_classifier": "edges-ner-ontonotes"
}
10/01 04:34:07 AM: Model specification:
10/01 04:34:07 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): BertEmbedderModule(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): NullPhraseLayer()
    (_dropout): Dropout(p=0.2)
  )
  (edges-ner-ontonotes_mdl): EdgeClassifierModule(
    (proj1): Conv1d(768, 256, kernel_size=(1,), stride=(1,))
    (span_extractor1): EndpointSpanExtractor()
    (classifier): Classifier(
      (classifier): Linear(in_features=512, out_features=18, bias=True)
    )
  )
)
10/01 04:34:07 AM: Model parameters:
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.embeddings.word_embeddings.weight: Non-trainable parameter, count 23440896 with torch.Size([30522, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.embeddings.position_embeddings.weight: Non-trainable parameter, count 393216 with torch.Size([512, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.embeddings.token_type_embeddings.weight: Non-trainable parameter, count 1536 with torch.Size([2, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.embeddings.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.0.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.1.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.2.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.3.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.4.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.5.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.6.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.7.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.8.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.9.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.10.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.query.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.key.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.self.value.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.attention.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([3072, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.intermediate.dense.bias: Non-trainable parameter, count 3072 with torch.Size([3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.weight: Non-trainable parameter, count 2359296 with torch.Size([768, 3072])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.weight: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.encoder.layer.11.output.LayerNorm.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.weight: Non-trainable parameter, count 589824 with torch.Size([768, 768])
10/01 04:34:07 AM: 	sent_encoder._text_field_embedder.model.pooler.dense.bias: Non-trainable parameter, count 768 with torch.Size([768])
10/01 04:34:07 AM: 	edges-ner-ontonotes_mdl.proj1.weight: Trainable parameter, count 196608 with torch.Size([256, 768, 1])
10/01 04:34:07 AM: 	edges-ner-ontonotes_mdl.proj1.bias: Trainable parameter, count 256 with torch.Size([256])
10/01 04:34:07 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.weight: Trainable parameter, count 9216 with torch.Size([18, 512])
10/01 04:34:07 AM: 	edges-ner-ontonotes_mdl.classifier.classifier.bias: Trainable parameter, count 18 with torch.Size([18])
10/01 04:34:07 AM: Total number of parameters: 109688338 (1.09688e+08)
10/01 04:34:07 AM: Number of trainable parameters: 206098 (206098)
10/01 04:34:07 AM: Finished building model in 9.662s
10/01 04:34:07 AM: Will run the following steps for this experiment:
Re-training model for individual target tasks 
Evaluating model on tasks: edges-ner-ontonotes 

10/01 04:34:09 AM: patience = 9
10/01 04:34:09 AM: val_interval = 1000
10/01 04:34:09 AM: max_vals = 250
10/01 04:34:09 AM: cuda_device = 0
10/01 04:34:09 AM: grad_norm = 5.0
10/01 04:34:09 AM: grad_clipping = None
10/01 04:34:09 AM: lr_decay = 0.99
10/01 04:34:09 AM: min_lr = 1e-06
10/01 04:34:09 AM: keep_all_checkpoints = 0
10/01 04:34:09 AM: val_data_limit = 5000
10/01 04:34:09 AM: max_epochs = -1
10/01 04:34:09 AM: dec_val_scale = 250
10/01 04:34:09 AM: training_data_fraction = 1
10/01 04:34:09 AM: type = adam
10/01 04:34:09 AM: parameter_groups = None
10/01 04:34:09 AM: Number of trainable parameters: 206098
10/01 04:34:09 AM: infer_type_and_cast = True
10/01 04:34:09 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:09 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:09 AM: lr = 0.0001
10/01 04:34:09 AM: amsgrad = True
10/01 04:34:09 AM: type = reduce_on_plateau
10/01 04:34:09 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:09 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:09 AM: mode = max
10/01 04:34:09 AM: factor = 0.5
10/01 04:34:09 AM: patience = 3
10/01 04:34:09 AM: threshold = 0.0001
10/01 04:34:09 AM: threshold_mode = abs
10/01 04:34:09 AM: verbose = True
10/01 04:34:09 AM: type = adam
10/01 04:34:09 AM: parameter_groups = None
10/01 04:34:09 AM: Number of trainable parameters: 206098
10/01 04:34:09 AM: infer_type_and_cast = True
10/01 04:34:09 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:09 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:09 AM: lr = 0.0001
10/01 04:34:09 AM: amsgrad = True
10/01 04:34:09 AM: type = reduce_on_plateau
10/01 04:34:09 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
10/01 04:34:09 AM: CURRENTLY DEFINED PARAMETERS: 
10/01 04:34:09 AM: mode = max
10/01 04:34:09 AM: factor = 0.5
10/01 04:34:09 AM: patience = 3
10/01 04:34:09 AM: threshold = 0.0001
10/01 04:34:09 AM: threshold_mode = abs
10/01 04:34:09 AM: verbose = True
10/01 04:34:09 AM: Starting training without restoring from a checkpoint.
10/01 04:34:09 AM: Training examples per task, before any subsampling: {'edges-ner-ontonotes': 49706}
10/01 04:34:09 AM: Beginning training with stopping criteria based on metric: edges-ner-ontonotes_f1
10/01 04:34:19 AM: Update 202: task edges-ner-ontonotes, batch 202 (202): mcc: 0.0037, acc: 0.0008, precision: 0.0604, recall: 0.0326, f1: 0.0423, edges-ner-ontonotes_loss: 0.2774
10/01 04:34:29 AM: Update 377: task edges-ner-ontonotes, batch 377 (377): mcc: 0.0027, acc: 0.0004, precision: 0.0604, recall: 0.0166, f1: 0.0261, edges-ner-ontonotes_loss: 0.2322
10/01 04:34:39 AM: Update 567: task edges-ner-ontonotes, batch 567 (567): mcc: 0.0021, acc: 0.0003, precision: 0.0604, recall: 0.0104, f1: 0.0177, edges-ner-ontonotes_loss: 0.2139
10/01 04:34:49 AM: Update 726: task edges-ner-ontonotes, batch 726 (726): mcc: 0.0018, acc: 0.0002, precision: 0.0604, recall: 0.0078, f1: 0.0137, edges-ner-ontonotes_loss: 0.2055
10/01 04:34:59 AM: Update 910: task edges-ner-ontonotes, batch 910 (910): mcc: 0.0016, acc: 0.0001, precision: 0.0604, recall: 0.0061, f1: 0.0110, edges-ner-ontonotes_loss: 0.1992
10/01 04:35:05 AM: ***** Step 1000 / Validation 1 *****
10/01 04:35:05 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:35:05 AM: Validating...
10/01 04:35:09 AM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1940
10/01 04:35:19 AM: Best result seen so far for edges-ner-ontonotes.
10/01 04:35:19 AM: Best result seen so far for micro.
10/01 04:35:19 AM: Best result seen so far for macro.
10/01 04:35:19 AM: Updating LR scheduler:
10/01 04:35:19 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:35:19 AM: 	# validation passes without improvement: 0
10/01 04:35:19 AM: edges-ner-ontonotes_loss: training: 0.197167 validation: 0.182915
10/01 04:35:19 AM: macro_avg: validation: 0.000000
10/01 04:35:19 AM: micro_avg: validation: 0.000000
10/01 04:35:19 AM: edges-ner-ontonotes_mcc: training: 0.001512 validation: 0.000000
10/01 04:35:19 AM: edges-ner-ontonotes_acc: training: 0.000135 validation: 0.000000
10/01 04:35:19 AM: edges-ner-ontonotes_precision: training: 0.060434 validation: 0.000000
10/01 04:35:19 AM: edges-ner-ontonotes_recall: training: 0.005456 validation: 0.000000
10/01 04:35:19 AM: edges-ner-ontonotes_f1: training: 0.010009 validation: 0.000000
10/01 04:35:19 AM: Global learning rate: 0.0001
10/01 04:35:19 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:35:19 AM: Update 1012: task edges-ner-ontonotes, batch 12 (1012): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1766
10/01 04:35:29 AM: Update 1194: task edges-ner-ontonotes, batch 194 (1194): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1753
10/01 04:35:39 AM: Update 1355: task edges-ner-ontonotes, batch 355 (1355): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1772
10/01 04:35:49 AM: Update 1542: task edges-ner-ontonotes, batch 542 (1542): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1783
10/01 04:35:59 AM: Update 1745: task edges-ner-ontonotes, batch 745 (1745): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1784
10/01 04:36:09 AM: Update 1924: task edges-ner-ontonotes, batch 924 (1924): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1783
10/01 04:36:14 AM: ***** Step 2000 / Validation 2 *****
10/01 04:36:14 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:36:14 AM: Validating...
10/01 04:36:19 AM: Evaluate: task edges-ner-ontonotes, batch 77 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1788
10/01 04:36:26 AM: Updating LR scheduler:
10/01 04:36:26 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:36:26 AM: 	# validation passes without improvement: 1
10/01 04:36:26 AM: edges-ner-ontonotes_loss: training: 0.178179 validation: 0.179554
10/01 04:36:26 AM: macro_avg: validation: 0.000000
10/01 04:36:26 AM: micro_avg: validation: 0.000000
10/01 04:36:26 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:36:26 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:36:26 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:36:26 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:36:26 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:36:26 AM: Global learning rate: 0.0001
10/01 04:36:26 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:36:29 AM: Update 2058: task edges-ner-ontonotes, batch 58 (2058): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1751
10/01 04:36:39 AM: Update 2217: task edges-ner-ontonotes, batch 217 (2217): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1758
10/01 04:36:49 AM: Update 2397: task edges-ner-ontonotes, batch 397 (2397): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1746
10/01 04:36:59 AM: Update 2548: task edges-ner-ontonotes, batch 548 (2548): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1744
10/01 04:37:09 AM: Update 2733: task edges-ner-ontonotes, batch 733 (2733): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1744
10/01 04:37:20 AM: Update 2887: task edges-ner-ontonotes, batch 887 (2887): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1749
10/01 04:37:26 AM: ***** Step 3000 / Validation 3 *****
10/01 04:37:26 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:37:26 AM: Validating...
10/01 04:37:30 AM: Evaluate: task edges-ner-ontonotes, batch 53 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1803
10/01 04:37:39 AM: Updating LR scheduler:
10/01 04:37:39 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:37:39 AM: 	# validation passes without improvement: 2
10/01 04:37:39 AM: edges-ner-ontonotes_loss: training: 0.175428 validation: 0.178582
10/01 04:37:39 AM: macro_avg: validation: 0.000000
10/01 04:37:39 AM: micro_avg: validation: 0.000000
10/01 04:37:39 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:37:39 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:37:39 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:37:39 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:37:39 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:37:39 AM: Global learning rate: 0.0001
10/01 04:37:39 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:37:40 AM: Update 3021: task edges-ner-ontonotes, batch 21 (3021): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1797
10/01 04:37:50 AM: Update 3203: task edges-ner-ontonotes, batch 203 (3203): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1788
10/01 04:38:01 AM: Update 3426: task edges-ner-ontonotes, batch 426 (3426): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1784
10/01 04:38:11 AM: Update 3616: task edges-ner-ontonotes, batch 616 (3616): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1774
10/01 04:38:21 AM: Update 3779: task edges-ner-ontonotes, batch 779 (3779): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1769
10/01 04:38:31 AM: Update 3966: task edges-ner-ontonotes, batch 966 (3966): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1761
10/01 04:38:33 AM: ***** Step 4000 / Validation 4 *****
10/01 04:38:33 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:38:33 AM: Validating...
10/01 04:38:41 AM: Evaluate: task edges-ner-ontonotes, batch 106 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1851
10/01 04:38:45 AM: Updating LR scheduler:
10/01 04:38:45 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:38:45 AM: 	# validation passes without improvement: 3
10/01 04:38:45 AM: edges-ner-ontonotes_loss: training: 0.175999 validation: 0.180634
10/01 04:38:45 AM: macro_avg: validation: 0.000000
10/01 04:38:45 AM: micro_avg: validation: 0.000000
10/01 04:38:45 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:38:45 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:38:45 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:38:45 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:38:45 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:38:45 AM: Global learning rate: 0.0001
10/01 04:38:45 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:38:51 AM: Update 4068: task edges-ner-ontonotes, batch 68 (4068): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1730
10/01 04:39:01 AM: Update 4250: task edges-ner-ontonotes, batch 250 (4250): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1732
10/01 04:39:11 AM: Update 4424: task edges-ner-ontonotes, batch 424 (4424): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1744
10/01 04:39:21 AM: Update 4610: task edges-ner-ontonotes, batch 610 (4610): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1759
10/01 04:39:31 AM: Update 4803: task edges-ner-ontonotes, batch 803 (4803): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1763
10/01 04:39:41 AM: Update 4982: task edges-ner-ontonotes, batch 982 (4982): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1766
10/01 04:39:42 AM: ***** Step 5000 / Validation 5 *****
10/01 04:39:42 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:39:42 AM: Validating...
10/01 04:39:52 AM: Evaluate: task edges-ner-ontonotes, batch 113 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1775
10/01 04:39:55 AM: Updating LR scheduler:
10/01 04:39:55 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:39:55 AM: 	# validation passes without improvement: 0
10/01 04:39:55 AM: edges-ner-ontonotes_loss: training: 0.176597 validation: 0.178942
10/01 04:39:55 AM: macro_avg: validation: 0.000000
10/01 04:39:55 AM: micro_avg: validation: 0.000000
10/01 04:39:55 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:39:55 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:39:55 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:39:55 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:39:55 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:39:55 AM: Global learning rate: 5e-05
10/01 04:39:55 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:40:02 AM: Update 5116: task edges-ner-ontonotes, batch 116 (5116): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1752
10/01 04:40:12 AM: Update 5296: task edges-ner-ontonotes, batch 296 (5296): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1748
10/01 04:40:22 AM: Update 5475: task edges-ner-ontonotes, batch 475 (5475): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1739
10/01 04:40:32 AM: Update 5608: task edges-ner-ontonotes, batch 608 (5608): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1737
10/01 04:40:42 AM: Update 5789: task edges-ner-ontonotes, batch 789 (5789): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1735
10/01 04:40:52 AM: Update 5955: task edges-ner-ontonotes, batch 955 (5955): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1739
10/01 04:40:54 AM: ***** Step 6000 / Validation 6 *****
10/01 04:40:54 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:40:54 AM: Validating...
10/01 04:41:02 AM: Evaluate: task edges-ner-ontonotes, batch 97 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1783
10/01 04:41:07 AM: Updating LR scheduler:
10/01 04:41:07 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:41:07 AM: 	# validation passes without improvement: 1
10/01 04:41:07 AM: edges-ner-ontonotes_loss: training: 0.174139 validation: 0.177216
10/01 04:41:07 AM: macro_avg: validation: 0.000000
10/01 04:41:07 AM: micro_avg: validation: 0.000000
10/01 04:41:07 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:41:07 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:41:07 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:41:07 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:41:07 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:41:07 AM: Global learning rate: 5e-05
10/01 04:41:07 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:41:12 AM: Update 6089: task edges-ner-ontonotes, batch 89 (6089): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1784
10/01 04:41:22 AM: Update 6253: task edges-ner-ontonotes, batch 253 (6253): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1784
10/01 04:41:32 AM: Update 6476: task edges-ner-ontonotes, batch 476 (6476): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1779
10/01 04:41:42 AM: Update 6637: task edges-ner-ontonotes, batch 637 (6637): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1776
10/01 04:41:52 AM: Update 6813: task edges-ner-ontonotes, batch 813 (6813): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1767
10/01 04:42:02 AM: Update 6977: task edges-ner-ontonotes, batch 977 (6977): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1763
10/01 04:42:03 AM: ***** Step 7000 / Validation 7 *****
10/01 04:42:03 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:42:03 AM: Validating...
10/01 04:42:12 AM: Evaluate: task edges-ner-ontonotes, batch 112 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1831
10/01 04:42:16 AM: Updating LR scheduler:
10/01 04:42:16 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:42:16 AM: 	# validation passes without improvement: 2
10/01 04:42:16 AM: edges-ner-ontonotes_loss: training: 0.176167 validation: 0.179500
10/01 04:42:16 AM: macro_avg: validation: 0.000000
10/01 04:42:16 AM: micro_avg: validation: 0.000000
10/01 04:42:16 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:42:16 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:42:16 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:42:16 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:42:16 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:42:16 AM: Global learning rate: 5e-05
10/01 04:42:16 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:42:22 AM: Update 7111: task edges-ner-ontonotes, batch 111 (7111): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1717
10/01 04:42:32 AM: Update 7255: task edges-ner-ontonotes, batch 255 (7255): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1728
10/01 04:42:42 AM: Update 7437: task edges-ner-ontonotes, batch 437 (7437): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1725
10/01 04:42:52 AM: Update 7604: task edges-ner-ontonotes, batch 604 (7604): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1741
10/01 04:43:03 AM: Update 7781: task edges-ner-ontonotes, batch 781 (7781): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1752
10/01 04:43:12 AM: ***** Step 8000 / Validation 8 *****
10/01 04:43:12 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:43:12 AM: Validating...
10/01 04:43:13 AM: Evaluate: task edges-ner-ontonotes, batch 4 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.2057
10/01 04:43:23 AM: Evaluate: task edges-ner-ontonotes, batch 126 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1779
10/01 04:43:25 AM: Updating LR scheduler:
10/01 04:43:25 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:43:25 AM: 	# validation passes without improvement: 3
10/01 04:43:25 AM: edges-ner-ontonotes_loss: training: 0.175625 validation: 0.179190
10/01 04:43:25 AM: macro_avg: validation: 0.000000
10/01 04:43:25 AM: micro_avg: validation: 0.000000
10/01 04:43:25 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:43:25 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:43:25 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:43:25 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:43:25 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:43:25 AM: Global learning rate: 5e-05
10/01 04:43:25 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:43:33 AM: Update 8133: task edges-ner-ontonotes, batch 133 (8133): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1768
10/01 04:43:43 AM: Update 8321: task edges-ner-ontonotes, batch 321 (8321): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1753
10/01 04:43:53 AM: Update 8486: task edges-ner-ontonotes, batch 486 (8486): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1750
10/01 04:44:03 AM: Update 8666: task edges-ner-ontonotes, batch 666 (8666): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1740
10/01 04:44:13 AM: Update 8833: task edges-ner-ontonotes, batch 833 (8833): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1737
10/01 04:44:22 AM: ***** Step 9000 / Validation 9 *****
10/01 04:44:22 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:44:22 AM: Validating...
10/01 04:44:23 AM: Evaluate: task edges-ner-ontonotes, batch 7 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.2101
10/01 04:44:33 AM: Evaluate: task edges-ner-ontonotes, batch 128 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1807
10/01 04:44:35 AM: Updating LR scheduler:
10/01 04:44:35 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:44:35 AM: 	# validation passes without improvement: 0
10/01 04:44:35 AM: edges-ner-ontonotes_loss: training: 0.173553 validation: 0.178907
10/01 04:44:35 AM: macro_avg: validation: 0.000000
10/01 04:44:35 AM: micro_avg: validation: 0.000000
10/01 04:44:35 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:44:35 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:44:35 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:44:35 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:44:35 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:44:35 AM: Global learning rate: 2.5e-05
10/01 04:44:35 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:44:43 AM: Update 9116: task edges-ner-ontonotes, batch 116 (9116): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1781
10/01 04:44:53 AM: Update 9297: task edges-ner-ontonotes, batch 297 (9297): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1786
10/01 04:45:03 AM: Update 9471: task edges-ner-ontonotes, batch 471 (9471): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1784
10/01 04:45:13 AM: Update 9661: task edges-ner-ontonotes, batch 661 (9661): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1781
10/01 04:45:23 AM: Update 9852: task edges-ner-ontonotes, batch 852 (9852): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1771
10/01 04:45:32 AM: ***** Step 10000 / Validation 10 *****
10/01 04:45:32 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:45:32 AM: Validating...
10/01 04:45:33 AM: Evaluate: task edges-ner-ontonotes, batch 8 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1993
10/01 04:45:43 AM: Evaluate: task edges-ner-ontonotes, batch 129 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1771
10/01 04:45:45 AM: Updating LR scheduler:
10/01 04:45:45 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:45:45 AM: 	# validation passes without improvement: 1
10/01 04:45:45 AM: edges-ner-ontonotes_loss: training: 0.176868 validation: 0.176529
10/01 04:45:45 AM: macro_avg: validation: 0.000000
10/01 04:45:45 AM: micro_avg: validation: 0.000000
10/01 04:45:45 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:45:45 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:45:45 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:45:45 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:45:45 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:45:45 AM: Global learning rate: 2.5e-05
10/01 04:45:45 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:45:53 AM: Update 10141: task edges-ner-ontonotes, batch 141 (10141): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1723
10/01 04:46:03 AM: Update 10293: task edges-ner-ontonotes, batch 293 (10293): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1720
10/01 04:46:13 AM: Update 10472: task edges-ner-ontonotes, batch 472 (10472): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1722
10/01 04:46:23 AM: Update 10638: task edges-ner-ontonotes, batch 638 (10638): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1729
10/01 04:46:33 AM: Update 10821: task edges-ner-ontonotes, batch 821 (10821): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1742
10/01 04:46:43 AM: Update 10997: task edges-ner-ontonotes, batch 997 (10997): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1751
10/01 04:46:43 AM: ***** Step 11000 / Validation 11 *****
10/01 04:46:43 AM: edges-ner-ontonotes: trained on 1000 batches, 0.644 epochs
10/01 04:46:43 AM: Validating...
10/01 04:46:53 AM: Evaluate: task edges-ner-ontonotes, batch 115 (157): mcc: 0.0000, acc: 0.0000, precision: 0.0000, recall: 0.0000, f1: 0.0000, edges-ner-ontonotes_loss: 0.1757
10/01 04:46:57 AM: Updating LR scheduler:
10/01 04:46:57 AM: 	Best result seen so far for macro_avg: 0.000
10/01 04:46:57 AM: 	# validation passes without improvement: 2
10/01 04:46:57 AM: Ran out of early stopping patience. Stopping training.
10/01 04:46:57 AM: edges-ner-ontonotes_loss: training: 0.175118 validation: 0.176817
10/01 04:46:57 AM: macro_avg: validation: 0.000000
10/01 04:46:57 AM: micro_avg: validation: 0.000000
10/01 04:46:57 AM: edges-ner-ontonotes_mcc: training: 0.000000 validation: 0.000000
10/01 04:46:57 AM: edges-ner-ontonotes_acc: training: 0.000000 validation: 0.000000
10/01 04:46:57 AM: edges-ner-ontonotes_precision: training: 0.000000 validation: 0.000000
10/01 04:46:57 AM: edges-ner-ontonotes_recall: training: 0.000000 validation: 0.000000
10/01 04:46:57 AM: edges-ner-ontonotes_f1: training: 0.000000 validation: 0.000000
10/01 04:46:57 AM: Global learning rate: 2.5e-05
10/01 04:46:57 AM: Saving checkpoints to: ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:46:57 AM: Stopped training after 11 validation checks
10/01 04:46:57 AM: Trained edges-ner-ontonotes for 11000 batches or 7.079 epochs
10/01 04:46:57 AM: ***** VALIDATION RESULTS *****
10/01 04:46:57 AM: edges-ner-ontonotes_f1 (for best val pass 1): edges-ner-ontonotes_loss: 0.18292, macro_avg: 0.00000, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.00000, edges-ner-ontonotes_acc: 0.00000, edges-ner-ontonotes_precision: 0.00000, edges-ner-ontonotes_recall: 0.00000, edges-ner-ontonotes_f1: 0.00000
10/01 04:46:57 AM: micro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.18292, macro_avg: 0.00000, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.00000, edges-ner-ontonotes_acc: 0.00000, edges-ner-ontonotes_precision: 0.00000, edges-ner-ontonotes_recall: 0.00000, edges-ner-ontonotes_f1: 0.00000
10/01 04:46:57 AM: macro_avg (for best val pass 1): edges-ner-ontonotes_loss: 0.18292, macro_avg: 0.00000, micro_avg: 0.00000, edges-ner-ontonotes_mcc: 0.00000, edges-ner-ontonotes_acc: 0.00000, edges-ner-ontonotes_precision: 0.00000, edges-ner-ontonotes_recall: 0.00000, edges-ner-ontonotes_f1: 0.00000
10/01 04:46:57 AM: Evaluating...
10/01 04:46:57 AM: Loaded model state from ./experiments/ner-ontonotes-allstrings-top/run/edges-ner-ontonotes/model_state_target_train_val_1.best.th
10/01 04:46:57 AM: Evaluating on: edges-ner-ontonotes, split: val
10/01 04:47:17 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
10/01 04:47:17 AM: Finished evaluating on: edges-ner-ontonotes
10/01 04:47:17 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'val'
10/01 04:47:17 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:47:17 AM: Wrote all preds for split 'val' to ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:47:17 AM: Evaluating on: edges-ner-ontonotes, split: test
10/01 04:47:30 AM: Task 'edges-ner-ontonotes': sorting predictions by 'idx'
10/01 04:47:30 AM: Finished evaluating on: edges-ner-ontonotes
10/01 04:47:30 AM: Task 'edges-ner-ontonotes': joining predictions with input split 'test'
10/01 04:47:31 AM: Task 'edges-ner-ontonotes': Wrote predictions to ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:47:31 AM: Wrote all preds for split 'test' to ./experiments/ner-ontonotes-allstrings-top/run
10/01 04:47:31 AM: Writing results for split 'val' to ./experiments/ner-ontonotes-allstrings-top/results.tsv
10/01 04:47:31 AM: micro_avg: 0.000, macro_avg: 0.000, edges-ner-ontonotes_mcc: 0.000, edges-ner-ontonotes_acc: 0.000, edges-ner-ontonotes_precision: 0.000, edges-ner-ontonotes_recall: 0.000, edges-ner-ontonotes_f1: 0.000
10/01 04:47:31 AM: Done!
10/01 04:47:31 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 16, in <module>
    main(sys.argv[1:])
  File "/srv/home/ericwallace/jiant/jiant/__main__.py", line 610, in main
    exit("Done exited")
  File "/home/ericwallace/miniconda3/envs/jiant/lib/python3.6/_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: Done exited
